{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to MkDocs For full documentation visit mkdocs.org . Commands mkdocs new [dir-name] - Create a new project. mkdocs serve - Start the live-reloading docs server. mkdocs build - Build the documentation site. mkdocs -h - Print help message and exit. Project layout mkdocs.yml # The configuration file. docs/ index.md # The documentation homepage. ... # Other markdown pages, images and other files.","title":"Home"},{"location":"#welcome-to-mkdocs","text":"For full documentation visit mkdocs.org .","title":"Welcome to MkDocs"},{"location":"#commands","text":"mkdocs new [dir-name] - Create a new project. mkdocs serve - Start the live-reloading docs server. mkdocs build - Build the documentation site. mkdocs -h - Print help message and exit.","title":"Commands"},{"location":"#project-layout","text":"mkdocs.yml # The configuration file. docs/ index.md # The documentation homepage. ... # Other markdown pages, images and other files.","title":"Project layout"},{"location":"git-helper/ncd-git-backup/","text":"Backing up Nokia Git Server this procedure, works for NCD 24.9 mp1 pp1. Instructions on backing up Nokia Git Server backup utility will generate the following outputs database.sql file containing the PostgreSQL database content Git repository data Kubernetes secrets required for the upgrade backup_information.yml generated by the backup tool Example of backup_information.yml :db_version: 20240508085441 :backup_created_at: 2024-06-13 13:57:08 +0000 :gitlab_version: 17.0.1 :tar_version: tar (GNU tar) 1.34 :installation_type: gitlab-helm-chart :skipped: builds,pages,registry,uploads,artifacts,lfs,packages, external_diffs,terraform_state,pages,ci_secure_files :repositories_server_side: false Prerequisites To back up Nokia Git Server and the necessary Helm plugins for Backup and Recovery (CBUR), CBUR must be installed on your cluster. CBUR must be enabled in the Nokia Git Server installation. remove plugins for helm (backup and restore) login to bastion host and login to cluster here . [root@ncputility ~]# source /root/panhubrc WARNING: Using insecure TLS client config. Setting this option is not supported! Login successful. You have access to 101 projects, the list has been suppressed. You can list all projects with 'oc projects' Using project \"default\". [root@ncputility ~ panhub_rc]$ list of plugins installed on helm here. [root@ncputility ~ panhub_rc]$ helm plugin list NAME VERSION DESCRIPTION backup 0.1.2 backup/restore releases in a namespace to/from a file [root@ncputility ~ panhub_rc]$ remove using following command on helm #helm plugin remove backup [root@ncputility ~ panhub_rc]$ helm plugin remove backup Uninstalled plugin: backup [root@ncputility ~ panhub_rc]$ Install plugins for helm (backup and restore) login to bastion host and login to cluster here . [root@ncputility ~]# source /root/panhubrc WARNING: Using insecure TLS client config. Setting this option is not supported! Login successful. You have access to 101 projects, the list has been suppressed. You can list all projects with 'oc projects' Using project \"default\". [root@ncputility ~ panhub_rc]$ list of plugins installed on helm here. [root@ncputility ~ panhub_rc]$ helm plugin list NAME VERSION DESCRIPTION [root@ncputility ~ panhub_rc]$ Install the latest version of the Helm backup and restore plugins. For the Helm backup and restore plugins, use the version that is compatible with the respective Backup and Recovery chart release. [root@ncputility ~ panhub_rc]$ export HELM_HOME=$HOME/.helm [root@ncputility ~ panhub_rc]$ cd [root@ncputility ~ panhub_rc]$ WORK_DIR=`mktemp -d` [root@ncputility ~ panhub_rc]$ mkdir -p $WORK_DIR/backup [root@ncputility ~ panhub_rc]$ mkdir -p $WORK_DIR/restore [root@ncputility ~ panhub_rc]$ tar xf /root/ncd/NCD_24.9_Git_Server_ORB-RC/ncd-git-server-product/INSTALL_MEDIA/backup-3.7.4.tgz -C $WORK_DIR/backup [root@ncputility ~ panhub_rc]$ tar xf /root/ncd/NCD_24.9_Git_Server_ORB-RC/ncd-git-server-product/INSTALL_MEDIA/restore-3.7.4.tgz -C $WORK_DIR/restore [root@ncputility ~ panhub_rc]$ helm plugin install $WORK_DIR/backup Installed plugin: backup [root@ncputility ~ panhub_rc]$ helm plugin install $WORK_DIR/restore Installed plugin: restore [root@ncputility ~ panhub_rc]$ rm -rf $WORK_DIR [root@ncputility ~ panhub_rc]$ a. check the status of installed helm plugins. ``` [root@ncputility ~ panhub_rc]$ helm plugin list NAME VERSION DESCRIPTION backup 3.7.4 Plugin responsible for the backup of helm releases and k8s namespaces with CBUR. restore 3.7.4 Plugin responsible for restoring helm releases and k8s namespaces with CBUR. [root@ncputility ~ panhub_rc]$ ``` Configure RBAC for CRDs. (only when you deploy using tenant credentials) For tenants only using Backup and Recovery, permission to read, create, modify and delete BrPolicy and BrHook is sufficient. However, tenants installing their own namespaces for Backup and Recovery also need permission to read brpolices/status. kind: ClusterRole apiVersion: rbac.authorization.k8s.io/v1 metadata: name: admin:brpolices labels: rbac.authorization.k8s.io/aggregate-to-admin: \"true\" rbac.authorization.k8s.io/aggregate-to-edit: \"true\" rules: - apiGroups: [\"cbur.csf.nokia.com\"] resources: [\"brpolices\", \"brhooks\"] verbs: [\"get\", \"list\", \"watch\", \"create\", \"update\", \"patch\", \"delete\"] - apiGroups: [\"cbur.csf.nokia.com\"] resources: [\"brpolices/status\"] verbs: [\"get\", \"list\", \"watch\", \"update\", \"patch\"] --- kind: ClusterRole apiVersion: rbac.authorization.k8s.io/v1 metadata: name: view:brpolices labels: rbac.authorization.k8s.io/aggregate-to-view: \"true\" rules: - apiGroups: [\"cbur.csf.nokia.com\"] resources: [\"brpolices\", \"brhooks\", \"brpolices/status\"] verbs: [\"get\", \"list\", \"watch\"] Backup Nokia Git Server Notes: The CBUR master service (CBUR_MASTER_SERVICE) must be specified in either of the following formats: http(s)://dns_name:port http(s)://ipv4:port http(s)://[ipv6]:port http(s)://dns_name http(s)://ingress/ingresspath here is the command to run the backup of git server helm backup -t ncd-git -a none -n paclypancdgit01 -x http://172.20.8.113:80 helm backup -n paclypancdgit01 -t ncd-git -a none -x http://172.20.8.113:80 login to bastion host and login to cluster here . [root@ncputility ~]# source /root/panhubrc WARNING: Using insecure TLS client config. Setting this option is not supported! Login successful. You have access to 101 projects, the list has been suppressed. You can list all projects with 'oc projects' Using project \"default\". [root@ncputility ~ panhub_rc]$ Find the namespace and release name of git server and cbut details from here 'helm list -A ' [root@ncputility ~ panhub_rc]$ helm list -A NAME NAMESPACE REVISION UPDATED STATUS CHART APP VERSION cbur-crds paclypancdcbur01 1 2025-02-27 14:18:04.313208995 -0500 EST deployed cbur-crds-2.6.0 2.6.0 ncd-cbur paclypancdcbur01 1 2025-02-27 14:24:56.578242699 -0500 EST deployed cbur-1.18.1 1.13.1 ncd-git paclypancdgit01 1 2025-02-27 14:45:25.489107042 -0500 EST deployed ncd-git-server-24.9.1-7.g30f1acf 17.3.3 ncd-postgresql paclypancddb01 1 2025-02-27 14:30:27.65639258 -0500 EST deployed postgresql-ha-24.9.1-1009.g19e2a92 24.9.1-1009.g19e2a92 ncd-redis paclypancddb01 1 2025-02-27 14:37:11.651840036 -0500 EST deployed ncd-redis-24.9.1-1009.g19e2a92 24.9.1-1009.g19e2a92 [root@ncputility ~ panhub_rc]$ trigger the backup job using following command here Uri should be having cbur structure name in it [root@ncputility ~ panhub_rc]$ helm backup -n paclypancdgit01 -a none -t ncd-git -x http://cbur.apps.panclyphub01.mnc020.mcc714/cbur { \"kind\": \"Status\", \"apiVersion\": \"v1\", \"metadata\": {}, \"status\": \"Success\", \"message\": \"backupHelmRelease task = a80ed466-254c-4644-8d19-9a0cdf813fbe is on!\", \"reason\": \"\", \"details\": { \"a80ed466-254c-4644-8d19-9a0cdf813fbe\": { \"name\": \"ncd-git\", \"namespace\": \"paclypancdgit01\", \"timestamp\": \"20250402104554\", \"backup_data\": {}, \"helm_version\": 3, \"request\": \"backupHelmRelease\" } }, \"code\": 202 } [root@ncputility ~ panhub_rc]$ here is an example with --verbose mode (optinal for troubleshooting) [root@ncputility ~ panhub_rc]$ helm backup -n paclypancdgit01 -a none -t ncd-git -x http://cbur.apps.panclyphub01.mnc020.mcc714/cbur --verbose ******* + exit_func 0 + '[' -d /tmp/certs.1974896 ']' + rm -rf /tmp/certs.1974896 + exit 0 [root@ncputility ~ panhub_rc]$ check the status of the backup using cbur br policy status [root@ncputility ~ panhub_rc]$ kubectl -n paclypancdgit01 get brpolices.cbur.csf.nokia.com ncd-git-toolbox NAME AGE ncd-git-toolbox 33d [root@ncputility ~ panhub_rc]$ kubectl -n paclypancdgit01 get brpolices.cbur.csf.nokia.com ncd-git-toolbox -o yaml [root@ncputility ~ panhub_rc]$ look for backup files exported. , right now backup files are saved locally on the gitserver(paclypancdgit01) namespace within ncd-git-toolbox pod. but we need this backup files to be saved outside the cluster, like sftp server or something, so that we can restore it on new git server. [root@ncputility ~ panhub_rc]$ oc -n paclypancdgit01 exec -it ncd-git-toolbox-56b88dd4c7-27hb5 -- bash Defaulted container \"toolbox\" out of: toolbox, cbura-sidecar, certificates (init), configure (init) git@ncd-git-toolbox-56b88dd4c7-27hb5:/$ cd /srv/gitlab/tmp/backups git@ncd-git-toolbox-56b88dd4c7-27hb5:/srv/gitlab/tmp/backups$ ls backup_information.yml db repositories git@ncd-git-toolbox-56b88dd4c7-27hb5:/srv/gitlab/tmp/backups$ ls -la total 4 drwxr-sr-x. 4 git git 66 Apr 2 10:46 . drwxrwsrwx. 3 root git 21 Apr 2 10:46 .. -rw-r--r--. 1 git git 318 Apr 2 10:46 backup_information.yml drwxr-sr-x. 2 git git 29 Apr 2 10:46 db drwx--S---. 4 git git 38 Apr 2 10:46 repositories git@ncd-git-toolbox-56b88dd4c7-27hb5:/srv/gitlab/tmp/backups$ Creating user for remote backup login to the linux host and used useradd and passwd commands to create user and password here [root@ncputility ~ panhub_rc]$ useradd gitserverbackup [root@ncputility ~ panhub_rc]$ passwd gitserverbackup Changing password for user gitserverbackup. New password: BAD PASSWORD: The password is shorter than 8 characters Retype new password: passwd: all authentication tokens updated successfully. [root@ncputility ~ panhub_rc]$ create a public key on the new user authrazation file. [root@ncputility ~ pancwl_rc]$ su - gitserverbackup [gitserverbackup@ncputility ~]$ pwd /home/gitserverbackup [gitserverbackup@ncputility ~]$ [gitserverbackup@ncputility ~]$ ssh-keygen -t rsa Generating public/private rsa key pair. Enter file in which to save the key (/home/gitserverbackup/.ssh/id_rsa): Created directory '/home/gitserverbackup/.ssh'. Enter passphrase (empty for no passphrase): Enter same passphrase again: Your identification has been saved in /home/gitserverbackup/.ssh/id_rsa Your public key has been saved in /home/gitserverbackup/.ssh/id_rsa.pub The key fingerprint is: SHA256:oF5K3Ky1tLDmb5ALID4FXyLPnlTtX97ZpJyFE0r3Z2A gitserverbackup@ncputility.panclyphub01.mnc020.mcc714 The key's randomart image is: +---[RSA 3072]----+ | . | | o . o . . oE | | * + o . o.+. | |o B + o o o +o| |o.+ *.* S o o O..| | o.=oO o . . * . | | ..*oo | | o. . | | .o. | +----[SHA256]-----+ [gitserverbackup@ncputility ~]$ take copy of the cbur public using following oc command [root@ncputility ~ panhub_rc]$ helm list -A NAME NAMESPACE REVISION UPDATED STATUS CHART APP VERSION cbur-crds paclypancdcbur01 1 2025-02-27 14:18:04.313208995 -0500 EST deployed cbur-crds-2.6.0 2.6.0 ncd-cbur paclypancdcbur01 2 2025-04-02 04:17:30.755392541 -0500 EST deployed cbur-1.18.1 1.13.1 ncd-git paclypancdgit01 1 2025-02-27 14:45:25.489107042 -0500 EST deployed ncd-git-server-24.9.1-7.g30f1acf 17.3.3 ncd-postgresql paclypancddb01 1 2025-02-27 14:30:27.65639258 -0500 EST deployed postgresql-ha-24.9.1-1009.g19e2a92 24.9.1-1009.g19e2a92 ncd-redis paclypancddb01 1 2025-02-27 14:37:11.651840036 -0500 EST deployed ncd-redis-24.9.1-1009.g19e2a92 24.9.1-1009.g19e2a92 [root@ncputility ~ panhub_rc]$ [root@ncputility ~ panhub_rc]$ oc get secret -n paclypancdcbur01 cburm-ssh-public-key -o jsonpath={.data.ssh_public_key} | base64 -d ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQDUH73f7kf8ey2UEkgeN/IJbEBDJPgricRlsD7d7pB0RAQGLx/fNZT15VRX2qOvLydsTbtcxz28Uzryy5zAmAB9z0zGuYKaSo80bS7bXjIsKc71fGD6NvvfSBBLQ1GCk0mFIjn06XmkRJOgqtgOvq66HQEcGSJQ6jq3NQzGERe+VrCk1VWbyfr0vtqqasmKChgr0dAh+0f07lUdpbR9XzEnOG20LNCAcffEBPXXccSEz/huPHOV0Kjfe7rtaKj5ZoIkFlFETPTz4HoKPlZcfxp/s94yICXk++TiI9+mF2SVYuEUWqx00p1DTa79dmUncTaz1c5nshaq4bNdJcPkoDdp[root@ncputility ~ panhub_rc]$ here i am using ssh-copy-id command to auto create an authzation file [gitserverbackup@ncputility ~]$ ssh-copy-id localhost /usr/bin/ssh-copy-id: INFO: Source of key(s) to be installed: \"/home/gitserverbackup/.ssh/id_rsa.pub\" The authenticity of host 'localhost (::1)' can't be established. ED25519 key fingerprint is SHA256:KXMGNVqIYHOtvcd+VqGq/5d/t5UWWGKxPVkuffCOD9I. This key is not known by any other names Are you sure you want to continue connecting (yes/no/[fingerprint])? yes /usr/bin/ssh-copy-id: INFO: attempting to log in with the new key(s), to filter out any that are already installed /usr/bin/ssh-copy-id: INFO: 1 key(s) remain to be installed -- if you are prompted now it is to install the new keys gitserverbackup@localhost's password: Number of key(s) added: 1 Now try logging into the machine, with: \"ssh 'localhost'\" and check to make sure that only the key(s) you wanted were added. [gitserverbackup@ncputility ~]$ cd .ssh/ [gitserverbackup@ncputility .ssh]$ ll total 20 -rw-------. 1 gitserverbackup gitserverbackup 607 Apr 2 08:14 authorized_keys -rw-------. 1 gitserverbackup gitserverbackup 2655 Apr 2 08:14 id_rsa -rw-r--r--. 1 gitserverbackup gitserverbackup 607 Apr 2 08:14 id_rsa.pub -rw-------. 1 gitserverbackup gitserverbackup 825 Apr 2 08:14 known_hosts -rw-r--r--. 1 gitserverbackup gitserverbackup 91 Apr 2 08:14 known_hosts.old [gitserverbackup@ncputility .ssh]$ vi authorized_keys #<- this updated based on step.3 output> [gitserverbackup@ncputility .ssh]$ Creating remote backup on NCD git Create an git server backup on a remote server, so create a secret with sftp details. [root@ncputility ~ panhub_rc]$ oc create secret generic bastionhostpan \\ --namespace=paclypancdcbur01 \\ --from-literal=port=\"22\" \\ --from-literal=host=\"10.89.100.66\" \\ --from-literal=mode=\"sftp\" \\ --from-literal=username=\"gitserverbackup\" \\ --from-literal=path=\"/home/gitserverbackup\" \\ --from-literal=strictHostKeyChecking=\"no\" \\ --from-literal=hostKey=\"\" secret/bastionhostpan created [root@ncputility ~ panhub_rc]$ make sure, it's avaiable via get command. [root@ncputility ~ panhub_rc]$ oc get secret -n paclypancdcbur01 NAME TYPE DATA AGE bastionhostpan Opaque 7 10s cbur-redis Opaque 1 33d cburm-ssh-public-key Opaque 1 33d my-pull-secret kubernetes.io/dockerconfigjson 1 33d sh.helm.release.v1.cbur-crds.v1 helm.sh/release.v1 1 33d sh.helm.release.v1.ncd-cbur.v1 helm.sh/release.v1 1 33d sh.helm.release.v1.ncd-cbur.v2 helm.sh/release.v1 1 4h14m sh.helm.release.v1.ncd-cbur.v3 helm.sh/release.v1 1 14m [root@ncputility ~ panhub_rc]$ update on the git cbur side. get the backup of cbur repo values file. and update the ssh:credentialName [root@ncputility ~ panhub_rc]$ helm get values ncd-cbur -n paclypancdcbur01 > rr.yaml [root@ncputility ~ panhub_rc]$ SSH: credentialName: bastionhostpan [root@ncputility ~ panhub_rc]$ helm upgrade ncd-cbur -n paclypancdcbur01 /root/ncd/NCD_24.9_Git_Server_ORB-RC/ncd-git-server-product/helmcharts/cbur-1.18.1.tgz -f rr.yaml --debug Similar to git cbur modification, need to update the git=server chart as well. [root@ncputility ~ panhub_rc]$ helm get values ncd-git -n paclypancdgit01 > rr.yaml [root@ncputility ~ panhub_rc]$ update: cbur: brPolicy: apiVersion: cbur.csf.nokia.com/v1 spec: autoEnableCron: false autoUpdateCron: false backend: mode: sftp cronSpec: 0 0 * * * dataEncryption: enable: false ignoreFileChanged: false maxiCopy: 3 weight: 5 [root@ncputility ~ panhub_rc]$ helm upgrade ncd-git -n paclypancdgit01 /root/ncd/NCD_24.9_Git_Server_ORB-RC/ncd-git-server-product/helmcharts/ncd-git-server-24.9.1-7.g30f1acf.tgz -f rr.yaml --debug --timeout 20m validate there is no pods are in pending or crashloop error here . Trigger the backup on remote desination using follow method to trigger the backup [root@ncputility ~ panhub_rc]$ helm backup -n paclypancdgit01 -a none -t ncd-git -x http://cbur.apps.panclyphub01.mnc020.mcc714/cbur --backend sftp --verbose [root@ncputility ~ panhub_rc]$ login to that SFTP server and check for backup files. [gitserverbackup@ncputility 20250402134019_SFTP_paclypancdgit01_ncd-git-toolbox]$ pwd /home/gitserverbackup/BACKUP/sftp/paclypancdgit01/DEPLOYMENT_ncd-git-toolbox/20250402134019_SFTP_paclypancdgit01_ncd-git-toolbox [gitserverbackup@ncputility 20250402134019_SFTP_paclypancdgit01_ncd-git-toolbox]$ ll total 2476 -rw-r--r--. 1 gitserverbackup gitserverbackup 7846 Apr 2 08:40 20250402134019_SFTP_paclypancdgit01_ncd-git-toolbox_Secrets.tar.gz -rw-r--r--. 1 gitserverbackup gitserverbackup 2523279 Apr 2 08:41 20250402134019_SFTP_paclypancdgit01_ncd-git-toolbox_volume.tar.gz [gitserverbackup@ncputility 20250402134019_SFTP_paclypancdgit01_ncd-git-toolbox]$ Troubleshooting Certificate error copy crt file used during installation. [root@ncputility denmark]# cp -rp ca.crt /etc/pki/ca-trust/source/anchors/ [root@ncputility denmark]# sudo update-ca-trust [root@ncputility denmark]# backup job failure if Error looks like this, then problem on the namespace spoce. [root@ncputility ~ hn_hub_rc]$ helm backup -n hnevocdgit01 -a none -t ncd-git -x http://cbur.apps.hnevocphub01.mnc002.mcc708/cbur { \"kind\": \"Status\", \"apiVersion\": \"v1\", \"metadata\": {}, \"status\": \"Failure\", \"message\": \"Failed to get helm release ncd-git in namespace hnevocdgit01\", \"reason\": \"\", \"details\": {}, \"code\": 500 } [root@ncputility ~ hn_hub_rc]$ to resolve this issue, get the values file and update the namespace scope list. include git-server namespace too. [root@ncputility ~ hn_hub_rc]$ helm get values ncd-cbur -n hnevocdcbur01 > rr.yaml retry it, it will work now [root@ncputility ~ hn_hub_rc]$ helm backup -n hnevocdgit01 -a none -t ncd-git -x http://cbur.apps.hnevocphub01.mnc002.mcc708/cbur { \"kind\": \"Status\", \"apiVersion\": \"v1\", \"metadata\": {}, \"status\": \"Success\", \"message\": \"backupHelmRelease task = cc857932-d83d-4e2d-9493-1252595da3f2 is on!\", \"reason\": \"\", \"details\": { \"cc857932-d83d-4e2d-9493-1252595da3f2\": { \"name\": \"ncd-git\", \"namespace\": \"hnevocdgit01\", \"timestamp\": \"20250403005800\", \"backup_data\": {}, \"helm_version\": 3, \"request\": \"backupHelmRelease\" } }, \"code\": 202 } [root@ncputility ~ hn_hub_rc]$ Reference NCDFM tickets https://jiradc2.ext.net.nokia.com/browse/NCDFM-3023","title":"Git Backup"},{"location":"git-helper/ncd-git-backup/#backing-up-nokia-git-server","text":"this procedure, works for NCD 24.9 mp1 pp1.","title":"Backing up Nokia Git Server"},{"location":"git-helper/ncd-git-backup/#instructions-on-backing-up-nokia-git-server","text":"","title":"Instructions on backing up Nokia Git Server"},{"location":"git-helper/ncd-git-backup/#backup-utility-will-generate-the-following-outputs","text":"database.sql file containing the PostgreSQL database content Git repository data Kubernetes secrets required for the upgrade backup_information.yml generated by the backup tool Example of backup_information.yml :db_version: 20240508085441 :backup_created_at: 2024-06-13 13:57:08 +0000 :gitlab_version: 17.0.1 :tar_version: tar (GNU tar) 1.34 :installation_type: gitlab-helm-chart :skipped: builds,pages,registry,uploads,artifacts,lfs,packages, external_diffs,terraform_state,pages,ci_secure_files :repositories_server_side: false","title":"backup utility will generate the following outputs"},{"location":"git-helper/ncd-git-backup/#prerequisites","text":"To back up Nokia Git Server and the necessary Helm plugins for Backup and Recovery (CBUR), CBUR must be installed on your cluster. CBUR must be enabled in the Nokia Git Server installation.","title":"Prerequisites"},{"location":"git-helper/ncd-git-backup/#remove-plugins-for-helm-backup-and-restore","text":"login to bastion host and login to cluster here . [root@ncputility ~]# source /root/panhubrc WARNING: Using insecure TLS client config. Setting this option is not supported! Login successful. You have access to 101 projects, the list has been suppressed. You can list all projects with 'oc projects' Using project \"default\". [root@ncputility ~ panhub_rc]$ list of plugins installed on helm here. [root@ncputility ~ panhub_rc]$ helm plugin list NAME VERSION DESCRIPTION backup 0.1.2 backup/restore releases in a namespace to/from a file [root@ncputility ~ panhub_rc]$ remove using following command on helm #helm plugin remove backup [root@ncputility ~ panhub_rc]$ helm plugin remove backup Uninstalled plugin: backup [root@ncputility ~ panhub_rc]$","title":"remove plugins for helm (backup and restore)"},{"location":"git-helper/ncd-git-backup/#install-plugins-for-helm-backup-and-restore","text":"login to bastion host and login to cluster here . [root@ncputility ~]# source /root/panhubrc WARNING: Using insecure TLS client config. Setting this option is not supported! Login successful. You have access to 101 projects, the list has been suppressed. You can list all projects with 'oc projects' Using project \"default\". [root@ncputility ~ panhub_rc]$ list of plugins installed on helm here. [root@ncputility ~ panhub_rc]$ helm plugin list NAME VERSION DESCRIPTION [root@ncputility ~ panhub_rc]$ Install the latest version of the Helm backup and restore plugins. For the Helm backup and restore plugins, use the version that is compatible with the respective Backup and Recovery chart release. [root@ncputility ~ panhub_rc]$ export HELM_HOME=$HOME/.helm [root@ncputility ~ panhub_rc]$ cd [root@ncputility ~ panhub_rc]$ WORK_DIR=`mktemp -d` [root@ncputility ~ panhub_rc]$ mkdir -p $WORK_DIR/backup [root@ncputility ~ panhub_rc]$ mkdir -p $WORK_DIR/restore [root@ncputility ~ panhub_rc]$ tar xf /root/ncd/NCD_24.9_Git_Server_ORB-RC/ncd-git-server-product/INSTALL_MEDIA/backup-3.7.4.tgz -C $WORK_DIR/backup [root@ncputility ~ panhub_rc]$ tar xf /root/ncd/NCD_24.9_Git_Server_ORB-RC/ncd-git-server-product/INSTALL_MEDIA/restore-3.7.4.tgz -C $WORK_DIR/restore [root@ncputility ~ panhub_rc]$ helm plugin install $WORK_DIR/backup Installed plugin: backup [root@ncputility ~ panhub_rc]$ helm plugin install $WORK_DIR/restore Installed plugin: restore [root@ncputility ~ panhub_rc]$ rm -rf $WORK_DIR [root@ncputility ~ panhub_rc]$ a. check the status of installed helm plugins. ``` [root@ncputility ~ panhub_rc]$ helm plugin list NAME VERSION DESCRIPTION backup 3.7.4 Plugin responsible for the backup of helm releases and k8s namespaces with CBUR. restore 3.7.4 Plugin responsible for restoring helm releases and k8s namespaces with CBUR. [root@ncputility ~ panhub_rc]$ ``` Configure RBAC for CRDs. (only when you deploy using tenant credentials) For tenants only using Backup and Recovery, permission to read, create, modify and delete BrPolicy and BrHook is sufficient. However, tenants installing their own namespaces for Backup and Recovery also need permission to read brpolices/status. kind: ClusterRole apiVersion: rbac.authorization.k8s.io/v1 metadata: name: admin:brpolices labels: rbac.authorization.k8s.io/aggregate-to-admin: \"true\" rbac.authorization.k8s.io/aggregate-to-edit: \"true\" rules: - apiGroups: [\"cbur.csf.nokia.com\"] resources: [\"brpolices\", \"brhooks\"] verbs: [\"get\", \"list\", \"watch\", \"create\", \"update\", \"patch\", \"delete\"] - apiGroups: [\"cbur.csf.nokia.com\"] resources: [\"brpolices/status\"] verbs: [\"get\", \"list\", \"watch\", \"update\", \"patch\"] --- kind: ClusterRole apiVersion: rbac.authorization.k8s.io/v1 metadata: name: view:brpolices labels: rbac.authorization.k8s.io/aggregate-to-view: \"true\" rules: - apiGroups: [\"cbur.csf.nokia.com\"] resources: [\"brpolices\", \"brhooks\", \"brpolices/status\"] verbs: [\"get\", \"list\", \"watch\"]","title":"Install plugins for helm (backup and restore)"},{"location":"git-helper/ncd-git-backup/#backup-nokia-git-server","text":"Notes: The CBUR master service (CBUR_MASTER_SERVICE) must be specified in either of the following formats: http(s)://dns_name:port http(s)://ipv4:port http(s)://[ipv6]:port http(s)://dns_name http(s)://ingress/ingresspath here is the command to run the backup of git server helm backup -t ncd-git -a none -n paclypancdgit01 -x http://172.20.8.113:80 helm backup -n paclypancdgit01 -t ncd-git -a none -x http://172.20.8.113:80 login to bastion host and login to cluster here . [root@ncputility ~]# source /root/panhubrc WARNING: Using insecure TLS client config. Setting this option is not supported! Login successful. You have access to 101 projects, the list has been suppressed. You can list all projects with 'oc projects' Using project \"default\". [root@ncputility ~ panhub_rc]$ Find the namespace and release name of git server and cbut details from here 'helm list -A ' [root@ncputility ~ panhub_rc]$ helm list -A NAME NAMESPACE REVISION UPDATED STATUS CHART APP VERSION cbur-crds paclypancdcbur01 1 2025-02-27 14:18:04.313208995 -0500 EST deployed cbur-crds-2.6.0 2.6.0 ncd-cbur paclypancdcbur01 1 2025-02-27 14:24:56.578242699 -0500 EST deployed cbur-1.18.1 1.13.1 ncd-git paclypancdgit01 1 2025-02-27 14:45:25.489107042 -0500 EST deployed ncd-git-server-24.9.1-7.g30f1acf 17.3.3 ncd-postgresql paclypancddb01 1 2025-02-27 14:30:27.65639258 -0500 EST deployed postgresql-ha-24.9.1-1009.g19e2a92 24.9.1-1009.g19e2a92 ncd-redis paclypancddb01 1 2025-02-27 14:37:11.651840036 -0500 EST deployed ncd-redis-24.9.1-1009.g19e2a92 24.9.1-1009.g19e2a92 [root@ncputility ~ panhub_rc]$ trigger the backup job using following command here Uri should be having cbur structure name in it [root@ncputility ~ panhub_rc]$ helm backup -n paclypancdgit01 -a none -t ncd-git -x http://cbur.apps.panclyphub01.mnc020.mcc714/cbur { \"kind\": \"Status\", \"apiVersion\": \"v1\", \"metadata\": {}, \"status\": \"Success\", \"message\": \"backupHelmRelease task = a80ed466-254c-4644-8d19-9a0cdf813fbe is on!\", \"reason\": \"\", \"details\": { \"a80ed466-254c-4644-8d19-9a0cdf813fbe\": { \"name\": \"ncd-git\", \"namespace\": \"paclypancdgit01\", \"timestamp\": \"20250402104554\", \"backup_data\": {}, \"helm_version\": 3, \"request\": \"backupHelmRelease\" } }, \"code\": 202 } [root@ncputility ~ panhub_rc]$ here is an example with --verbose mode (optinal for troubleshooting) [root@ncputility ~ panhub_rc]$ helm backup -n paclypancdgit01 -a none -t ncd-git -x http://cbur.apps.panclyphub01.mnc020.mcc714/cbur --verbose ******* + exit_func 0 + '[' -d /tmp/certs.1974896 ']' + rm -rf /tmp/certs.1974896 + exit 0 [root@ncputility ~ panhub_rc]$ check the status of the backup using cbur br policy status [root@ncputility ~ panhub_rc]$ kubectl -n paclypancdgit01 get brpolices.cbur.csf.nokia.com ncd-git-toolbox NAME AGE ncd-git-toolbox 33d [root@ncputility ~ panhub_rc]$ kubectl -n paclypancdgit01 get brpolices.cbur.csf.nokia.com ncd-git-toolbox -o yaml [root@ncputility ~ panhub_rc]$ look for backup files exported. , right now backup files are saved locally on the gitserver(paclypancdgit01) namespace within ncd-git-toolbox pod. but we need this backup files to be saved outside the cluster, like sftp server or something, so that we can restore it on new git server. [root@ncputility ~ panhub_rc]$ oc -n paclypancdgit01 exec -it ncd-git-toolbox-56b88dd4c7-27hb5 -- bash Defaulted container \"toolbox\" out of: toolbox, cbura-sidecar, certificates (init), configure (init) git@ncd-git-toolbox-56b88dd4c7-27hb5:/$ cd /srv/gitlab/tmp/backups git@ncd-git-toolbox-56b88dd4c7-27hb5:/srv/gitlab/tmp/backups$ ls backup_information.yml db repositories git@ncd-git-toolbox-56b88dd4c7-27hb5:/srv/gitlab/tmp/backups$ ls -la total 4 drwxr-sr-x. 4 git git 66 Apr 2 10:46 . drwxrwsrwx. 3 root git 21 Apr 2 10:46 .. -rw-r--r--. 1 git git 318 Apr 2 10:46 backup_information.yml drwxr-sr-x. 2 git git 29 Apr 2 10:46 db drwx--S---. 4 git git 38 Apr 2 10:46 repositories git@ncd-git-toolbox-56b88dd4c7-27hb5:/srv/gitlab/tmp/backups$","title":"Backup Nokia Git Server"},{"location":"git-helper/ncd-git-backup/#creating-user-for-remote-backup","text":"login to the linux host and used useradd and passwd commands to create user and password here [root@ncputility ~ panhub_rc]$ useradd gitserverbackup [root@ncputility ~ panhub_rc]$ passwd gitserverbackup Changing password for user gitserverbackup. New password: BAD PASSWORD: The password is shorter than 8 characters Retype new password: passwd: all authentication tokens updated successfully. [root@ncputility ~ panhub_rc]$ create a public key on the new user authrazation file. [root@ncputility ~ pancwl_rc]$ su - gitserverbackup [gitserverbackup@ncputility ~]$ pwd /home/gitserverbackup [gitserverbackup@ncputility ~]$ [gitserverbackup@ncputility ~]$ ssh-keygen -t rsa Generating public/private rsa key pair. Enter file in which to save the key (/home/gitserverbackup/.ssh/id_rsa): Created directory '/home/gitserverbackup/.ssh'. Enter passphrase (empty for no passphrase): Enter same passphrase again: Your identification has been saved in /home/gitserverbackup/.ssh/id_rsa Your public key has been saved in /home/gitserverbackup/.ssh/id_rsa.pub The key fingerprint is: SHA256:oF5K3Ky1tLDmb5ALID4FXyLPnlTtX97ZpJyFE0r3Z2A gitserverbackup@ncputility.panclyphub01.mnc020.mcc714 The key's randomart image is: +---[RSA 3072]----+ | . | | o . o . . oE | | * + o . o.+. | |o B + o o o +o| |o.+ *.* S o o O..| | o.=oO o . . * . | | ..*oo | | o. . | | .o. | +----[SHA256]-----+ [gitserverbackup@ncputility ~]$ take copy of the cbur public using following oc command [root@ncputility ~ panhub_rc]$ helm list -A NAME NAMESPACE REVISION UPDATED STATUS CHART APP VERSION cbur-crds paclypancdcbur01 1 2025-02-27 14:18:04.313208995 -0500 EST deployed cbur-crds-2.6.0 2.6.0 ncd-cbur paclypancdcbur01 2 2025-04-02 04:17:30.755392541 -0500 EST deployed cbur-1.18.1 1.13.1 ncd-git paclypancdgit01 1 2025-02-27 14:45:25.489107042 -0500 EST deployed ncd-git-server-24.9.1-7.g30f1acf 17.3.3 ncd-postgresql paclypancddb01 1 2025-02-27 14:30:27.65639258 -0500 EST deployed postgresql-ha-24.9.1-1009.g19e2a92 24.9.1-1009.g19e2a92 ncd-redis paclypancddb01 1 2025-02-27 14:37:11.651840036 -0500 EST deployed ncd-redis-24.9.1-1009.g19e2a92 24.9.1-1009.g19e2a92 [root@ncputility ~ panhub_rc]$ [root@ncputility ~ panhub_rc]$ oc get secret -n paclypancdcbur01 cburm-ssh-public-key -o jsonpath={.data.ssh_public_key} | base64 -d ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQDUH73f7kf8ey2UEkgeN/IJbEBDJPgricRlsD7d7pB0RAQGLx/fNZT15VRX2qOvLydsTbtcxz28Uzryy5zAmAB9z0zGuYKaSo80bS7bXjIsKc71fGD6NvvfSBBLQ1GCk0mFIjn06XmkRJOgqtgOvq66HQEcGSJQ6jq3NQzGERe+VrCk1VWbyfr0vtqqasmKChgr0dAh+0f07lUdpbR9XzEnOG20LNCAcffEBPXXccSEz/huPHOV0Kjfe7rtaKj5ZoIkFlFETPTz4HoKPlZcfxp/s94yICXk++TiI9+mF2SVYuEUWqx00p1DTa79dmUncTaz1c5nshaq4bNdJcPkoDdp[root@ncputility ~ panhub_rc]$ here i am using ssh-copy-id command to auto create an authzation file [gitserverbackup@ncputility ~]$ ssh-copy-id localhost /usr/bin/ssh-copy-id: INFO: Source of key(s) to be installed: \"/home/gitserverbackup/.ssh/id_rsa.pub\" The authenticity of host 'localhost (::1)' can't be established. ED25519 key fingerprint is SHA256:KXMGNVqIYHOtvcd+VqGq/5d/t5UWWGKxPVkuffCOD9I. This key is not known by any other names Are you sure you want to continue connecting (yes/no/[fingerprint])? yes /usr/bin/ssh-copy-id: INFO: attempting to log in with the new key(s), to filter out any that are already installed /usr/bin/ssh-copy-id: INFO: 1 key(s) remain to be installed -- if you are prompted now it is to install the new keys gitserverbackup@localhost's password: Number of key(s) added: 1 Now try logging into the machine, with: \"ssh 'localhost'\" and check to make sure that only the key(s) you wanted were added. [gitserverbackup@ncputility ~]$ cd .ssh/ [gitserverbackup@ncputility .ssh]$ ll total 20 -rw-------. 1 gitserverbackup gitserverbackup 607 Apr 2 08:14 authorized_keys -rw-------. 1 gitserverbackup gitserverbackup 2655 Apr 2 08:14 id_rsa -rw-r--r--. 1 gitserverbackup gitserverbackup 607 Apr 2 08:14 id_rsa.pub -rw-------. 1 gitserverbackup gitserverbackup 825 Apr 2 08:14 known_hosts -rw-r--r--. 1 gitserverbackup gitserverbackup 91 Apr 2 08:14 known_hosts.old [gitserverbackup@ncputility .ssh]$ vi authorized_keys #<- this updated based on step.3 output> [gitserverbackup@ncputility .ssh]$","title":"Creating user for remote backup"},{"location":"git-helper/ncd-git-backup/#creating-remote-backup-on-ncd-git","text":"Create an git server backup on a remote server, so create a secret with sftp details. [root@ncputility ~ panhub_rc]$ oc create secret generic bastionhostpan \\ --namespace=paclypancdcbur01 \\ --from-literal=port=\"22\" \\ --from-literal=host=\"10.89.100.66\" \\ --from-literal=mode=\"sftp\" \\ --from-literal=username=\"gitserverbackup\" \\ --from-literal=path=\"/home/gitserverbackup\" \\ --from-literal=strictHostKeyChecking=\"no\" \\ --from-literal=hostKey=\"\" secret/bastionhostpan created [root@ncputility ~ panhub_rc]$ make sure, it's avaiable via get command. [root@ncputility ~ panhub_rc]$ oc get secret -n paclypancdcbur01 NAME TYPE DATA AGE bastionhostpan Opaque 7 10s cbur-redis Opaque 1 33d cburm-ssh-public-key Opaque 1 33d my-pull-secret kubernetes.io/dockerconfigjson 1 33d sh.helm.release.v1.cbur-crds.v1 helm.sh/release.v1 1 33d sh.helm.release.v1.ncd-cbur.v1 helm.sh/release.v1 1 33d sh.helm.release.v1.ncd-cbur.v2 helm.sh/release.v1 1 4h14m sh.helm.release.v1.ncd-cbur.v3 helm.sh/release.v1 1 14m [root@ncputility ~ panhub_rc]$","title":"Creating remote backup on NCD git"},{"location":"git-helper/ncd-git-backup/#update-on-the-git-cbur-side","text":"get the backup of cbur repo values file. and update the ssh:credentialName [root@ncputility ~ panhub_rc]$ helm get values ncd-cbur -n paclypancdcbur01 > rr.yaml [root@ncputility ~ panhub_rc]$ SSH: credentialName: bastionhostpan [root@ncputility ~ panhub_rc]$ helm upgrade ncd-cbur -n paclypancdcbur01 /root/ncd/NCD_24.9_Git_Server_ORB-RC/ncd-git-server-product/helmcharts/cbur-1.18.1.tgz -f rr.yaml --debug Similar to git cbur modification, need to update the git=server chart as well. [root@ncputility ~ panhub_rc]$ helm get values ncd-git -n paclypancdgit01 > rr.yaml [root@ncputility ~ panhub_rc]$ update: cbur: brPolicy: apiVersion: cbur.csf.nokia.com/v1 spec: autoEnableCron: false autoUpdateCron: false backend: mode: sftp cronSpec: 0 0 * * * dataEncryption: enable: false ignoreFileChanged: false maxiCopy: 3 weight: 5 [root@ncputility ~ panhub_rc]$ helm upgrade ncd-git -n paclypancdgit01 /root/ncd/NCD_24.9_Git_Server_ORB-RC/ncd-git-server-product/helmcharts/ncd-git-server-24.9.1-7.g30f1acf.tgz -f rr.yaml --debug --timeout 20m validate there is no pods are in pending or crashloop error here .","title":"update on the git cbur side."},{"location":"git-helper/ncd-git-backup/#trigger-the-backup-on-remote-desination","text":"using follow method to trigger the backup [root@ncputility ~ panhub_rc]$ helm backup -n paclypancdgit01 -a none -t ncd-git -x http://cbur.apps.panclyphub01.mnc020.mcc714/cbur --backend sftp --verbose [root@ncputility ~ panhub_rc]$ login to that SFTP server and check for backup files. [gitserverbackup@ncputility 20250402134019_SFTP_paclypancdgit01_ncd-git-toolbox]$ pwd /home/gitserverbackup/BACKUP/sftp/paclypancdgit01/DEPLOYMENT_ncd-git-toolbox/20250402134019_SFTP_paclypancdgit01_ncd-git-toolbox [gitserverbackup@ncputility 20250402134019_SFTP_paclypancdgit01_ncd-git-toolbox]$ ll total 2476 -rw-r--r--. 1 gitserverbackup gitserverbackup 7846 Apr 2 08:40 20250402134019_SFTP_paclypancdgit01_ncd-git-toolbox_Secrets.tar.gz -rw-r--r--. 1 gitserverbackup gitserverbackup 2523279 Apr 2 08:41 20250402134019_SFTP_paclypancdgit01_ncd-git-toolbox_volume.tar.gz [gitserverbackup@ncputility 20250402134019_SFTP_paclypancdgit01_ncd-git-toolbox]$","title":"Trigger the backup on remote desination"},{"location":"git-helper/ncd-git-backup/#troubleshooting","text":"","title":"Troubleshooting"},{"location":"git-helper/ncd-git-backup/#certificate-error","text":"copy crt file used during installation. [root@ncputility denmark]# cp -rp ca.crt /etc/pki/ca-trust/source/anchors/ [root@ncputility denmark]# sudo update-ca-trust [root@ncputility denmark]#","title":"Certificate error"},{"location":"git-helper/ncd-git-backup/#backup-job-failure","text":"if Error looks like this, then problem on the namespace spoce. [root@ncputility ~ hn_hub_rc]$ helm backup -n hnevocdgit01 -a none -t ncd-git -x http://cbur.apps.hnevocphub01.mnc002.mcc708/cbur { \"kind\": \"Status\", \"apiVersion\": \"v1\", \"metadata\": {}, \"status\": \"Failure\", \"message\": \"Failed to get helm release ncd-git in namespace hnevocdgit01\", \"reason\": \"\", \"details\": {}, \"code\": 500 } [root@ncputility ~ hn_hub_rc]$ to resolve this issue, get the values file and update the namespace scope list. include git-server namespace too. [root@ncputility ~ hn_hub_rc]$ helm get values ncd-cbur -n hnevocdcbur01 > rr.yaml retry it, it will work now [root@ncputility ~ hn_hub_rc]$ helm backup -n hnevocdgit01 -a none -t ncd-git -x http://cbur.apps.hnevocphub01.mnc002.mcc708/cbur { \"kind\": \"Status\", \"apiVersion\": \"v1\", \"metadata\": {}, \"status\": \"Success\", \"message\": \"backupHelmRelease task = cc857932-d83d-4e2d-9493-1252595da3f2 is on!\", \"reason\": \"\", \"details\": { \"cc857932-d83d-4e2d-9493-1252595da3f2\": { \"name\": \"ncd-git\", \"namespace\": \"hnevocdgit01\", \"timestamp\": \"20250403005800\", \"backup_data\": {}, \"helm_version\": 3, \"request\": \"backupHelmRelease\" } }, \"code\": 202 } [root@ncputility ~ hn_hub_rc]$","title":"backup job failure"},{"location":"git-helper/ncd-git-backup/#reference","text":"NCDFM tickets https://jiradc2.ext.net.nokia.com/browse/NCDFM-3023","title":"Reference"},{"location":"git-helper/ncd-git-restore/","text":"Backing up Nokia Git Server this procedure, works for NCD 24.9 mp1 pp1. Instructions on backing up Nokia Git Server","title":"Git Restore"},{"location":"git-helper/ncd-git-restore/#backing-up-nokia-git-server","text":"this procedure, works for NCD 24.9 mp1 pp1.","title":"Backing up Nokia Git Server"},{"location":"git-helper/ncd-git-restore/#instructions-on-backing-up-nokia-git-server","text":"","title":"Instructions on backing up Nokia Git Server"},{"location":"git-helper/readme/","text":"This document to help pushing you git code without any issues Set Your Name and Email Globally (One-Time Setup) If the issue is related to your identity (name and email), configure it using: # git config --global user.name \"venkatapathiraj Ravichandran\" # git config --global user.email \"rajurraju400@gmail.com\" This will apply to all repositories on your system. Enable Git Credential Caching a. If the issue is related to authentication (username/password), you can cache your credentials using Git's credential helper. Here's how: For a Temporary Cache (Default 15 Minutes) #git config --global credential.helper cache b. Git will store your credentials in memory for 15 minutes. c. You can adjust the cache timeout (in seconds) like this: git config --global credential.helper 'cache --timeout=3600' For a Persistent Cache a. To store your credentials securely and never be prompted again: git config --global credential.helper store b. Your credentials will be stored in a plain-text file (~/.git-credentials). c. Be cautious with this method if you're using a shared machine. Configure Git to Skip SSL Verification (Not Recommended for Production) If you're in a controlled environment where you trust the source, and you want to bypass SSL verification temporarily, you can configure Git to skip SSL verification: git config --global http.sslVerify false","title":"This document to help pushing you git code without any issues"},{"location":"git-helper/readme/#this-document-to-help-pushing-you-git-code-without-any-issues","text":"Set Your Name and Email Globally (One-Time Setup) If the issue is related to your identity (name and email), configure it using: # git config --global user.name \"venkatapathiraj Ravichandran\" # git config --global user.email \"rajurraju400@gmail.com\" This will apply to all repositories on your system. Enable Git Credential Caching a. If the issue is related to authentication (username/password), you can cache your credentials using Git's credential helper. Here's how: For a Temporary Cache (Default 15 Minutes) #git config --global credential.helper cache b. Git will store your credentials in memory for 15 minutes. c. You can adjust the cache timeout (in seconds) like this: git config --global credential.helper 'cache --timeout=3600' For a Persistent Cache a. To store your credentials securely and never be prompted again: git config --global credential.helper store b. Your credentials will be stored in a plain-text file (~/.git-credentials). c. Be cautious with this method if you're using a shared machine. Configure Git to Skip SSL Verification (Not Recommended for Production) If you're in a controlled environment where you trust the source, and you want to bypass SSL verification temporarily, you can configure Git to skip SSL verification: git config --global http.sslVerify false","title":"This document to help pushing you git code without any issues"},{"location":"openshift/CNF-onboarding-support/image-tls-issue/","text":"Container Image download issue due to TLS problem. Creating tls certificate on the OCP cluster as additional tls newly This step will resolve issue on Linux OS level(bastion host level) In order to be able to push images (for example) from the infra node to this Quay, the rootCA certificate shall be put to the trusted list of the client. The Quay is exposed using the OCP\u2019s default ingress controller, route, so its rootCA shall be fetched (if it was not swapped already). from bastion host, login to respective cluster and find the respective CWL cluster quay url. [root@dom14npv101-infra-manager ~ vlabrc]# oc get route -A |grep -i quay quay-registry quay-registry-quay quay-registry.apps.ncpvnpvlab1.pnwlab.nsn-rdnet.net quay-registry-quay-app http edge/Redirect None quay-registry quay-registry-quay-builder quay-registry-quay-builder-quay-registry.apps.ncpvnpvlab1.pnwlab.nsn-rdnet.net quay-registry-quay-app grpc edge/Redirect None [root@dom14npv101-infra-manager ~ vlabrc]# first locally resolve it, so make dir and download the cert (this is resolve on that particular linux server.) [root@dom14npv101-infra-manager ~ vlabrc]# sudo mkdir -p /etc/containers/certs.d/quay-registry.apps.ncpvnpvlab1.pnwlab.nsn-rdnet.net [root@dom14npv101-infra-manager ~ vlabrc]# create the ingress certificate to local host [root@dom14npv101-infra-manager ~ vlabrc]# oc get secrets -n openshift-ingress-operator router-ca \\ -o jsonpath=\"{.data['tls\\.crt']}\" | base64 -d | \\ sudo tee -a \\ /etc/containers/certs.d/quay-registry.apps.ncpvnpvlab1.pnwlab.nsn-rdnet.net/ca.crt -----BEGIN CERTIFICATE----- MIIDDDCCAfSgAwIBAgIBATANBgkqhkiG9w0BAQsFADAmMSQwIgYDVQQDDBtpbmdy ZXNzLW9wZXJhdG9yQDE3NDE2MjI1OTkwHhcNMjUwMzEwMTYwMzE4WhcNMjcwMzEw MTYwMzE5WjAmMSQwIgYDVQQDDBtpbmdyZXNzLW9wZXJhdG9yQDE3NDE2MjI1OTkw ggEiMA0GCSqGSIb3DQEBAQUAA4IBDwAwggEKAoIBAQCtDCDqA6Cx+MvulZtYNleK T3e4OVlQrR8zytgIQIvlLpeGoqsoNWEe/ZukJ35fco6LA42RcIiI+F5aI6zZ4+F8 81ZEpsSRBx8VUMsZCdkb5mA8tl5vJjV+GC1tRlohk6KWqYoR5VJVLbggFer95efv xyny/BCYXmU2CSHSmRQRnwAI6cX0K7QgEB0kMHaFjEta16UnwzKdhNbaj5rn0aTm hLSGYLvPMx9RVZswjqOqrju0Aovfv9ZzzE++e6+KH9/jZr2HepK62ZZdGznbmnzu Al0D+ILaVj8DiFpcwUIaSaRxUVlphAUmm530GLbKrBdQGSsWcJTo4ixf8R2wYo69 AgMBAAGjRTBDMA4GA1UdDwEB/wQEAwICpDASBgNVHRMBAf8ECDAGAQH/AgEAMB0G A1UdDgQWBBQMuq1odXMd4OlIU4vG8Kfu/aLltjANBgkqhkiG9w0BAQsFAAOCAQEA BVerN+fxay+kk9uei+bQIpryakFstJ5ApuB1wDKgLY3LucwbzXhaE48i9TEOoNlB 32ugNpShYQoOyVMMAvQNQG69HNu0KDJHYGDMAs4seGIsMwqityS6Zgv8T3xo176g mR0y74yiK1ImtnUAaAPt7NNFflhpZafzhY24k4L3AVNEjMKI9B2SgUJAscmXkNIZ Dri+EILpba6MzmeLdE3sVTaOIRberr6yTKbZQskaci+twaO7r83hD3E3xwGJB823 Zu+B2i/txKbBrFeKUpppfrg7zCsyqM1UwFtenuj1yj3qECJVwe1Lr8SctrzpJc+J ryyB1JeEPQWwewI1j7QXqg== -----END CERTIFICATE----- [root@dom14npv101-infra-manager ~ vlabrc]# On the OCP cluster level changes, TLS Creation Similarly if images would be pulled on the HUB cluster, its ingress\u2019 rootCA shall be put into the image.config.openshift.io/cluster CR, as the registry is using self-signed certificate by default and treated as an insecure registry, more precisely the OCP\u2019s ingress (if it is not swapped yet). In order to overcome issues the following commands: [root@dom14npv101-infra-manager ~ vlabrc]# oc get secrets -n openshift-ingress-operator router-ca \\ -o jsonpath=\"{.data['tls\\.crt']}\" | base64 -d > ingress_ca.crt [root@dom14npv101-infra-manager ~ vlabrc]# create an cm to enforce the certificate here. [root@dom14npv101-infra-manager ~ vlabrc]# oc create configmap registry-cas -n openshift-config \\ --from-file=quay-registry.apps.ncpvnpvlab1.pnwlab.nsn-rdnet.net=ingress_ca.crt configmap/registry-cas created [root@dom14npv101-infra-manager ~ vlabrc]# now create an image patch to cluster iamge config. [root@dom14npv101-infra-manager ~ vlabrc]# oc patch image.config.openshift.io/cluster --patch '{\"spec\": {\"additionalTrustedCA\":{\"name\":\"registry-cas\"}}}' \\ --type=merge image.config.openshift.io/cluster patched [root@dom14npv101-infra-manager ~ vlabrc]# Creating tls certificate on the OCP cluster as additional tls ADD oc apply is the preferred way to update an existing resource, including ConfigMaps. This command will update the ConfigMap with the new data while keeping existing data intact. If you don\u2019t have a YAML file, you can first export the current ConfigMap to a file, edit it, and then apply the changes: oc get configmap registry-cas -n openshift-config -o yaml > registry-cas.yaml oc apply -f registry-cas.yaml If you need to update the existing ConfigMap with new or modified data directly from the command line, you can force the update with oc create configmap using the --dry-run and --force flags: oc create configmap registry-cas -n openshift-config \\ --from-file=harbor.ncdvnpv.ncpvnpvmgt.pnwlab.nsn-rdnet.net=ingress_ca.crt \\ --dry-run=client -o yaml | oc replace -f - Using oc patch (for small changes) Optional extra steps oc patch configmap registry-cas -n openshift-config \\ --type='json' \\ -p='[{\"op\": \"replace\", \"path\": \"/data/harbor.ncdvnpv.ncpvnpvmgt.pnwlab.nsn-rdnet.net\", \"value\": \"ingress_ca.crt\"}]'","title":"Image TLS Issue"},{"location":"openshift/CNF-onboarding-support/image-tls-issue/#container-image-download-issue-due-to-tls-problem","text":"","title":"Container Image download issue due to TLS problem."},{"location":"openshift/CNF-onboarding-support/image-tls-issue/#creating-tls-certificate-on-the-ocp-cluster-as-additional-tls-newly","text":"","title":"Creating tls certificate on the OCP cluster as additional tls newly"},{"location":"openshift/CNF-onboarding-support/image-tls-issue/#this-step-will-resolve-issue-on-linux-os-levelbastion-host-level","text":"In order to be able to push images (for example) from the infra node to this Quay, the rootCA certificate shall be put to the trusted list of the client. The Quay is exposed using the OCP\u2019s default ingress controller, route, so its rootCA shall be fetched (if it was not swapped already). from bastion host, login to respective cluster and find the respective CWL cluster quay url. [root@dom14npv101-infra-manager ~ vlabrc]# oc get route -A |grep -i quay quay-registry quay-registry-quay quay-registry.apps.ncpvnpvlab1.pnwlab.nsn-rdnet.net quay-registry-quay-app http edge/Redirect None quay-registry quay-registry-quay-builder quay-registry-quay-builder-quay-registry.apps.ncpvnpvlab1.pnwlab.nsn-rdnet.net quay-registry-quay-app grpc edge/Redirect None [root@dom14npv101-infra-manager ~ vlabrc]# first locally resolve it, so make dir and download the cert (this is resolve on that particular linux server.) [root@dom14npv101-infra-manager ~ vlabrc]# sudo mkdir -p /etc/containers/certs.d/quay-registry.apps.ncpvnpvlab1.pnwlab.nsn-rdnet.net [root@dom14npv101-infra-manager ~ vlabrc]# create the ingress certificate to local host [root@dom14npv101-infra-manager ~ vlabrc]# oc get secrets -n openshift-ingress-operator router-ca \\ -o jsonpath=\"{.data['tls\\.crt']}\" | base64 -d | \\ sudo tee -a \\ /etc/containers/certs.d/quay-registry.apps.ncpvnpvlab1.pnwlab.nsn-rdnet.net/ca.crt -----BEGIN CERTIFICATE----- MIIDDDCCAfSgAwIBAgIBATANBgkqhkiG9w0BAQsFADAmMSQwIgYDVQQDDBtpbmdy ZXNzLW9wZXJhdG9yQDE3NDE2MjI1OTkwHhcNMjUwMzEwMTYwMzE4WhcNMjcwMzEw MTYwMzE5WjAmMSQwIgYDVQQDDBtpbmdyZXNzLW9wZXJhdG9yQDE3NDE2MjI1OTkw ggEiMA0GCSqGSIb3DQEBAQUAA4IBDwAwggEKAoIBAQCtDCDqA6Cx+MvulZtYNleK T3e4OVlQrR8zytgIQIvlLpeGoqsoNWEe/ZukJ35fco6LA42RcIiI+F5aI6zZ4+F8 81ZEpsSRBx8VUMsZCdkb5mA8tl5vJjV+GC1tRlohk6KWqYoR5VJVLbggFer95efv xyny/BCYXmU2CSHSmRQRnwAI6cX0K7QgEB0kMHaFjEta16UnwzKdhNbaj5rn0aTm hLSGYLvPMx9RVZswjqOqrju0Aovfv9ZzzE++e6+KH9/jZr2HepK62ZZdGznbmnzu Al0D+ILaVj8DiFpcwUIaSaRxUVlphAUmm530GLbKrBdQGSsWcJTo4ixf8R2wYo69 AgMBAAGjRTBDMA4GA1UdDwEB/wQEAwICpDASBgNVHRMBAf8ECDAGAQH/AgEAMB0G A1UdDgQWBBQMuq1odXMd4OlIU4vG8Kfu/aLltjANBgkqhkiG9w0BAQsFAAOCAQEA BVerN+fxay+kk9uei+bQIpryakFstJ5ApuB1wDKgLY3LucwbzXhaE48i9TEOoNlB 32ugNpShYQoOyVMMAvQNQG69HNu0KDJHYGDMAs4seGIsMwqityS6Zgv8T3xo176g mR0y74yiK1ImtnUAaAPt7NNFflhpZafzhY24k4L3AVNEjMKI9B2SgUJAscmXkNIZ Dri+EILpba6MzmeLdE3sVTaOIRberr6yTKbZQskaci+twaO7r83hD3E3xwGJB823 Zu+B2i/txKbBrFeKUpppfrg7zCsyqM1UwFtenuj1yj3qECJVwe1Lr8SctrzpJc+J ryyB1JeEPQWwewI1j7QXqg== -----END CERTIFICATE----- [root@dom14npv101-infra-manager ~ vlabrc]#","title":"This step will resolve issue on Linux OS level(bastion host level)"},{"location":"openshift/CNF-onboarding-support/image-tls-issue/#on-the-ocp-cluster-level-changes-tls-creation","text":"Similarly if images would be pulled on the HUB cluster, its ingress\u2019 rootCA shall be put into the image.config.openshift.io/cluster CR, as the registry is using self-signed certificate by default and treated as an insecure registry, more precisely the OCP\u2019s ingress (if it is not swapped yet). In order to overcome issues the following commands: [root@dom14npv101-infra-manager ~ vlabrc]# oc get secrets -n openshift-ingress-operator router-ca \\ -o jsonpath=\"{.data['tls\\.crt']}\" | base64 -d > ingress_ca.crt [root@dom14npv101-infra-manager ~ vlabrc]# create an cm to enforce the certificate here. [root@dom14npv101-infra-manager ~ vlabrc]# oc create configmap registry-cas -n openshift-config \\ --from-file=quay-registry.apps.ncpvnpvlab1.pnwlab.nsn-rdnet.net=ingress_ca.crt configmap/registry-cas created [root@dom14npv101-infra-manager ~ vlabrc]# now create an image patch to cluster iamge config. [root@dom14npv101-infra-manager ~ vlabrc]# oc patch image.config.openshift.io/cluster --patch '{\"spec\": {\"additionalTrustedCA\":{\"name\":\"registry-cas\"}}}' \\ --type=merge image.config.openshift.io/cluster patched [root@dom14npv101-infra-manager ~ vlabrc]#","title":"On the OCP cluster level changes, TLS Creation"},{"location":"openshift/CNF-onboarding-support/image-tls-issue/#creating-tls-certificate-on-the-ocp-cluster-as-additional-tls-add","text":"oc apply is the preferred way to update an existing resource, including ConfigMaps. This command will update the ConfigMap with the new data while keeping existing data intact. If you don\u2019t have a YAML file, you can first export the current ConfigMap to a file, edit it, and then apply the changes: oc get configmap registry-cas -n openshift-config -o yaml > registry-cas.yaml oc apply -f registry-cas.yaml If you need to update the existing ConfigMap with new or modified data directly from the command line, you can force the update with oc create configmap using the --dry-run and --force flags: oc create configmap registry-cas -n openshift-config \\ --from-file=harbor.ncdvnpv.ncpvnpvmgt.pnwlab.nsn-rdnet.net=ingress_ca.crt \\ --dry-run=client -o yaml | oc replace -f - Using oc patch (for small changes) Optional extra steps oc patch configmap registry-cas -n openshift-config \\ --type='json' \\ -p='[{\"op\": \"replace\", \"path\": \"/data/harbor.ncdvnpv.ncpvnpvmgt.pnwlab.nsn-rdnet.net\", \"value\": \"ingress_ca.crt\"}]'","title":"Creating tls certificate on the OCP cluster as additional tls ADD"},{"location":"openshift/CNF-onboarding-support/oclogin-tls-issue/","text":"Oc login ssl error on OCP To login to NWC cluster, oauth-serving-cert (second certificate) to be copied to /etc/pki/ca-trust/source/anchors/ and then run sudo update-ca-trust extract command. It should be done for each NWC cluster. To get the certificate, execute below command on NWC cluster. [root@ncputility ~ panhub_rc]$ oc get configmaps -n openshift-config-managed oauth-serving-cert -o yaml apiVersion: v1 data: ca-bundle.crt: |2 -----BEGIN CERTIFICATE----- MIIDeTCCAmGgAwIBAgIIItt373u4MZMwDQYJKoZIhvcNAQELBQAwJjEkMCIGA1UE AwwbaW5ncmVzcy1vcGVyYXRvckAxNzQwMDc0NTQ0MB4XDTI1MDIyMDE4MDIyM1oX DTI3MDIyMDE4MDIyNFowLDEqMCgGA1UEAwwhKi5hcHBzLnBhbmNseXBodWIwMS5t bmMwMjAubWNjNzE0MIIBIjANBgkqhkiG9w0BAQEFAAOCAQ8AMIIBCgKCAQEAoNne 91mbAMQEjKAyOBWp0wGYpELbNKgYtgUGqL0zteOA3WI+opnJnpTPztlqr4Xqddyw EAuPS7gjUqplbwJROO0lQ+sDSVqfOCC7wvrkd6pI/0jxWPK9WgnSZt1lmiJ9L0Rs s7H5iVdUq8hvIfI9ZUzvr2BUGi9StdABRFoxk1R+BF6yRRiQnxhyqhYjPOuzV4GM blfDAvo3yqFoMOHo0DTZQXcRLQnbt2a3ApPHcsLgyBjTmOMlPilRSHtVFivQP2Pd VRhZGSsAANk7aJyCvHZ+oMo0DLUqmBgikHpgm9TAv6M+oX0kbfdqfMci8sEF7Vqj 9fK5l19t+zZaXTnH3QIDAQABo4GkMIGhMA4GA1UdDwEB/wQEAwIFoDATBgNVHSUE DDAKBggrBgEFBQcDATAMBgNVHRMBAf8EAjAAMB0GA1UdDgQWBBTsyQRlbeyo2H/V f887YPhF8jVixDAfBgNVHSMEGDAWgBTZS/SzkBfiHQ5/Gy+b4g1XJOHkojAsBgNV HREEJTAjgiEqLmFwcHMucGFuY2x5cGh1YjAxLm1uYzAyMC5tY2M3MTQwDQYJKoZI hvcNAQELBQADggEBAE6kBjPoA3RJI09pYfUzQlEyKQrnudNTu+O61ZspCPvafp4s 4py8hyS/pzkp7611KfmJnzXjiBjw6qzcE5lye4coO5vwplYDbZUTCn9bz30+2g1O wpA1ZOLLTHet11+i0FG0m4AJq4OXEjHuA1K2+AyfzG0TsogT88WstndoNPtGrYWJ pj1kQYbVqwBtCU/jhKbXycEQ+UdeICRuNp5FbBcJ/ZrxJRmJa/zUkT6tHWMvzlsI IPrTpL7BEkoRtWPYhcW4gL70XgkmahuX2bssG7C2IJxN2DNTvmFsMSWHzQNc7AaD ND+v4E+mn2zKhzdQyOB4Mx6H4LH5cEHZsLfVal8= -----END CERTIFICATE----- -----BEGIN CERTIFICATE----- MIIDDDCCAfSgAwIBAgIBATANBgkqhkiG9w0BAQsFADAmMSQwIgYDVQQDDBtpbmdy ZXNzLW9wZXJhdG9yQDE3NDAwNzQ1NDQwHhcNMjUwMjIwMTgwMjIzWhcNMjcwMjIw MTgwMjI0WjAmMSQwIgYDVQQDDBtpbmdyZXNzLW9wZXJhdG9yQDE3NDAwNzQ1NDQw ggEiMA0GCSqGSIb3DQEBAQUAA4IBDwAwggEKAoIBAQDjXWFSVPshwihivZhaTrB7 0boUOw2j3Ut/J6eSm+JA+xVl05L4XD8+C8VSst+f32Pe42Lso2hrovY1dT7IBX3g 6S9Nnd2iRc9vC/qHcQkGQ6krIYSQ48aCH7UuahCpTqxEp+MwmhCTQngN2maTJBe3 0E6K2IL4zXSi0Iuj08BOnH/w4pJxeWhyngXDf2SeA88EmU3juHLshHrAND84uou2 /sOStIAhFwU+o887dSgx7iERy+6nczSDB9Qvq11cUSS4RPC82bNnxcc+BrynDwZ/ eggs0OEaj/1cHu/svKZHX9gUKrqz80wF8YGZLLgI2oPAf2VJorvtkJAErzgttroF AgMBAAGjRTBDMA4GA1UdDwEB/wQEAwICpDASBgNVHRMBAf8ECDAGAQH/AgEAMB0G A1UdDgQWBBTZS/SzkBfiHQ5/Gy+b4g1XJOHkojANBgkqhkiG9w0BAQsFAAOCAQEA UwPhAbzTWZIlBsMHAL+8jvxM8qxc6HDhayAD4gbCE65vHYgSizost02vRfpOPQq1 D6HM8JjifS3KHd6E6chdTbrHI0W8pMJJPon5akCJf/uGeGDl+2wKfmVC6UoV7hC3 pcUzm3JKwsNJbjS5rxL8f5a8bNdIFfLQKuyRpnVX2CsNHvh+WJzynQ+PUJ6zCa7y x5AJxca2PTnBKRoVTAyumT1suluI9f4GRYnxTE/qIKRZRs+uT3kIl/N9VX+GbjGb pPszJ+p6N6Arl1BqJP1DdLin2IFGZL39pTyifm5GP+Vou2aHPuHZDVoCdxsFKup+ gUY2KeKz0UManwubPQNnKA== -----END CERTIFICATE----- kind: ConfigMap metadata: annotations: openshift.io/owning-component: apiserver-auth creationTimestamp: \"2025-02-20T18:02:31Z\" name: oauth-serving-cert namespace: openshift-config-managed resourceVersion: \"73869405\" uid: a08c0137-033d-41d0-9c79-8f345662140c [root@ncputility ~ panhub_rc]$ Then copy second certificate and put it in a file. Move this file to /etc/pki/ca-trust/source/anchors/ Post that, run update-ca-trust extrace command. Create separate linux user and do oc login kubeadmin, password and api url (find from the kubeconfig) file.","title":"OCLogin TLS Issue"},{"location":"openshift/CNF-onboarding-support/oclogin-tls-issue/#oc-login-ssl-error-on-ocp","text":"To login to NWC cluster, oauth-serving-cert (second certificate) to be copied to /etc/pki/ca-trust/source/anchors/ and then run sudo update-ca-trust extract command. It should be done for each NWC cluster. To get the certificate, execute below command on NWC cluster. [root@ncputility ~ panhub_rc]$ oc get configmaps -n openshift-config-managed oauth-serving-cert -o yaml apiVersion: v1 data: ca-bundle.crt: |2 -----BEGIN CERTIFICATE----- MIIDeTCCAmGgAwIBAgIIItt373u4MZMwDQYJKoZIhvcNAQELBQAwJjEkMCIGA1UE AwwbaW5ncmVzcy1vcGVyYXRvckAxNzQwMDc0NTQ0MB4XDTI1MDIyMDE4MDIyM1oX DTI3MDIyMDE4MDIyNFowLDEqMCgGA1UEAwwhKi5hcHBzLnBhbmNseXBodWIwMS5t bmMwMjAubWNjNzE0MIIBIjANBgkqhkiG9w0BAQEFAAOCAQ8AMIIBCgKCAQEAoNne 91mbAMQEjKAyOBWp0wGYpELbNKgYtgUGqL0zteOA3WI+opnJnpTPztlqr4Xqddyw EAuPS7gjUqplbwJROO0lQ+sDSVqfOCC7wvrkd6pI/0jxWPK9WgnSZt1lmiJ9L0Rs s7H5iVdUq8hvIfI9ZUzvr2BUGi9StdABRFoxk1R+BF6yRRiQnxhyqhYjPOuzV4GM blfDAvo3yqFoMOHo0DTZQXcRLQnbt2a3ApPHcsLgyBjTmOMlPilRSHtVFivQP2Pd VRhZGSsAANk7aJyCvHZ+oMo0DLUqmBgikHpgm9TAv6M+oX0kbfdqfMci8sEF7Vqj 9fK5l19t+zZaXTnH3QIDAQABo4GkMIGhMA4GA1UdDwEB/wQEAwIFoDATBgNVHSUE DDAKBggrBgEFBQcDATAMBgNVHRMBAf8EAjAAMB0GA1UdDgQWBBTsyQRlbeyo2H/V f887YPhF8jVixDAfBgNVHSMEGDAWgBTZS/SzkBfiHQ5/Gy+b4g1XJOHkojAsBgNV HREEJTAjgiEqLmFwcHMucGFuY2x5cGh1YjAxLm1uYzAyMC5tY2M3MTQwDQYJKoZI hvcNAQELBQADggEBAE6kBjPoA3RJI09pYfUzQlEyKQrnudNTu+O61ZspCPvafp4s 4py8hyS/pzkp7611KfmJnzXjiBjw6qzcE5lye4coO5vwplYDbZUTCn9bz30+2g1O wpA1ZOLLTHet11+i0FG0m4AJq4OXEjHuA1K2+AyfzG0TsogT88WstndoNPtGrYWJ pj1kQYbVqwBtCU/jhKbXycEQ+UdeICRuNp5FbBcJ/ZrxJRmJa/zUkT6tHWMvzlsI IPrTpL7BEkoRtWPYhcW4gL70XgkmahuX2bssG7C2IJxN2DNTvmFsMSWHzQNc7AaD ND+v4E+mn2zKhzdQyOB4Mx6H4LH5cEHZsLfVal8= -----END CERTIFICATE----- -----BEGIN CERTIFICATE----- MIIDDDCCAfSgAwIBAgIBATANBgkqhkiG9w0BAQsFADAmMSQwIgYDVQQDDBtpbmdy ZXNzLW9wZXJhdG9yQDE3NDAwNzQ1NDQwHhcNMjUwMjIwMTgwMjIzWhcNMjcwMjIw MTgwMjI0WjAmMSQwIgYDVQQDDBtpbmdyZXNzLW9wZXJhdG9yQDE3NDAwNzQ1NDQw ggEiMA0GCSqGSIb3DQEBAQUAA4IBDwAwggEKAoIBAQDjXWFSVPshwihivZhaTrB7 0boUOw2j3Ut/J6eSm+JA+xVl05L4XD8+C8VSst+f32Pe42Lso2hrovY1dT7IBX3g 6S9Nnd2iRc9vC/qHcQkGQ6krIYSQ48aCH7UuahCpTqxEp+MwmhCTQngN2maTJBe3 0E6K2IL4zXSi0Iuj08BOnH/w4pJxeWhyngXDf2SeA88EmU3juHLshHrAND84uou2 /sOStIAhFwU+o887dSgx7iERy+6nczSDB9Qvq11cUSS4RPC82bNnxcc+BrynDwZ/ eggs0OEaj/1cHu/svKZHX9gUKrqz80wF8YGZLLgI2oPAf2VJorvtkJAErzgttroF AgMBAAGjRTBDMA4GA1UdDwEB/wQEAwICpDASBgNVHRMBAf8ECDAGAQH/AgEAMB0G A1UdDgQWBBTZS/SzkBfiHQ5/Gy+b4g1XJOHkojANBgkqhkiG9w0BAQsFAAOCAQEA UwPhAbzTWZIlBsMHAL+8jvxM8qxc6HDhayAD4gbCE65vHYgSizost02vRfpOPQq1 D6HM8JjifS3KHd6E6chdTbrHI0W8pMJJPon5akCJf/uGeGDl+2wKfmVC6UoV7hC3 pcUzm3JKwsNJbjS5rxL8f5a8bNdIFfLQKuyRpnVX2CsNHvh+WJzynQ+PUJ6zCa7y x5AJxca2PTnBKRoVTAyumT1suluI9f4GRYnxTE/qIKRZRs+uT3kIl/N9VX+GbjGb pPszJ+p6N6Arl1BqJP1DdLin2IFGZL39pTyifm5GP+Vou2aHPuHZDVoCdxsFKup+ gUY2KeKz0UManwubPQNnKA== -----END CERTIFICATE----- kind: ConfigMap metadata: annotations: openshift.io/owning-component: apiserver-auth creationTimestamp: \"2025-02-20T18:02:31Z\" name: oauth-serving-cert namespace: openshift-config-managed resourceVersion: \"73869405\" uid: a08c0137-033d-41d0-9c79-8f345662140c [root@ncputility ~ panhub_rc]$ Then copy second certificate and put it in a file. Move this file to /etc/pki/ca-trust/source/anchors/ Post that, run update-ca-trust extrace command. Create separate linux user and do oc login kubeadmin, password and api url (find from the kubeconfig) file.","title":"Oc login ssl error on OCP"},{"location":"openshift/CNF-onboarding-support/proxy-cache-pod/","text":"Proxy-cache quay based pod creation proxy-cache TLS issue. Login in to CWL cluster and make sure mcp and nodes are looks good at this point, if any issue, need to be resolved first. [root@ncputility ~ pancwl_rc]$ oc get nodes NAME STATUS ROLES AGE VERSION appworker0.panclypcwl01.mnc020.mcc714 Ready appworker,appworker-mcp-a,worker 38d v1.29.6+aba1e8d appworker1.panclypcwl01.mnc020.mcc714 Ready appworker,appworker-mcp-a,worker 38d v1.29.6+aba1e8d appworker10.panclypcwl01.mnc020.mcc714 Ready appworker,appworker-mcp-b,worker 37d v1.29.6+aba1e8d appworker11.panclypcwl01.mnc020.mcc714 Ready appworker,appworker-mcp-b,worker 37d v1.29.6+aba1e8d appworker12.panclypcwl01.mnc020.mcc714 Ready appworker,appworker-mcp-b,worker 37d v1.29.6+aba1e8d appworker13.panclypcwl01.mnc020.mcc714 Ready appworker,appworker-mcp-b,worker 37d v1.29.6+aba1e8d appworker14.panclypcwl01.mnc020.mcc714 Ready appworker,appworker-mcp-b,worker 37d v1.29.6+aba1e8d appworker15.panclypcwl01.mnc020.mcc714 Ready appworker,appworker-mcp-b,worker 37d v1.29.6+aba1e8d appworker16.panclypcwl01.mnc020.mcc714 Ready appworker,appworker-mcp-b,worker 37d v1.29.6+aba1e8d appworker17.panclypcwl01.mnc020.mcc714 Ready appworker,appworker-mcp-b,worker 37d v1.29.6+aba1e8d appworker19.panclypcwl01.mnc020.mcc714 Ready appworker,appworker-mcp-c,worker 37d v1.29.6+aba1e8d appworker2.panclypcwl01.mnc020.mcc714 Ready appworker,appworker-mcp-a,worker 30d v1.29.6+aba1e8d appworker20.panclypcwl01.mnc020.mcc714 Ready appworker,appworker-mcp-c,worker 37d v1.29.6+aba1e8d appworker21.panclypcwl01.mnc020.mcc714 Ready appworker,appworker-mcp-c,worker 37d v1.29.6+aba1e8d appworker22.panclypcwl01.mnc020.mcc714 Ready appworker,appworker-mcp-c,worker 37d v1.29.6+aba1e8d appworker23.panclypcwl01.mnc020.mcc714 Ready appworker,appworker-mcp-c,worker 37d v1.29.6+aba1e8d appworker24.panclypcwl01.mnc020.mcc714 Ready appworker,appworker-mcp-c,worker 37d v1.29.6+aba1e8d appworker25.panclypcwl01.mnc020.mcc714 Ready appworker,appworker-mcp-c,worker 37d v1.29.6+aba1e8d appworker26.panclypcwl01.mnc020.mcc714 Ready appworker,appworker-mcp-c,worker 37d v1.29.6+aba1e8d appworker27.panclypcwl01.mnc020.mcc714 Ready appworker,appworker-mcp-d,worker 37d v1.29.6+aba1e8d appworker28.panclypcwl01.mnc020.mcc714 Ready appworker,appworker-mcp-d,worker 37d v1.29.6+aba1e8d appworker29.panclypcwl01.mnc020.mcc714 Ready appworker,appworker-mcp-d,worker 37d v1.29.6+aba1e8d appworker3.panclypcwl01.mnc020.mcc714 Ready appworker,appworker-mcp-a,worker 38d v1.29.6+aba1e8d appworker30.panclypcwl01.mnc020.mcc714 Ready appworker,appworker-mcp-d,worker 37d v1.29.6+aba1e8d appworker31.panclypcwl01.mnc020.mcc714 Ready appworker,appworker-mcp-d,worker 37d v1.29.6+aba1e8d appworker32.panclypcwl01.mnc020.mcc714 Ready appworker,appworker-mcp-d,worker 37d v1.29.6+aba1e8d appworker33.panclypcwl01.mnc020.mcc714 Ready appworker,appworker-mcp-d,worker 37d v1.29.6+aba1e8d appworker34.panclypcwl01.mnc020.mcc714 Ready appworker,appworker-mcp-d,worker 37d v1.29.6+aba1e8d appworker4.panclypcwl01.mnc020.mcc714 Ready appworker,appworker-mcp-a,worker 38d v1.29.6+aba1e8d appworker5.panclypcwl01.mnc020.mcc714 Ready appworker,appworker-mcp-a,worker 38d v1.29.6+aba1e8d appworker6.panclypcwl01.mnc020.mcc714 Ready appworker,appworker-mcp-a,worker 38d v1.29.6+aba1e8d appworker7.panclypcwl01.mnc020.mcc714 Ready appworker,appworker-mcp-a,worker 37d v1.29.6+aba1e8d appworker9.panclypcwl01.mnc020.mcc714 Ready appworker,appworker-mcp-b,worker 37d v1.29.6+aba1e8d gateway1.panclypcwl01.mnc020.mcc714 Ready gateway,gateway-mcp-a,worker 38d v1.29.6+aba1e8d gateway2.panclypcwl01.mnc020.mcc714 Ready gateway,gateway-mcp-a,worker 38d v1.29.6+aba1e8d gateway3.panclypcwl01.mnc020.mcc714 Ready gateway,gateway-mcp-b,worker 38d v1.29.6+aba1e8d gateway4.panclypcwl01.mnc020.mcc714 Ready gateway,gateway-mcp-b,worker 38d v1.29.6+aba1e8d master0.panclypcwl01.mnc020.mcc714 Ready control-plane,master,monitor 38d v1.29.6+aba1e8d master1.panclypcwl01.mnc020.mcc714 Ready control-plane,master,monitor 38d v1.29.6+aba1e8d master2.panclypcwl01.mnc020.mcc714 Ready control-plane,master,monitor 38d v1.29.6+aba1e8d storage0.panclypcwl01.mnc020.mcc714 Ready storage,worker 38d v1.29.6+aba1e8d storage1.panclypcwl01.mnc020.mcc714 Ready storage,worker 38d v1.29.6+aba1e8d storage2.panclypcwl01.mnc020.mcc714 Ready storage,worker 38d v1.29.6+aba1e8d storage3.panclypcwl01.mnc020.mcc714 Ready storage,worker 38d v1.29.6+aba1e8d storage4.panclypcwl01.mnc020.mcc714 Ready storage,worker 38d v1.29.6+aba1e8d [root@ncputility ~ pancwl_rc]$ oc get mcp NAME CONFIG UPDATED UPDATING DEGRADED MACHINECOUNT READYMACHINECOUNT UPDATEDMACHINECOUNT DEGRADEDMACHINECOUNT AGE appworker-mcp-a rendered-appworker-mcp-a-0e9dd6df593dcd5016bbe7d601119bf4 True False False 8 8 8 0 37d appworker-mcp-b rendered-appworker-mcp-b-0e9dd6df593dcd5016bbe7d601119bf4 True False False 9 9 9 0 37d appworker-mcp-c rendered-appworker-mcp-c-5afb864664d3b10530b54b3153a1a61e True False False 8 8 8 0 29h appworker-mcp-d rendered-appworker-mcp-d-5afb864664d3b10530b54b3153a1a61e True False False 8 8 8 0 29h gateway-mcp-a rendered-gateway-mcp-a-c81254a16575de9053ae543c4f1ba3fb True False False 2 2 2 0 37d gateway-mcp-b rendered-gateway-mcp-b-3be41ecbbe09004c35ca04a4309cabf0 True False False 2 2 2 0 29h master rendered-master-114f60e6be691323222ea11e72de0bcf True False False 3 3 3 0 38d storage rendered-storage-dc2d8a34080bce1400e11bb1fb098693 True False False 5 5 5 0 37d worker rendered-worker-68cb1df39185f7ad80fda7915e4c5a42 True False False 0 0 0 0 38d [root@ncputility ~ pancwl_rc]$ login to hub cluster and run this command [root@ncputility ~ pancwl_rc]$ source /root/panhubrc WARNING: Using insecure TLS client config. Setting this option is not supported! Login successful. You have access to 105 projects, the list has been suppressed. You can list all projects with 'oc projects' Using project \"default\". [root@ncputility ~ panhub_rc]$ oc get secret -n openshift-ingress-operator router-ca -o \"jsonpath={.data['tls\\.crt']}\" | base64 -d -----BEGIN CERTIFICATE----- MIIDDDCCAfSgAwIBAgIBATANBgkqhkiG9w0BAQsFADAmMSQwIgYDVQQDDBtpbmdy ZXNzLW9wZXJhdG9yQDE3NDAwNzQ1NDQwHhcNMjUwMjIwMTgwMjIzWhcNMjcwMjIw MTgwMjI0WjAmMSQwIgYDVQQDDBtpbmdyZXNzLW9wZXJhdG9yQDE3NDAwNzQ1NDQw ggEiMA0GCSqGSIb3DQEBAQUAA4IBDwAwggEKAoIBAQDjXWFSVPshwihivZhaTrB7 0boUOw2j3Ut/J6eSm+JA+xVl05L4XD8+C8VSst+f32Pe42Lso2hrovY1dT7IBX3g 6S9Nnd2iRc9vC/qHcQkGQ6krIYSQ48aCH7UuahCpTqxEp+MwmhCTQngN2maTJBe3 0E6K2IL4zXSi0Iuj08BOnH/w4pJxeWhyngXDf2SeA88EmU3juHLshHrAND84uou2 /sOStIAhFwU+o887dSgx7iERy+6nczSDB9Qvq11cUSS4RPC82bNnxcc+BrynDwZ/ eggs0OEaj/1cHu/svKZHX9gUKrqz80wF8YGZLLgI2oPAf2VJorvtkJAErzgttroF AgMBAAGjRTBDMA4GA1UdDwEB/wQEAwICpDASBgNVHRMBAf8ECDAGAQH/AgEAMB0G A1UdDgQWBBTZS/SzkBfiHQ5/Gy+b4g1XJOHkojANBgkqhkiG9w0BAQsFAAOCAQEA UwPhAbzTWZIlBsMHAL+8jvxM8qxc6HDhayAD4gbCE65vHYgSizost02vRfpOPQq1 D6HM8JjifS3KHd6E6chdTbrHI0W8pMJJPon5akCJf/uGeGDl+2wKfmVC6UoV7hC3 pcUzm3JKwsNJbjS5rxL8f5a8bNdIFfLQKuyRpnVX2CsNHvh+WJzynQ+PUJ6zCa7y x5AJxca2PTnBKRoVTAyumT1suluI9f4GRYnxTE/qIKRZRs+uT3kIl/N9VX+GbjGb pPszJ+p6N6Arl1BqJP1DdLin2IFGZL39pTyifm5GP+Vou2aHPuHZDVoCdxsFKup+ gUY2KeKz0UManwubPQNnKA== -----END CERTIFICATE----- [root@ncputility ~ panhub_rc]$ login to workload cluster and run this command [root@ncputility ~ pancwl_rc]$ source /root/pancwlrc WARNING: Using insecure TLS client config. Setting this option is not supported! Login successful. You have access to 115 projects, the list has been suppressed. You can list all projects with 'oc projects' Using project \"nokia\". [root@ncputility ~ pancwl_rc]$ oc get cm -n openshift-config user-ca-bundle -o yaml apiVersion: v1 data: ca-bundle.crt: | -----BEGIN CERTIFICATE----- MIIDDDCCAfSgAwIBAgIBATANBgkqhkiG9w0BAQsFADAmMSQwIgYDVQQDDBtpbmdy ZXNzLW9wZXJhdG9yQDE3NDAwNzQ1NDQwHhcNMjUwMjIwMTgwMjIzWhcNMjcwMjIw MTgwMjI0WjAmMSQwIgYDVQQDDBtpbmdyZXNzLW9wZXJhdG9yQDE3NDAwNzQ1NDQw ggEiMA0GCSqGSIb3DQEBAQUAA4IBDwAwggEKAoIBAQDjXWFSVPshwihivZhaTrB7 0boUOw2j3Ut/J6eSm+JA+xVl05L4XD8+C8VSst+f32Pe42Lso2hrovY1dT7IBX3g 6S9Nnd2iRc9vC/qHcQkGQ6krIYSQ48aCH7UuahCpTqxEp+MwmhCTQngN2maTJBe3 0E6K2IL4zXSi0Iuj08BOnH/w4pJxeWhyngXDf2SeA88EmU3juHLshHrAND84uou2 /sOStIAhFwU+o887dSgx7iERy+6nczSDB9Qvq11cUSS4RPC82bNnxcc+BrynDwZ/ eggs0OEaj/1cHu/svKZHX9gUKrqz80wF8YGZLLgI2oPAf2VJorvtkJAErzgttroF AgMBAAGjRTBDMA4GA1UdDwEB/wQEAwICpDASBgNVHRMBAf8ECDAGAQH/AgEAMB0G A1UdDgQWBBTZS/SzkBfiHQ5/Gy+b4g1XJOHkojANBgkqhkiG9w0BAQsFAAOCAQEA UwPhAbzTWZIlBsMHAL+8jvxM8qxc6HDhayAD4gbCE65vHYgSizost02vRfpOPQq1 D6HM8JjifS3KHd6E6chdTbrHI0W8pMJJPon5akCJf/uGeGDl+2wKfmVC6UoV7hC3 pcUzm3JKwsNJbjS5rxL8f5a8bNdIFfLQKuyRpnVX2CsNHvh+WJzynQ+PUJ6zCa7y x5AJxca2PTnBKRoVTAyumT1suluI9f4GRYnxTE/qIKRZRs+uT3kIl/N9VX+GbjGb pPszJ+p6N6Arl1BqJP1DdLin2IFGZL39pTyifm5GP+Vou2aHPuHZDVoCdxsFKup+ gUY2KeKz0UManwubPQNnKA== -----END CERTIFICATE----- kind: ConfigMap metadata: annotations: openshift.io/owning-component: End User creationTimestamp: \"2025-03-03T08:56:26Z\" name: user-ca-bundle namespace: openshift-config resourceVersion: \"43318895\" uid: 6624e5ad-93b5-418e-8c1e-7a91c724a760 [root@ncputility ~ pancwl_rc]$ oc get proxy cluster -o yaml apiVersion: config.openshift.io/v1 kind: Proxy metadata: creationTimestamp: \"2025-03-03T08:54:59Z\" generation: 2 name: cluster resourceVersion: \"48312416\" uid: ceaebe02-9bd3-4361-847e-1b880ebb85de spec: trustedCA: name: user-ca-bundle <---------------------------- this should be patched. if missing. status: {} [root@ncputility ~ pancwl_rc]$ login to workload cluster add tls of hub to cwl. [root@ncputility ~ pancwl_rc]$ oc patch proxy cluster --patch '{\"spec\":{\"trustedCA\":{\"name\":\"user-ca-bundle\"}}}' --type=merge [root@ncputility ~ pancwl_rc]$ oc get proxy cluster -o yaml apiVersion: config.openshift.io/v1 kind: Proxy metadata: creationTimestamp: \"2025-03-03T08:54:59Z\" generation: 2 name: cluster resourceVersion: \"48312416\" uid: ceaebe02-9bd3-4361-847e-1b880ebb85de spec: trustedCA: name: user-ca-bundle status: {} [root@ncputility ~ pancwl_rc]$ Configure the proxy cache on the registry level. hub quay configiration Open up the hub quay url CWL quay configuration Open CWL quay login via super account and created an user called cnfowners also fix the passwd. Create an org as same as hub quay. Set the proxy cache configuration for the organization create robot account and default permission Create a new team for image pull user Set default permission for the pull user (optional) \u2192 Default Permissions \u2192 + Create Default Permission Extend the global image pull secret During the Managed cluster installation, the global pull secret is configured. If the 2nd Hub Quay account and the cache account are not prepared in advance, these accounts need to be added. In case of mirrored registries, only the global pull secret can be used. It is not possible to add project specific pull secrets. For more information, see chapter Image configuration resources in document Images, available in OpenShift Container Platform Product documentation. Testing pod creation using proxy-cache quay. Login the namespace with cluster admin access to grand rights for an scc. [root@ncputility ~ pancwl_rc]$ source /root/pancwlrc WARNING: Using insecure TLS client config. Setting this option is not supported! Login successful. You have access to 116 projects, the list has been suppressed. You can list all projects with 'oc projects' Using project \"default\". [root@ncputility ~ pancwl_rc]$ Grand admin rights to project. if missed during project creation phase. [root@ncputility ~ pancwl_rc]$ oc policy add-role-to-user admin nokia -n nokia clusterrole.rbac.authorization.k8s.io/admin added: \"nokia\" [root@ncputility ~ pancwl_rc]$ Also grand scc role to default service account via anyuid. [root@ncputility ~ pancwl_rc]$ oc adm policy add-scc-to-user anyuid -z default -n nokia clusterrole.rbac.authorization.k8s.io/system:openshift:scc:anyuid added: \"default\" [root@ncputility ~ pancwl_rc]$ login to cnf tenant here . [root@ncputility ~ pancwl_rc]$ oc login -u nokia -p nokia@123 WARNING: Using insecure TLS client config. Setting this option is not supported! Login successful. You have one project on this server: \"nokia\" Using project \"nokia\". [root@ncputility ~ pancwl_rc]$ run an pod using proxy-cache url [root@ncputility ~ pancwl_rc]$ oc run podpingtest3 --image=ephemeral.url/cnfimages/testimage01:latest --restart=Never -- tail -f /dev/null pod/podpingtest3 created [root@ncputility ~ pancwl_rc]$ oc get pods NAME READY STATUS RESTARTS AGE podpingtest3 0/1 ContainerCreating 0 4s [root@ncputility ~ pancwl_rc]$ oc get pods NAME READY STATUS RESTARTS AGE podpingtest3 0/1 ContainerCreating 0 6s [root@ncputility ~ pancwl_rc]$ oc get pods NAME READY STATUS RESTARTS AGE podpingtest3 0/1 ContainerCreating 0 8s [root@ncputility ~ pancwl_rc]$ oc get pods NAME READY STATUS RESTARTS AGE podpingtest3 0/1 ContainerCreating 0 9s [root@ncputility ~ pancwl_rc]$ oc get pods NAME READY STATUS RESTARTS AGE podpingtest3 0/1 ContainerCreating 0 11s [root@ncputility ~ pancwl_rc]$ oc get pods NAME READY STATUS RESTARTS AGE podpingtest3 0/1 ContainerCreating 0 13s [root@ncputility ~ pancwl_rc]$ validate the pod status and make sure it's getting the image via proxy-cache. [root@ncputility ~ pancwl_rc]$ oc get pods NAME READY STATUS RESTARTS AGE podpingtest3 1/1 Running 0 5m1s [root@ncputility ~ pancwl_rc]$ [root@ncputility ~ pancwl_rc]$ oc describe pod podpingtest3 Name: podpingtest3 Namespace: nokia Priority: 0 Service Account: default Node: appworker9.panclypcwl01.mnc020.mcc714/10.89.96.35 Start Time: Fri, 04 Apr 2025 08:04:47 -0500 Labels: run=podpingtest3 Annotations: k8s.ovn.org/pod-networks: {\"default\":{\"ip_addresses\":[\"172.19.21.252/23\"],\"mac_address\":\"0a:58:ac:13:15:fc\",\"gateway_ips\":[\"172.19.20.1\"],\"routes\":[{\"dest\":\"172.16.... k8s.v1.cni.cncf.io/network-status: [{ \"name\": \"ovn-kubernetes\", \"interface\": \"eth0\", \"ips\": [ \"172.19.21.252\" ], \"mac\": \"0a:58:ac:13:15:fc\", \"default\": true, \"dns\": {} }] openshift.io/scc: anyuid Status: Running IP: 172.19.21.252 IPs: IP: 172.19.21.252 Containers: podpingtest3: Container ID: cri-o://749055ef608c6f30be42248c63889fd85377928389dae2e29eed50919cc2ee79 Image: ephemeral.url/cnfimages/testimage01:latest Image ID: ephemeral.url/cnfimages/testimage01@sha256:32666e0234f88377a91de56bb78f2d4f8df45b4f99c1c2dc9ee1d134c84f4753 Port: <none> Host Port: <none> Args: tail -f /dev/null State: Running Started: Fri, 04 Apr 2025 08:05:05 -0500 Ready: True Restart Count: 0 Environment: <none> Mounts: /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-mp46w (ro) Conditions: Type Status PodReadyToStartContainers True Initialized True Ready True ContainersReady True PodScheduled True Volumes: kube-api-access-mp46w: Type: Projected (a volume that contains injected data from multiple sources) TokenExpirationSeconds: 3607 ConfigMapName: kube-root-ca.crt ConfigMapOptional: <nil> DownwardAPI: true ConfigMapName: openshift-service-ca.crt ConfigMapOptional: <nil> QoS Class: BestEffort Node-Selectors: <none> Tolerations: node.kubernetes.io/not-ready:NoExecute op=Exists for 60s node.kubernetes.io/unreachable:NoExecute op=Exists for 60s Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal Scheduled 6m5s default-scheduler Successfully assigned nokia/podpingtest3 to appworker9.panclypcwl01.mnc020.mcc714 Normal AddedInterface 6m4s multus Add eth0 [172.19.21.252/23] from ovn-kubernetes Normal Pulling 6m4s kubelet Pulling image \"ephemeral.url/cnfimages/testimage01:latest\" Normal Pulled 5m47s kubelet Successfully pulled image \"ephemeral.url/cnfimages/testimage01:latest\" in 17.521s (17.521s including waiting) Normal Created 5m47s kubelet Created container podpingtest3 Normal Started 5m47s kubelet Started container podpingtest3 [root@ncputility ~ pancwl_rc]$ CNF image upload using pod command login to hub quay using cnfowners and org as cnfimages. [root@ncputility ~ pancwl_rc]$ podman login quay-registry.apps.panclyphub01.mnc020.mcc714 -u cnfowners -p cnfowners Login Succeeded! [root@ncputility ~ pancwl_rc]$ load the container images to log registry [root@ncputility ~ pancwl_rc]$ podman load -i <filename>.tar^C [root@ncputility ~ pancwl_rc]$ tag the image to your registry here [root@ncputility ~ pancwl_rc]$ podman tag quay-registry.apps.panclyphub01.mnc020.mcc714/cnfimages/testimage01 quay-registry.apps.panclyphub01.mnc020.mcc714/cnfimagesnew/testimage01:latest [root@ncputility ~ pancwl_rc]$ push the image to remore registry using podman push command here . [root@ncputility ~ pancwl_rc]$ podman push quay-registry.apps.panclyphub01.mnc020.mcc714/cnfimagesnew/testimage01:latest Getting image source signatures Copying blob 1af69dabfc93 done | Copying blob 53f86715cdba done | Copying blob b6361360b38a done | Copying config d39b33df22 done | Writing manifest to image destination [root@ncputility ~ pancwl_rc]$","title":"Proxy Cache Pod"},{"location":"openshift/CNF-onboarding-support/proxy-cache-pod/#proxy-cache-quay-based-pod-creation","text":"","title":"Proxy-cache quay based pod creation"},{"location":"openshift/CNF-onboarding-support/proxy-cache-pod/#proxy-cache-tls-issue","text":"Login in to CWL cluster and make sure mcp and nodes are looks good at this point, if any issue, need to be resolved first. [root@ncputility ~ pancwl_rc]$ oc get nodes NAME STATUS ROLES AGE VERSION appworker0.panclypcwl01.mnc020.mcc714 Ready appworker,appworker-mcp-a,worker 38d v1.29.6+aba1e8d appworker1.panclypcwl01.mnc020.mcc714 Ready appworker,appworker-mcp-a,worker 38d v1.29.6+aba1e8d appworker10.panclypcwl01.mnc020.mcc714 Ready appworker,appworker-mcp-b,worker 37d v1.29.6+aba1e8d appworker11.panclypcwl01.mnc020.mcc714 Ready appworker,appworker-mcp-b,worker 37d v1.29.6+aba1e8d appworker12.panclypcwl01.mnc020.mcc714 Ready appworker,appworker-mcp-b,worker 37d v1.29.6+aba1e8d appworker13.panclypcwl01.mnc020.mcc714 Ready appworker,appworker-mcp-b,worker 37d v1.29.6+aba1e8d appworker14.panclypcwl01.mnc020.mcc714 Ready appworker,appworker-mcp-b,worker 37d v1.29.6+aba1e8d appworker15.panclypcwl01.mnc020.mcc714 Ready appworker,appworker-mcp-b,worker 37d v1.29.6+aba1e8d appworker16.panclypcwl01.mnc020.mcc714 Ready appworker,appworker-mcp-b,worker 37d v1.29.6+aba1e8d appworker17.panclypcwl01.mnc020.mcc714 Ready appworker,appworker-mcp-b,worker 37d v1.29.6+aba1e8d appworker19.panclypcwl01.mnc020.mcc714 Ready appworker,appworker-mcp-c,worker 37d v1.29.6+aba1e8d appworker2.panclypcwl01.mnc020.mcc714 Ready appworker,appworker-mcp-a,worker 30d v1.29.6+aba1e8d appworker20.panclypcwl01.mnc020.mcc714 Ready appworker,appworker-mcp-c,worker 37d v1.29.6+aba1e8d appworker21.panclypcwl01.mnc020.mcc714 Ready appworker,appworker-mcp-c,worker 37d v1.29.6+aba1e8d appworker22.panclypcwl01.mnc020.mcc714 Ready appworker,appworker-mcp-c,worker 37d v1.29.6+aba1e8d appworker23.panclypcwl01.mnc020.mcc714 Ready appworker,appworker-mcp-c,worker 37d v1.29.6+aba1e8d appworker24.panclypcwl01.mnc020.mcc714 Ready appworker,appworker-mcp-c,worker 37d v1.29.6+aba1e8d appworker25.panclypcwl01.mnc020.mcc714 Ready appworker,appworker-mcp-c,worker 37d v1.29.6+aba1e8d appworker26.panclypcwl01.mnc020.mcc714 Ready appworker,appworker-mcp-c,worker 37d v1.29.6+aba1e8d appworker27.panclypcwl01.mnc020.mcc714 Ready appworker,appworker-mcp-d,worker 37d v1.29.6+aba1e8d appworker28.panclypcwl01.mnc020.mcc714 Ready appworker,appworker-mcp-d,worker 37d v1.29.6+aba1e8d appworker29.panclypcwl01.mnc020.mcc714 Ready appworker,appworker-mcp-d,worker 37d v1.29.6+aba1e8d appworker3.panclypcwl01.mnc020.mcc714 Ready appworker,appworker-mcp-a,worker 38d v1.29.6+aba1e8d appworker30.panclypcwl01.mnc020.mcc714 Ready appworker,appworker-mcp-d,worker 37d v1.29.6+aba1e8d appworker31.panclypcwl01.mnc020.mcc714 Ready appworker,appworker-mcp-d,worker 37d v1.29.6+aba1e8d appworker32.panclypcwl01.mnc020.mcc714 Ready appworker,appworker-mcp-d,worker 37d v1.29.6+aba1e8d appworker33.panclypcwl01.mnc020.mcc714 Ready appworker,appworker-mcp-d,worker 37d v1.29.6+aba1e8d appworker34.panclypcwl01.mnc020.mcc714 Ready appworker,appworker-mcp-d,worker 37d v1.29.6+aba1e8d appworker4.panclypcwl01.mnc020.mcc714 Ready appworker,appworker-mcp-a,worker 38d v1.29.6+aba1e8d appworker5.panclypcwl01.mnc020.mcc714 Ready appworker,appworker-mcp-a,worker 38d v1.29.6+aba1e8d appworker6.panclypcwl01.mnc020.mcc714 Ready appworker,appworker-mcp-a,worker 38d v1.29.6+aba1e8d appworker7.panclypcwl01.mnc020.mcc714 Ready appworker,appworker-mcp-a,worker 37d v1.29.6+aba1e8d appworker9.panclypcwl01.mnc020.mcc714 Ready appworker,appworker-mcp-b,worker 37d v1.29.6+aba1e8d gateway1.panclypcwl01.mnc020.mcc714 Ready gateway,gateway-mcp-a,worker 38d v1.29.6+aba1e8d gateway2.panclypcwl01.mnc020.mcc714 Ready gateway,gateway-mcp-a,worker 38d v1.29.6+aba1e8d gateway3.panclypcwl01.mnc020.mcc714 Ready gateway,gateway-mcp-b,worker 38d v1.29.6+aba1e8d gateway4.panclypcwl01.mnc020.mcc714 Ready gateway,gateway-mcp-b,worker 38d v1.29.6+aba1e8d master0.panclypcwl01.mnc020.mcc714 Ready control-plane,master,monitor 38d v1.29.6+aba1e8d master1.panclypcwl01.mnc020.mcc714 Ready control-plane,master,monitor 38d v1.29.6+aba1e8d master2.panclypcwl01.mnc020.mcc714 Ready control-plane,master,monitor 38d v1.29.6+aba1e8d storage0.panclypcwl01.mnc020.mcc714 Ready storage,worker 38d v1.29.6+aba1e8d storage1.panclypcwl01.mnc020.mcc714 Ready storage,worker 38d v1.29.6+aba1e8d storage2.panclypcwl01.mnc020.mcc714 Ready storage,worker 38d v1.29.6+aba1e8d storage3.panclypcwl01.mnc020.mcc714 Ready storage,worker 38d v1.29.6+aba1e8d storage4.panclypcwl01.mnc020.mcc714 Ready storage,worker 38d v1.29.6+aba1e8d [root@ncputility ~ pancwl_rc]$ oc get mcp NAME CONFIG UPDATED UPDATING DEGRADED MACHINECOUNT READYMACHINECOUNT UPDATEDMACHINECOUNT DEGRADEDMACHINECOUNT AGE appworker-mcp-a rendered-appworker-mcp-a-0e9dd6df593dcd5016bbe7d601119bf4 True False False 8 8 8 0 37d appworker-mcp-b rendered-appworker-mcp-b-0e9dd6df593dcd5016bbe7d601119bf4 True False False 9 9 9 0 37d appworker-mcp-c rendered-appworker-mcp-c-5afb864664d3b10530b54b3153a1a61e True False False 8 8 8 0 29h appworker-mcp-d rendered-appworker-mcp-d-5afb864664d3b10530b54b3153a1a61e True False False 8 8 8 0 29h gateway-mcp-a rendered-gateway-mcp-a-c81254a16575de9053ae543c4f1ba3fb True False False 2 2 2 0 37d gateway-mcp-b rendered-gateway-mcp-b-3be41ecbbe09004c35ca04a4309cabf0 True False False 2 2 2 0 29h master rendered-master-114f60e6be691323222ea11e72de0bcf True False False 3 3 3 0 38d storage rendered-storage-dc2d8a34080bce1400e11bb1fb098693 True False False 5 5 5 0 37d worker rendered-worker-68cb1df39185f7ad80fda7915e4c5a42 True False False 0 0 0 0 38d [root@ncputility ~ pancwl_rc]$ login to hub cluster and run this command [root@ncputility ~ pancwl_rc]$ source /root/panhubrc WARNING: Using insecure TLS client config. Setting this option is not supported! Login successful. You have access to 105 projects, the list has been suppressed. You can list all projects with 'oc projects' Using project \"default\". [root@ncputility ~ panhub_rc]$ oc get secret -n openshift-ingress-operator router-ca -o \"jsonpath={.data['tls\\.crt']}\" | base64 -d -----BEGIN CERTIFICATE----- MIIDDDCCAfSgAwIBAgIBATANBgkqhkiG9w0BAQsFADAmMSQwIgYDVQQDDBtpbmdy ZXNzLW9wZXJhdG9yQDE3NDAwNzQ1NDQwHhcNMjUwMjIwMTgwMjIzWhcNMjcwMjIw MTgwMjI0WjAmMSQwIgYDVQQDDBtpbmdyZXNzLW9wZXJhdG9yQDE3NDAwNzQ1NDQw ggEiMA0GCSqGSIb3DQEBAQUAA4IBDwAwggEKAoIBAQDjXWFSVPshwihivZhaTrB7 0boUOw2j3Ut/J6eSm+JA+xVl05L4XD8+C8VSst+f32Pe42Lso2hrovY1dT7IBX3g 6S9Nnd2iRc9vC/qHcQkGQ6krIYSQ48aCH7UuahCpTqxEp+MwmhCTQngN2maTJBe3 0E6K2IL4zXSi0Iuj08BOnH/w4pJxeWhyngXDf2SeA88EmU3juHLshHrAND84uou2 /sOStIAhFwU+o887dSgx7iERy+6nczSDB9Qvq11cUSS4RPC82bNnxcc+BrynDwZ/ eggs0OEaj/1cHu/svKZHX9gUKrqz80wF8YGZLLgI2oPAf2VJorvtkJAErzgttroF AgMBAAGjRTBDMA4GA1UdDwEB/wQEAwICpDASBgNVHRMBAf8ECDAGAQH/AgEAMB0G A1UdDgQWBBTZS/SzkBfiHQ5/Gy+b4g1XJOHkojANBgkqhkiG9w0BAQsFAAOCAQEA UwPhAbzTWZIlBsMHAL+8jvxM8qxc6HDhayAD4gbCE65vHYgSizost02vRfpOPQq1 D6HM8JjifS3KHd6E6chdTbrHI0W8pMJJPon5akCJf/uGeGDl+2wKfmVC6UoV7hC3 pcUzm3JKwsNJbjS5rxL8f5a8bNdIFfLQKuyRpnVX2CsNHvh+WJzynQ+PUJ6zCa7y x5AJxca2PTnBKRoVTAyumT1suluI9f4GRYnxTE/qIKRZRs+uT3kIl/N9VX+GbjGb pPszJ+p6N6Arl1BqJP1DdLin2IFGZL39pTyifm5GP+Vou2aHPuHZDVoCdxsFKup+ gUY2KeKz0UManwubPQNnKA== -----END CERTIFICATE----- [root@ncputility ~ panhub_rc]$ login to workload cluster and run this command [root@ncputility ~ pancwl_rc]$ source /root/pancwlrc WARNING: Using insecure TLS client config. Setting this option is not supported! Login successful. You have access to 115 projects, the list has been suppressed. You can list all projects with 'oc projects' Using project \"nokia\". [root@ncputility ~ pancwl_rc]$ oc get cm -n openshift-config user-ca-bundle -o yaml apiVersion: v1 data: ca-bundle.crt: | -----BEGIN CERTIFICATE----- MIIDDDCCAfSgAwIBAgIBATANBgkqhkiG9w0BAQsFADAmMSQwIgYDVQQDDBtpbmdy ZXNzLW9wZXJhdG9yQDE3NDAwNzQ1NDQwHhcNMjUwMjIwMTgwMjIzWhcNMjcwMjIw MTgwMjI0WjAmMSQwIgYDVQQDDBtpbmdyZXNzLW9wZXJhdG9yQDE3NDAwNzQ1NDQw ggEiMA0GCSqGSIb3DQEBAQUAA4IBDwAwggEKAoIBAQDjXWFSVPshwihivZhaTrB7 0boUOw2j3Ut/J6eSm+JA+xVl05L4XD8+C8VSst+f32Pe42Lso2hrovY1dT7IBX3g 6S9Nnd2iRc9vC/qHcQkGQ6krIYSQ48aCH7UuahCpTqxEp+MwmhCTQngN2maTJBe3 0E6K2IL4zXSi0Iuj08BOnH/w4pJxeWhyngXDf2SeA88EmU3juHLshHrAND84uou2 /sOStIAhFwU+o887dSgx7iERy+6nczSDB9Qvq11cUSS4RPC82bNnxcc+BrynDwZ/ eggs0OEaj/1cHu/svKZHX9gUKrqz80wF8YGZLLgI2oPAf2VJorvtkJAErzgttroF AgMBAAGjRTBDMA4GA1UdDwEB/wQEAwICpDASBgNVHRMBAf8ECDAGAQH/AgEAMB0G A1UdDgQWBBTZS/SzkBfiHQ5/Gy+b4g1XJOHkojANBgkqhkiG9w0BAQsFAAOCAQEA UwPhAbzTWZIlBsMHAL+8jvxM8qxc6HDhayAD4gbCE65vHYgSizost02vRfpOPQq1 D6HM8JjifS3KHd6E6chdTbrHI0W8pMJJPon5akCJf/uGeGDl+2wKfmVC6UoV7hC3 pcUzm3JKwsNJbjS5rxL8f5a8bNdIFfLQKuyRpnVX2CsNHvh+WJzynQ+PUJ6zCa7y x5AJxca2PTnBKRoVTAyumT1suluI9f4GRYnxTE/qIKRZRs+uT3kIl/N9VX+GbjGb pPszJ+p6N6Arl1BqJP1DdLin2IFGZL39pTyifm5GP+Vou2aHPuHZDVoCdxsFKup+ gUY2KeKz0UManwubPQNnKA== -----END CERTIFICATE----- kind: ConfigMap metadata: annotations: openshift.io/owning-component: End User creationTimestamp: \"2025-03-03T08:56:26Z\" name: user-ca-bundle namespace: openshift-config resourceVersion: \"43318895\" uid: 6624e5ad-93b5-418e-8c1e-7a91c724a760 [root@ncputility ~ pancwl_rc]$ oc get proxy cluster -o yaml apiVersion: config.openshift.io/v1 kind: Proxy metadata: creationTimestamp: \"2025-03-03T08:54:59Z\" generation: 2 name: cluster resourceVersion: \"48312416\" uid: ceaebe02-9bd3-4361-847e-1b880ebb85de spec: trustedCA: name: user-ca-bundle <---------------------------- this should be patched. if missing. status: {} [root@ncputility ~ pancwl_rc]$ login to workload cluster add tls of hub to cwl. [root@ncputility ~ pancwl_rc]$ oc patch proxy cluster --patch '{\"spec\":{\"trustedCA\":{\"name\":\"user-ca-bundle\"}}}' --type=merge [root@ncputility ~ pancwl_rc]$ oc get proxy cluster -o yaml apiVersion: config.openshift.io/v1 kind: Proxy metadata: creationTimestamp: \"2025-03-03T08:54:59Z\" generation: 2 name: cluster resourceVersion: \"48312416\" uid: ceaebe02-9bd3-4361-847e-1b880ebb85de spec: trustedCA: name: user-ca-bundle status: {} [root@ncputility ~ pancwl_rc]$","title":"proxy-cache TLS issue."},{"location":"openshift/CNF-onboarding-support/proxy-cache-pod/#configure-the-proxy-cache-on-the-registry-level","text":"","title":"Configure the proxy cache on the registry level."},{"location":"openshift/CNF-onboarding-support/proxy-cache-pod/#hub-quay-configiration","text":"Open up the hub quay url","title":"hub quay configiration"},{"location":"openshift/CNF-onboarding-support/proxy-cache-pod/#cwl-quay-configuration","text":"Open CWL quay login via super account and created an user called cnfowners also fix the passwd. Create an org as same as hub quay. Set the proxy cache configuration for the organization create robot account and default permission Create a new team for image pull user Set default permission for the pull user (optional) \u2192 Default Permissions \u2192 + Create Default Permission Extend the global image pull secret During the Managed cluster installation, the global pull secret is configured. If the 2nd Hub Quay account and the cache account are not prepared in advance, these accounts need to be added. In case of mirrored registries, only the global pull secret can be used. It is not possible to add project specific pull secrets. For more information, see chapter Image configuration resources in document Images, available in OpenShift Container Platform Product documentation.","title":"CWL quay configuration"},{"location":"openshift/CNF-onboarding-support/proxy-cache-pod/#testing-pod-creation-using-proxy-cache-quay","text":"Login the namespace with cluster admin access to grand rights for an scc. [root@ncputility ~ pancwl_rc]$ source /root/pancwlrc WARNING: Using insecure TLS client config. Setting this option is not supported! Login successful. You have access to 116 projects, the list has been suppressed. You can list all projects with 'oc projects' Using project \"default\". [root@ncputility ~ pancwl_rc]$ Grand admin rights to project. if missed during project creation phase. [root@ncputility ~ pancwl_rc]$ oc policy add-role-to-user admin nokia -n nokia clusterrole.rbac.authorization.k8s.io/admin added: \"nokia\" [root@ncputility ~ pancwl_rc]$ Also grand scc role to default service account via anyuid. [root@ncputility ~ pancwl_rc]$ oc adm policy add-scc-to-user anyuid -z default -n nokia clusterrole.rbac.authorization.k8s.io/system:openshift:scc:anyuid added: \"default\" [root@ncputility ~ pancwl_rc]$ login to cnf tenant here . [root@ncputility ~ pancwl_rc]$ oc login -u nokia -p nokia@123 WARNING: Using insecure TLS client config. Setting this option is not supported! Login successful. You have one project on this server: \"nokia\" Using project \"nokia\". [root@ncputility ~ pancwl_rc]$ run an pod using proxy-cache url [root@ncputility ~ pancwl_rc]$ oc run podpingtest3 --image=ephemeral.url/cnfimages/testimage01:latest --restart=Never -- tail -f /dev/null pod/podpingtest3 created [root@ncputility ~ pancwl_rc]$ oc get pods NAME READY STATUS RESTARTS AGE podpingtest3 0/1 ContainerCreating 0 4s [root@ncputility ~ pancwl_rc]$ oc get pods NAME READY STATUS RESTARTS AGE podpingtest3 0/1 ContainerCreating 0 6s [root@ncputility ~ pancwl_rc]$ oc get pods NAME READY STATUS RESTARTS AGE podpingtest3 0/1 ContainerCreating 0 8s [root@ncputility ~ pancwl_rc]$ oc get pods NAME READY STATUS RESTARTS AGE podpingtest3 0/1 ContainerCreating 0 9s [root@ncputility ~ pancwl_rc]$ oc get pods NAME READY STATUS RESTARTS AGE podpingtest3 0/1 ContainerCreating 0 11s [root@ncputility ~ pancwl_rc]$ oc get pods NAME READY STATUS RESTARTS AGE podpingtest3 0/1 ContainerCreating 0 13s [root@ncputility ~ pancwl_rc]$ validate the pod status and make sure it's getting the image via proxy-cache. [root@ncputility ~ pancwl_rc]$ oc get pods NAME READY STATUS RESTARTS AGE podpingtest3 1/1 Running 0 5m1s [root@ncputility ~ pancwl_rc]$ [root@ncputility ~ pancwl_rc]$ oc describe pod podpingtest3 Name: podpingtest3 Namespace: nokia Priority: 0 Service Account: default Node: appworker9.panclypcwl01.mnc020.mcc714/10.89.96.35 Start Time: Fri, 04 Apr 2025 08:04:47 -0500 Labels: run=podpingtest3 Annotations: k8s.ovn.org/pod-networks: {\"default\":{\"ip_addresses\":[\"172.19.21.252/23\"],\"mac_address\":\"0a:58:ac:13:15:fc\",\"gateway_ips\":[\"172.19.20.1\"],\"routes\":[{\"dest\":\"172.16.... k8s.v1.cni.cncf.io/network-status: [{ \"name\": \"ovn-kubernetes\", \"interface\": \"eth0\", \"ips\": [ \"172.19.21.252\" ], \"mac\": \"0a:58:ac:13:15:fc\", \"default\": true, \"dns\": {} }] openshift.io/scc: anyuid Status: Running IP: 172.19.21.252 IPs: IP: 172.19.21.252 Containers: podpingtest3: Container ID: cri-o://749055ef608c6f30be42248c63889fd85377928389dae2e29eed50919cc2ee79 Image: ephemeral.url/cnfimages/testimage01:latest Image ID: ephemeral.url/cnfimages/testimage01@sha256:32666e0234f88377a91de56bb78f2d4f8df45b4f99c1c2dc9ee1d134c84f4753 Port: <none> Host Port: <none> Args: tail -f /dev/null State: Running Started: Fri, 04 Apr 2025 08:05:05 -0500 Ready: True Restart Count: 0 Environment: <none> Mounts: /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-mp46w (ro) Conditions: Type Status PodReadyToStartContainers True Initialized True Ready True ContainersReady True PodScheduled True Volumes: kube-api-access-mp46w: Type: Projected (a volume that contains injected data from multiple sources) TokenExpirationSeconds: 3607 ConfigMapName: kube-root-ca.crt ConfigMapOptional: <nil> DownwardAPI: true ConfigMapName: openshift-service-ca.crt ConfigMapOptional: <nil> QoS Class: BestEffort Node-Selectors: <none> Tolerations: node.kubernetes.io/not-ready:NoExecute op=Exists for 60s node.kubernetes.io/unreachable:NoExecute op=Exists for 60s Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal Scheduled 6m5s default-scheduler Successfully assigned nokia/podpingtest3 to appworker9.panclypcwl01.mnc020.mcc714 Normal AddedInterface 6m4s multus Add eth0 [172.19.21.252/23] from ovn-kubernetes Normal Pulling 6m4s kubelet Pulling image \"ephemeral.url/cnfimages/testimage01:latest\" Normal Pulled 5m47s kubelet Successfully pulled image \"ephemeral.url/cnfimages/testimage01:latest\" in 17.521s (17.521s including waiting) Normal Created 5m47s kubelet Created container podpingtest3 Normal Started 5m47s kubelet Started container podpingtest3 [root@ncputility ~ pancwl_rc]$","title":"Testing pod creation using proxy-cache quay."},{"location":"openshift/CNF-onboarding-support/proxy-cache-pod/#cnf-image-upload-using-pod-command","text":"login to hub quay using cnfowners and org as cnfimages. [root@ncputility ~ pancwl_rc]$ podman login quay-registry.apps.panclyphub01.mnc020.mcc714 -u cnfowners -p cnfowners Login Succeeded! [root@ncputility ~ pancwl_rc]$ load the container images to log registry [root@ncputility ~ pancwl_rc]$ podman load -i <filename>.tar^C [root@ncputility ~ pancwl_rc]$ tag the image to your registry here [root@ncputility ~ pancwl_rc]$ podman tag quay-registry.apps.panclyphub01.mnc020.mcc714/cnfimages/testimage01 quay-registry.apps.panclyphub01.mnc020.mcc714/cnfimagesnew/testimage01:latest [root@ncputility ~ pancwl_rc]$ push the image to remore registry using podman push command here . [root@ncputility ~ pancwl_rc]$ podman push quay-registry.apps.panclyphub01.mnc020.mcc714/cnfimagesnew/testimage01:latest Getting image source signatures Copying blob 1af69dabfc93 done | Copying blob 53f86715cdba done | Copying blob b6361360b38a done | Copying config d39b33df22 done | Writing manifest to image destination [root@ncputility ~ pancwl_rc]$","title":"CNF image upload using pod command"},{"location":"openshift/backup-restore/ACM-GEO-RED/","text":"","title":"ACM GEO Red"},{"location":"openshift/backup-restore/ACM-localbackup/","text":"both cluster active hub: [root@ncputility ~ panhub_rc]$ source /root/panhubrc WARNING: Using insecure TLS client config. Setting this option is not supported! Login successful. You have access to 108 projects, the list has been suppressed. You can list all projects with 'oc projects' Using project \"default\". [root@ncputility ~ panhub_rc]$ oc edit multiclusterhubs.operator.open-cluster-management.io -n open-cluster-management multiclusterhub multiclusterhub.operator.open-cluster-management.io/multiclusterhub edited [root@ncputility ~ panhub_rc]$ oc edit multiclusterhubs.operator.open-cluster-management.io -n open-cluster-management multiclusterhub Edit cancelled, no changes made. [root@ncputility ~ panhub_rc]$ Active hub: [root@dom14npv101-infra-manager ~ hubrc]# cd /root/raj/amc-backup [root@dom14npv101-infra-manager ~ hubrc]# ll total 24 drwxr-xr-x. 7 root root 4096 May 1 17:46 ../ -rw-r--r--. 1 root root 257 May 1 17:47 bucketclass-noobaa-default-backing-store.yaml -rw-r--r--. 1 root root 290 May 1 17:48 ObjectBucketClaim.yaml -rw-r--r--. 1 root root 116 May 1 17:50 secret.txt -rw-r--r--. 1 root root 293 May 1 17:52 secret.yaml -rw-r--r--. 1 root root 2329 May 1 17:58 DataProtectionApplication.yaml drwxr-xr-x. 2 root root 164 May 1 17:58 ./ [root@dom14npv101-infra-manager ~ hubrc]# oc apply -f secret.yaml secret/cloud-credentials created [root@dom14npv101-infra-manager ~ hubrc]# oc apply -f DataProtectionApplication.yaml dataprotectionapplication.oadp.openshift.io/velero-acm-backup created [root@dom14npv101-infra-manager ~ hubrc]# oc get backupstoragelocations.velero.io No resources found in ncd-cbur namespace. [root@dom14npv101-infra-manager ~ hubrc]# oc get backupstoragelocations.velero.io -n open-cluster-management-backup NAME PHASE LAST VALIDATED AGE DEFAULT velero-acm-backup-1 Available 22s 33s true [root@dom14npv101-infra-manager ~ hubrc]# oc get appprojects.argoproj.io -n openshift-gitops | grep -v default NAME AGE site-config-project 59d site-policy-project 59d [root@dom14npv101-infra-manager ~ hubrc]# oc get appprojects.argoproj.io -n openshift-gitops -l velero.io/exclude-from-backup NAME AGE site-config-project 59d site-policy-project 59d [root@dom14npv101-infra-manager ~ hubrc]# oc get applications.argoproj.io -n openshift-gitops -l velero.io/exclude-from-backup NAME SYNC STATUS HEALTH STATUS ncpvnpvlab1-site-configs Synced Healthy ncpvnpvlab1-site-policies Synced Healthy ncpvnpvmgt-site-configs Synced Healthy ncpvnpvmgt-site-policies Synced Healthy [root@dom14npv101-infra-manager ~ hubrc]# cat > backup.yaml apiVersion: cluster.open-cluster-management.io/v1beta1 kind: BackupSchedule metadata: name: schedule-acm namespace: open-cluster-management-backup spec: veleroSchedule: 0 22 * * * veleroTtl: 120h ^C [root@dom14npv101-infra-manager ~ hubrc]# date Thu May 1 06:04:49 PM UTC 2025 [root@dom14npv101-infra-manager ~ hubrc]# vi backup.yaml [root@dom14npv101-infra-manager ~ hubrc]# oc apply -f backup.yaml backupschedule.cluster.open-cluster-management.io/schedule-acm created [root@dom14npv101-infra-manager ~ hubrc]# oc get backupstoragelocations.velero.io -A NAMESPACE NAME PHASE LAST VALIDATED AGE DEFAULT open-cluster-management-backup velero-acm-backup-1 Available 4s 5m15s true [root@dom14npv101-infra-manager ~ hubrc]# oc describe backupstoragelocations.velero.io velero-acm-backup-1 -n open-cluster-management-backup Name: velero-acm-backup-1 Namespace: open-cluster-management-backup Labels: app.kubernetes.io/component=bsl app.kubernetes.io/instance=velero-acm-backup-1 app.kubernetes.io/managed-by=oadp-operator app.kubernetes.io/name=oadp-operator-velero openshift.io/oadp=True openshift.io/oadp-registry=True Annotations: API Version: velero.io/v1 Kind: BackupStorageLocation Metadata: Creation Timestamp: 2025-05-01T18:02:31Z Generation: 13 Owner References: API Version: oadp.openshift.io/v1alpha1 Block Owner Deletion: true Controller: true Kind: DataProtectionApplication Name: velero-acm-backup UID: 89734e12-0144-4376-954d-cb13cde17515 Resource Version: 142419914 UID: 52b6b341-5e36-4e02-bc98-15622be65673 Spec: Config: Checksum Algorithm: Profile: default Region: none s3ForcePathStyle: true s3Url: https://s3-openshift-storage.apps.ncpvnpvmgt.pnwlab.nsn-rdnet.net Credential: Key: cloud Name: cloud-credentials Default: true Object Storage: Bucket: acm-backups-bcf1990d-0846-4a6d-8518-67db81ebee63 Ca Cert: LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSURERENDQWZTZ0F3SUJBZ0lCQVRBTkJna3Foa2lHOXcwQkFRc0ZBREFtTVNRd0lnWURWUVFEREJ0cGJtZHkKWlhOekxXOXdaWEpoZEc5eVFERTNOREV4TXpJek16RXdIaGNOTWpVd016QTBNak0xTWpF d1doY05NamN3TXpBMApNak0xTWpFeFdqQW1NU1F3SWdZRFZRUUREQnRwYm1keVpYTnpMVzl3WlhKaGRHOXlRREUzTkRFeE16SXpNekV3CmdnRWlNQTBHQ1NxR1NJYjNEUUVCQVFVQUE0SUJEd0F3Z2dFS0FvSUJBUURLOUhrUkd4VWNPRnZjcHVZc1dLdWQKd2ltMFBWc3B3VU 1LNXgyejhBQWJ1eGhuOTRFWThWbk9tSkJUVkhqRnIvbmI0SjRzYldzN3JBbXZINWpjbTdzbApMYW1wWU1uUWlSamtrdE5FSVV3Rm9LVm9VU3ZRSWtiTHJlZk1hcjJYSENOOEU0dHVHeXA3U1c1YjBMRjc4cUY5CktIY0ttMXBDVi9NR3lyU1RtMkx0SmtBcnM5d0lsL0ZPYmox UEcvUmsvQThtRHZhalBmSUVGbU8yMHduWEQ5bWcKZDVIVk1ZVzkyWWRWVDZPR0FWMEZUNCtJNzNibEdyK3pqQUJzMklxTnUzQ3h0cHlucXMvVVV6RGFodGRvc2ZxSgpWeHhaTUFaZlRENTk0UUtzMFZhamR2aTU1Z1pPejVBQXRLSG96Rm85TWk5dklXblpwR3Z0T2xINnIyVE ZmSytmCkFnTUJBQUdqUlRCRE1BNEdBMVVkRHdFQi93UUVBd0lDcERBU0JnTlZIUk1CQWY4RUNEQUdBUUgvQWdFQU1CMEcKQTFVZERnUVdCQlRJNVRqeVp2aUJ1NE0wNzd6Mk9PY0lDRmkwMURBTkJna3Foa2lHOXcwQkFRc0ZBQU9DQVFFQQpuNG96eWZha0sySUFqb3dFSlZE bFNMMlp4YVJuWmJFcjVLanhJbjhiQ0tjaUdBK0h2UURuY0UzK1BzSTJDZGxpCjhXQlR4ZnJ3aFoxTVZ2YjVySmlsTXpZUklQamJaanUrbitaNlB0SGJYMEZDb2Q0elpaYkZBOFQvNlZXUkJSSmUKL3VMaXU3VVRENjJQRDgydVlNSmJFTDNTa1V6b1U5T2NXMSt1S1R3UG56NG VFblVvNzVnbVlUWnBybkhKSEttNAowUVljTlF6RndHR3JnQnNuTy8wRW94Z2Roa09keENlWFBqRUpURnZXRTdpRjhYWTlKQVZKMVpCcStuZUNoMmRCCkowRGRkaGZjMDhtUnpHbStkVXRyeCtZMnRoSXBxWVUreTN1WDZaM09TcDVNeFIvQzMvelRGR2pqK3pKUmNIOTIKUmtH c2V5bGNCYXpYbmVvWXgzdEVxUT09Ci0tLS0tRU5EIENFUlRJRklDQVRFLS0tLS0K Prefix: velero Provider: aws Status: Last Synced Time: 2025-05-01T18:07:42Z Last Validation Time: 2025-05-01T18:07:42Z Phase: Available Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal BackupStorageLocationReconciled 5m33s DPA-controller performed created on backupstoragelocation open-cluster-management-backup/velero-acm-backup-1 [root@dom14npv101-infra-manager ~ hubrc]# oc get pods -n open-cluster-management-backup NAME READY STATUS RESTARTS AGE cluster-backup-chart-clusterbackup-698656f7f-cqxxj 1/1 Running 0 24d cluster-backup-chart-clusterbackup-698656f7f-dvdz8 1/1 Running 0 24d node-agent-brq4q 1/1 Running 0 6m6s node-agent-j9xm8 1/1 Running 0 6m6s node-agent-mlrwk 1/1 Running 0 6m6s node-agent-nkb7v 1/1 Running 0 6m6s node-agent-sgqxw 1/1 Running 0 6m6s openshift-adp-controller-manager-74f799649f-v2slw 1/1 Running 0 24d velero-549fbfb95d-s966g 1/1 Running 0 6m6s [root@dom14npv101-infra-manager ~ hubrc]# oc -n open-cluster-management-backup exec -it velero-549fbfb95d-s966g -- ./velero backup-location get Defaulted container \"velero\" out of: velero, openshift-velero-plugin (init), velero-plugin-for-aws (init) NAME PROVIDER BUCKET/PREFIX PHASE LAST VALIDATED ACCESS MODE DEFAULT velero-acm-backup-1 aws acm-backups-bcf1990d-0846-4a6d-8518-67db81ebee63/velero Available 2025-05-01 18:08:42 +0000 UTC ReadWrite true [root@dom14npv101-infra-manager ~ hubrc]# oc -n open-cluster-management-backup exec -it velero-549fbfb95d-s966g -- ./velero backup get Defaulted container \"velero\" out of: velero, openshift-velero-plugin (init), velero-plugin-for-aws (init) NAME STATUS ERRORS WARNINGS CREATED EXPIRES STORAGE LOCATION SELECTOR acm-credentials-schedule-20250501180616 Completed 0 0 2025-05-01 18:06:17 +0000 UTC 4d velero-acm-backup-1 acm-managed-clusters-schedule-20250501180616 Completed 0 0 2025-05-01 18:06:18 +0000 UTC 4d velero-acm-backup-1 acm-resources-generic-schedule-20250501180616 Completed 0 0 2025-05-01 18:06:20 +0000 UTC 4d velero-acm-backup-1 cluster.open-cluster-management.io/backup acm-resources-schedule-20250501180616 Completed 0 0 2025-05-01 18:06:29 +0000 UTC 4d velero-acm-backup-1 !cluster.open-cluster-management.io/backup,!policy.open-cluste r-management.io/root-policy acm-validation-policy-schedule-20250501180616 Completed 0 0 2025-05-01 18:06:30 +0000 UTC 1d velero-acm-backup-1 [root@dom14npv101-infra-manager ~ hubrc]# date Thu May 1 06:09:39 PM UTC 2025 [root@dom14npv101-infra-manager ~ hubrc]#","title":"ACM Local Backup"},{"location":"openshift/backup-restore/AWS-S3/","text":"[root@dom14npv101-infra-manager ~ hubrc]# oc -n open-cluster-management-backup exec -it velero-549fbfb95d-s966g -- ./velero backup-location get Defaulted container \"velero\" out of: velero, openshift-velero-plugin (init), velero-plugin-for-aws (init) NAME PROVIDER BUCKET/PREFIX PHASE LAST VALIDATED ACCESS MODE DEFAULT velero-acm-backup-1 aws acm-backups-bcf1990d-0846-4a6d-8518-67db81ebee63/velero Available 2025-05-01 18:20:42 +0000 UTC ReadWrite true [root@dom14npv101-infra-manager ~ hubrc]# oc -n open-cluster-management-backup exec -it velero-549fbfb95d-s966g -- ./velero backup get Defaulted container \"velero\" out of: velero, openshift-velero-plugin (init), velero-plugin-for-aws (init) NAME STATUS ERRORS WARNINGS CREATED EXPIRES STORAGE LOCATION SELECTOR acm-credentials-schedule-20250501180616 Completed 0 0 2025-05-01 18:06:17 +0000 UTC 4d velero-acm-backup-1 acm-managed-clusters-schedule-20250501180616 Completed 0 0 2025-05-01 18:06:18 +0000 UTC 4d velero-acm-backup-1 acm-resources-generic-schedule-20250501180616 Completed 0 0 2025-05-01 18:06:20 +0000 UTC 4d velero-acm-backup-1 cluster.open-cluster-management.io/backup acm-resources-schedule-20250501180616 Completed 0 0 2025-05-01 18:06:29 +0000 UTC 4d velero-acm-backup-1 !cluster.open-cluster-management.io/backup,!policy.open-cluste r-management.io/root-policy acm-validation-policy-schedule-20250501180616 Completed 0 0 2025-05-01 18:06:30 +0000 UTC 23h velero-acm-backup-1 [root@dom14npv101-infra-manager ~ hubrc]# oc -n open-cluster-management-backup exec -it velero-549fbfb95d-s966g -- ./velero backup get Defaulted container \"velero\" out of: velero, openshift-velero-plugin (init), velero-plugin-for-aws (init) NAME STATUS ERRORS WARNINGS CREATED EXPIRES STORAGE LOCATION SELECTOR acm-credentials-schedule-20250501180616 Completed 0 0 2025-05-01 18:06:17 +0000 UTC 4d velero-acm-backup-1 acm-managed-clusters-schedule-20250501180616 Completed 0 0 2025-05-01 18:06:18 +0000 UTC 4d velero-acm-backup-1 acm-resources-generic-schedule-20250501180616 Completed 0 0 2025-05-01 18:06:20 +0000 UTC 4d velero-acm-backup-1 cluster.open-cluster-management.io/backup acm-resources-schedule-20250501180616 Completed 0 0 2025-05-01 18:06:29 +0000 UTC 4d velero-acm-backup-1 !cluster.open-cluster-management.io/backup,!policy.open-cluste r-management.io/root-policy acm-validation-policy-schedule-20250501180616 Completed 0 0 2025-05-01 18:06:30 +0000 UTC 22h velero-acm-backup-1 [root@dom14npv101-infra-manager ~ hubrc]# curl \"https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip\" -o \"awscliv2.zip\" % Total % Received % Xferd Average Speed Time Time Time Current Dload Upload Total Spent Left Speed 100 67.2M 100 67.2M 0 0 20.2M 0 0:00:03 0:00:03 --:--:-- 20.2M [root@dom14npv101-infra-manager ~ hubrc]# unzip awscliv2.zip Archive: awscliv2.zip creating: aws/ creating: aws/dist/ inflating: aws/install inflating: aws/README.md inflating: aws/THIRD_PARTY_LICENSES creating: aws/dist/awscli/ creating: aws/dist/cryptography/ creating: aws/dist/docutils/ creating: aws/dist/lib-dynload/ inflating: aws/dist/aws inflating: aws/dist/aws_completer inflating: aws/dist/libpython3.13.so.1.0 inflating: aws/dist/_awscrt.abi3.so inflating: aws/dist/_cffi_backend.cpython-313-x86_64-linux-gnu.so inflating: aws/dist/_ruamel_yaml.cpython-313-x86_64-linux-gnu.so inflating: aws/dist/libz.so.1 inflating: aws/dist/liblzma.so.5 inflating: aws/dist/libbz2.so.1 inflating: aws/dist/libffi.so.6 inflating: aws/dist/libuuid.so.1 [root@dom14npv101-infra-manager ~ hubrc]# sudo ./aws/install You can now run: /usr/local/bin/aws --version [root@dom14npv101-infra-manager ~ hubrc]# aws --version aws-cli/2.27.6 Python/3.13.2 Linux/5.14.0-427.13.1.el9_4.x86_64 exe/x86_64.rhel.9 [root@dom14npv101-infra-manager ~ hubrc]# root@dom14npv101-infra-manager ~ hubrc]# aws configure --profile oadp AWS Access Key ID [None]: I35Rzin4xFf58GaCkJlC AWS Secret Access Key [None]: vaimKLwBnO5glSe+f4AKgwJTtYEMOPMnfon540LZ Default region name [None]: Default output format [None]: [root@dom14npv101-infra-manager ~ hubrc]# aws --endpoint-url $AWS_ENDPOINT_URL s3 ls s3://acm-backups-bcf1990d-0846-4a6d-8518-67db81ebee63/velero/ SSL validation failed for https://s3-openshift-storage.apps.ncpvnpvmgt.pnwlab.nsn-rdnet.net:443/acm-backups-bcf1990d-0846-4a6d-8518-67db81ebee63?list-type=2&prefix=velero%2F&delimiter=%2F&encoding-type=url [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: self signed certificate in certificate chain (_ssl.c:1028) [root@dom14npv101-infra-manager ~ hubrc]# [root@dom14npv101-infra-manager ~ hubrc]# aws --endpoint-url \"$AWS_ENDPOINT_URL\" --no-verify-ssl s3 ls s3://acm-backups-bcf1990d-0846-4a6d-8518-67db81ebee63/velero/ urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 's3-openshift-storage.apps.ncpvnpvmgt.pnwlab.nsn-rdnet.net'. Adding certificate verification is strongl y advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings PRE backups/ [root@dom14npv101-infra-manager ~ hubrc]# aws --endpoint-url \"$AWS_ENDPOINT_URL\" --no-verify-ssl s3 ls s3://acm-backups-bcf1990d-0846-4a6d-8518-67db81ebee63/velero/backups/ urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 's3-openshift-storage.apps.ncpvnpvmgt.pnwlab.nsn-rdnet.net'. Adding certificate verification is strongl y advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings PRE acm-credentials-schedule-20250501180616/ PRE acm-managed-clusters-schedule-20250501180616/ PRE acm-resources-generic-schedule-20250501180616/ PRE acm-resources-schedule-20250501180616/ PRE acm-validation-policy-schedule-20250501180616/ [root@dom14npv101-infra-manager ~ hubrc]# aws --endpoint-url \"$AWS_ENDPOINT_URL\" --no-verify-ssl s3 ls s3://acm-backups-bcf1990d-0846-4a6d-8518-67db81ebee63/velero/backups/acm-credentials-schedule-202505011 80616/ urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 's3-openshift-storage.apps.ncpvnpvmgt.pnwlab.nsn-rdnet.net'. Adding certificate verification is strongl y advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings 2025-05-01 18:06:18 29 acm-credentials-schedule-20250501180616-csi-volumesnapshotclasses.json.gz 2025-05-01 18:06:18 27 acm-credentials-schedule-20250501180616-csi-volumesnapshotcontents.json.gz 2025-05-01 18:06:18 29 acm-credentials-schedule-20250501180616-csi-volumesnapshots.json.gz 2025-05-01 18:06:18 27 acm-credentials-schedule-20250501180616-itemoperations.json.gz 2025-05-01 18:06:18 11604 acm-credentials-schedule-20250501180616-logs.gz 2025-05-01 18:06:18 29 acm-credentials-schedule-20250501180616-podvolumebackups.json.gz 2025-05-01 18:06:18 327 acm-credentials-schedule-20250501180616-resource-list.json.gz 2025-05-01 18:06:18 49 acm-credentials-schedule-20250501180616-results.gz 2025-05-01 18:06:18 27 acm-credentials-schedule-20250501180616-volumeinfo.json.gz 2025-05-01 18:06:18 29 acm-credentials-schedule-20250501180616-volumesnapshots.json.gz 2025-05-01 18:06:18 46601 acm-credentials-schedule-20250501180616.tar.gz 2025-05-01 18:06:18 4428 velero-backup.json [root@dom14npv101-infra-manager ~ hubrc]#","title":"AWS S3 Backup"},{"location":"openshift/backup-restore/etcd-backup/","text":"Control plane backup Backing up etcd etcd is the key-value store for OpenShift Container Platform, which persists the state of all resource objects. Back up your cluster\u2019s etcd data regularly and store in a secure location ideally outside the OpenShift Container Platform environment. Do not take an etcd backup before the first certificate rotation completes, which occurs 24 hours after installation, otherwise the backup will contain expired certificates. It is also recommended to take etcd backups during non-peak usage hours because the etcd snapshot has a high I/O cost. Be sure to take an etcd backup after you upgrade your cluster. This is important because when you restore your cluster, you must use an etcd backup that was taken from the same z-stream release. For example, an OpenShift Container Platform 4.y.z cluster must use an etcd backup that was taken from 4.y.z. Backing up etcd data Prerequisites You have access to the cluster as a user with the cluster-admin role. You have checked whether the cluster-wide proxy is enabled. Procedure Start a debug session for a control plane node: oc debug node/<node_name> Change your root directory to /host: chroot /host If the cluster-wide proxy is enabled, be sure that you have exported the NO_PROXY, HTTP_PROXY, and HTTPS_PROXY environment variables. (optional) Run the cluster-backup.sh script and pass in the location to save the backup to. sh-4.4# /usr/local/bin/cluster-backup.sh /home/core/assets/backup Example script output found latest kube-apiserver: /etc/kubernetes/static-pod-resources/kube-apiserver-pod-6 found latest kube-controller-manager: /etc/kubernetes/static-pod-resources/kube-controller-manager-pod-7 found latest kube-scheduler: /etc/kubernetes/static-pod-resources/kube-scheduler-pod-6 found latest etcd: /etc/kubernetes/static-pod-resources/etcd-pod-3 ede95fe6b88b87ba86a03c15e669fb4aa5bf0991c180d3c6895ce72eaade54a1 etcdctl version: 3.4.14 API version: 3.4 {\"level\":\"info\",\"ts\":1624647639.0188997,\"caller\":\"snapshot/v3_snapshot.go:119\",\"msg\":\"created temporary db file\",\"path\":\"/home/core/assets/backup/snapshot_2021-06-25_190035.db.part\"} {\"level\":\"info\",\"ts\":\"2021-06-25T19:00:39.030Z\",\"caller\":\"clientv3/maintenance.go:200\",\"msg\":\"opened snapshot stream; downloading\"} {\"level\":\"info\",\"ts\":1624647639.0301006,\"caller\":\"snapshot/v3_snapshot.go:127\",\"msg\":\"fetching snapshot\",\"endpoint\":\"https://10.0.0.5:2379\"} {\"level\":\"info\",\"ts\":\"2021-06-25T19:00:40.215Z\",\"caller\":\"clientv3/maintenance.go:208\",\"msg\":\"completed snapshot read; closing\"} {\"level\":\"info\",\"ts\":1624647640.6032252,\"caller\":\"snapshot/v3_snapshot.go:142\",\"msg\":\"fetched snapshot\",\"endpoint\":\"https://10.0.0.5:2379\",\"size\":\"114 MB\",\"took\":1.584090459} {\"level\":\"info\",\"ts\":1624647640.6047094,\"caller\":\"snapshot/v3_snapshot.go:152\",\"msg\":\"saved\",\"path\":\"/home/core/assets/backup/snapshot_2021-06-25_190035.db\"} Snapshot saved at /home/core/assets/backup/snapshot_2021-06-25_190035.db {\"hash\":3866667823,\"revision\":31407,\"totalKey\":12828,\"totalSize\":114446336} snapshot db and kube resources are successfully saved to /home/core/assets/backup automated steps to run and copy the file locally backupdir=/tmp/backup-etcd/ servername=$(oc get nodes |grep -i master|awk '{print $1}'|head -1) [ -d $backupdir/${servername} ] || mkdir -p $backupdir/${servername} execout=$(oc debug -t node/$servername -- chroot /host bash -c '/usr/local/bin/cluster-backup.sh /home/core/assets/backup') # filename=$(echo $execout | awk -F'path\":\"' '{print $2}' | awk -F'\"' '{print $1}') #<--- not working sleep 20 oc debug -t node/$servername -- chroot /host bash -c 'chown core:core /home/core/assets/backup/*.db' scp -rp core@$servername:${filename} $backupdir/${servername}/ Reference ETCD backup and restore","title":"ETCD Backup"},{"location":"openshift/backup-restore/etcd-backup/#control-plane-backup","text":"","title":"Control plane backup"},{"location":"openshift/backup-restore/etcd-backup/#backing-up-etcd","text":"etcd is the key-value store for OpenShift Container Platform, which persists the state of all resource objects. Back up your cluster\u2019s etcd data regularly and store in a secure location ideally outside the OpenShift Container Platform environment. Do not take an etcd backup before the first certificate rotation completes, which occurs 24 hours after installation, otherwise the backup will contain expired certificates. It is also recommended to take etcd backups during non-peak usage hours because the etcd snapshot has a high I/O cost. Be sure to take an etcd backup after you upgrade your cluster. This is important because when you restore your cluster, you must use an etcd backup that was taken from the same z-stream release. For example, an OpenShift Container Platform 4.y.z cluster must use an etcd backup that was taken from 4.y.z.","title":"Backing up etcd"},{"location":"openshift/backup-restore/etcd-backup/#backing-up-etcd-data","text":"","title":"Backing up etcd data"},{"location":"openshift/backup-restore/etcd-backup/#prerequisites","text":"You have access to the cluster as a user with the cluster-admin role. You have checked whether the cluster-wide proxy is enabled.","title":"Prerequisites"},{"location":"openshift/backup-restore/etcd-backup/#procedure","text":"Start a debug session for a control plane node: oc debug node/<node_name> Change your root directory to /host: chroot /host If the cluster-wide proxy is enabled, be sure that you have exported the NO_PROXY, HTTP_PROXY, and HTTPS_PROXY environment variables. (optional) Run the cluster-backup.sh script and pass in the location to save the backup to. sh-4.4# /usr/local/bin/cluster-backup.sh /home/core/assets/backup Example script output found latest kube-apiserver: /etc/kubernetes/static-pod-resources/kube-apiserver-pod-6 found latest kube-controller-manager: /etc/kubernetes/static-pod-resources/kube-controller-manager-pod-7 found latest kube-scheduler: /etc/kubernetes/static-pod-resources/kube-scheduler-pod-6 found latest etcd: /etc/kubernetes/static-pod-resources/etcd-pod-3 ede95fe6b88b87ba86a03c15e669fb4aa5bf0991c180d3c6895ce72eaade54a1 etcdctl version: 3.4.14 API version: 3.4 {\"level\":\"info\",\"ts\":1624647639.0188997,\"caller\":\"snapshot/v3_snapshot.go:119\",\"msg\":\"created temporary db file\",\"path\":\"/home/core/assets/backup/snapshot_2021-06-25_190035.db.part\"} {\"level\":\"info\",\"ts\":\"2021-06-25T19:00:39.030Z\",\"caller\":\"clientv3/maintenance.go:200\",\"msg\":\"opened snapshot stream; downloading\"} {\"level\":\"info\",\"ts\":1624647639.0301006,\"caller\":\"snapshot/v3_snapshot.go:127\",\"msg\":\"fetching snapshot\",\"endpoint\":\"https://10.0.0.5:2379\"} {\"level\":\"info\",\"ts\":\"2021-06-25T19:00:40.215Z\",\"caller\":\"clientv3/maintenance.go:208\",\"msg\":\"completed snapshot read; closing\"} {\"level\":\"info\",\"ts\":1624647640.6032252,\"caller\":\"snapshot/v3_snapshot.go:142\",\"msg\":\"fetched snapshot\",\"endpoint\":\"https://10.0.0.5:2379\",\"size\":\"114 MB\",\"took\":1.584090459} {\"level\":\"info\",\"ts\":1624647640.6047094,\"caller\":\"snapshot/v3_snapshot.go:152\",\"msg\":\"saved\",\"path\":\"/home/core/assets/backup/snapshot_2021-06-25_190035.db\"} Snapshot saved at /home/core/assets/backup/snapshot_2021-06-25_190035.db {\"hash\":3866667823,\"revision\":31407,\"totalKey\":12828,\"totalSize\":114446336} snapshot db and kube resources are successfully saved to /home/core/assets/backup","title":"Procedure"},{"location":"openshift/backup-restore/etcd-backup/#automated-steps-to-run-and-copy-the-file-locally","text":"backupdir=/tmp/backup-etcd/ servername=$(oc get nodes |grep -i master|awk '{print $1}'|head -1) [ -d $backupdir/${servername} ] || mkdir -p $backupdir/${servername} execout=$(oc debug -t node/$servername -- chroot /host bash -c '/usr/local/bin/cluster-backup.sh /home/core/assets/backup') # filename=$(echo $execout | awk -F'path\":\"' '{print $2}' | awk -F'\"' '{print $1}') #<--- not working sleep 20 oc debug -t node/$servername -- chroot /host bash -c 'chown core:core /home/core/assets/backup/*.db' scp -rp core@$servername:${filename} $backupdir/${servername}/","title":"automated steps to run and copy the file locally"},{"location":"openshift/backup-restore/etcd-backup/#reference","text":"ETCD backup and restore","title":"Reference"},{"location":"openshift/backup-restore/etcd-restore/","text":"Replacing an unhealthy etcd member Reference ETCD backup and restore","title":"ETCD Restore"},{"location":"openshift/backup-restore/etcd-restore/#replacing-an-unhealthy-etcd-member","text":"","title":"Replacing an unhealthy etcd member"},{"location":"openshift/backup-restore/etcd-restore/#reference","text":"ETCD backup and restore","title":"Reference"},{"location":"openshift/deployment/readme/","text":"update","title":"Deployment"},{"location":"openshift/maintenace/cluster-stop-start/","text":"","title":"Cluster Stop/Start"},{"location":"openshift/maintenace/reboot-nodes/","text":"Rebooting nodes on the openshift cluster steps to show, boot on master, worker/gateway and storage. Master node reboot login to right cluster using cluster-admin based role. [root@dom14npv101-infra-manager ~ management]# source /root/raj/managementrc WARNING: Using insecure TLS client config. Setting this option is not supported! Login successful. You have access to 99 projects, the list has been suppressed. You can list all projects with 'oc projects' Using project \"default\". [root@dom14npv101-infra-manager ~ management]# get the list of storage nodes and also check the ceph status. [root@dom14npv101-infra-manager ~ management]# oc get nodes |grep -i master ncpvnpvmgt-master-101.ncpvnpvmgt.pnwlab.nsn-rdnet.net Ready control-plane,master,monitor 32d v1.29.10+67d3387 ncpvnpvmgt-master-102.ncpvnpvmgt.pnwlab.nsn-rdnet.net Ready control-plane,master,monitor 32d v1.29.10+67d3387 ncpvnpvmgt-master-103.ncpvnpvmgt.pnwlab.nsn-rdnet.net Ready control-plane,master,monitor 32d v1.29.10+67d3387 [root@dom14npv101-infra-manager ~ management]# drain the node completely node=ncpvnpvmgt-master-101.ncpvnpvmgt.pnwlab.nsn-rdnet.net oc adm drain $node --ignore-daemonsets --delete-emptydir-data --force oc adm drain $node --ignore-daemonsets --delete-emptydir-data --force --disable-eviction trigger the shutdown now. oc debug node/$node -- chroot /host shutdown -r +1 once this server came up, waiting for server to be in ready state oc wait --for=condition=Ready node/$node --timeout=800s uncordon the node and make it schedulable now. oc adm uncordon $node Storage node reboot login to right cluster using cluster-admin based role. [root@dom14npv101-infra-manager ~ management]# source /root/raj/managementrc WARNING: Using insecure TLS client config. Setting this option is not supported! Login successful. You have access to 99 projects, the list has been suppressed. You can list all projects with 'oc projects' Using project \"default\". [root@dom14npv101-infra-manager ~ management]# get the list of storage nodes and also check the ceph status. [root@dom14npv101-infra-manager ~ vlabrc]# oc get no |grep -i storage ncpvnpvlab1-storage-101.ncpvnpvlab1.pnwlab.nsn-rdnet.net Ready storage,worker 24d v1.29.10+67d3387 ncpvnpvlab1-storage-102.ncpvnpvlab1.pnwlab.nsn-rdnet.net Ready storage,worker 26d v1.29.10+67d3387 ncpvnpvlab1-storage-103.ncpvnpvlab1.pnwlab.nsn-rdnet.net Ready storage,worker 26d v1.29.10+67d3387 ncpvnpvlab1-storage-201.ncpvnpvlab1.pnwlab.nsn-rdnet.net Ready storage,worker 24d v1.29.10+67d3387 ncpvnpvlab1-storage-202.ncpvnpvlab1.pnwlab.nsn-rdnet.net Ready storage,worker 26d v1.29.10+67d3387 ncpvnpvlab1-storage-203.ncpvnpvlab1.pnwlab.nsn-rdnet.net Ready storage,worker 26d v1.29.10+67d3387 [root@dom14npv101-infra-manager ~ vlabrc]# [root@dom14npv101-infra-manager ~ vlabrc]# oc exec -it $(oc get pod -n openshift-storage -l app=rook-ceph-operator -o name) -n openshift-storage -- ceph -s -c /var/lib/rook/openshift-storage/openshift-storage.config define a variable called and value as respective storage node, easy run of reboot [root@dom14npv101-infra-manager ~ vlabrc]# node=ncpvnpvlab1-storage-203.ncpvnpvlab1.pnwlab.nsn-rdnet.net get the list ceph storage related pods [root@dom14npv101-infra-manager ~ vlabrc]# oc -n openshift-storage get po -o wide | grep -e mon -e osd | grep ${node}|grep -iv complete|awk '{print $1}' rook-ceph-osd-12-589c6485c9-ptqs7 rook-ceph-osd-16-74cbc54b4f-tg8lw rook-ceph-osd-2-7fcc4ff664-hjc2w rook-ceph-osd-26-67d86fbf5d-lqdv8 rook-ceph-osd-30-78fb4f588b-474jn rook-ceph-osd-37-bd549c676-jn5ld rook-ceph-osd-40-5b4ddb6d7d-7qd8z rook-ceph-osd-6-5b64cf8d49-n55zf [root@dom14npv101-infra-manager ~ vlabrc]# use notepad++ to create these following commands to remove those pods on that host. [root@dom14npv101-infra-manager ~ vlabrc]# oc -n openshift-storage scale deployment rook-ceph-osd-12 --replicas=0 oc -n openshift-storage scale deployment rook-ceph-osd-16 --replicas=0 oc -n openshift-storage scale deployment rook-ceph-osd-2 --replicas=0 oc -n openshift-storage scale deployment rook-ceph-osd-26 --replicas=0 oc -n openshift-storage scale deployment rook-ceph-osd-30 --replicas=0 oc -n openshift-storage scale deployment rook-ceph-osd-37 --replicas=0 oc -n openshift-storage scale deployment rook-ceph-osd-40 --replicas=0 oc -n openshift-storage scale deployment rook-ceph-osd-6 --replicas=0 deployment.apps/rook-ceph-osd-12 scaled deployment.apps/rook-ceph-osd-16 scaled deployment.apps/rook-ceph-osd-2 scaled deployment.apps/rook-ceph-osd-26 scaled deployment.apps/rook-ceph-osd-30 scaled deployment.apps/rook-ceph-osd-37 scaled deployment.apps/rook-ceph-osd-40 scaled deployment.apps/rook-ceph-osd-6 scaled [root@dom14npv101-infra-manager ~ vlabrc]# Completely drain that storage node, using oc adm command [root@dom14npv101-infra-manager ~ vlabrc]#oc adm drain ${node} --delete-emptydir-data --ignore-daemonsets=true --timeout=500s --force node/ncpvnpvlab1-storage-203.ncpvnpvlab1.pnwlab.nsn-rdnet.net cordoned Warning: ignoring DaemonSet-managed Pods: openshift-cluster-node-tuning-operator/tuned-kqqfn, openshift-dns/dns-default-27nmr, openshift-dns/node-resolver-4dcng, openshift-image-registry/node-ca-2xtbk, openshift-ingress-canary/ingress-canary-285m2, openshift-local-storage/diskmaker-discovery-zmmwj, openshift-local-storage/diskmaker-manager-dvpjq, openshift-logging/collector-7g5zv, openshift-machine-config-operator/machine-config-daemon-c7c4v, openshift-monitoring/node-exporter-gsh5l, openshift-multus/multus-6q7c8, openshift-multus/multus-additional-cni-plugins-pwq85, openshift-multus/network-metrics-daemon-f6flp, openshift-multus/whereabouts-reconciler-fjpmv, openshift-network-diagnostics/network-check-target-9h7cj, openshift-network-operator/iptables-alerter-km8jq, openshift-nmstate/nmstate-handler-6hmz9, openshift-operators/istio-cni-node-v2-5-4qpnk, openshift-ovn-kubernetes/ovnkube-node-w672f, openshift-storage/csi-cephfsplugin-ck4mf, openshift-storage/csi-rbdplugin-gkmjm evicting pod openshift-storage/rook-ceph-osd-6-5b64cf8d49-n55zf evicting pod openshift-compliance/openscap-pod-24dd5998a72563f75be98757ffbe02e424df617e evicting pod openshift-storage/rook-ceph-crashcollector-9c7a6e51d7dc201a808c754612468a82-j84n5 evicting pod openshift-storage/rook-ceph-mds-ocs-storagecluster-cephfilesystem-b-78f7bbf8bn4rl evicting pod openshift-storage/rook-ceph-mgr-b-5468b7cf-nx4d4 evicting pod openshift-storage/rook-ceph-osd-40-5b4ddb6d7d-7qd8z evicting pod openshift-storage/rook-ceph-exporter-9c7a6e51d7dc201a808c754612468a82-c854b4r2tn4 evicting pod openshift-compliance/openscap-pod-5f54688367da49304dfd83a2e0d564582b073f2b pod/openscap-pod-5f54688367da49304dfd83a2e0d564582b073f2b evicted pod/openscap-pod-24dd5998a72563f75be98757ffbe02e424df617e evicted pod/rook-ceph-crashcollector-9c7a6e51d7dc201a808c754612468a82-j84n5 evicted pod/rook-ceph-mgr-b-5468b7cf-nx4d4 evicted pod/rook-ceph-exporter-9c7a6e51d7dc201a808c754612468a82-c854b4r2tn4 evicted pod/rook-ceph-mds-ocs-storagecluster-cephfilesystem-b-78f7bbf8bn4rl evicted node/ncpvnpvlab1-storage-203.ncpvnpvlab1.pnwlab.nsn-rdnet.net drained [root@dom14npv101-infra-manager ~ vlabrc]# reboot the respective storage node using following command. [root@dom14npv101-infra-manager ~ vlabrc]#oc debug node/${node} -- chroot /host systemctl reboot waiting for node to be fully up. then check the kubelet status [root@dom14npv101-infra-manager ~ vlabrc]# oc get no |grep -i storage ncpvnpvlab1-storage-101.ncpvnpvlab1.pnwlab.nsn-rdnet.net Ready storage,worker 24d v1.29.10+67d3387 ncpvnpvlab1-storage-102.ncpvnpvlab1.pnwlab.nsn-rdnet.net Ready storage,worker 26d v1.29.10+67d3387 ncpvnpvlab1-storage-103.ncpvnpvlab1.pnwlab.nsn-rdnet.net Ready storage,worker 26d v1.29.10+67d3387 ncpvnpvlab1-storage-201.ncpvnpvlab1.pnwlab.nsn-rdnet.net Ready storage,worker 24d v1.29.10+67d3387 ncpvnpvlab1-storage-202.ncpvnpvlab1.pnwlab.nsn-rdnet.net Ready storage,worker 26d v1.29.10+67d3387 ncpvnpvlab1-storage-203.ncpvnpvlab1.pnwlab.nsn-rdnet.net Ready,sechd-disable storage,worker 26d v1.29.10+67d3387 [root@dom14npv101-infra-manager ~ vlabrc]# uncordon the storage node. [root@dom14npv101-infra-manager ~ vlabrc]# oc adm uncordon ncpvnpvlab1-storage-203.ncpvnpvlab1.pnwlab.nsn-rdnet.net check the ceph health and wait for all osd to be fully up. give 10mins [root@dom14npv101-infra-manager ~ vlabrc]# oc exec -it $(oc get pod -n openshift-storage -l app=rook-ceph-operator -o name) -n openshift-storage -- ceph -s -c /var/lib/rook/openshift-storage/openshift-storage.config","title":"Reboot Nodes"},{"location":"openshift/maintenace/reboot-nodes/#rebooting-nodes-on-the-openshift-cluster","text":"steps to show, boot on master, worker/gateway and storage.","title":"Rebooting nodes on the openshift cluster"},{"location":"openshift/maintenace/reboot-nodes/#master-node-reboot","text":"login to right cluster using cluster-admin based role. [root@dom14npv101-infra-manager ~ management]# source /root/raj/managementrc WARNING: Using insecure TLS client config. Setting this option is not supported! Login successful. You have access to 99 projects, the list has been suppressed. You can list all projects with 'oc projects' Using project \"default\". [root@dom14npv101-infra-manager ~ management]# get the list of storage nodes and also check the ceph status. [root@dom14npv101-infra-manager ~ management]# oc get nodes |grep -i master ncpvnpvmgt-master-101.ncpvnpvmgt.pnwlab.nsn-rdnet.net Ready control-plane,master,monitor 32d v1.29.10+67d3387 ncpvnpvmgt-master-102.ncpvnpvmgt.pnwlab.nsn-rdnet.net Ready control-plane,master,monitor 32d v1.29.10+67d3387 ncpvnpvmgt-master-103.ncpvnpvmgt.pnwlab.nsn-rdnet.net Ready control-plane,master,monitor 32d v1.29.10+67d3387 [root@dom14npv101-infra-manager ~ management]# drain the node completely node=ncpvnpvmgt-master-101.ncpvnpvmgt.pnwlab.nsn-rdnet.net oc adm drain $node --ignore-daemonsets --delete-emptydir-data --force oc adm drain $node --ignore-daemonsets --delete-emptydir-data --force --disable-eviction trigger the shutdown now. oc debug node/$node -- chroot /host shutdown -r +1 once this server came up, waiting for server to be in ready state oc wait --for=condition=Ready node/$node --timeout=800s uncordon the node and make it schedulable now. oc adm uncordon $node","title":"Master node reboot"},{"location":"openshift/maintenace/reboot-nodes/#storage-node-reboot","text":"login to right cluster using cluster-admin based role. [root@dom14npv101-infra-manager ~ management]# source /root/raj/managementrc WARNING: Using insecure TLS client config. Setting this option is not supported! Login successful. You have access to 99 projects, the list has been suppressed. You can list all projects with 'oc projects' Using project \"default\". [root@dom14npv101-infra-manager ~ management]# get the list of storage nodes and also check the ceph status. [root@dom14npv101-infra-manager ~ vlabrc]# oc get no |grep -i storage ncpvnpvlab1-storage-101.ncpvnpvlab1.pnwlab.nsn-rdnet.net Ready storage,worker 24d v1.29.10+67d3387 ncpvnpvlab1-storage-102.ncpvnpvlab1.pnwlab.nsn-rdnet.net Ready storage,worker 26d v1.29.10+67d3387 ncpvnpvlab1-storage-103.ncpvnpvlab1.pnwlab.nsn-rdnet.net Ready storage,worker 26d v1.29.10+67d3387 ncpvnpvlab1-storage-201.ncpvnpvlab1.pnwlab.nsn-rdnet.net Ready storage,worker 24d v1.29.10+67d3387 ncpvnpvlab1-storage-202.ncpvnpvlab1.pnwlab.nsn-rdnet.net Ready storage,worker 26d v1.29.10+67d3387 ncpvnpvlab1-storage-203.ncpvnpvlab1.pnwlab.nsn-rdnet.net Ready storage,worker 26d v1.29.10+67d3387 [root@dom14npv101-infra-manager ~ vlabrc]# [root@dom14npv101-infra-manager ~ vlabrc]# oc exec -it $(oc get pod -n openshift-storage -l app=rook-ceph-operator -o name) -n openshift-storage -- ceph -s -c /var/lib/rook/openshift-storage/openshift-storage.config define a variable called and value as respective storage node, easy run of reboot [root@dom14npv101-infra-manager ~ vlabrc]# node=ncpvnpvlab1-storage-203.ncpvnpvlab1.pnwlab.nsn-rdnet.net get the list ceph storage related pods [root@dom14npv101-infra-manager ~ vlabrc]# oc -n openshift-storage get po -o wide | grep -e mon -e osd | grep ${node}|grep -iv complete|awk '{print $1}' rook-ceph-osd-12-589c6485c9-ptqs7 rook-ceph-osd-16-74cbc54b4f-tg8lw rook-ceph-osd-2-7fcc4ff664-hjc2w rook-ceph-osd-26-67d86fbf5d-lqdv8 rook-ceph-osd-30-78fb4f588b-474jn rook-ceph-osd-37-bd549c676-jn5ld rook-ceph-osd-40-5b4ddb6d7d-7qd8z rook-ceph-osd-6-5b64cf8d49-n55zf [root@dom14npv101-infra-manager ~ vlabrc]# use notepad++ to create these following commands to remove those pods on that host. [root@dom14npv101-infra-manager ~ vlabrc]# oc -n openshift-storage scale deployment rook-ceph-osd-12 --replicas=0 oc -n openshift-storage scale deployment rook-ceph-osd-16 --replicas=0 oc -n openshift-storage scale deployment rook-ceph-osd-2 --replicas=0 oc -n openshift-storage scale deployment rook-ceph-osd-26 --replicas=0 oc -n openshift-storage scale deployment rook-ceph-osd-30 --replicas=0 oc -n openshift-storage scale deployment rook-ceph-osd-37 --replicas=0 oc -n openshift-storage scale deployment rook-ceph-osd-40 --replicas=0 oc -n openshift-storage scale deployment rook-ceph-osd-6 --replicas=0 deployment.apps/rook-ceph-osd-12 scaled deployment.apps/rook-ceph-osd-16 scaled deployment.apps/rook-ceph-osd-2 scaled deployment.apps/rook-ceph-osd-26 scaled deployment.apps/rook-ceph-osd-30 scaled deployment.apps/rook-ceph-osd-37 scaled deployment.apps/rook-ceph-osd-40 scaled deployment.apps/rook-ceph-osd-6 scaled [root@dom14npv101-infra-manager ~ vlabrc]# Completely drain that storage node, using oc adm command [root@dom14npv101-infra-manager ~ vlabrc]#oc adm drain ${node} --delete-emptydir-data --ignore-daemonsets=true --timeout=500s --force node/ncpvnpvlab1-storage-203.ncpvnpvlab1.pnwlab.nsn-rdnet.net cordoned Warning: ignoring DaemonSet-managed Pods: openshift-cluster-node-tuning-operator/tuned-kqqfn, openshift-dns/dns-default-27nmr, openshift-dns/node-resolver-4dcng, openshift-image-registry/node-ca-2xtbk, openshift-ingress-canary/ingress-canary-285m2, openshift-local-storage/diskmaker-discovery-zmmwj, openshift-local-storage/diskmaker-manager-dvpjq, openshift-logging/collector-7g5zv, openshift-machine-config-operator/machine-config-daemon-c7c4v, openshift-monitoring/node-exporter-gsh5l, openshift-multus/multus-6q7c8, openshift-multus/multus-additional-cni-plugins-pwq85, openshift-multus/network-metrics-daemon-f6flp, openshift-multus/whereabouts-reconciler-fjpmv, openshift-network-diagnostics/network-check-target-9h7cj, openshift-network-operator/iptables-alerter-km8jq, openshift-nmstate/nmstate-handler-6hmz9, openshift-operators/istio-cni-node-v2-5-4qpnk, openshift-ovn-kubernetes/ovnkube-node-w672f, openshift-storage/csi-cephfsplugin-ck4mf, openshift-storage/csi-rbdplugin-gkmjm evicting pod openshift-storage/rook-ceph-osd-6-5b64cf8d49-n55zf evicting pod openshift-compliance/openscap-pod-24dd5998a72563f75be98757ffbe02e424df617e evicting pod openshift-storage/rook-ceph-crashcollector-9c7a6e51d7dc201a808c754612468a82-j84n5 evicting pod openshift-storage/rook-ceph-mds-ocs-storagecluster-cephfilesystem-b-78f7bbf8bn4rl evicting pod openshift-storage/rook-ceph-mgr-b-5468b7cf-nx4d4 evicting pod openshift-storage/rook-ceph-osd-40-5b4ddb6d7d-7qd8z evicting pod openshift-storage/rook-ceph-exporter-9c7a6e51d7dc201a808c754612468a82-c854b4r2tn4 evicting pod openshift-compliance/openscap-pod-5f54688367da49304dfd83a2e0d564582b073f2b pod/openscap-pod-5f54688367da49304dfd83a2e0d564582b073f2b evicted pod/openscap-pod-24dd5998a72563f75be98757ffbe02e424df617e evicted pod/rook-ceph-crashcollector-9c7a6e51d7dc201a808c754612468a82-j84n5 evicted pod/rook-ceph-mgr-b-5468b7cf-nx4d4 evicted pod/rook-ceph-exporter-9c7a6e51d7dc201a808c754612468a82-c854b4r2tn4 evicted pod/rook-ceph-mds-ocs-storagecluster-cephfilesystem-b-78f7bbf8bn4rl evicted node/ncpvnpvlab1-storage-203.ncpvnpvlab1.pnwlab.nsn-rdnet.net drained [root@dom14npv101-infra-manager ~ vlabrc]# reboot the respective storage node using following command. [root@dom14npv101-infra-manager ~ vlabrc]#oc debug node/${node} -- chroot /host systemctl reboot waiting for node to be fully up. then check the kubelet status [root@dom14npv101-infra-manager ~ vlabrc]# oc get no |grep -i storage ncpvnpvlab1-storage-101.ncpvnpvlab1.pnwlab.nsn-rdnet.net Ready storage,worker 24d v1.29.10+67d3387 ncpvnpvlab1-storage-102.ncpvnpvlab1.pnwlab.nsn-rdnet.net Ready storage,worker 26d v1.29.10+67d3387 ncpvnpvlab1-storage-103.ncpvnpvlab1.pnwlab.nsn-rdnet.net Ready storage,worker 26d v1.29.10+67d3387 ncpvnpvlab1-storage-201.ncpvnpvlab1.pnwlab.nsn-rdnet.net Ready storage,worker 24d v1.29.10+67d3387 ncpvnpvlab1-storage-202.ncpvnpvlab1.pnwlab.nsn-rdnet.net Ready storage,worker 26d v1.29.10+67d3387 ncpvnpvlab1-storage-203.ncpvnpvlab1.pnwlab.nsn-rdnet.net Ready,sechd-disable storage,worker 26d v1.29.10+67d3387 [root@dom14npv101-infra-manager ~ vlabrc]# uncordon the storage node. [root@dom14npv101-infra-manager ~ vlabrc]# oc adm uncordon ncpvnpvlab1-storage-203.ncpvnpvlab1.pnwlab.nsn-rdnet.net check the ceph health and wait for all osd to be fully up. give 10mins [root@dom14npv101-infra-manager ~ vlabrc]# oc exec -it $(oc get pod -n openshift-storage -l app=rook-ceph-operator -o name) -n openshift-storage -- ceph -s -c /var/lib/rook/openshift-storage/openshift-storage.config","title":"Storage node reboot"},{"location":"openshift/networking/metalb-troubleshooting/","text":"metalb configuration troubleshooting login to cluster [root@ncputility ~ pancwl_rc]$ source /root/pancwlrc WARNING: Using insecure TLS client config. Setting this option is not supported! Login successful. You have access to 113 projects, the list has been suppressed. You can list all projects with 'oc projects' Using project \"ncd01pan\". check for metallb-system namespace exist [root@ncputility ~ pancwl_rc]$ oc get ns | grep -i metallb metallb-system Active 26d [root@ncputility ~ pancwl_rc]$ check the status of the pods on the metallb-system namespace here. [root@ncputility ~ pancwl_rc]$ oc get pods -n metallb-system NAME READY STATUS RESTARTS AGE controller-5785bc85cb-qpk8h 2/2 Running 0 24d metallb-operator-controller-manager-7bf5d8978d-clpd7 1/1 Running 0 26d metallb-operator-webhook-server-86784c6c8c-49ncp 1/1 Running 0 26d speaker-dlrn5 6/6 Running 0 24d speaker-g2g77 6/6 Running 0 24d speaker-jzbw7 6/6 Running 0 24d speaker-pjstl 6/6 Running 0 24d check for bfdprofile on this cluster here transmit interval and receiver interval should be equal from local and remote end. oc -n metallb-system get BFDProfile -o wide NAME PASSIVE MODE TRANSMIT INTERVAL RECEIVE INTERVAL MULTIPLIER ncp-metallb-oam-pa-hn-bfd-profile true 300 300 3 ncp-metallb-oam-pa-ni-bfd-profile true 300 300 3 ncp-metallb-oam-pa-pa-bfd-profile true 300 300 3 ncp-metallb-oam-pa-sv-bfd-profile true 300 300 3 make sure, desination having the backward route configured here . [root@ncputility ~ pancwl_rc]$ oc -n metallb-system get nncp -o wide NAME STATUS REASON backward-route-for-oam-pa-pa-metallb-vlan104 Available SuccessfullyConfigured ncp-metallb-oam-pa-hn-route-for-switches-105 Available SuccessfullyConfigured ncp-metallb-oam-pa-ni-route-for-switches-107 Available SuccessfullyConfigured ncp-metallb-oam-pa-pa-route-for-switches-104 Available SuccessfullyConfigured ncp-metallb-oam-pa-sv-route-for-switches-106 Available SuccessfullyConfigured tenant-bond-bgp-oam-vlan104-gateway-0 Available SuccessfullyConfigured tenant-bond-bgp-oam-vlan104-gateway-1 Available SuccessfullyConfigured tenant-bond-bgp-oam-vlan104-gateway-2 Available SuccessfullyConfigured ** output omitted ** tenantvlan-373 Available SuccessfullyConfigured tenantvlan-374 Available SuccessfullyConfigured check for ipaddresspool exist here, so that application can create their service as loadbalancer here . [root@ncputility ~ pancwl_rc]$ oc -n metallb-system get IPAddressPool -o wide NAME AUTO ASSIGN AVOID BUGGY IPS ADDRESSES ncp-metallb-oam-pa-hn-addresspool false false [\"10.89.147.128/28\"] ncp-metallb-oam-pa-ni-addresspool false false [\"10.86.10.16/28\"] ncp-metallb-oam-pa-pa-addresspool false false [\"10.89.101.128/27\",\"10.89.97.208/28\"] ncp-metallb-oam-pa-sv-addresspool false false [\"10.85.186.240/28\"] check for bgppeer are up on the metallb speakers thats important for this communication [root@ncputility ~ pancwl_rc]$ oc -n metallb-system get BGPPeer -o wide NAME ADDRESS ASN BFD PROFILE MULTI HOPS ncp-metallb-oam-pa-hn-bgp-peer-1 10.89.147.194 4200000320 ncp-metallb-oam-pa-hn-bfd-profile ncp-metallb-oam-pa-hn-bgp-peer-2 10.89.147.195 4200000320 ncp-metallb-oam-pa-hn-bfd-profile ncp-metallb-oam-pa-ni-bgp-peer-1 10.86.10.98 4200000320 ncp-metallb-oam-pa-ni-bfd-profile ncp-metallb-oam-pa-ni-bgp-peer-2 10.86.10.99 4200000320 ncp-metallb-oam-pa-ni-bfd-profile ncp-metallb-oam-pa-pa-bgp-peer-1 10.89.97.162 4200000320 ncp-metallb-oam-pa-pa-bfd-profile ncp-metallb-oam-pa-pa-bgp-peer-2 10.89.97.163 4200000320 ncp-metallb-oam-pa-pa-bfd-profile ncp-metallb-oam-pa-sv-bgp-peer-1 10.85.187.34 4200000320 ncp-metallb-oam-pa-sv-bfd-profile ncp-metallb-oam-pa-sv-bgp-peer-2 10.85.187.35 4200000320 ncp-metallb-oam-pa-sv-bfd-profile check for BGPAdvertisement are created on this cluster and it should be in the metallb-system namespace. [root@ncputility ~ pancwl_rc]$ oc -n metallb-system get BGPAdvertisement -o wide NAME IPADDRESSPOOLS IPADDRESSPOOL SELECTORS PEERS NODE SELECTORS ncp-metallb-oam-pa-hn-bgp-advertisement [\"ncp-metallb-oam-pa-hn-addresspool\"] [\"ncp-metallb-oam-pa-hn-bgp-peer-1\",\"ncp-metallb-oam-pa-hn-bgp-peer-2\"] ncp-metallb-oam-pa-ni-bgp-advertisement [\"ncp-metallb-oam-pa-ni-addresspool\"] [\"ncp-metallb-oam-pa-ni-bgp-peer-1\",\"ncp-metallb-oam-pa-ni-bgp-peer-2\"] ncp-metallb-oam-pa-pa-bgp-advertisement [\"ncp-metallb-oam-pa-pa-addresspool\"] [\"ncp-metallb-oam-pa-pa-bgp-peer-1\",\"ncp-metallb-oam-pa-pa-bgp-peer-2\"] ncp-metallb-oam-pa-sv-bgp-advertisement [\"ncp-metallb-oam-pa-sv-addresspool\"] [\"ncp-metallb-oam-pa-sv-bgp-peer-1\",\"ncp-metallb-oam-pa-sv-bgp-peer-2\"] No, error from the container logs on this namespace . [root@ncputility ~ pancwl_rc]$ oc -n metallb-system logs -l component=speaker Defaulted container \"speaker\" out of: speaker, frr, reloader, frr-metrics, kube-rbac-proxy, kube-rbac-proxy-frr, cp-frr-files (init), cp-reloader (init), cp-metrics (init) Defaulted container \"speaker\" out of: speaker, frr, reloader, frr-metrics, kube-rbac-proxy, kube-rbac-proxy-frr, cp-frr-files (init), cp-reloader (init), cp-metrics (init) Defaulted container \"speaker\" out of: speaker, frr, reloader, frr-metrics, kube-rbac-proxy, kube-rbac-proxy-frr, cp-frr-files (init), cp-reloader (init), cp-metrics (init) Defaulted container \"speaker\" out of: speaker, frr, reloader, frr-metrics, kube-rbac-proxy, kube-rbac-proxy-frr, cp-frr-files (init), cp-reloader (init), cp-metrics (init) {\"caller\":\"node_controller.go:46\",\"controller\":\"NodeReconciler\",\"level\":\"info\",\"start reconcile\":\"/gateway4.panclypcwl01.mnc020.mcc714\",\"ts\":\"2025-03-31T04:44:07Z\"} {\"caller\":\"node_controller.go:69\",\"controller\":\"NodeReconciler\",\"end reconcile\":\"/gateway4.panclypcwl01.mnc020.mcc714\",\"level\":\"info\",\"ts\":\"2025-03-31T04:44:07Z\"} {\"caller\":\"node_controller.go:46\",\"controller\":\"NodeReconciler\",\"level\":\"info\",\"start reconcile\":\"/gateway3.panclypcwl01.mnc020.mcc714\",\"ts\":\"2025-03-31T04:44:12Z\"} {\"caller\":\"node_controller.go:69\",\"controller\":\"NodeReconciler\",\"end reconcile\":\"/gateway3.panclypcwl01.mnc020.mcc714\",\"level\":\"info\",\"ts\":\"2025-03-31T04:44:12Z\"} {\"caller\":\"node_controller.go:46\",\"controller\":\"NodeReconciler\",\"level\":\"info\",\"start reconcile\":\"/appworker16.panclypcwl01.mnc020.mcc714\",\"ts\":\"2025-03-31T04:44:21Z\"} {\"caller\":\"node_controller.go:69\",\"controller\":\"NodeReconciler\",\"end reconcile\":\"/appworker16.panclypcwl01.mnc020.mcc714\",\"level\":\"info\",\"ts\":\"2025-03-31T04:44:21Z\"} {\"caller\":\"node_controller.go:46\",\"controller\":\"NodeReconciler\",\"level\":\"info\",\"start reconcile\":\"/appworker20.panclypcwl01.mnc020.mcc714\",\"ts\":\"2025-03-31T04:44:34Z\"} {\"caller\":\"node_controller.go:69\",\"controller\":\"NodeReconciler\",\"end reconcile\":\"/appworker20.panclypcwl01.mnc020.mcc714\",\"level\":\"info\",\"ts\":\"2025-03-31T04:44:34Z\"} {\"caller\":\"node_controller.go:46\",\"controller\":\"NodeReconciler\",\"level\":\"info\",\"start reconcile\":\"/appworker28.panclypcwl01.mnc020.mcc714\",\"ts\":\"2025-03-31T04:44:44Z\"} {\"caller\":\"node_controller.go:69\",\"controller\":\"NodeReconciler\",\"end reconcile\":\"/appworker28.panclypcwl01.mnc020.mcc714\",\"level\":\"info\",\"ts\":\"2025-03-31T04:44:44Z\"} {\"caller\":\"node_controller.go:69\",\"controller\":\"NodeReconciler\",\"end reconcile\":\"/gateway4.panclypcwl01.mnc020.mcc714\",\"level\":\"info\",\"ts\":\"2025-03-31T04:44:07Z\"} {\"caller\":\"node_controller.go:46\",\"controller\":\"NodeReconciler\",\"level\":\"info\",\"start reconcile\":\"/gateway3.panclypcwl01.mnc020.mcc714\",\"ts\":\"2025-03-31T04:44:12Z\"} {\"caller\":\"speakerlist.go:274\",\"level\":\"info\",\"msg\":\"triggering discovery\",\"op\":\"memberDiscovery\",\"ts\":\"2025-03-31T04:44:12Z\"} {\"caller\":\"node_controller.go:69\",\"controller\":\"NodeReconciler\",\"end reconcile\":\"/gateway3.panclypcwl01.mnc020.mcc714\",\"level\":\"info\",\"ts\":\"2025-03-31T04:44:12Z\"} {\"caller\":\"node_controller.go:46\",\"controller\":\"NodeReconciler\",\"level\":\"info\",\"start reconcile\":\"/appworker16.panclypcwl01.mnc020.mcc714\",\"ts\":\"2025-03-31T04:44:21Z\"} {\"caller\":\"node_controller.go:69\",\"controller\":\"NodeReconciler\",\"end reconcile\":\"/appworker16.panclypcwl01.mnc020.mcc714\",\"level\":\"info\",\"ts\":\"2025-03-31T04:44:21Z\"} {\"caller\":\"node_controller.go:46\",\"controller\":\"NodeReconciler\",\"level\":\"info\",\"start reconcile\":\"/appworker20.panclypcwl01.mnc020.mcc714\",\"ts\":\"2025-03-31T04:44:34Z\"} {\"caller\":\"node_controller.go:69\",\"controller\":\"NodeReconciler\",\"end reconcile\":\"/appworker20.panclypcwl01.mnc020.mcc714\",\"level\":\"info\",\"ts\":\"2025-03-31T04:44:34Z\"} {\"caller\":\"node_controller.go:46\",\"controller\":\"NodeReconciler\",\"level\":\"info\",\"start reconcile\":\"/appworker28.panclypcwl01.mnc020.mcc714\",\"ts\":\"2025-03-31T04:44:44Z\"} {\"caller\":\"node_controller.go:69\",\"controller\":\"NodeReconciler\",\"end reconcile\":\"/appworker28.panclypcwl01.mnc020.mcc714\",\"level\":\"info\",\"ts\":\"2025-03-31T04:44:44Z\"} {\"caller\":\"node_controller.go:46\",\"controller\":\"NodeReconciler\",\"level\":\"info\",\"start reconcile\":\"/gateway4.panclypcwl01.mnc020.mcc714\",\"ts\":\"2025-03-31T04:44:07Z\"} {\"caller\":\"node_controller.go:69\",\"controller\":\"NodeReconciler\",\"end reconcile\":\"/gateway4.panclypcwl01.mnc020.mcc714\",\"level\":\"info\",\"ts\":\"2025-03-31T04:44:07Z\"} {\"caller\":\"node_controller.go:46\",\"controller\":\"NodeReconciler\",\"level\":\"info\",\"start reconcile\":\"/gateway3.panclypcwl01.mnc020.mcc714\",\"ts\":\"2025-03-31T04:44:12Z\"} {\"caller\":\"node_controller.go:69\",\"controller\":\"NodeReconciler\",\"end reconcile\":\"/gateway3.panclypcwl01.mnc020.mcc714\",\"level\":\"info\",\"ts\":\"2025-03-31T04:44:12Z\"} {\"caller\":\"node_controller.go:46\",\"controller\":\"NodeReconciler\",\"level\":\"info\",\"start reconcile\":\"/appworker16.panclypcwl01.mnc020.mcc714\",\"ts\":\"2025-03-31T04:44:21Z\"} {\"caller\":\"node_controller.go:69\",\"controller\":\"NodeReconciler\",\"end reconcile\":\"/appworker16.panclypcwl01.mnc020.mcc714\",\"level\":\"info\",\"ts\":\"2025-03-31T04:44:21Z\"} {\"caller\":\"node_controller.go:46\",\"controller\":\"NodeReconciler\",\"level\":\"info\",\"start reconcile\":\"/appworker20.panclypcwl01.mnc020.mcc714\",\"ts\":\"2025-03-31T04:44:34Z\"} {\"caller\":\"node_controller.go:69\",\"controller\":\"NodeReconciler\",\"end reconcile\":\"/appworker20.panclypcwl01.mnc020.mcc714\",\"level\":\"info\",\"ts\":\"2025-03-31T04:44:34Z\"} {\"caller\":\"node_controller.go:46\",\"controller\":\"NodeReconciler\",\"level\":\"info\",\"start reconcile\":\"/appworker28.panclypcwl01.mnc020.mcc714\",\"ts\":\"2025-03-31T04:44:44Z\"} {\"caller\":\"node_controller.go:69\",\"controller\":\"NodeReconciler\",\"end reconcile\":\"/appworker28.panclypcwl01.mnc020.mcc714\",\"level\":\"info\",\"ts\":\"2025-03-31T04:44:44Z\"} {\"caller\":\"speakerlist.go:274\",\"level\":\"info\",\"msg\":\"triggering discovery\",\"op\":\"memberDiscovery\",\"ts\":\"2025-03-31T04:44:07Z\"} {\"caller\":\"node_controller.go:69\",\"controller\":\"NodeReconciler\",\"end reconcile\":\"/gateway4.panclypcwl01.mnc020.mcc714\",\"level\":\"info\",\"ts\":\"2025-03-31T04:44:07Z\"} {\"caller\":\"node_controller.go:46\",\"controller\":\"NodeReconciler\",\"level\":\"info\",\"start reconcile\":\"/gateway3.panclypcwl01.mnc020.mcc714\",\"ts\":\"2025-03-31T04:44:12Z\"} {\"caller\":\"node_controller.go:69\",\"controller\":\"NodeReconciler\",\"end reconcile\":\"/gateway3.panclypcwl01.mnc020.mcc714\",\"level\":\"info\",\"ts\":\"2025-03-31T04:44:12Z\"} {\"caller\":\"node_controller.go:46\",\"controller\":\"NodeReconciler\",\"level\":\"info\",\"start reconcile\":\"/appworker16.panclypcwl01.mnc020.mcc714\",\"ts\":\"2025-03-31T04:44:21Z\"} {\"caller\":\"node_controller.go:69\",\"controller\":\"NodeReconciler\",\"end reconcile\":\"/appworker16.panclypcwl01.mnc020.mcc714\",\"level\":\"info\",\"ts\":\"2025-03-31T04:44:21Z\"} {\"caller\":\"node_controller.go:46\",\"controller\":\"NodeReconciler\",\"level\":\"info\",\"start reconcile\":\"/appworker20.panclypcwl01.mnc020.mcc714\",\"ts\":\"2025-03-31T04:44:34Z\"} {\"caller\":\"node_controller.go:69\",\"controller\":\"NodeReconciler\",\"end reconcile\":\"/appworker20.panclypcwl01.mnc020.mcc714\",\"level\":\"info\",\"ts\":\"2025-03-31T04:44:34Z\"} {\"caller\":\"node_controller.go:46\",\"controller\":\"NodeReconciler\",\"level\":\"info\",\"start reconcile\":\"/appworker28.panclypcwl01.mnc020.mcc714\",\"ts\":\"2025-03-31T04:44:44Z\"} {\"caller\":\"node_controller.go:69\",\"controller\":\"NodeReconciler\",\"end reconcile\":\"/appworker28.panclypcwl01.mnc020.mcc714\",\"level\":\"info\",\"ts\":\"2025-03-31T04:44:44Z\"} just showing an backwards route here for comparison and your destination should be exist here . [root@ncputility ~ pancwl_rc]$ oc get nncp -A -o yaml apiVersion: v1 items: - apiVersion: nmstate.io/v1 kind: NodeNetworkConfigurationPolicy metadata: annotations: kubectl.kubernetes.io/last-applied-configuration: | {\"apiVersion\":\"nmstate.io/v1\",\"kind\":\"NodeNetworkConfigurationPolicy\",\"metadata\":{\"annotations\":{},\"name\":\"backward-route-for-oam-pa-pa-metallb-vlan104\"},\"spec\":{\"desiredState\":{\"routes\":{\"config\":[{\"destination\":\"10.89.100.66/32\",\"metric\":150,\"next-hop-address\":\"10.89.97.161\",\"next-hop-interface\":\"vlan104\",\"table-id\":254},{\"destination\":\"10.89.27.4/32\",\"metric\":150,\"next-hop-address\":\"10.89.97.161\",\"next-hop-interface\":\"vlan104\",\"table-id\":254}]}},\"nodeSelector\":{\"node-role.kubernetes.io/gateway\":\"\"}}} nmstate.io/webhook-mutating-timestamp: \"1743107464714866687\" creationTimestamp: \"2025-03-27T20:31:04Z\" generation: 1 name: backward-route-for-oam-pa-pa-metallb-vlan104 resourceVersion: \"36077334\" uid: d72a67aa-44d0-403f-8ed5-29a49994eaf6 spec: desiredState: routes: config: - destination: 10.89.100.66/32 metric: 150 next-hop-address: 10.89.97.161 next-hop-interface: vlan104 table-id: 254 - destination: 10.89.27.4/32 metric: 150 next-hop-address: 10.89.97.161 next-hop-interface: vlan104 table-id: 254 nodeSelector: node-role.kubernetes.io/gateway: \"\" status: conditions: - lastHeartbeatTime: \"2025-03-27T20:32:10Z\" lastTransitionTime: \"2025-03-27T20:32:10Z\" message: 4/4 nodes successfully configured reason: SuccessfullyConfigured status: \"True\" type: Available - lastHeartbeatTime: \"2025-03-27T20:32:10Z\" lastTransitionTime: \"2025-03-27T20:32:10Z\" reason: SuccessfullyConfigured status: \"False\" type: Degraded - lastHeartbeatTime: \"2025-03-27T20:32:10Z\" lastTransitionTime: \"2025-03-27T20:32:10Z\" reason: ConfigurationProgressing status: \"False\" type: Progressing lastUnavailableNodeCountUpdate: \"2025-03-27T20:32:09Z\" Check the status of local and remote configuration by login into the metallb-system namespace. bdf and bgp should be fully up [root@ncputility ~ pancwl_rc]$ oc get -n metallb-system pods -l component=speaker -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES speaker-dlrn5 6/6 Running 0 24d 10.89.96.18 gateway2.panclypcwl01.mnc020.mcc714 <none> <none> speaker-g2g77 6/6 Running 0 24d 10.89.96.19 gateway3.panclypcwl01.mnc020.mcc714 <none> <none> speaker-jzbw7 6/6 Running 0 24d 10.89.96.17 gateway1.panclypcwl01.mnc020.mcc714 <none> <none> speaker-pjstl 6/6 Running 0 24d 10.89.96.20 gateway4.panclypcwl01.mnc020.mcc714 <none> <none> [root@ncputility ~ pancwl_rc]$ oc exec -n metallb-system speaker-dlrn5 -c frr -- vtysh -c \"show running-config\" Building configuration... Current configuration: ! frr version 8.3.1 frr defaults traditional hostname gateway2.panclypcwl01.mnc020.mcc714 log file /etc/frr/frr.log informational log timestamp precision 3 service integrated-vtysh-config ! router bgp 4200000320 no bgp ebgp-requires-policy no bgp default ipv4-unicast no bgp network import-check neighbor 10.85.187.34 remote-as 4200000320 neighbor 10.85.187.34 bfd neighbor 10.85.187.34 bfd profile ncp-metallb-oam-pa-sv-bfd-profile neighbor 10.85.187.34 timers 30 90 neighbor 10.85.187.35 remote-as 4200000320 neighbor 10.85.187.35 bfd neighbor 10.85.187.35 bfd profile ncp-metallb-oam-pa-sv-bfd-profile neighbor 10.85.187.35 timers 30 90 neighbor 10.86.10.98 remote-as 4200000320 neighbor 10.86.10.98 bfd neighbor 10.86.10.98 bfd profile ncp-metallb-oam-pa-ni-bfd-profile neighbor 10.86.10.98 timers 30 90 neighbor 10.86.10.99 remote-as 4200000320 neighbor 10.86.10.99 bfd neighbor 10.86.10.99 bfd profile ncp-metallb-oam-pa-ni-bfd-profile neighbor 10.86.10.99 timers 30 90 neighbor 10.89.97.162 remote-as 4200000320 neighbor 10.89.97.162 bfd neighbor 10.89.97.162 bfd profile ncp-metallb-oam-pa-pa-bfd-profile neighbor 10.89.97.162 timers 30 90 neighbor 10.89.97.163 remote-as 4200000320 neighbor 10.89.97.163 bfd neighbor 10.89.97.163 bfd profile ncp-metallb-oam-pa-pa-bfd-profile neighbor 10.89.97.163 timers 30 90 neighbor 10.89.147.194 remote-as 4200000320 neighbor 10.89.147.194 bfd neighbor 10.89.147.194 bfd profile ncp-metallb-oam-pa-hn-bfd-profile neighbor 10.89.147.194 timers 30 90 neighbor 10.89.147.195 remote-as 4200000320 neighbor 10.89.147.195 bfd neighbor 10.89.147.195 bfd profile ncp-metallb-oam-pa-hn-bfd-profile neighbor 10.89.147.195 timers 30 90 ! address-family ipv4 unicast network 10.89.97.210/32 neighbor 10.85.187.34 activate neighbor 10.85.187.34 route-map 10.85.187.34-in in neighbor 10.85.187.34 route-map 10.85.187.34-out out neighbor 10.85.187.35 activate neighbor 10.85.187.35 route-map 10.85.187.35-in in neighbor 10.85.187.35 route-map 10.85.187.35-out out neighbor 10.86.10.98 activate neighbor 10.86.10.98 route-map 10.86.10.98-in in neighbor 10.86.10.98 route-map 10.86.10.98-out out neighbor 10.86.10.99 activate neighbor 10.86.10.99 route-map 10.86.10.99-in in neighbor 10.86.10.99 route-map 10.86.10.99-out out neighbor 10.89.97.162 activate neighbor 10.89.97.162 route-map 10.89.97.162-in in neighbor 10.89.97.162 route-map 10.89.97.162-out out neighbor 10.89.97.163 activate neighbor 10.89.97.163 route-map 10.89.97.163-in in neighbor 10.89.97.163 route-map 10.89.97.163-out out neighbor 10.89.147.194 activate neighbor 10.89.147.194 route-map 10.89.147.194-in in neighbor 10.89.147.194 route-map 10.89.147.194-out out neighbor 10.89.147.195 activate neighbor 10.89.147.195 route-map 10.89.147.195-in in neighbor 10.89.147.195 route-map 10.89.147.195-out out exit-address-family ! address-family ipv6 unicast neighbor 10.85.187.34 activate neighbor 10.85.187.34 route-map 10.85.187.34-in in neighbor 10.85.187.34 route-map 10.85.187.34-out out neighbor 10.85.187.35 activate neighbor 10.85.187.35 route-map 10.85.187.35-in in neighbor 10.85.187.35 route-map 10.85.187.35-out out neighbor 10.86.10.98 activate neighbor 10.86.10.98 route-map 10.86.10.98-in in neighbor 10.86.10.98 route-map 10.86.10.98-out out neighbor 10.86.10.99 activate neighbor 10.86.10.99 route-map 10.86.10.99-in in neighbor 10.86.10.99 route-map 10.86.10.99-out out neighbor 10.89.97.162 activate neighbor 10.89.97.162 route-map 10.89.97.162-in in neighbor 10.89.97.162 route-map 10.89.97.162-out out neighbor 10.89.97.163 activate neighbor 10.89.97.163 route-map 10.89.97.163-in in neighbor 10.89.97.163 route-map 10.89.97.163-out out neighbor 10.89.147.194 activate neighbor 10.89.147.194 route-map 10.89.147.194-in in neighbor 10.89.147.194 route-map 10.89.147.194-out out neighbor 10.89.147.195 activate neighbor 10.89.147.195 route-map 10.89.147.195-in in neighbor 10.89.147.195 route-map 10.89.147.195-out out exit-address-family exit ! ip prefix-list 10.89.97.162-pl-ipv4 seq 1 permit 10.89.97.210/32 ip prefix-list 10.89.97.163-pl-ipv4 seq 1 permit 10.89.97.210/32 ip prefix-list 10.85.187.34-pl-ipv4 seq 1 deny any ip prefix-list 10.85.187.35-pl-ipv4 seq 1 deny any ip prefix-list 10.86.10.98-pl-ipv4 seq 1 deny any ip prefix-list 10.86.10.99-pl-ipv4 seq 1 deny any ip prefix-list 10.89.147.194-pl-ipv4 seq 1 deny any ip prefix-list 10.89.147.195-pl-ipv4 seq 1 deny any ! ipv6 prefix-list 10.89.97.162-pl-ipv4 seq 2 deny any ipv6 prefix-list 10.89.97.163-pl-ipv4 seq 2 deny any ipv6 prefix-list 10.85.187.34-pl-ipv4 seq 2 deny any ipv6 prefix-list 10.85.187.35-pl-ipv4 seq 2 deny any ipv6 prefix-list 10.86.10.98-pl-ipv4 seq 2 deny any ipv6 prefix-list 10.86.10.99-pl-ipv4 seq 2 deny any ipv6 prefix-list 10.89.147.194-pl-ipv4 seq 2 deny any ipv6 prefix-list 10.89.147.195-pl-ipv4 seq 2 deny any ! route-map 10.85.187.34-in deny 20 exit ! route-map 10.85.187.34-out permit 1 match ip address prefix-list 10.85.187.34-pl-ipv4 exit ! route-map 10.85.187.34-out permit 2 match ipv6 address prefix-list 10.85.187.34-pl-ipv4 exit ! route-map 10.85.187.35-in deny 20 exit ! route-map 10.85.187.35-out permit 1 match ip address prefix-list 10.85.187.35-pl-ipv4 exit ! route-map 10.85.187.35-out permit 2 match ipv6 address prefix-list 10.85.187.35-pl-ipv4 exit ! route-map 10.86.10.98-in deny 20 exit ! route-map 10.86.10.98-out permit 1 match ip address prefix-list 10.86.10.98-pl-ipv4 exit ! route-map 10.86.10.98-out permit 2 match ipv6 address prefix-list 10.86.10.98-pl-ipv4 exit ! route-map 10.86.10.99-in deny 20 exit ! route-map 10.86.10.99-out permit 1 match ip address prefix-list 10.86.10.99-pl-ipv4 exit ! route-map 10.86.10.99-out permit 2 match ipv6 address prefix-list 10.86.10.99-pl-ipv4 exit ! route-map 10.89.147.194-in deny 20 exit ! route-map 10.89.147.194-out permit 1 match ip address prefix-list 10.89.147.194-pl-ipv4 exit ! route-map 10.89.147.194-out permit 2 match ipv6 address prefix-list 10.89.147.194-pl-ipv4 exit ! route-map 10.89.147.195-in deny 20 exit ! route-map 10.89.147.195-out permit 1 match ip address prefix-list 10.89.147.195-pl-ipv4 exit ! route-map 10.89.147.195-out permit 2 match ipv6 address prefix-list 10.89.147.195-pl-ipv4 exit ! route-map 10.89.97.162-in deny 20 exit ! route-map 10.89.97.162-out permit 1 match ip address prefix-list 10.89.97.162-pl-ipv4 exit ! route-map 10.89.97.162-out permit 2 match ipv6 address prefix-list 10.89.97.162-pl-ipv4 exit ! route-map 10.89.97.163-in deny 20 exit ! route-map 10.89.97.163-out permit 1 match ip address prefix-list 10.89.97.163-pl-ipv4 exit ! route-map 10.89.97.163-out permit 2 match ipv6 address prefix-list 10.89.97.163-pl-ipv4 exit ! ip nht resolve-via-default ! ipv6 nht resolve-via-default ! bfd profile ncp-metallb-oam-pa-hn-bfd-profile passive-mode exit ! profile ncp-metallb-oam-pa-ni-bfd-profile passive-mode exit ! profile ncp-metallb-oam-pa-pa-bfd-profile passive-mode exit ! profile ncp-metallb-oam-pa-sv-bfd-profile passive-mode exit ! exit ! end [root@ncputility ~ pancwl_rc]$ oc exec -n metallb-system speaker-dlrn5 -c frr -- vtysh -c \"show bgp summary\" IPv4 Unicast Summary (VRF default): BGP router identifier 172.16.2.2, local AS number 4200000320 vrf-id 0 BGP table version 1 RIB entries 1, using 192 bytes of memory Peers 8, using 5788 KiB of memory Neighbor V AS MsgRcvd MsgSent TblVer InQ OutQ Up/Down State/PfxRcd PfxSnt Desc 10.85.187.34 4 4200000320 0 0 0 0 0 never Active 0 N/A 10.85.187.35 4 4200000320 0 0 0 0 0 never Active 0 N/A 10.86.10.98 4 4200000320 0 0 0 0 0 never Active 0 N/A 10.86.10.99 4 4200000320 0 0 0 0 0 never Active 0 N/A 10.89.97.162 4 4200000320 7306 7307 0 0 0 2d12h50m 0 1 N/A 10.89.97.163 4 4200000320 7305 7307 0 0 0 2d12h50m 0 1 N/A 10.89.147.194 4 4200000320 0 0 0 0 0 never Active 0 N/A 10.89.147.195 4 4200000320 0 0 0 0 0 never Active 0 N/A Total number of neighbors 8 IPv6 Unicast Summary (VRF default): BGP router identifier 172.16.2.2, local AS number 4200000320 vrf-id 0 BGP table version 0 RIB entries 0, using 0 bytes of memory Peers 8, using 5788 KiB of memory Neighbor V AS MsgRcvd MsgSent TblVer InQ OutQ Up/Down State/PfxRcd PfxSnt Desc 10.85.187.34 4 4200000320 0 0 0 0 0 never Active 0 N/A 10.85.187.35 4 4200000320 0 0 0 0 0 never Active 0 N/A 10.86.10.98 4 4200000320 0 0 0 0 0 never Active 0 N/A 10.86.10.99 4 4200000320 0 0 0 0 0 never Active 0 N/A 10.89.97.162 4 4200000320 7306 7307 0 0 0 2d12h50m NoNeg NoNeg N/A 10.89.97.163 4 4200000320 7305 7307 0 0 0 2d12h50m NoNeg NoNeg N/A 10.89.147.194 4 4200000320 0 0 0 0 0 never Active 0 N/A 10.89.147.195 4 4200000320 0 0 0 0 0 never Active 0 N/A Total number of neighbors 8 [root@ncputility ~ pancwl_rc]$ oc exec -n metallb-system speaker-dlrn5 -c frr -- vtysh -c \"show bfd peers brief\" Session count: 2 SessionId LocalAddress PeerAddress Status ========= ============ =========== ====== 62069252 10.89.97.165 10.89.97.162 down 3159552171 10.89.97.165 10.89.97.163 down [root@ncputility ~ pancwl_rc]$ oc exec -n metallb-system speaker-dlrn5 -c frr -- vtysh -c \"show bfd peer\" BFD Peers: peer 10.89.97.162 local-address 10.89.97.165 vrf default interface vlan104 ID: 62069252 Remote ID: 0 Passive mode Status: down Downtime: 2 day(s), 10 hour(s), 3 minute(s), 16 second(s) Diagnostics: ok Remote diagnostics: ok Peer Type: dynamic Local timers: Detect-multiplier: 3 Receive interval: 300ms Transmission interval: 300ms Echo receive interval: 50ms Echo transmission interval: disabled Remote timers: Detect-multiplier: 3 Receive interval: 1000ms Transmission interval: 1000ms Echo receive interval: disabled peer 10.89.97.163 local-address 10.89.97.165 vrf default interface vlan104 ID: 3159552171 Remote ID: 0 Passive mode Status: down Downtime: 2 day(s), 10 hour(s), 3 minute(s), 16 second(s) Diagnostics: ok Remote diagnostics: ok Peer Type: dynamic Local timers: Detect-multiplier: 3 Receive interval: 300ms Transmission interval: 300ms Echo receive interval: 50ms Echo transmission interval: disabled Remote timers: Detect-multiplier: 3 Receive interval: 1000ms Transmission interval: 1000ms Echo receive interval: disabled [root@ncputility ~ pancwl_rc]$ oc exec -n metallb-system speaker-dlrn5 -c frr -- vtysh -c \"show ip bgp neighbors\" BGP neighbor is 10.85.187.34, remote AS 4200000320, local AS 4200000320, internal link BGP version 4, remote router ID 0.0.0.0, local router ID 172.16.2.2 BGP state = Active Last read 2d10h03m, Last write never Hold time is 90, keepalive interval is 30 seconds Configured hold time is 90, keepalive interval is 30 seconds Configured conditional advertisements interval is 60 seconds Graceful restart information: Local GR Mode: Helper* Remote GR Mode: NotApplicable R bit: False N bit: False Timers: Configured Restart Time(sec): 120 Received Restart Time(sec): 0 Message statistics: Inq depth is 0 Outq depth is 0 Sent Rcvd Opens: 0 0 Notifications: 0 0 Updates: 0 0 Keepalives: 0 0 Route Refresh: 0 0 Capability: 0 0 Total: 0 0 Minimum time between advertisement runs is 0 seconds For address family: IPv4 Unicast Not part of any update group Community attribute sent to this neighbor(all) Inbound path policy configured Outbound path policy configured Route map for incoming advertisements is *10.85.187.34-in Route map for outgoing advertisements is *10.85.187.34-out 0 accepted prefixes For address family: IPv6 Unicast Not part of any update group Community attribute sent to this neighbor(all) Inbound path policy configured Outbound path policy configured Route map for incoming advertisements is *10.85.187.34-in Route map for outgoing advertisements is *10.85.187.34-out 0 accepted prefixes Connections established 0; dropped 0 Last reset 2d10h03m, Waiting for peer OPEN BGP Connect Retry Timer in Seconds: 120 Next connect timer due in 80 seconds Read thread: off Write thread: off FD used: -1 BFD: Type: multi hop Detect Multiplier: 3, Min Rx interval: 300, Min Tx interval: 300 Status: Unknown, Last update: never BGP neighbor is 10.85.187.35, remote AS 4200000320, local AS 4200000320, internal link BGP version 4, remote router ID 0.0.0.0, local router ID 172.16.2.2 BGP state = Active Last read 2d10h03m, Last write never Hold time is 90, keepalive interval is 30 seconds Configured hold time is 90, keepalive interval is 30 seconds Configured conditional advertisements interval is 60 seconds Graceful restart information: Local GR Mode: Helper* Remote GR Mode: NotApplicable R bit: False N bit: False Timers: Configured Restart Time(sec): 120 Received Restart Time(sec): 0 Message statistics: Inq depth is 0 Outq depth is 0 Sent Rcvd Opens: 0 0 Notifications: 0 0 Updates: 0 0 Keepalives: 0 0 Route Refresh: 0 0 Capability: 0 0 Total: 0 0 Minimum time between advertisement runs is 0 seconds For address family: IPv4 Unicast Not part of any update group Community attribute sent to this neighbor(all) Inbound path policy configured Outbound path policy configured Route map for incoming advertisements is *10.85.187.35-in Route map for outgoing advertisements is *10.85.187.35-out 0 accepted prefixes For address family: IPv6 Unicast Not part of any update group Community attribute sent to this neighbor(all) Inbound path policy configured Outbound path policy configured Route map for incoming advertisements is *10.85.187.35-in Route map for outgoing advertisements is *10.85.187.35-out 0 accepted prefixes Connections established 0; dropped 0 Last reset 2d10h03m, Waiting for peer OPEN BGP Connect Retry Timer in Seconds: 120 Next connect timer due in 80 seconds Read thread: off Write thread: off FD used: -1 BFD: Type: multi hop Detect Multiplier: 3, Min Rx interval: 300, Min Tx interval: 300 Status: Unknown, Last update: never BGP neighbor is 10.86.10.98, remote AS 4200000320, local AS 4200000320, internal link BGP version 4, remote router ID 0.0.0.0, local router ID 172.16.2.2 BGP state = Active Last read 2d10h03m, Last write never Hold time is 90, keepalive interval is 30 seconds Configured hold time is 90, keepalive interval is 30 seconds Configured conditional advertisements interval is 60 seconds Graceful restart information: Local GR Mode: Helper* Remote GR Mode: NotApplicable R bit: False N bit: False Timers: Configured Restart Time(sec): 120 Received Restart Time(sec): 0 Message statistics: Inq depth is 0 Outq depth is 0 Sent Rcvd Opens: 0 0 Notifications: 0 0 Updates: 0 0 Keepalives: 0 0 Route Refresh: 0 0 Capability: 0 0 Total: 0 0 Minimum time between advertisement runs is 0 seconds For address family: IPv4 Unicast Not part of any update group Community attribute sent to this neighbor(all) Inbound path policy configured Outbound path policy configured Route map for incoming advertisements is *10.86.10.98-in Route map for outgoing advertisements is *10.86.10.98-out 0 accepted prefixes For address family: IPv6 Unicast Not part of any update group Community attribute sent to this neighbor(all) Inbound path policy configured Outbound path policy configured Route map for incoming advertisements is *10.86.10.98-in Route map for outgoing advertisements is *10.86.10.98-out 0 accepted prefixes Connections established 0; dropped 0 Last reset 2d10h03m, Waiting for peer OPEN BGP Connect Retry Timer in Seconds: 120 Next connect timer due in 80 seconds Read thread: off Write thread: off FD used: -1 BFD: Type: multi hop Detect Multiplier: 3, Min Rx interval: 300, Min Tx interval: 300 Status: Unknown, Last update: never BGP neighbor is 10.86.10.99, remote AS 4200000320, local AS 4200000320, internal link BGP version 4, remote router ID 0.0.0.0, local router ID 172.16.2.2 BGP state = Active Last read 2d10h03m, Last write never Hold time is 90, keepalive interval is 30 seconds Configured hold time is 90, keepalive interval is 30 seconds Configured conditional advertisements interval is 60 seconds Graceful restart information: Local GR Mode: Helper* Remote GR Mode: NotApplicable R bit: False N bit: False Timers: Configured Restart Time(sec): 120 Received Restart Time(sec): 0 Message statistics: Inq depth is 0 Outq depth is 0 Sent Rcvd Opens: 0 0 Notifications: 0 0 Updates: 0 0 Keepalives: 0 0 Route Refresh: 0 0 Capability: 0 0 Total: 0 0 Minimum time between advertisement runs is 0 seconds For address family: IPv4 Unicast Not part of any update group Community attribute sent to this neighbor(all) Inbound path policy configured Outbound path policy configured Route map for incoming advertisements is *10.86.10.99-in Route map for outgoing advertisements is *10.86.10.99-out 0 accepted prefixes For address family: IPv6 Unicast Not part of any update group Community attribute sent to this neighbor(all) Inbound path policy configured Outbound path policy configured Route map for incoming advertisements is *10.86.10.99-in Route map for outgoing advertisements is *10.86.10.99-out 0 accepted prefixes Connections established 0; dropped 0 Last reset 2d10h03m, Waiting for peer OPEN BGP Connect Retry Timer in Seconds: 120 Next connect timer due in 80 seconds Read thread: off Write thread: off FD used: -1 BFD: Type: multi hop Detect Multiplier: 3, Min Rx interval: 300, Min Tx interval: 300 Status: Unknown, Last update: never BGP neighbor is 10.89.97.162, remote AS 4200000320, local AS 4200000320, internal link BGP version 4, remote router ID 10.29.90.34, local router ID 172.16.2.2 BGP state = Established, up for 2d12h53m Last read 00:00:00, Last write 00:00:03 Hold time is 90, keepalive interval is 30 seconds Configured hold time is 90, keepalive interval is 30 seconds Configured conditional advertisements interval is 60 seconds Neighbor capabilities: 4 Byte AS: advertised and received Extended Message: advertised AddPath: IPv4 Unicast: RX advertised IPv6 Unicast: RX advertised Long-lived Graceful Restart: advertised Route refresh: advertised and received(new) Enhanced Route Refresh: advertised Address Family IPv4 Unicast: advertised and received Address Family IPv6 Unicast: advertised Hostname Capability: advertised (name: gateway2.panclypcwl01.mnc020.mcc714,domain name: n/a) not received Graceful Restart Capability: advertised and received Remote Restart timer is 300 seconds Address families by peer: none Graceful restart information: End-of-RIB send: IPv4 Unicast End-of-RIB received: IPv4 Unicast Local GR Mode: Helper* Remote GR Mode: Helper R bit: False N bit: False Timers: Configured Restart Time(sec): 120 Received Restart Time(sec): 300 IPv4 Unicast: F bit: False End-of-RIB sent: Yes End-of-RIB sent after update: Yes End-of-RIB received: Yes Timers: Configured Stale Path Time(sec): 360 IPv6 Unicast: F bit: False End-of-RIB sent: No End-of-RIB sent after update: No End-of-RIB received: No Timers: Configured Stale Path Time(sec): 360 Message statistics: Inq depth is 0 Outq depth is 0 Sent Rcvd Opens: 3 1 Notifications: 0 0 Updates: 2 2 Keepalives: 7307 7308 Route Refresh: 0 0 Capability: 0 0 Total: 7312 7311 Minimum time between advertisement runs is 0 seconds For address family: IPv4 Unicast Update group 3, subgroup 3 Packet Queue length 0 Community attribute sent to this neighbor(all) Inbound path policy configured Outbound path policy configured Route map for incoming advertisements is *10.89.97.162-in Route map for outgoing advertisements is *10.89.97.162-out 0 accepted prefixes For address family: IPv6 Unicast Not part of any update group Community attribute sent to this neighbor(all) Inbound path policy configured Outbound path policy configured Route map for incoming advertisements is *10.89.97.162-in Route map for outgoing advertisements is *10.89.97.162-out 0 accepted prefixes Connections established 1; dropped 0 Last reset 2d12h55m, Waiting for peer OPEN Local host: 10.89.97.165, Local port: 37680 Foreign host: 10.89.97.162, Foreign port: 179 Nexthop: 10.89.97.165 Nexthop global: :: Nexthop local: :: BGP connection: shared network BGP Connect Retry Timer in Seconds: 120 Read thread: on Write thread: on FD used: 26 BFD: Type: single hop Detect Multiplier: 3, Min Rx interval: 300, Min Tx interval: 300 Status: Down, Last update: 2:10:03:42 BGP neighbor is 10.89.97.163, remote AS 4200000320, local AS 4200000320, internal link BGP version 4, remote router ID 10.29.90.38, local router ID 172.16.2.2 BGP state = Established, up for 2d12h53m Last read 00:00:29, Last write 00:00:03 Hold time is 90, keepalive interval is 30 seconds Configured hold time is 90, keepalive interval is 30 seconds Configured conditional advertisements interval is 60 seconds Neighbor capabilities: 4 Byte AS: advertised and received Extended Message: advertised AddPath: IPv4 Unicast: RX advertised IPv6 Unicast: RX advertised Long-lived Graceful Restart: advertised Route refresh: advertised and received(new) Enhanced Route Refresh: advertised Address Family IPv4 Unicast: advertised and received Address Family IPv6 Unicast: advertised Hostname Capability: advertised (name: gateway2.panclypcwl01.mnc020.mcc714,domain name: n/a) not received Graceful Restart Capability: advertised and received Remote Restart timer is 300 seconds Address families by peer: none Graceful restart information: End-of-RIB send: IPv4 Unicast End-of-RIB received: IPv4 Unicast Local GR Mode: Helper* Remote GR Mode: Helper R bit: False N bit: False Timers: Configured Restart Time(sec): 120 Received Restart Time(sec): 300 IPv4 Unicast: F bit: False End-of-RIB sent: Yes End-of-RIB sent after update: Yes End-of-RIB received: Yes Timers: Configured Stale Path Time(sec): 360 IPv6 Unicast: F bit: False End-of-RIB sent: No End-of-RIB sent after update: No End-of-RIB received: No Timers: Configured Stale Path Time(sec): 360 Message statistics: Inq depth is 0 Outq depth is 0 Sent Rcvd Opens: 3 1 Notifications: 0 0 Updates: 2 1 Keepalives: 7307 7307 Route Refresh: 0 0 Capability: 0 0 Total: 7312 7309 Minimum time between advertisement runs is 0 seconds For address family: IPv4 Unicast Update group 4, subgroup 4 Packet Queue length 0 Community attribute sent to this neighbor(all) Inbound path policy configured Outbound path policy configured Route map for incoming advertisements is *10.89.97.163-in Route map for outgoing advertisements is *10.89.97.163-out 0 accepted prefixes For address family: IPv6 Unicast Not part of any update group Community attribute sent to this neighbor(all) Inbound path policy configured Outbound path policy configured Route map for incoming advertisements is *10.89.97.163-in Route map for outgoing advertisements is *10.89.97.163-out 0 accepted prefixes Connections established 1; dropped 0 Last reset 2d12h55m, Waiting for peer OPEN Local host: 10.89.97.165, Local port: 54688 Foreign host: 10.89.97.163, Foreign port: 179 Nexthop: 10.89.97.165 Nexthop global: :: Nexthop local: :: BGP connection: shared network BGP Connect Retry Timer in Seconds: 120 Read thread: on Write thread: on FD used: 25 BFD: Type: single hop Detect Multiplier: 3, Min Rx interval: 300, Min Tx interval: 300 Status: Down, Last update: 2:10:03:42 BGP neighbor is 10.89.147.194, remote AS 4200000320, local AS 4200000320, internal link BGP version 4, remote router ID 0.0.0.0, local router ID 172.16.2.2 BGP state = Active Last read 2d10h03m, Last write never Hold time is 90, keepalive interval is 30 seconds Configured hold time is 90, keepalive interval is 30 seconds Configured conditional advertisements interval is 60 seconds Graceful restart information: Local GR Mode: Helper* Remote GR Mode: NotApplicable R bit: False N bit: False Timers: Configured Restart Time(sec): 120 Received Restart Time(sec): 0 Message statistics: Inq depth is 0 Outq depth is 0 Sent Rcvd Opens: 0 0 Notifications: 0 0 Updates: 0 0 Keepalives: 0 0 Route Refresh: 0 0 Capability: 0 0 Total: 0 0 Minimum time between advertisement runs is 0 seconds For address family: IPv4 Unicast Not part of any update group Community attribute sent to this neighbor(all) Inbound path policy configured Outbound path policy configured Route map for incoming advertisements is *10.89.147.194-in Route map for outgoing advertisements is *10.89.147.194-out 0 accepted prefixes For address family: IPv6 Unicast Not part of any update group Community attribute sent to this neighbor(all) Inbound path policy configured Outbound path policy configured Route map for incoming advertisements is *10.89.147.194-in Route map for outgoing advertisements is *10.89.147.194-out 0 accepted prefixes Connections established 0; dropped 0 Last reset 2d10h03m, Waiting for peer OPEN BGP Connect Retry Timer in Seconds: 120 Next connect timer due in 80 seconds Read thread: off Write thread: off FD used: -1 BFD: Type: multi hop Detect Multiplier: 3, Min Rx interval: 300, Min Tx interval: 300 Status: Unknown, Last update: never BGP neighbor is 10.89.147.195, remote AS 4200000320, local AS 4200000320, internal link BGP version 4, remote router ID 0.0.0.0, local router ID 172.16.2.2 BGP state = Active Last read 2d10h03m, Last write never Hold time is 90, keepalive interval is 30 seconds Configured hold time is 90, keepalive interval is 30 seconds Configured conditional advertisements interval is 60 seconds Graceful restart information: Local GR Mode: Helper* Remote GR Mode: NotApplicable R bit: False N bit: False Timers: Configured Restart Time(sec): 120 Received Restart Time(sec): 0 Message statistics: Inq depth is 0 Outq depth is 0 Sent Rcvd Opens: 0 0 Notifications: 0 0 Updates: 0 0 Keepalives: 0 0 Route Refresh: 0 0 Capability: 0 0 Total: 0 0 Minimum time between advertisement runs is 0 seconds For address family: IPv4 Unicast Not part of any update group Community attribute sent to this neighbor(all) Inbound path policy configured Outbound path policy configured Route map for incoming advertisements is *10.89.147.195-in Route map for outgoing advertisements is *10.89.147.195-out 0 accepted prefixes For address family: IPv6 Unicast Not part of any update group Community attribute sent to this neighbor(all) Inbound path policy configured Outbound path policy configured Route map for incoming advertisements is *10.89.147.195-in Route map for outgoing advertisements is *10.89.147.195-out 0 accepted prefixes Connections established 0; dropped 0 Last reset 2d10h03m, Waiting for peer OPEN BGP Connect Retry Timer in Seconds: 120 Next connect timer due in 80 seconds Read thread: off Write thread: off FD used: -1 BFD: Type: multi hop Detect Multiplier: 3, Min Rx interval: 300, Min Tx interval: 300 Status: Unknown, Last update: never some more additional test to show the local testing here [root@ncputility ~ pancwl_rc]$ oc get svc -A -o wide |grep -i loadbalan ncom01pan ncom01pan-citm-ingress LoadBalancer 172.20.138.133 10.89.97.210 80:32432/TCP,443:32622/TCP,2309:30135/TCP 10d app=citm-ingress,component=controller,release=ncom01pan-citm-ingress [root@ncputility ~ pancwl_rc]$ nslookup 10.89.97.210 210.97.89.10.in-addr.arpa name = ncom01.panclyncom01.mnc020.mcc714. [root@ncputility ~ pancwl_rc]$ curl -k https://ncom01.panclyncom01.mnc020.mcc714/ ^C [root@ncputility ~ pancwl_rc]$ ip r get 10.89.97.210 10.89.97.210 via 10.89.100.65 dev br308 src 10.89.100.66 uid 0 cache [root@ncputility ~ pancwl_rc]$ tracepath 10.89.97.210 1?: [LOCALHOST] pmtu 1500 1: _gateway 0.184ms 1: _gateway 0.290ms 2: no reply 3: 10.89.97.129 0.284ms asymm 1 4: no reply 5: 10.89.97.129 0.264ms asymm 1 6: no reply 7: 10.89.97.129 0.281ms asymm 1 8: no reply 9: 10.89.97.129 0.279ms asymm 1 10: no reply 11: 10.89.97.129 0.269ms asymm 1 12: no reply 13: 10.89.97.129 0.280ms asymm 1 14: no reply 15: 10.89.97.129 0.234ms asymm 1 16: no reply 17: 10.89.97.129 0.297ms asymm 1 18: no reply 19: 10.89.97.129 0.328ms asymm 1 20: no reply 21: 10.89.97.129 0.292ms asymm 1 22: no reply 23: 10.89.97.129 0.336ms asymm 1 24: no reply 25: 10.89.97.129 0.332ms asymm 1 26: no reply 27: 10.89.97.129 0.330ms asymm 1 28: no reply 29: 10.89.97.129 0.332ms asymm 1 ^C [root@ncputility ~ pancwl_rc]$ oc get -n metallb-system pods -l component=speaker -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES speaker-dlrn5 6/6 Running 0 24d 10.89.96.18 gateway2.panclypcwl01.mnc020.mcc714 <none> <none> speaker-g2g77 6/6 Running 0 24d 10.89.96.19 gateway3.panclypcwl01.mnc020.mcc714 <none> <none> speaker-jzbw7 6/6 Running 0 24d 10.89.96.17 gateway1.panclypcwl01.mnc020.mcc714 <none> <none> speaker-pjstl 6/6 Running 0 24d 10.89.96.20 gateway4.panclypcwl01.mnc020.mcc714 <none> <none> [root@ncputility ~ pancwl_rc]$ oc -n metallb-system exec -it -c frr vtysh error: you must specify at least one command for the container [root@ncputility ~ pancwl_rc]$ oc -n metallb-system exec -it speaker-dlrn5 -c frr vtysh kubectl exec [POD] [COMMAND] is DEPRECATED and will be removed in a future version. Use kubectl exec [POD] -- [COMMAND] instead. Hello, this is FRRouting (version 8.3.1). Copyright 1996-2005 Kunihiro Ishiguro, et al. gateway2.panclypcwl01.mnc020.mcc714# curl % Unknown command: curl gateway2.panclypcwl01.mnc020.mcc714# exit [root@ncputility ~ pancwl_rc]$ oc debug -t node/gateway3.panclypcwl01.mnc020.mcc714 Temporary namespace openshift-debug-8j8fb is created for debugging node... Starting pod/gateway3panclypcwl01mnc020mcc714-debug-qsrhj ... To use host binaries, run `chroot /host` Pod IP: 10.89.96.19 If you don't see a command prompt, try pressing enter. sh-5.1# chroot /host sh-5.1# ping 10.89.97.163 PING 10.89.97.163 (10.89.97.163) 56(84) bytes of data. From 10.89.97.166 icmp_seq=1 Destination Host Unreachable From 10.89.97.166 icmp_seq=2 Destination Host Unreachable From 10.89.97.166 icmp_seq=3 Destination Host Unreachable ^C --- 10.89.97.163 ping statistics --- 4 packets transmitted, 0 received, +3 errors, 100% packet loss, time 3090ms pipe 3 sh-5.1# ping 10.89.97.162 PING 10.89.97.162 (10.89.97.162) 56(84) bytes of data. From 10.89.97.166 icmp_seq=1 Destination Host Unreachable From 10.89.97.166 icmp_seq=2 Destination Host Unreachable From 10.89.97.166 icmp_seq=3 Destination Host Unreachable From 10.89.97.166 icmp_seq=4 Destination Host Unreachable ^C --- 10.89.97.162 ping statistics --- 5 packets transmitted, 0 received, +4 errors, 100% packet loss, time 4064ms pipe 4 sh-5.1# ip r g 10.89.97.163 10.89.97.163 via 10.89.97.161 dev vlan104 src 10.89.97.166 uid 0 cache sh-5.1# ip r g 10.89.97.162 10.89.97.162 via 10.89.97.161 dev vlan104 src 10.89.97.166 uid 0 cache sh-5.1# curl -k https://ncom01.panclyncom01.mnc020.mcc714/ <!doctype html><html lang=\"en\"><head><meta charset=\"utf-8\"><meta name=\"viewport\" content=\"width=device-width,initial-scale=1,shrink-to-fit=no\"><meta name=\"theme-color\" content=\"#000000\"><link rel=\"manifest\" href=\"/manifest.json\"><link rel=\"shortcut icon\" href=\"/favicon.ico\"><title>Nokia Cloud Operations Manager</title><link href=\"/static/css/27.2dbd0a91.chunk.css\" rel=\"stylesheet\"><link href=\"/static/css/main.167b01c4.chunk.css\" rel=\"stylesheet\"></head><body><noscript>You need to enable JavaScript to run this app.</noscript><div id=\"root\"></div><script>!function(e){function t(t){for(var r,o,l=t[0],c=t[1],s=t[2],u=0,d=[];u<l.length;u++)o=l[u],Object.prototype.hasOwnProperty.call(n,o)&&n[o]&&d.push(n[o][0]),n[o]=0;for(r in c)Object.prototype.hasOwnProperty.call(c,r)&&(e[r]=c[r]);for(f&&f(t);d.length;)d.shift()();return i.push.apply(i,s||[]),a()}function a(){for(var e,t=0;t<i.length;t++){for(var a=i[t],r=!0,o=1;o<a.length;o++){var c=a[o];0!==n[c]&&(r=!1)}r&&(i.splice(t--,1),e=l(l.s=a[0]))}return e}var r={},o={25:0},n={25:0},i=[];function l(t){if(r[t])return r[t].exports;var a=r[t]={i:t,l:!1,exports:{}};return e[t].call(a.exports,a,a.exports,l),a.l=!0,a.exports}l.e=function(e){var t=[];o[e]?t.push(o[e]):0!==o[e]&&{2:1,5:1,6:1,7:1,8:1,11:1,12:1,13:1,14:1,15:1,16:1,17:1,18:1,19:1,20:1,21:1,22:1,26:1}[e]&&t.push(o[e]=new Promise((function(t,a){for(var r=\"static/css/\"+({5:\"[AlarmDetailsPage]\",6:\"[AlarmPage]\",7:\"[CatalogDetailsPage]\",8:\"[CatalogTablePage]\",9:\"[CloudFlowLogin]\",10:\"[CloudFlowLogout]\",11:\"[CsvExporterDetailsPage]\",12:\"[DashboardPage]\",13:\"[ExecutionDetailsPage]\",14:\"[JobDetailsPage]\",15:\"[JobsTablePage]\",16:\"[ManagedWorkloadsTablePage]\",17:\"[NSDetailsContainerPage]\",18:\"[OperationsDetailsPage]\",19:\"[ResourceCompositionDetailsPage]\",20:\"[ResourceCompositionsPage]\",21:\"[ResourcesTablePage]\",22:\"[VimDetailsPage]\",23:\"[loginPage]\"}[e]||e)+\".\"+{0:\"31d6cfe0\",1:\"31d6cfe0\",2:\"559272a8\",3:\"31d6cfe0\",4:\"31d6cfe0\",5:\"affca9cb\",6:\"affca9cb\",7:\"91932cb4\",8:\"bf9ba349\",9:\"31d6cfe0\",10:\"31d6cfe0\",11:\"a0ed0644\",12:\"b2dca462\",13:\"b77ab692\",14:\"4ca14441\",15:\"f59232c1\",16:\"c1e044f2\",17:\"398f0759\",18:\"d8693b3e\",19:\"7b4dfbce\",20:\"d9277a02\",21:\"03edf692\",22:\"b6690f64\",23:\"31d6cfe0\",26:\"4391d164\",28:\"31d6cfe0\",29:\"31d6cfe0\"}[e]+\".chunk.css\",n=l.p+r,i=document.getElementsByTagName(\"link\"),c=0;c<i.length;c++){var s=(f=i[c]).getAttribute(\"data-href\")||f.getAttribute(\"href\");if(\"stylesheet\"===f.rel&&(s===r||s===n))return t()}var u=document.getElementsByTagName(\"style\");for(c=0;c<u.length;c++){var f;if((s=(f=u[c]).getAttribute(\"data-href\"))===r||s===n)return t()}var d=document.createElement(\"link\");d.rel=\"stylesheet\",d.type=\"text/css\",d.onload=t,d.onerror=function(t){var r=t&&t.target&&t.target.src||n,i=new Error(\"Loading CSS chunk \"+e+\" failed.\\n(\"+r+\")\");i.code=\"CSS_CHUNK_LOAD_FAILED\",i.request=r,delete o[e],d.parentNode.removeChild(d),a(i)},d.href=n,document.getElementsByTagName(\"head\")[0].appendChild(d)})).then((function(){o[e]=0})));var a=n[e];if(0!==a)if(a)t.push(a[2]);else{var r=new Promise((function(t,r){a=n[e]=[t,r]}));t.push(a[2]=r);var i,c=document.createElement(\"script\");c.charset=\"utf-8\",c.timeout=120,l.nc&&c.setAttribute(\"nonce\",l.nc),c.src=function(e){return l.p+\"static/js/\"+({5:\"[AlarmDetailsPage]\",6:\"[AlarmPage]\",7:\"[CatalogDetailsPage]\",8:\"[CatalogTablePage]\",9:\"[CloudFlowLogin]\",10:\"[CloudFlowLogout]\",11:\"[CsvExporterDetailsPage]\",12:\"[DashboardPage]\",13:\"[ExecutionDetailsPage]\",14:\"[JobDetailsPage]\",15:\"[JobsTablePage]\",16:\"[ManagedWorkloadsTablePage]\",17:\"[NSDetailsContainerPage]\",18:\"[OperationsDetailsPage]\",19:\"[ResourceCompositionDetailsPage]\",20:\"[ResourceCompositionsPage]\",21:\"[ResourcesTablePage]\",22:\"[VimDetailsPage]\",23:\"[loginPage]\"}[e]||e)+\".\"+{0:\"9ea1f8f1\",1:\"d7647e6d\",2:\"49e46739\",3:\"6bb32e31\",4:\"5066ed38\",5:\"3e390b6c\",6:\"88b37376\",7:\"e7438c70\",8:\"f547dc3b\",9:\"c157abf8\",10:\"c3c2011c\",11:\"62ea6190\",12:\"c002dc82\",13:\"f0d807bf\",14:\"cee9c63a\",15:\"737e42f0\",16:\"2aae2df5\",17:\"4a2ec46d\",18:\"4777a959\",19:\"f28d098b\",20:\"7e96e980\",21:\"abc77c4e\",22:\"9d55d492\",23:\"44757b03\",26:\"c04605d3\",28:\"52a01c15\",29:\"f0708fbb\"}[e]+\".chunk.js\"}(e);var s=new Error;i=function(t){c.onerror=c.onload=null,clearTimeout(u);var a=n[e];if(0!==a){if(a){var r=t&&(\"load\"===t.type?\"missing\":t.type),o=t&&t.target&&t.target.src;s.message=\"Loading chunk \"+e+\" failed.\\n(\"+r+\": \"+o+\")\",s.name=\"ChunkLoadError\",s.type=r,s.request=o,a[1](s)}n[e]=void 0}};var u=setTimeout((function(){i({type:\"timeout\",target:c})}),12e4);c.onerror=c.onload=i,document.head.appendChild(c)}return Promise.all(t)},l.m=e,l.c=r,l.d=function(e,t,a){l.o(e,t)||Object.defineProperty(e,t,{enumerable:!0,get:a})},l.r=function(e){\"undefined\"!=typeof Symbol&&Symbol.toStringTag&&Object.defineProperty(e,Symbol.toStringTag,{value:\"Module\"}),Object.defineProperty(e,\"__esModule\",{value:!0})},l.t=function(e,t){if(1&t&&(e=l(e)),8&t)return e;if(4&t&&\"object\"==typeof e&&e&&e.__esModule)return e;var a=Object.create(null);if(l.r(a),Object.defineProperty(a,\"default\",{enumerable:!0,value:e}),2&t&&\"string\"!=typeof e)for(var r in e)l.d(a,r,function(t){return e[t]}.bind(null,r));return a},l.n=function(e){var t=e&&e.__esModule?function(){return e.default}:function(){return e};return l.d(t,\"a\",t),t},l.o=function(e,t){return Object.prototype.hasOwnProperty.call(e,t)},l.p=\"/\",l.oe=function(e){throw console.error(e),e};var c=this.webpackJsonpfrontend=this.webpackJsonpfrontend||[],s=c.push.bind(c);c.push=t,c=c.slice();for(var u=0;u<c.length;u++)t(c[u]);var f=s;a()}([])</script><script src=\"/static/js/27.08ffca3e.chunk.js\"></script><script src=\"/static/js/main.90334f28.chunk.js\"></script></body></html>sh-5.1#","title":"MetalLB Troubleshooting"},{"location":"openshift/networking/metalb-troubleshooting/#metalb-configuration-troubleshooting","text":"login to cluster [root@ncputility ~ pancwl_rc]$ source /root/pancwlrc WARNING: Using insecure TLS client config. Setting this option is not supported! Login successful. You have access to 113 projects, the list has been suppressed. You can list all projects with 'oc projects' Using project \"ncd01pan\". check for metallb-system namespace exist [root@ncputility ~ pancwl_rc]$ oc get ns | grep -i metallb metallb-system Active 26d [root@ncputility ~ pancwl_rc]$ check the status of the pods on the metallb-system namespace here. [root@ncputility ~ pancwl_rc]$ oc get pods -n metallb-system NAME READY STATUS RESTARTS AGE controller-5785bc85cb-qpk8h 2/2 Running 0 24d metallb-operator-controller-manager-7bf5d8978d-clpd7 1/1 Running 0 26d metallb-operator-webhook-server-86784c6c8c-49ncp 1/1 Running 0 26d speaker-dlrn5 6/6 Running 0 24d speaker-g2g77 6/6 Running 0 24d speaker-jzbw7 6/6 Running 0 24d speaker-pjstl 6/6 Running 0 24d check for bfdprofile on this cluster here transmit interval and receiver interval should be equal from local and remote end. oc -n metallb-system get BFDProfile -o wide NAME PASSIVE MODE TRANSMIT INTERVAL RECEIVE INTERVAL MULTIPLIER ncp-metallb-oam-pa-hn-bfd-profile true 300 300 3 ncp-metallb-oam-pa-ni-bfd-profile true 300 300 3 ncp-metallb-oam-pa-pa-bfd-profile true 300 300 3 ncp-metallb-oam-pa-sv-bfd-profile true 300 300 3 make sure, desination having the backward route configured here . [root@ncputility ~ pancwl_rc]$ oc -n metallb-system get nncp -o wide NAME STATUS REASON backward-route-for-oam-pa-pa-metallb-vlan104 Available SuccessfullyConfigured ncp-metallb-oam-pa-hn-route-for-switches-105 Available SuccessfullyConfigured ncp-metallb-oam-pa-ni-route-for-switches-107 Available SuccessfullyConfigured ncp-metallb-oam-pa-pa-route-for-switches-104 Available SuccessfullyConfigured ncp-metallb-oam-pa-sv-route-for-switches-106 Available SuccessfullyConfigured tenant-bond-bgp-oam-vlan104-gateway-0 Available SuccessfullyConfigured tenant-bond-bgp-oam-vlan104-gateway-1 Available SuccessfullyConfigured tenant-bond-bgp-oam-vlan104-gateway-2 Available SuccessfullyConfigured ** output omitted ** tenantvlan-373 Available SuccessfullyConfigured tenantvlan-374 Available SuccessfullyConfigured check for ipaddresspool exist here, so that application can create their service as loadbalancer here . [root@ncputility ~ pancwl_rc]$ oc -n metallb-system get IPAddressPool -o wide NAME AUTO ASSIGN AVOID BUGGY IPS ADDRESSES ncp-metallb-oam-pa-hn-addresspool false false [\"10.89.147.128/28\"] ncp-metallb-oam-pa-ni-addresspool false false [\"10.86.10.16/28\"] ncp-metallb-oam-pa-pa-addresspool false false [\"10.89.101.128/27\",\"10.89.97.208/28\"] ncp-metallb-oam-pa-sv-addresspool false false [\"10.85.186.240/28\"] check for bgppeer are up on the metallb speakers thats important for this communication [root@ncputility ~ pancwl_rc]$ oc -n metallb-system get BGPPeer -o wide NAME ADDRESS ASN BFD PROFILE MULTI HOPS ncp-metallb-oam-pa-hn-bgp-peer-1 10.89.147.194 4200000320 ncp-metallb-oam-pa-hn-bfd-profile ncp-metallb-oam-pa-hn-bgp-peer-2 10.89.147.195 4200000320 ncp-metallb-oam-pa-hn-bfd-profile ncp-metallb-oam-pa-ni-bgp-peer-1 10.86.10.98 4200000320 ncp-metallb-oam-pa-ni-bfd-profile ncp-metallb-oam-pa-ni-bgp-peer-2 10.86.10.99 4200000320 ncp-metallb-oam-pa-ni-bfd-profile ncp-metallb-oam-pa-pa-bgp-peer-1 10.89.97.162 4200000320 ncp-metallb-oam-pa-pa-bfd-profile ncp-metallb-oam-pa-pa-bgp-peer-2 10.89.97.163 4200000320 ncp-metallb-oam-pa-pa-bfd-profile ncp-metallb-oam-pa-sv-bgp-peer-1 10.85.187.34 4200000320 ncp-metallb-oam-pa-sv-bfd-profile ncp-metallb-oam-pa-sv-bgp-peer-2 10.85.187.35 4200000320 ncp-metallb-oam-pa-sv-bfd-profile check for BGPAdvertisement are created on this cluster and it should be in the metallb-system namespace. [root@ncputility ~ pancwl_rc]$ oc -n metallb-system get BGPAdvertisement -o wide NAME IPADDRESSPOOLS IPADDRESSPOOL SELECTORS PEERS NODE SELECTORS ncp-metallb-oam-pa-hn-bgp-advertisement [\"ncp-metallb-oam-pa-hn-addresspool\"] [\"ncp-metallb-oam-pa-hn-bgp-peer-1\",\"ncp-metallb-oam-pa-hn-bgp-peer-2\"] ncp-metallb-oam-pa-ni-bgp-advertisement [\"ncp-metallb-oam-pa-ni-addresspool\"] [\"ncp-metallb-oam-pa-ni-bgp-peer-1\",\"ncp-metallb-oam-pa-ni-bgp-peer-2\"] ncp-metallb-oam-pa-pa-bgp-advertisement [\"ncp-metallb-oam-pa-pa-addresspool\"] [\"ncp-metallb-oam-pa-pa-bgp-peer-1\",\"ncp-metallb-oam-pa-pa-bgp-peer-2\"] ncp-metallb-oam-pa-sv-bgp-advertisement [\"ncp-metallb-oam-pa-sv-addresspool\"] [\"ncp-metallb-oam-pa-sv-bgp-peer-1\",\"ncp-metallb-oam-pa-sv-bgp-peer-2\"] No, error from the container logs on this namespace . [root@ncputility ~ pancwl_rc]$ oc -n metallb-system logs -l component=speaker Defaulted container \"speaker\" out of: speaker, frr, reloader, frr-metrics, kube-rbac-proxy, kube-rbac-proxy-frr, cp-frr-files (init), cp-reloader (init), cp-metrics (init) Defaulted container \"speaker\" out of: speaker, frr, reloader, frr-metrics, kube-rbac-proxy, kube-rbac-proxy-frr, cp-frr-files (init), cp-reloader (init), cp-metrics (init) Defaulted container \"speaker\" out of: speaker, frr, reloader, frr-metrics, kube-rbac-proxy, kube-rbac-proxy-frr, cp-frr-files (init), cp-reloader (init), cp-metrics (init) Defaulted container \"speaker\" out of: speaker, frr, reloader, frr-metrics, kube-rbac-proxy, kube-rbac-proxy-frr, cp-frr-files (init), cp-reloader (init), cp-metrics (init) {\"caller\":\"node_controller.go:46\",\"controller\":\"NodeReconciler\",\"level\":\"info\",\"start reconcile\":\"/gateway4.panclypcwl01.mnc020.mcc714\",\"ts\":\"2025-03-31T04:44:07Z\"} {\"caller\":\"node_controller.go:69\",\"controller\":\"NodeReconciler\",\"end reconcile\":\"/gateway4.panclypcwl01.mnc020.mcc714\",\"level\":\"info\",\"ts\":\"2025-03-31T04:44:07Z\"} {\"caller\":\"node_controller.go:46\",\"controller\":\"NodeReconciler\",\"level\":\"info\",\"start reconcile\":\"/gateway3.panclypcwl01.mnc020.mcc714\",\"ts\":\"2025-03-31T04:44:12Z\"} {\"caller\":\"node_controller.go:69\",\"controller\":\"NodeReconciler\",\"end reconcile\":\"/gateway3.panclypcwl01.mnc020.mcc714\",\"level\":\"info\",\"ts\":\"2025-03-31T04:44:12Z\"} {\"caller\":\"node_controller.go:46\",\"controller\":\"NodeReconciler\",\"level\":\"info\",\"start reconcile\":\"/appworker16.panclypcwl01.mnc020.mcc714\",\"ts\":\"2025-03-31T04:44:21Z\"} {\"caller\":\"node_controller.go:69\",\"controller\":\"NodeReconciler\",\"end reconcile\":\"/appworker16.panclypcwl01.mnc020.mcc714\",\"level\":\"info\",\"ts\":\"2025-03-31T04:44:21Z\"} {\"caller\":\"node_controller.go:46\",\"controller\":\"NodeReconciler\",\"level\":\"info\",\"start reconcile\":\"/appworker20.panclypcwl01.mnc020.mcc714\",\"ts\":\"2025-03-31T04:44:34Z\"} {\"caller\":\"node_controller.go:69\",\"controller\":\"NodeReconciler\",\"end reconcile\":\"/appworker20.panclypcwl01.mnc020.mcc714\",\"level\":\"info\",\"ts\":\"2025-03-31T04:44:34Z\"} {\"caller\":\"node_controller.go:46\",\"controller\":\"NodeReconciler\",\"level\":\"info\",\"start reconcile\":\"/appworker28.panclypcwl01.mnc020.mcc714\",\"ts\":\"2025-03-31T04:44:44Z\"} {\"caller\":\"node_controller.go:69\",\"controller\":\"NodeReconciler\",\"end reconcile\":\"/appworker28.panclypcwl01.mnc020.mcc714\",\"level\":\"info\",\"ts\":\"2025-03-31T04:44:44Z\"} {\"caller\":\"node_controller.go:69\",\"controller\":\"NodeReconciler\",\"end reconcile\":\"/gateway4.panclypcwl01.mnc020.mcc714\",\"level\":\"info\",\"ts\":\"2025-03-31T04:44:07Z\"} {\"caller\":\"node_controller.go:46\",\"controller\":\"NodeReconciler\",\"level\":\"info\",\"start reconcile\":\"/gateway3.panclypcwl01.mnc020.mcc714\",\"ts\":\"2025-03-31T04:44:12Z\"} {\"caller\":\"speakerlist.go:274\",\"level\":\"info\",\"msg\":\"triggering discovery\",\"op\":\"memberDiscovery\",\"ts\":\"2025-03-31T04:44:12Z\"} {\"caller\":\"node_controller.go:69\",\"controller\":\"NodeReconciler\",\"end reconcile\":\"/gateway3.panclypcwl01.mnc020.mcc714\",\"level\":\"info\",\"ts\":\"2025-03-31T04:44:12Z\"} {\"caller\":\"node_controller.go:46\",\"controller\":\"NodeReconciler\",\"level\":\"info\",\"start reconcile\":\"/appworker16.panclypcwl01.mnc020.mcc714\",\"ts\":\"2025-03-31T04:44:21Z\"} {\"caller\":\"node_controller.go:69\",\"controller\":\"NodeReconciler\",\"end reconcile\":\"/appworker16.panclypcwl01.mnc020.mcc714\",\"level\":\"info\",\"ts\":\"2025-03-31T04:44:21Z\"} {\"caller\":\"node_controller.go:46\",\"controller\":\"NodeReconciler\",\"level\":\"info\",\"start reconcile\":\"/appworker20.panclypcwl01.mnc020.mcc714\",\"ts\":\"2025-03-31T04:44:34Z\"} {\"caller\":\"node_controller.go:69\",\"controller\":\"NodeReconciler\",\"end reconcile\":\"/appworker20.panclypcwl01.mnc020.mcc714\",\"level\":\"info\",\"ts\":\"2025-03-31T04:44:34Z\"} {\"caller\":\"node_controller.go:46\",\"controller\":\"NodeReconciler\",\"level\":\"info\",\"start reconcile\":\"/appworker28.panclypcwl01.mnc020.mcc714\",\"ts\":\"2025-03-31T04:44:44Z\"} {\"caller\":\"node_controller.go:69\",\"controller\":\"NodeReconciler\",\"end reconcile\":\"/appworker28.panclypcwl01.mnc020.mcc714\",\"level\":\"info\",\"ts\":\"2025-03-31T04:44:44Z\"} {\"caller\":\"node_controller.go:46\",\"controller\":\"NodeReconciler\",\"level\":\"info\",\"start reconcile\":\"/gateway4.panclypcwl01.mnc020.mcc714\",\"ts\":\"2025-03-31T04:44:07Z\"} {\"caller\":\"node_controller.go:69\",\"controller\":\"NodeReconciler\",\"end reconcile\":\"/gateway4.panclypcwl01.mnc020.mcc714\",\"level\":\"info\",\"ts\":\"2025-03-31T04:44:07Z\"} {\"caller\":\"node_controller.go:46\",\"controller\":\"NodeReconciler\",\"level\":\"info\",\"start reconcile\":\"/gateway3.panclypcwl01.mnc020.mcc714\",\"ts\":\"2025-03-31T04:44:12Z\"} {\"caller\":\"node_controller.go:69\",\"controller\":\"NodeReconciler\",\"end reconcile\":\"/gateway3.panclypcwl01.mnc020.mcc714\",\"level\":\"info\",\"ts\":\"2025-03-31T04:44:12Z\"} {\"caller\":\"node_controller.go:46\",\"controller\":\"NodeReconciler\",\"level\":\"info\",\"start reconcile\":\"/appworker16.panclypcwl01.mnc020.mcc714\",\"ts\":\"2025-03-31T04:44:21Z\"} {\"caller\":\"node_controller.go:69\",\"controller\":\"NodeReconciler\",\"end reconcile\":\"/appworker16.panclypcwl01.mnc020.mcc714\",\"level\":\"info\",\"ts\":\"2025-03-31T04:44:21Z\"} {\"caller\":\"node_controller.go:46\",\"controller\":\"NodeReconciler\",\"level\":\"info\",\"start reconcile\":\"/appworker20.panclypcwl01.mnc020.mcc714\",\"ts\":\"2025-03-31T04:44:34Z\"} {\"caller\":\"node_controller.go:69\",\"controller\":\"NodeReconciler\",\"end reconcile\":\"/appworker20.panclypcwl01.mnc020.mcc714\",\"level\":\"info\",\"ts\":\"2025-03-31T04:44:34Z\"} {\"caller\":\"node_controller.go:46\",\"controller\":\"NodeReconciler\",\"level\":\"info\",\"start reconcile\":\"/appworker28.panclypcwl01.mnc020.mcc714\",\"ts\":\"2025-03-31T04:44:44Z\"} {\"caller\":\"node_controller.go:69\",\"controller\":\"NodeReconciler\",\"end reconcile\":\"/appworker28.panclypcwl01.mnc020.mcc714\",\"level\":\"info\",\"ts\":\"2025-03-31T04:44:44Z\"} {\"caller\":\"speakerlist.go:274\",\"level\":\"info\",\"msg\":\"triggering discovery\",\"op\":\"memberDiscovery\",\"ts\":\"2025-03-31T04:44:07Z\"} {\"caller\":\"node_controller.go:69\",\"controller\":\"NodeReconciler\",\"end reconcile\":\"/gateway4.panclypcwl01.mnc020.mcc714\",\"level\":\"info\",\"ts\":\"2025-03-31T04:44:07Z\"} {\"caller\":\"node_controller.go:46\",\"controller\":\"NodeReconciler\",\"level\":\"info\",\"start reconcile\":\"/gateway3.panclypcwl01.mnc020.mcc714\",\"ts\":\"2025-03-31T04:44:12Z\"} {\"caller\":\"node_controller.go:69\",\"controller\":\"NodeReconciler\",\"end reconcile\":\"/gateway3.panclypcwl01.mnc020.mcc714\",\"level\":\"info\",\"ts\":\"2025-03-31T04:44:12Z\"} {\"caller\":\"node_controller.go:46\",\"controller\":\"NodeReconciler\",\"level\":\"info\",\"start reconcile\":\"/appworker16.panclypcwl01.mnc020.mcc714\",\"ts\":\"2025-03-31T04:44:21Z\"} {\"caller\":\"node_controller.go:69\",\"controller\":\"NodeReconciler\",\"end reconcile\":\"/appworker16.panclypcwl01.mnc020.mcc714\",\"level\":\"info\",\"ts\":\"2025-03-31T04:44:21Z\"} {\"caller\":\"node_controller.go:46\",\"controller\":\"NodeReconciler\",\"level\":\"info\",\"start reconcile\":\"/appworker20.panclypcwl01.mnc020.mcc714\",\"ts\":\"2025-03-31T04:44:34Z\"} {\"caller\":\"node_controller.go:69\",\"controller\":\"NodeReconciler\",\"end reconcile\":\"/appworker20.panclypcwl01.mnc020.mcc714\",\"level\":\"info\",\"ts\":\"2025-03-31T04:44:34Z\"} {\"caller\":\"node_controller.go:46\",\"controller\":\"NodeReconciler\",\"level\":\"info\",\"start reconcile\":\"/appworker28.panclypcwl01.mnc020.mcc714\",\"ts\":\"2025-03-31T04:44:44Z\"} {\"caller\":\"node_controller.go:69\",\"controller\":\"NodeReconciler\",\"end reconcile\":\"/appworker28.panclypcwl01.mnc020.mcc714\",\"level\":\"info\",\"ts\":\"2025-03-31T04:44:44Z\"} just showing an backwards route here for comparison and your destination should be exist here . [root@ncputility ~ pancwl_rc]$ oc get nncp -A -o yaml apiVersion: v1 items: - apiVersion: nmstate.io/v1 kind: NodeNetworkConfigurationPolicy metadata: annotations: kubectl.kubernetes.io/last-applied-configuration: | {\"apiVersion\":\"nmstate.io/v1\",\"kind\":\"NodeNetworkConfigurationPolicy\",\"metadata\":{\"annotations\":{},\"name\":\"backward-route-for-oam-pa-pa-metallb-vlan104\"},\"spec\":{\"desiredState\":{\"routes\":{\"config\":[{\"destination\":\"10.89.100.66/32\",\"metric\":150,\"next-hop-address\":\"10.89.97.161\",\"next-hop-interface\":\"vlan104\",\"table-id\":254},{\"destination\":\"10.89.27.4/32\",\"metric\":150,\"next-hop-address\":\"10.89.97.161\",\"next-hop-interface\":\"vlan104\",\"table-id\":254}]}},\"nodeSelector\":{\"node-role.kubernetes.io/gateway\":\"\"}}} nmstate.io/webhook-mutating-timestamp: \"1743107464714866687\" creationTimestamp: \"2025-03-27T20:31:04Z\" generation: 1 name: backward-route-for-oam-pa-pa-metallb-vlan104 resourceVersion: \"36077334\" uid: d72a67aa-44d0-403f-8ed5-29a49994eaf6 spec: desiredState: routes: config: - destination: 10.89.100.66/32 metric: 150 next-hop-address: 10.89.97.161 next-hop-interface: vlan104 table-id: 254 - destination: 10.89.27.4/32 metric: 150 next-hop-address: 10.89.97.161 next-hop-interface: vlan104 table-id: 254 nodeSelector: node-role.kubernetes.io/gateway: \"\" status: conditions: - lastHeartbeatTime: \"2025-03-27T20:32:10Z\" lastTransitionTime: \"2025-03-27T20:32:10Z\" message: 4/4 nodes successfully configured reason: SuccessfullyConfigured status: \"True\" type: Available - lastHeartbeatTime: \"2025-03-27T20:32:10Z\" lastTransitionTime: \"2025-03-27T20:32:10Z\" reason: SuccessfullyConfigured status: \"False\" type: Degraded - lastHeartbeatTime: \"2025-03-27T20:32:10Z\" lastTransitionTime: \"2025-03-27T20:32:10Z\" reason: ConfigurationProgressing status: \"False\" type: Progressing lastUnavailableNodeCountUpdate: \"2025-03-27T20:32:09Z\" Check the status of local and remote configuration by login into the metallb-system namespace. bdf and bgp should be fully up [root@ncputility ~ pancwl_rc]$ oc get -n metallb-system pods -l component=speaker -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES speaker-dlrn5 6/6 Running 0 24d 10.89.96.18 gateway2.panclypcwl01.mnc020.mcc714 <none> <none> speaker-g2g77 6/6 Running 0 24d 10.89.96.19 gateway3.panclypcwl01.mnc020.mcc714 <none> <none> speaker-jzbw7 6/6 Running 0 24d 10.89.96.17 gateway1.panclypcwl01.mnc020.mcc714 <none> <none> speaker-pjstl 6/6 Running 0 24d 10.89.96.20 gateway4.panclypcwl01.mnc020.mcc714 <none> <none> [root@ncputility ~ pancwl_rc]$ oc exec -n metallb-system speaker-dlrn5 -c frr -- vtysh -c \"show running-config\" Building configuration... Current configuration: ! frr version 8.3.1 frr defaults traditional hostname gateway2.panclypcwl01.mnc020.mcc714 log file /etc/frr/frr.log informational log timestamp precision 3 service integrated-vtysh-config ! router bgp 4200000320 no bgp ebgp-requires-policy no bgp default ipv4-unicast no bgp network import-check neighbor 10.85.187.34 remote-as 4200000320 neighbor 10.85.187.34 bfd neighbor 10.85.187.34 bfd profile ncp-metallb-oam-pa-sv-bfd-profile neighbor 10.85.187.34 timers 30 90 neighbor 10.85.187.35 remote-as 4200000320 neighbor 10.85.187.35 bfd neighbor 10.85.187.35 bfd profile ncp-metallb-oam-pa-sv-bfd-profile neighbor 10.85.187.35 timers 30 90 neighbor 10.86.10.98 remote-as 4200000320 neighbor 10.86.10.98 bfd neighbor 10.86.10.98 bfd profile ncp-metallb-oam-pa-ni-bfd-profile neighbor 10.86.10.98 timers 30 90 neighbor 10.86.10.99 remote-as 4200000320 neighbor 10.86.10.99 bfd neighbor 10.86.10.99 bfd profile ncp-metallb-oam-pa-ni-bfd-profile neighbor 10.86.10.99 timers 30 90 neighbor 10.89.97.162 remote-as 4200000320 neighbor 10.89.97.162 bfd neighbor 10.89.97.162 bfd profile ncp-metallb-oam-pa-pa-bfd-profile neighbor 10.89.97.162 timers 30 90 neighbor 10.89.97.163 remote-as 4200000320 neighbor 10.89.97.163 bfd neighbor 10.89.97.163 bfd profile ncp-metallb-oam-pa-pa-bfd-profile neighbor 10.89.97.163 timers 30 90 neighbor 10.89.147.194 remote-as 4200000320 neighbor 10.89.147.194 bfd neighbor 10.89.147.194 bfd profile ncp-metallb-oam-pa-hn-bfd-profile neighbor 10.89.147.194 timers 30 90 neighbor 10.89.147.195 remote-as 4200000320 neighbor 10.89.147.195 bfd neighbor 10.89.147.195 bfd profile ncp-metallb-oam-pa-hn-bfd-profile neighbor 10.89.147.195 timers 30 90 ! address-family ipv4 unicast network 10.89.97.210/32 neighbor 10.85.187.34 activate neighbor 10.85.187.34 route-map 10.85.187.34-in in neighbor 10.85.187.34 route-map 10.85.187.34-out out neighbor 10.85.187.35 activate neighbor 10.85.187.35 route-map 10.85.187.35-in in neighbor 10.85.187.35 route-map 10.85.187.35-out out neighbor 10.86.10.98 activate neighbor 10.86.10.98 route-map 10.86.10.98-in in neighbor 10.86.10.98 route-map 10.86.10.98-out out neighbor 10.86.10.99 activate neighbor 10.86.10.99 route-map 10.86.10.99-in in neighbor 10.86.10.99 route-map 10.86.10.99-out out neighbor 10.89.97.162 activate neighbor 10.89.97.162 route-map 10.89.97.162-in in neighbor 10.89.97.162 route-map 10.89.97.162-out out neighbor 10.89.97.163 activate neighbor 10.89.97.163 route-map 10.89.97.163-in in neighbor 10.89.97.163 route-map 10.89.97.163-out out neighbor 10.89.147.194 activate neighbor 10.89.147.194 route-map 10.89.147.194-in in neighbor 10.89.147.194 route-map 10.89.147.194-out out neighbor 10.89.147.195 activate neighbor 10.89.147.195 route-map 10.89.147.195-in in neighbor 10.89.147.195 route-map 10.89.147.195-out out exit-address-family ! address-family ipv6 unicast neighbor 10.85.187.34 activate neighbor 10.85.187.34 route-map 10.85.187.34-in in neighbor 10.85.187.34 route-map 10.85.187.34-out out neighbor 10.85.187.35 activate neighbor 10.85.187.35 route-map 10.85.187.35-in in neighbor 10.85.187.35 route-map 10.85.187.35-out out neighbor 10.86.10.98 activate neighbor 10.86.10.98 route-map 10.86.10.98-in in neighbor 10.86.10.98 route-map 10.86.10.98-out out neighbor 10.86.10.99 activate neighbor 10.86.10.99 route-map 10.86.10.99-in in neighbor 10.86.10.99 route-map 10.86.10.99-out out neighbor 10.89.97.162 activate neighbor 10.89.97.162 route-map 10.89.97.162-in in neighbor 10.89.97.162 route-map 10.89.97.162-out out neighbor 10.89.97.163 activate neighbor 10.89.97.163 route-map 10.89.97.163-in in neighbor 10.89.97.163 route-map 10.89.97.163-out out neighbor 10.89.147.194 activate neighbor 10.89.147.194 route-map 10.89.147.194-in in neighbor 10.89.147.194 route-map 10.89.147.194-out out neighbor 10.89.147.195 activate neighbor 10.89.147.195 route-map 10.89.147.195-in in neighbor 10.89.147.195 route-map 10.89.147.195-out out exit-address-family exit ! ip prefix-list 10.89.97.162-pl-ipv4 seq 1 permit 10.89.97.210/32 ip prefix-list 10.89.97.163-pl-ipv4 seq 1 permit 10.89.97.210/32 ip prefix-list 10.85.187.34-pl-ipv4 seq 1 deny any ip prefix-list 10.85.187.35-pl-ipv4 seq 1 deny any ip prefix-list 10.86.10.98-pl-ipv4 seq 1 deny any ip prefix-list 10.86.10.99-pl-ipv4 seq 1 deny any ip prefix-list 10.89.147.194-pl-ipv4 seq 1 deny any ip prefix-list 10.89.147.195-pl-ipv4 seq 1 deny any ! ipv6 prefix-list 10.89.97.162-pl-ipv4 seq 2 deny any ipv6 prefix-list 10.89.97.163-pl-ipv4 seq 2 deny any ipv6 prefix-list 10.85.187.34-pl-ipv4 seq 2 deny any ipv6 prefix-list 10.85.187.35-pl-ipv4 seq 2 deny any ipv6 prefix-list 10.86.10.98-pl-ipv4 seq 2 deny any ipv6 prefix-list 10.86.10.99-pl-ipv4 seq 2 deny any ipv6 prefix-list 10.89.147.194-pl-ipv4 seq 2 deny any ipv6 prefix-list 10.89.147.195-pl-ipv4 seq 2 deny any ! route-map 10.85.187.34-in deny 20 exit ! route-map 10.85.187.34-out permit 1 match ip address prefix-list 10.85.187.34-pl-ipv4 exit ! route-map 10.85.187.34-out permit 2 match ipv6 address prefix-list 10.85.187.34-pl-ipv4 exit ! route-map 10.85.187.35-in deny 20 exit ! route-map 10.85.187.35-out permit 1 match ip address prefix-list 10.85.187.35-pl-ipv4 exit ! route-map 10.85.187.35-out permit 2 match ipv6 address prefix-list 10.85.187.35-pl-ipv4 exit ! route-map 10.86.10.98-in deny 20 exit ! route-map 10.86.10.98-out permit 1 match ip address prefix-list 10.86.10.98-pl-ipv4 exit ! route-map 10.86.10.98-out permit 2 match ipv6 address prefix-list 10.86.10.98-pl-ipv4 exit ! route-map 10.86.10.99-in deny 20 exit ! route-map 10.86.10.99-out permit 1 match ip address prefix-list 10.86.10.99-pl-ipv4 exit ! route-map 10.86.10.99-out permit 2 match ipv6 address prefix-list 10.86.10.99-pl-ipv4 exit ! route-map 10.89.147.194-in deny 20 exit ! route-map 10.89.147.194-out permit 1 match ip address prefix-list 10.89.147.194-pl-ipv4 exit ! route-map 10.89.147.194-out permit 2 match ipv6 address prefix-list 10.89.147.194-pl-ipv4 exit ! route-map 10.89.147.195-in deny 20 exit ! route-map 10.89.147.195-out permit 1 match ip address prefix-list 10.89.147.195-pl-ipv4 exit ! route-map 10.89.147.195-out permit 2 match ipv6 address prefix-list 10.89.147.195-pl-ipv4 exit ! route-map 10.89.97.162-in deny 20 exit ! route-map 10.89.97.162-out permit 1 match ip address prefix-list 10.89.97.162-pl-ipv4 exit ! route-map 10.89.97.162-out permit 2 match ipv6 address prefix-list 10.89.97.162-pl-ipv4 exit ! route-map 10.89.97.163-in deny 20 exit ! route-map 10.89.97.163-out permit 1 match ip address prefix-list 10.89.97.163-pl-ipv4 exit ! route-map 10.89.97.163-out permit 2 match ipv6 address prefix-list 10.89.97.163-pl-ipv4 exit ! ip nht resolve-via-default ! ipv6 nht resolve-via-default ! bfd profile ncp-metallb-oam-pa-hn-bfd-profile passive-mode exit ! profile ncp-metallb-oam-pa-ni-bfd-profile passive-mode exit ! profile ncp-metallb-oam-pa-pa-bfd-profile passive-mode exit ! profile ncp-metallb-oam-pa-sv-bfd-profile passive-mode exit ! exit ! end [root@ncputility ~ pancwl_rc]$ oc exec -n metallb-system speaker-dlrn5 -c frr -- vtysh -c \"show bgp summary\" IPv4 Unicast Summary (VRF default): BGP router identifier 172.16.2.2, local AS number 4200000320 vrf-id 0 BGP table version 1 RIB entries 1, using 192 bytes of memory Peers 8, using 5788 KiB of memory Neighbor V AS MsgRcvd MsgSent TblVer InQ OutQ Up/Down State/PfxRcd PfxSnt Desc 10.85.187.34 4 4200000320 0 0 0 0 0 never Active 0 N/A 10.85.187.35 4 4200000320 0 0 0 0 0 never Active 0 N/A 10.86.10.98 4 4200000320 0 0 0 0 0 never Active 0 N/A 10.86.10.99 4 4200000320 0 0 0 0 0 never Active 0 N/A 10.89.97.162 4 4200000320 7306 7307 0 0 0 2d12h50m 0 1 N/A 10.89.97.163 4 4200000320 7305 7307 0 0 0 2d12h50m 0 1 N/A 10.89.147.194 4 4200000320 0 0 0 0 0 never Active 0 N/A 10.89.147.195 4 4200000320 0 0 0 0 0 never Active 0 N/A Total number of neighbors 8 IPv6 Unicast Summary (VRF default): BGP router identifier 172.16.2.2, local AS number 4200000320 vrf-id 0 BGP table version 0 RIB entries 0, using 0 bytes of memory Peers 8, using 5788 KiB of memory Neighbor V AS MsgRcvd MsgSent TblVer InQ OutQ Up/Down State/PfxRcd PfxSnt Desc 10.85.187.34 4 4200000320 0 0 0 0 0 never Active 0 N/A 10.85.187.35 4 4200000320 0 0 0 0 0 never Active 0 N/A 10.86.10.98 4 4200000320 0 0 0 0 0 never Active 0 N/A 10.86.10.99 4 4200000320 0 0 0 0 0 never Active 0 N/A 10.89.97.162 4 4200000320 7306 7307 0 0 0 2d12h50m NoNeg NoNeg N/A 10.89.97.163 4 4200000320 7305 7307 0 0 0 2d12h50m NoNeg NoNeg N/A 10.89.147.194 4 4200000320 0 0 0 0 0 never Active 0 N/A 10.89.147.195 4 4200000320 0 0 0 0 0 never Active 0 N/A Total number of neighbors 8 [root@ncputility ~ pancwl_rc]$ oc exec -n metallb-system speaker-dlrn5 -c frr -- vtysh -c \"show bfd peers brief\" Session count: 2 SessionId LocalAddress PeerAddress Status ========= ============ =========== ====== 62069252 10.89.97.165 10.89.97.162 down 3159552171 10.89.97.165 10.89.97.163 down [root@ncputility ~ pancwl_rc]$ oc exec -n metallb-system speaker-dlrn5 -c frr -- vtysh -c \"show bfd peer\" BFD Peers: peer 10.89.97.162 local-address 10.89.97.165 vrf default interface vlan104 ID: 62069252 Remote ID: 0 Passive mode Status: down Downtime: 2 day(s), 10 hour(s), 3 minute(s), 16 second(s) Diagnostics: ok Remote diagnostics: ok Peer Type: dynamic Local timers: Detect-multiplier: 3 Receive interval: 300ms Transmission interval: 300ms Echo receive interval: 50ms Echo transmission interval: disabled Remote timers: Detect-multiplier: 3 Receive interval: 1000ms Transmission interval: 1000ms Echo receive interval: disabled peer 10.89.97.163 local-address 10.89.97.165 vrf default interface vlan104 ID: 3159552171 Remote ID: 0 Passive mode Status: down Downtime: 2 day(s), 10 hour(s), 3 minute(s), 16 second(s) Diagnostics: ok Remote diagnostics: ok Peer Type: dynamic Local timers: Detect-multiplier: 3 Receive interval: 300ms Transmission interval: 300ms Echo receive interval: 50ms Echo transmission interval: disabled Remote timers: Detect-multiplier: 3 Receive interval: 1000ms Transmission interval: 1000ms Echo receive interval: disabled [root@ncputility ~ pancwl_rc]$ oc exec -n metallb-system speaker-dlrn5 -c frr -- vtysh -c \"show ip bgp neighbors\" BGP neighbor is 10.85.187.34, remote AS 4200000320, local AS 4200000320, internal link BGP version 4, remote router ID 0.0.0.0, local router ID 172.16.2.2 BGP state = Active Last read 2d10h03m, Last write never Hold time is 90, keepalive interval is 30 seconds Configured hold time is 90, keepalive interval is 30 seconds Configured conditional advertisements interval is 60 seconds Graceful restart information: Local GR Mode: Helper* Remote GR Mode: NotApplicable R bit: False N bit: False Timers: Configured Restart Time(sec): 120 Received Restart Time(sec): 0 Message statistics: Inq depth is 0 Outq depth is 0 Sent Rcvd Opens: 0 0 Notifications: 0 0 Updates: 0 0 Keepalives: 0 0 Route Refresh: 0 0 Capability: 0 0 Total: 0 0 Minimum time between advertisement runs is 0 seconds For address family: IPv4 Unicast Not part of any update group Community attribute sent to this neighbor(all) Inbound path policy configured Outbound path policy configured Route map for incoming advertisements is *10.85.187.34-in Route map for outgoing advertisements is *10.85.187.34-out 0 accepted prefixes For address family: IPv6 Unicast Not part of any update group Community attribute sent to this neighbor(all) Inbound path policy configured Outbound path policy configured Route map for incoming advertisements is *10.85.187.34-in Route map for outgoing advertisements is *10.85.187.34-out 0 accepted prefixes Connections established 0; dropped 0 Last reset 2d10h03m, Waiting for peer OPEN BGP Connect Retry Timer in Seconds: 120 Next connect timer due in 80 seconds Read thread: off Write thread: off FD used: -1 BFD: Type: multi hop Detect Multiplier: 3, Min Rx interval: 300, Min Tx interval: 300 Status: Unknown, Last update: never BGP neighbor is 10.85.187.35, remote AS 4200000320, local AS 4200000320, internal link BGP version 4, remote router ID 0.0.0.0, local router ID 172.16.2.2 BGP state = Active Last read 2d10h03m, Last write never Hold time is 90, keepalive interval is 30 seconds Configured hold time is 90, keepalive interval is 30 seconds Configured conditional advertisements interval is 60 seconds Graceful restart information: Local GR Mode: Helper* Remote GR Mode: NotApplicable R bit: False N bit: False Timers: Configured Restart Time(sec): 120 Received Restart Time(sec): 0 Message statistics: Inq depth is 0 Outq depth is 0 Sent Rcvd Opens: 0 0 Notifications: 0 0 Updates: 0 0 Keepalives: 0 0 Route Refresh: 0 0 Capability: 0 0 Total: 0 0 Minimum time between advertisement runs is 0 seconds For address family: IPv4 Unicast Not part of any update group Community attribute sent to this neighbor(all) Inbound path policy configured Outbound path policy configured Route map for incoming advertisements is *10.85.187.35-in Route map for outgoing advertisements is *10.85.187.35-out 0 accepted prefixes For address family: IPv6 Unicast Not part of any update group Community attribute sent to this neighbor(all) Inbound path policy configured Outbound path policy configured Route map for incoming advertisements is *10.85.187.35-in Route map for outgoing advertisements is *10.85.187.35-out 0 accepted prefixes Connections established 0; dropped 0 Last reset 2d10h03m, Waiting for peer OPEN BGP Connect Retry Timer in Seconds: 120 Next connect timer due in 80 seconds Read thread: off Write thread: off FD used: -1 BFD: Type: multi hop Detect Multiplier: 3, Min Rx interval: 300, Min Tx interval: 300 Status: Unknown, Last update: never BGP neighbor is 10.86.10.98, remote AS 4200000320, local AS 4200000320, internal link BGP version 4, remote router ID 0.0.0.0, local router ID 172.16.2.2 BGP state = Active Last read 2d10h03m, Last write never Hold time is 90, keepalive interval is 30 seconds Configured hold time is 90, keepalive interval is 30 seconds Configured conditional advertisements interval is 60 seconds Graceful restart information: Local GR Mode: Helper* Remote GR Mode: NotApplicable R bit: False N bit: False Timers: Configured Restart Time(sec): 120 Received Restart Time(sec): 0 Message statistics: Inq depth is 0 Outq depth is 0 Sent Rcvd Opens: 0 0 Notifications: 0 0 Updates: 0 0 Keepalives: 0 0 Route Refresh: 0 0 Capability: 0 0 Total: 0 0 Minimum time between advertisement runs is 0 seconds For address family: IPv4 Unicast Not part of any update group Community attribute sent to this neighbor(all) Inbound path policy configured Outbound path policy configured Route map for incoming advertisements is *10.86.10.98-in Route map for outgoing advertisements is *10.86.10.98-out 0 accepted prefixes For address family: IPv6 Unicast Not part of any update group Community attribute sent to this neighbor(all) Inbound path policy configured Outbound path policy configured Route map for incoming advertisements is *10.86.10.98-in Route map for outgoing advertisements is *10.86.10.98-out 0 accepted prefixes Connections established 0; dropped 0 Last reset 2d10h03m, Waiting for peer OPEN BGP Connect Retry Timer in Seconds: 120 Next connect timer due in 80 seconds Read thread: off Write thread: off FD used: -1 BFD: Type: multi hop Detect Multiplier: 3, Min Rx interval: 300, Min Tx interval: 300 Status: Unknown, Last update: never BGP neighbor is 10.86.10.99, remote AS 4200000320, local AS 4200000320, internal link BGP version 4, remote router ID 0.0.0.0, local router ID 172.16.2.2 BGP state = Active Last read 2d10h03m, Last write never Hold time is 90, keepalive interval is 30 seconds Configured hold time is 90, keepalive interval is 30 seconds Configured conditional advertisements interval is 60 seconds Graceful restart information: Local GR Mode: Helper* Remote GR Mode: NotApplicable R bit: False N bit: False Timers: Configured Restart Time(sec): 120 Received Restart Time(sec): 0 Message statistics: Inq depth is 0 Outq depth is 0 Sent Rcvd Opens: 0 0 Notifications: 0 0 Updates: 0 0 Keepalives: 0 0 Route Refresh: 0 0 Capability: 0 0 Total: 0 0 Minimum time between advertisement runs is 0 seconds For address family: IPv4 Unicast Not part of any update group Community attribute sent to this neighbor(all) Inbound path policy configured Outbound path policy configured Route map for incoming advertisements is *10.86.10.99-in Route map for outgoing advertisements is *10.86.10.99-out 0 accepted prefixes For address family: IPv6 Unicast Not part of any update group Community attribute sent to this neighbor(all) Inbound path policy configured Outbound path policy configured Route map for incoming advertisements is *10.86.10.99-in Route map for outgoing advertisements is *10.86.10.99-out 0 accepted prefixes Connections established 0; dropped 0 Last reset 2d10h03m, Waiting for peer OPEN BGP Connect Retry Timer in Seconds: 120 Next connect timer due in 80 seconds Read thread: off Write thread: off FD used: -1 BFD: Type: multi hop Detect Multiplier: 3, Min Rx interval: 300, Min Tx interval: 300 Status: Unknown, Last update: never BGP neighbor is 10.89.97.162, remote AS 4200000320, local AS 4200000320, internal link BGP version 4, remote router ID 10.29.90.34, local router ID 172.16.2.2 BGP state = Established, up for 2d12h53m Last read 00:00:00, Last write 00:00:03 Hold time is 90, keepalive interval is 30 seconds Configured hold time is 90, keepalive interval is 30 seconds Configured conditional advertisements interval is 60 seconds Neighbor capabilities: 4 Byte AS: advertised and received Extended Message: advertised AddPath: IPv4 Unicast: RX advertised IPv6 Unicast: RX advertised Long-lived Graceful Restart: advertised Route refresh: advertised and received(new) Enhanced Route Refresh: advertised Address Family IPv4 Unicast: advertised and received Address Family IPv6 Unicast: advertised Hostname Capability: advertised (name: gateway2.panclypcwl01.mnc020.mcc714,domain name: n/a) not received Graceful Restart Capability: advertised and received Remote Restart timer is 300 seconds Address families by peer: none Graceful restart information: End-of-RIB send: IPv4 Unicast End-of-RIB received: IPv4 Unicast Local GR Mode: Helper* Remote GR Mode: Helper R bit: False N bit: False Timers: Configured Restart Time(sec): 120 Received Restart Time(sec): 300 IPv4 Unicast: F bit: False End-of-RIB sent: Yes End-of-RIB sent after update: Yes End-of-RIB received: Yes Timers: Configured Stale Path Time(sec): 360 IPv6 Unicast: F bit: False End-of-RIB sent: No End-of-RIB sent after update: No End-of-RIB received: No Timers: Configured Stale Path Time(sec): 360 Message statistics: Inq depth is 0 Outq depth is 0 Sent Rcvd Opens: 3 1 Notifications: 0 0 Updates: 2 2 Keepalives: 7307 7308 Route Refresh: 0 0 Capability: 0 0 Total: 7312 7311 Minimum time between advertisement runs is 0 seconds For address family: IPv4 Unicast Update group 3, subgroup 3 Packet Queue length 0 Community attribute sent to this neighbor(all) Inbound path policy configured Outbound path policy configured Route map for incoming advertisements is *10.89.97.162-in Route map for outgoing advertisements is *10.89.97.162-out 0 accepted prefixes For address family: IPv6 Unicast Not part of any update group Community attribute sent to this neighbor(all) Inbound path policy configured Outbound path policy configured Route map for incoming advertisements is *10.89.97.162-in Route map for outgoing advertisements is *10.89.97.162-out 0 accepted prefixes Connections established 1; dropped 0 Last reset 2d12h55m, Waiting for peer OPEN Local host: 10.89.97.165, Local port: 37680 Foreign host: 10.89.97.162, Foreign port: 179 Nexthop: 10.89.97.165 Nexthop global: :: Nexthop local: :: BGP connection: shared network BGP Connect Retry Timer in Seconds: 120 Read thread: on Write thread: on FD used: 26 BFD: Type: single hop Detect Multiplier: 3, Min Rx interval: 300, Min Tx interval: 300 Status: Down, Last update: 2:10:03:42 BGP neighbor is 10.89.97.163, remote AS 4200000320, local AS 4200000320, internal link BGP version 4, remote router ID 10.29.90.38, local router ID 172.16.2.2 BGP state = Established, up for 2d12h53m Last read 00:00:29, Last write 00:00:03 Hold time is 90, keepalive interval is 30 seconds Configured hold time is 90, keepalive interval is 30 seconds Configured conditional advertisements interval is 60 seconds Neighbor capabilities: 4 Byte AS: advertised and received Extended Message: advertised AddPath: IPv4 Unicast: RX advertised IPv6 Unicast: RX advertised Long-lived Graceful Restart: advertised Route refresh: advertised and received(new) Enhanced Route Refresh: advertised Address Family IPv4 Unicast: advertised and received Address Family IPv6 Unicast: advertised Hostname Capability: advertised (name: gateway2.panclypcwl01.mnc020.mcc714,domain name: n/a) not received Graceful Restart Capability: advertised and received Remote Restart timer is 300 seconds Address families by peer: none Graceful restart information: End-of-RIB send: IPv4 Unicast End-of-RIB received: IPv4 Unicast Local GR Mode: Helper* Remote GR Mode: Helper R bit: False N bit: False Timers: Configured Restart Time(sec): 120 Received Restart Time(sec): 300 IPv4 Unicast: F bit: False End-of-RIB sent: Yes End-of-RIB sent after update: Yes End-of-RIB received: Yes Timers: Configured Stale Path Time(sec): 360 IPv6 Unicast: F bit: False End-of-RIB sent: No End-of-RIB sent after update: No End-of-RIB received: No Timers: Configured Stale Path Time(sec): 360 Message statistics: Inq depth is 0 Outq depth is 0 Sent Rcvd Opens: 3 1 Notifications: 0 0 Updates: 2 1 Keepalives: 7307 7307 Route Refresh: 0 0 Capability: 0 0 Total: 7312 7309 Minimum time between advertisement runs is 0 seconds For address family: IPv4 Unicast Update group 4, subgroup 4 Packet Queue length 0 Community attribute sent to this neighbor(all) Inbound path policy configured Outbound path policy configured Route map for incoming advertisements is *10.89.97.163-in Route map for outgoing advertisements is *10.89.97.163-out 0 accepted prefixes For address family: IPv6 Unicast Not part of any update group Community attribute sent to this neighbor(all) Inbound path policy configured Outbound path policy configured Route map for incoming advertisements is *10.89.97.163-in Route map for outgoing advertisements is *10.89.97.163-out 0 accepted prefixes Connections established 1; dropped 0 Last reset 2d12h55m, Waiting for peer OPEN Local host: 10.89.97.165, Local port: 54688 Foreign host: 10.89.97.163, Foreign port: 179 Nexthop: 10.89.97.165 Nexthop global: :: Nexthop local: :: BGP connection: shared network BGP Connect Retry Timer in Seconds: 120 Read thread: on Write thread: on FD used: 25 BFD: Type: single hop Detect Multiplier: 3, Min Rx interval: 300, Min Tx interval: 300 Status: Down, Last update: 2:10:03:42 BGP neighbor is 10.89.147.194, remote AS 4200000320, local AS 4200000320, internal link BGP version 4, remote router ID 0.0.0.0, local router ID 172.16.2.2 BGP state = Active Last read 2d10h03m, Last write never Hold time is 90, keepalive interval is 30 seconds Configured hold time is 90, keepalive interval is 30 seconds Configured conditional advertisements interval is 60 seconds Graceful restart information: Local GR Mode: Helper* Remote GR Mode: NotApplicable R bit: False N bit: False Timers: Configured Restart Time(sec): 120 Received Restart Time(sec): 0 Message statistics: Inq depth is 0 Outq depth is 0 Sent Rcvd Opens: 0 0 Notifications: 0 0 Updates: 0 0 Keepalives: 0 0 Route Refresh: 0 0 Capability: 0 0 Total: 0 0 Minimum time between advertisement runs is 0 seconds For address family: IPv4 Unicast Not part of any update group Community attribute sent to this neighbor(all) Inbound path policy configured Outbound path policy configured Route map for incoming advertisements is *10.89.147.194-in Route map for outgoing advertisements is *10.89.147.194-out 0 accepted prefixes For address family: IPv6 Unicast Not part of any update group Community attribute sent to this neighbor(all) Inbound path policy configured Outbound path policy configured Route map for incoming advertisements is *10.89.147.194-in Route map for outgoing advertisements is *10.89.147.194-out 0 accepted prefixes Connections established 0; dropped 0 Last reset 2d10h03m, Waiting for peer OPEN BGP Connect Retry Timer in Seconds: 120 Next connect timer due in 80 seconds Read thread: off Write thread: off FD used: -1 BFD: Type: multi hop Detect Multiplier: 3, Min Rx interval: 300, Min Tx interval: 300 Status: Unknown, Last update: never BGP neighbor is 10.89.147.195, remote AS 4200000320, local AS 4200000320, internal link BGP version 4, remote router ID 0.0.0.0, local router ID 172.16.2.2 BGP state = Active Last read 2d10h03m, Last write never Hold time is 90, keepalive interval is 30 seconds Configured hold time is 90, keepalive interval is 30 seconds Configured conditional advertisements interval is 60 seconds Graceful restart information: Local GR Mode: Helper* Remote GR Mode: NotApplicable R bit: False N bit: False Timers: Configured Restart Time(sec): 120 Received Restart Time(sec): 0 Message statistics: Inq depth is 0 Outq depth is 0 Sent Rcvd Opens: 0 0 Notifications: 0 0 Updates: 0 0 Keepalives: 0 0 Route Refresh: 0 0 Capability: 0 0 Total: 0 0 Minimum time between advertisement runs is 0 seconds For address family: IPv4 Unicast Not part of any update group Community attribute sent to this neighbor(all) Inbound path policy configured Outbound path policy configured Route map for incoming advertisements is *10.89.147.195-in Route map for outgoing advertisements is *10.89.147.195-out 0 accepted prefixes For address family: IPv6 Unicast Not part of any update group Community attribute sent to this neighbor(all) Inbound path policy configured Outbound path policy configured Route map for incoming advertisements is *10.89.147.195-in Route map for outgoing advertisements is *10.89.147.195-out 0 accepted prefixes Connections established 0; dropped 0 Last reset 2d10h03m, Waiting for peer OPEN BGP Connect Retry Timer in Seconds: 120 Next connect timer due in 80 seconds Read thread: off Write thread: off FD used: -1 BFD: Type: multi hop Detect Multiplier: 3, Min Rx interval: 300, Min Tx interval: 300 Status: Unknown, Last update: never some more additional test to show the local testing here [root@ncputility ~ pancwl_rc]$ oc get svc -A -o wide |grep -i loadbalan ncom01pan ncom01pan-citm-ingress LoadBalancer 172.20.138.133 10.89.97.210 80:32432/TCP,443:32622/TCP,2309:30135/TCP 10d app=citm-ingress,component=controller,release=ncom01pan-citm-ingress [root@ncputility ~ pancwl_rc]$ nslookup 10.89.97.210 210.97.89.10.in-addr.arpa name = ncom01.panclyncom01.mnc020.mcc714. [root@ncputility ~ pancwl_rc]$ curl -k https://ncom01.panclyncom01.mnc020.mcc714/ ^C [root@ncputility ~ pancwl_rc]$ ip r get 10.89.97.210 10.89.97.210 via 10.89.100.65 dev br308 src 10.89.100.66 uid 0 cache [root@ncputility ~ pancwl_rc]$ tracepath 10.89.97.210 1?: [LOCALHOST] pmtu 1500 1: _gateway 0.184ms 1: _gateway 0.290ms 2: no reply 3: 10.89.97.129 0.284ms asymm 1 4: no reply 5: 10.89.97.129 0.264ms asymm 1 6: no reply 7: 10.89.97.129 0.281ms asymm 1 8: no reply 9: 10.89.97.129 0.279ms asymm 1 10: no reply 11: 10.89.97.129 0.269ms asymm 1 12: no reply 13: 10.89.97.129 0.280ms asymm 1 14: no reply 15: 10.89.97.129 0.234ms asymm 1 16: no reply 17: 10.89.97.129 0.297ms asymm 1 18: no reply 19: 10.89.97.129 0.328ms asymm 1 20: no reply 21: 10.89.97.129 0.292ms asymm 1 22: no reply 23: 10.89.97.129 0.336ms asymm 1 24: no reply 25: 10.89.97.129 0.332ms asymm 1 26: no reply 27: 10.89.97.129 0.330ms asymm 1 28: no reply 29: 10.89.97.129 0.332ms asymm 1 ^C [root@ncputility ~ pancwl_rc]$ oc get -n metallb-system pods -l component=speaker -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES speaker-dlrn5 6/6 Running 0 24d 10.89.96.18 gateway2.panclypcwl01.mnc020.mcc714 <none> <none> speaker-g2g77 6/6 Running 0 24d 10.89.96.19 gateway3.panclypcwl01.mnc020.mcc714 <none> <none> speaker-jzbw7 6/6 Running 0 24d 10.89.96.17 gateway1.panclypcwl01.mnc020.mcc714 <none> <none> speaker-pjstl 6/6 Running 0 24d 10.89.96.20 gateway4.panclypcwl01.mnc020.mcc714 <none> <none> [root@ncputility ~ pancwl_rc]$ oc -n metallb-system exec -it -c frr vtysh error: you must specify at least one command for the container [root@ncputility ~ pancwl_rc]$ oc -n metallb-system exec -it speaker-dlrn5 -c frr vtysh kubectl exec [POD] [COMMAND] is DEPRECATED and will be removed in a future version. Use kubectl exec [POD] -- [COMMAND] instead. Hello, this is FRRouting (version 8.3.1). Copyright 1996-2005 Kunihiro Ishiguro, et al. gateway2.panclypcwl01.mnc020.mcc714# curl % Unknown command: curl gateway2.panclypcwl01.mnc020.mcc714# exit [root@ncputility ~ pancwl_rc]$ oc debug -t node/gateway3.panclypcwl01.mnc020.mcc714 Temporary namespace openshift-debug-8j8fb is created for debugging node... Starting pod/gateway3panclypcwl01mnc020mcc714-debug-qsrhj ... To use host binaries, run `chroot /host` Pod IP: 10.89.96.19 If you don't see a command prompt, try pressing enter. sh-5.1# chroot /host sh-5.1# ping 10.89.97.163 PING 10.89.97.163 (10.89.97.163) 56(84) bytes of data. From 10.89.97.166 icmp_seq=1 Destination Host Unreachable From 10.89.97.166 icmp_seq=2 Destination Host Unreachable From 10.89.97.166 icmp_seq=3 Destination Host Unreachable ^C --- 10.89.97.163 ping statistics --- 4 packets transmitted, 0 received, +3 errors, 100% packet loss, time 3090ms pipe 3 sh-5.1# ping 10.89.97.162 PING 10.89.97.162 (10.89.97.162) 56(84) bytes of data. From 10.89.97.166 icmp_seq=1 Destination Host Unreachable From 10.89.97.166 icmp_seq=2 Destination Host Unreachable From 10.89.97.166 icmp_seq=3 Destination Host Unreachable From 10.89.97.166 icmp_seq=4 Destination Host Unreachable ^C --- 10.89.97.162 ping statistics --- 5 packets transmitted, 0 received, +4 errors, 100% packet loss, time 4064ms pipe 4 sh-5.1# ip r g 10.89.97.163 10.89.97.163 via 10.89.97.161 dev vlan104 src 10.89.97.166 uid 0 cache sh-5.1# ip r g 10.89.97.162 10.89.97.162 via 10.89.97.161 dev vlan104 src 10.89.97.166 uid 0 cache sh-5.1# curl -k https://ncom01.panclyncom01.mnc020.mcc714/ <!doctype html><html lang=\"en\"><head><meta charset=\"utf-8\"><meta name=\"viewport\" content=\"width=device-width,initial-scale=1,shrink-to-fit=no\"><meta name=\"theme-color\" content=\"#000000\"><link rel=\"manifest\" href=\"/manifest.json\"><link rel=\"shortcut icon\" href=\"/favicon.ico\"><title>Nokia Cloud Operations Manager</title><link href=\"/static/css/27.2dbd0a91.chunk.css\" rel=\"stylesheet\"><link href=\"/static/css/main.167b01c4.chunk.css\" rel=\"stylesheet\"></head><body><noscript>You need to enable JavaScript to run this app.</noscript><div id=\"root\"></div><script>!function(e){function t(t){for(var r,o,l=t[0],c=t[1],s=t[2],u=0,d=[];u<l.length;u++)o=l[u],Object.prototype.hasOwnProperty.call(n,o)&&n[o]&&d.push(n[o][0]),n[o]=0;for(r in c)Object.prototype.hasOwnProperty.call(c,r)&&(e[r]=c[r]);for(f&&f(t);d.length;)d.shift()();return i.push.apply(i,s||[]),a()}function a(){for(var e,t=0;t<i.length;t++){for(var a=i[t],r=!0,o=1;o<a.length;o++){var c=a[o];0!==n[c]&&(r=!1)}r&&(i.splice(t--,1),e=l(l.s=a[0]))}return e}var r={},o={25:0},n={25:0},i=[];function l(t){if(r[t])return r[t].exports;var a=r[t]={i:t,l:!1,exports:{}};return e[t].call(a.exports,a,a.exports,l),a.l=!0,a.exports}l.e=function(e){var t=[];o[e]?t.push(o[e]):0!==o[e]&&{2:1,5:1,6:1,7:1,8:1,11:1,12:1,13:1,14:1,15:1,16:1,17:1,18:1,19:1,20:1,21:1,22:1,26:1}[e]&&t.push(o[e]=new Promise((function(t,a){for(var r=\"static/css/\"+({5:\"[AlarmDetailsPage]\",6:\"[AlarmPage]\",7:\"[CatalogDetailsPage]\",8:\"[CatalogTablePage]\",9:\"[CloudFlowLogin]\",10:\"[CloudFlowLogout]\",11:\"[CsvExporterDetailsPage]\",12:\"[DashboardPage]\",13:\"[ExecutionDetailsPage]\",14:\"[JobDetailsPage]\",15:\"[JobsTablePage]\",16:\"[ManagedWorkloadsTablePage]\",17:\"[NSDetailsContainerPage]\",18:\"[OperationsDetailsPage]\",19:\"[ResourceCompositionDetailsPage]\",20:\"[ResourceCompositionsPage]\",21:\"[ResourcesTablePage]\",22:\"[VimDetailsPage]\",23:\"[loginPage]\"}[e]||e)+\".\"+{0:\"31d6cfe0\",1:\"31d6cfe0\",2:\"559272a8\",3:\"31d6cfe0\",4:\"31d6cfe0\",5:\"affca9cb\",6:\"affca9cb\",7:\"91932cb4\",8:\"bf9ba349\",9:\"31d6cfe0\",10:\"31d6cfe0\",11:\"a0ed0644\",12:\"b2dca462\",13:\"b77ab692\",14:\"4ca14441\",15:\"f59232c1\",16:\"c1e044f2\",17:\"398f0759\",18:\"d8693b3e\",19:\"7b4dfbce\",20:\"d9277a02\",21:\"03edf692\",22:\"b6690f64\",23:\"31d6cfe0\",26:\"4391d164\",28:\"31d6cfe0\",29:\"31d6cfe0\"}[e]+\".chunk.css\",n=l.p+r,i=document.getElementsByTagName(\"link\"),c=0;c<i.length;c++){var s=(f=i[c]).getAttribute(\"data-href\")||f.getAttribute(\"href\");if(\"stylesheet\"===f.rel&&(s===r||s===n))return t()}var u=document.getElementsByTagName(\"style\");for(c=0;c<u.length;c++){var f;if((s=(f=u[c]).getAttribute(\"data-href\"))===r||s===n)return t()}var d=document.createElement(\"link\");d.rel=\"stylesheet\",d.type=\"text/css\",d.onload=t,d.onerror=function(t){var r=t&&t.target&&t.target.src||n,i=new Error(\"Loading CSS chunk \"+e+\" failed.\\n(\"+r+\")\");i.code=\"CSS_CHUNK_LOAD_FAILED\",i.request=r,delete o[e],d.parentNode.removeChild(d),a(i)},d.href=n,document.getElementsByTagName(\"head\")[0].appendChild(d)})).then((function(){o[e]=0})));var a=n[e];if(0!==a)if(a)t.push(a[2]);else{var r=new Promise((function(t,r){a=n[e]=[t,r]}));t.push(a[2]=r);var i,c=document.createElement(\"script\");c.charset=\"utf-8\",c.timeout=120,l.nc&&c.setAttribute(\"nonce\",l.nc),c.src=function(e){return l.p+\"static/js/\"+({5:\"[AlarmDetailsPage]\",6:\"[AlarmPage]\",7:\"[CatalogDetailsPage]\",8:\"[CatalogTablePage]\",9:\"[CloudFlowLogin]\",10:\"[CloudFlowLogout]\",11:\"[CsvExporterDetailsPage]\",12:\"[DashboardPage]\",13:\"[ExecutionDetailsPage]\",14:\"[JobDetailsPage]\",15:\"[JobsTablePage]\",16:\"[ManagedWorkloadsTablePage]\",17:\"[NSDetailsContainerPage]\",18:\"[OperationsDetailsPage]\",19:\"[ResourceCompositionDetailsPage]\",20:\"[ResourceCompositionsPage]\",21:\"[ResourcesTablePage]\",22:\"[VimDetailsPage]\",23:\"[loginPage]\"}[e]||e)+\".\"+{0:\"9ea1f8f1\",1:\"d7647e6d\",2:\"49e46739\",3:\"6bb32e31\",4:\"5066ed38\",5:\"3e390b6c\",6:\"88b37376\",7:\"e7438c70\",8:\"f547dc3b\",9:\"c157abf8\",10:\"c3c2011c\",11:\"62ea6190\",12:\"c002dc82\",13:\"f0d807bf\",14:\"cee9c63a\",15:\"737e42f0\",16:\"2aae2df5\",17:\"4a2ec46d\",18:\"4777a959\",19:\"f28d098b\",20:\"7e96e980\",21:\"abc77c4e\",22:\"9d55d492\",23:\"44757b03\",26:\"c04605d3\",28:\"52a01c15\",29:\"f0708fbb\"}[e]+\".chunk.js\"}(e);var s=new Error;i=function(t){c.onerror=c.onload=null,clearTimeout(u);var a=n[e];if(0!==a){if(a){var r=t&&(\"load\"===t.type?\"missing\":t.type),o=t&&t.target&&t.target.src;s.message=\"Loading chunk \"+e+\" failed.\\n(\"+r+\": \"+o+\")\",s.name=\"ChunkLoadError\",s.type=r,s.request=o,a[1](s)}n[e]=void 0}};var u=setTimeout((function(){i({type:\"timeout\",target:c})}),12e4);c.onerror=c.onload=i,document.head.appendChild(c)}return Promise.all(t)},l.m=e,l.c=r,l.d=function(e,t,a){l.o(e,t)||Object.defineProperty(e,t,{enumerable:!0,get:a})},l.r=function(e){\"undefined\"!=typeof Symbol&&Symbol.toStringTag&&Object.defineProperty(e,Symbol.toStringTag,{value:\"Module\"}),Object.defineProperty(e,\"__esModule\",{value:!0})},l.t=function(e,t){if(1&t&&(e=l(e)),8&t)return e;if(4&t&&\"object\"==typeof e&&e&&e.__esModule)return e;var a=Object.create(null);if(l.r(a),Object.defineProperty(a,\"default\",{enumerable:!0,value:e}),2&t&&\"string\"!=typeof e)for(var r in e)l.d(a,r,function(t){return e[t]}.bind(null,r));return a},l.n=function(e){var t=e&&e.__esModule?function(){return e.default}:function(){return e};return l.d(t,\"a\",t),t},l.o=function(e,t){return Object.prototype.hasOwnProperty.call(e,t)},l.p=\"/\",l.oe=function(e){throw console.error(e),e};var c=this.webpackJsonpfrontend=this.webpackJsonpfrontend||[],s=c.push.bind(c);c.push=t,c=c.slice();for(var u=0;u<c.length;u++)t(c[u]);var f=s;a()}([])</script><script src=\"/static/js/27.08ffca3e.chunk.js\"></script><script src=\"/static/js/main.90334f28.chunk.js\"></script></body></html>sh-5.1#","title":"metalb configuration troubleshooting"},{"location":"openshift/networking/metallb-configuration/","text":"","title":"MetalLB Configuration"},{"location":"openshift/storagemanagement/ceph-rebalanceissue/","text":"[root@dom14npv101-infra-manager ~ vlabrc]# oc rsh -n openshift-storage pod/$CEPH_TOOL_POD ceph health detail HEALTH_WARN 1 filesystem is degraded; 11 osds down; 3 hosts (23 osds) down; Reduced data availability: 247 pgs inactive; Degraded data redundancy: 5609127/17836029 objects degraded (31.448%), 811 pgs degraded, 839 pgs undersized; 1597 slow ops, oldest one blocked for 16139 sec, daemons [osd.0,osd.10,osd.13,osd.19,osd.22,osd.27,osd.29,osd.3,osd.35,osd.36]... have slow ops. [WRN] FS_DEGRADED: 1 filesystem is degraded fs ocs-storagecluster-cephfilesystem is degraded [WRN] OSD_DOWN: 11 osds down osd.7 (root=default,host=ncpvnpvlab1-storage-102-ncpvnpvlab1-pnwlab-nsn-rdnet-net) is down osd.9 (root=default,host=ncpvnpvlab1-storage-101-ncpvnpvlab1-pnwlab-nsn-rdnet-net) is down osd.28 (root=default,host=ncpvnpvlab1-storage-101-ncpvnpvlab1-pnwlab-nsn-rdnet-net) is down osd.30 (root=default,host=ncpvnpvlab1-storage-203-ncpvnpvlab1-pnwlab-nsn-rdnet-net) is down osd.31 (root=default,host=ncpvnpvlab1-storage-102-ncpvnpvlab1-pnwlab-nsn-rdnet-net) is down osd.33 (root=default,host=ncpvnpvlab1-storage-102-ncpvnpvlab1-pnwlab-nsn-rdnet-net) is down osd.34 (root=default,host=ncpvnpvlab1-storage-101-ncpvnpvlab1-pnwlab-nsn-rdnet-net) is down osd.37 (root=default,host=ncpvnpvlab1-storage-203-ncpvnpvlab1-pnwlab-nsn-rdnet-net) is down osd.40 (root=default,host=ncpvnpvlab1-storage-203-ncpvnpvlab1-pnwlab-nsn-rdnet-net) is down osd.41 (root=default,host=ncpvnpvlab1-storage-101-ncpvnpvlab1-pnwlab-nsn-rdnet-net) is down osd.44 (root=default,host=ncpvnpvlab1-storage-101-ncpvnpvlab1-pnwlab-nsn-rdnet-net) is down [WRN] OSD_HOST_DOWN: 3 hosts (23 osds) down host ncpvnpvlab1-storage-101-ncpvnpvlab1-pnwlab-nsn-rdnet-net (root=default) (8 osds) is down host ncpvnpvlab1-storage-203-ncpvnpvlab1-pnwlab-nsn-rdnet-net (root=default) (8 osds) is down host ncpvnpvlab1-storage-102-ncpvnpvlab1-pnwlab-nsn-rdnet-net (root=default) (7 osds) is down [WRN] PG_AVAILABILITY: Reduced data availability: 247 pgs inactive pg 9.91 is stuck inactive for 4h, current state unknown, last acting [] pg 9.93 is stuck inactive for 4h, current state undersized+degraded+peered, last acting [45] pg 9.97 is stuck inactive for 4h, current state undersized+degraded+peered, last acting [47] pg 9.9e is stuck inactive for 4h, current state unknown, last acting [] pg 9.9f is stuck inactive for 4h, current state undersized+degraded+peered, last acting [5] pg 9.a0 is stuck inactive for 4h, current state unknown, last acting [] pg 9.a6 is stuck inactive for 4h, current state undersized+degraded+peered, last acting [22] pg 9.ab is stuck inactive for 4h, current state undersized+degraded+peered, last acting [35] pg 9.ae is stuck inactive for 4h, current state undersized+degraded+peered, last acting [25] pg 9.af is stuck inactive for 4h, current state undersized+degraded+peered, last acting [8] pg 9.b2 is stuck inactive for 4h, current state unknown, last acting [] pg 9.b3 is stuck inactive for 4h, current state undersized+degraded+peered, last acting [35] pg 9.b4 is stuck inactive for 4h, current state unknown, last acting [] pg 9.c5 is stuck inactive for 4h, current state undersized+degraded+peered, last acting [42] pg 9.c6 is stuck inactive for 4h, current state undersized+degraded+peered, last acting [3] pg 9.c7 is stuck inactive for 4h, current state undersized+degraded+peered, last acting [46] pg 9.1e7 is stuck inactive for 4h, current state undersized+degraded+peered, last acting [46] pg 9.1eb is stuck inactive for 4h, current state undersized+degraded+peered, last acting [29] pg 9.1f5 is stuck inactive for 4h, current state undersized+degraded+peered, last acting [0] pg 9.1fa is stuck inactive for 4h, current state undersized+degraded+peered, last acting [22] pg 9.1fb is stuck inactive for 4h, current state unknown, last acting [] pg 11.95 is stuck inactive for 4h, current state undersized+degraded+peered, last acting [46] pg 11.98 is stuck inactive for 4h, current state undersized+degraded+peered, last acting [19] pg 11.9b is stuck inactive for 4h, current state undersized+degraded+peered, last acting [32] pg 11.a0 is stuck inactive for 4h, current state undersized+degraded+peered, last acting [36] pg 11.a1 is stuck inactive for 4h, current state unknown, last acting [] pg 11.a3 is stuck inactive for 4h, current state undersized+degraded+peered, last acting [3] pg 11.a8 is stuck inactive for 4h, current state undersized+degraded+peered, last acting [23] pg 11.ac is stuck inactive for 4h, current state undersized+degraded+peered, last acting [43] pg 11.b0 is stuck inactive for 4h, current state undersized+degraded+peered, last acting [39] pg 11.b3 is stuck inactive for 4h, current state undersized+degraded+peered, last acting [3] pg 11.b5 is stuck inactive for 4h, current state undersized+degraded+peered, last acting [10] pg 11.b9 is stuck inactive for 4h, current state undersized+degraded+peered, last acting [43] pg 11.bb is stuck inactive for 4h, current state unknown, last acting [] pg 11.bc is stuck inactive for 4h, current state undersized+degraded+peered, last acting [42] pg 11.c0 is stuck inactive for 4h, current state undersized+degraded+peered, last acting [15] pg 11.c2 is stuck inactive for 4h, current state undersized+degraded+peered, last acting [3] pg 11.c3 is stuck inactive for 4h, current state undersized+degraded+peered, last acting [3] pg 12.96 is stuck inactive for 4h, current state undersized+degraded+peered, last acting [23] pg 12.9b is stuck inactive for 4h, current state undersized+degraded+peered, last acting [8] pg 12.9c is stuck inactive for 4h, current state undersized+degraded+peered, last acting [38] pg 12.9d is stuck inactive for 4h, current state undersized+degraded+peered, last acting [10] pg 12.ab is stuck inactive for 4h, current state undersized+degraded+peered, last acting [36] pg 12.af is stuck inactive for 4h, current state undersized+degraded+peered, last acting [20] pg 12.b1 is stuck inactive for 4h, current state undersized+degraded+peered, last acting [0] pg 12.b4 is stuck inactive for 4h, current state undersized+degraded+peered, last acting [13] pg 12.b5 is stuck inactive for 4h, current state undersized+degraded+peered, last acting [38] pg 12.be is stuck inactive for 4h, current state undersized+degraded+peered, last acting [3] pg 12.c4 is stuck inactive for 4h, current state undersized+degraded+peered, last acting [23] pg 12.c5 is stuck inactive for 4h, current state undersized+degraded+peered, last acting [39] pg 12.c7 is stuck inactive for 4h, current state undersized+degraded+peered, last acting [13] [WRN] PG_DEGRADED: Degraded data redundancy: 5609127/17836029 objects degraded (31.448%), 811 pgs degraded, 839 pgs undersized pg 9.bf is active+undersized+degraded, acting [29,35] pg 9.c0 is stuck undersized for 4h, current state active+undersized+degraded, last acting [0,45] pg 9.c1 is stuck undersized for 3h, current state active+undersized+remapped, last acting [46,45] pg 9.c2 is stuck undersized for 4h, current state active+undersized+degraded, last acting [10,13] pg 9.c3 is stuck undersized for 4h, current state active+undersized+degraded, last acting [27,22] pg 9.c4 is stuck undersized for 4h, current state active+undersized+degraded, last acting [36,45] pg 9.c5 is stuck undersized for 4h, current state undersized+degraded+peered, last acting [42] pg 9.c6 is stuck undersized for 4h, current state undersized+degraded+peered, last acting [3] pg 9.c7 is stuck undersized for 4h, current state undersized+degraded+peered, last acting [46] pg 9.1e4 is stuck undersized for 4h, current state active+undersized+degraded, last acting [39,25] pg 9.1e5 is stuck undersized for 4h, current state active+undersized+degraded, last acting [10,43] pg 9.1e6 is stuck undersized for 4h, current state active+undersized+degraded, last acting [39,5] pg 9.1e7 is stuck undersized for 4h, current state undersized+degraded+peered, last acting [46] pg 9.1e8 is stuck undersized for 4h, current state active+undersized+degraded, last acting [23,8] pg 9.1e9 is stuck undersized for 4h, current state active+undersized+degraded, last acting [47,27] pg 9.1ea is stuck undersized for 4h, current state active+undersized+degraded, last acting [19,36] pg 9.1eb is stuck undersized for 4h, current state undersized+degraded+peered, last acting [29] pg 9.1ec is stuck undersized for 4h, current state active+undersized+degraded, last acting [3,42] pg 9.1ed is stuck undersized for 4h, current state active+undersized+degraded, last acting [15,43] pg 9.1ee is stuck undersized for 4h, current state active+undersized+degraded, last acting [19,36] pg 9.1ef is stuck undersized for 4h, current state active+undersized+degraded, last acting [0,20] pg 9.1f1 is stuck undersized for 4h, current state active+undersized+degraded, last acting [47,0] pg 9.1f2 is stuck undersized for 4h, current state active+undersized+degraded, last acting [42,35] pg 9.1f3 is stuck undersized for 4h, current state active+undersized+degraded, last acting [47,27] pg 9.1f4 is stuck undersized for 4h, current state active+undersized+degraded, last acting [36,46] pg 9.1f5 is stuck undersized for 4h, current state undersized+degraded+peered, last acting [0] pg 9.1f6 is stuck undersized for 3h, current state active+undersized+degraded, last acting [38,46] pg 9.1f7 is stuck undersized for 4h, current state active+undersized+degraded, last acting [3,0] pg 9.1f8 is stuck undersized for 4h, current state active+undersized+degraded, last acting [3,15] pg 9.1f9 is stuck undersized for 4h, current state active+undersized+degraded, last acting [15,43] pg 9.1fa is stuck undersized for 4h, current state undersized+degraded+peered, last acting [22] pg 9.1fc is stuck undersized for 4h, current state active+undersized+degraded, last acting [35,29] pg 9.1fd is stuck undersized for 4h, current state active+undersized+degraded, last acting [5,19] pg 9.1fe is stuck undersized for 4h, current state active+undersized+degraded, last acting [29,0] pg 9.1ff is stuck undersized for 4h, current state active+undersized+degraded, last acting [43,27] pg 11.bd is stuck undersized for 4h, current state active+undersized+degraded, last acting [10,42] pg 11.c0 is stuck undersized for 4h, current state undersized+degraded+peered, last acting [15] pg 11.c1 is stuck undersized for 4h, current state active+undersized+degraded, last acting [5,32] pg 11.c2 is stuck undersized for 4h, current state undersized+degraded+peered, last acting [3] pg 11.c3 is stuck undersized for 4h, current state undersized+degraded+peered, last acting [3] pg 11.c4 is stuck undersized for 4h, current state active+undersized+degraded, last acting [42,39] pg 11.c5 is stuck undersized for 4h, current state active+undersized+degraded, last acting [46,27] pg 11.c7 is stuck undersized for 4h, current state active+undersized+degraded, last acting [42,36] pg 12.c0 is stuck undersized for 4h, current state active+undersized+degraded, last acting [47,23] pg 12.c1 is stuck undersized for 4h, current state active+undersized+degraded, last acting [25,32] pg 12.c2 is stuck undersized for 4h, current state active+undersized+degraded, last acting [20,0] pg 12.c3 is stuck undersized for 4h, current state active+undersized+degraded, last acting [22,13] pg 12.c4 is stuck undersized for 4h, current state undersized+degraded+peered, last acting [23] pg 12.c5 is stuck undersized for 4h, current state undersized+degraded+peered, last acting [39] pg 12.c6 is stuck undersized for 4h, current state active+undersized+degraded, last acting [29,32] pg 12.c7 is stuck undersized for 4h, current state undersized+degraded+peered, last acting [13] [WRN] SLOW_OPS: 1597 slow ops, oldest one blocked for 16139 sec, daemons [osd.0,osd.10,osd.13,osd.19,osd.22,osd.27,osd.29,osd.3,osd.35,osd.36]... have slow ops. [root@dom14npv101-infra-manager ~ vlabrc]# oc rsh -n openshift-storage pod/$CEPH_TOOL_POD ceph -s cluster: id: a2b0c334-ba7c-4ae1-b3f5-c6d514f19bec health: HEALTH_WARN 1 filesystem is degraded 11 osds down 3 hosts (23 osds) down Reduced data availability: 247 pgs inactive Degraded data redundancy: 5609127/17836029 objects degraded (31.448%), 811 pgs degraded, 839 pgs undersized 1597 slow ops, oldest one blocked for 16159 sec, daemons [osd.0,osd.10,osd.13,osd.19,osd.22,osd.27,osd.29,osd.3,osd.35,osd.36]... have slow ops. services: mon: 3 daemons, quorum h,j,k (age 4h) mgr: a(active, since 4h), standbys: b mds: 1/1 daemons up, 1 standby osd: 47 osds: 24 up (since 4h), 35 in (since 4h); 42 remapped pgs data: volumes: 0/1 healthy, 1 recovering pools: 12 pools, 1097 pgs objects: 5.95M objects, 20 TiB usage: 41 TiB used, 99 TiB / 140 TiB avail pgs: 3.829% pgs unknown 18.687% pgs not active 5609127/17836029 objects degraded (31.448%) 279094/17836029 objects misplaced (1.565%) 610 active+undersized+degraded 201 undersized+degraded+peered 190 active+clean 42 unknown 26 active+clean+remapped 16 active+undersized+remapped 8 active+undersized 4 undersized+peered [root@dom14npv101-infra-manager ~ vlabrc]# oc rsh -n openshift-storage pod/$CEPH_TOOL_POD ceph -s E0501 23:49:07.749890 2538728 memcache.go:265] couldn't get current server API group list: the server has asked for the client to provide credentials E0501 23:49:07.787031 2538728 memcache.go:265] couldn't get current server API group list: the server has asked for the client to provide credentials E0501 23:49:07.822712 2538728 memcache.go:265] couldn't get current server API group list: the server has asked for the client to provide credentials E0501 23:49:07.855100 2538728 memcache.go:265] couldn't get current server API group list: the server has asked for the client to provide credentials error: You must be logged in to the server (the server has asked for the client to provide credentials) [root@dom14npv101-infra-manager ~ vlabrc]# source /root/raj/ alarms/ health.py ingress_ca.crt ncp-health.py security/ amc-backup/ htpasswdhub install-config new.py storage-logging-loki-compactor-0.yaml backup-etcd/ htpasswdmang kubeadmin.yaml oauth.yaml testing.txt beacon.k8s.worker.tar htpasswdnlab localcert.crt pull_secret_cwl_dockerconfigjson.json users.htpasswd cephvlanrc.yaml hubconfig management_new_health_output.txt registry-cas.yaml vlab1config fedora-tools.tar.gz hub_new_health_output.txt managementrc resourcequota.yaml vlabrc git/ hubrc managementrcconfig rr.txt vlabrc_new_health_output.txt [root@dom14npv101-infra-manager ~ vlabrc]# source /root/raj/vlabrc WARNING: Using insecure TLS client config. Setting this option is not supported! Login successful. You have access to 115 projects, the list has been suppressed. You can list all projects with 'oc projects' Using project \"openshift-storage\". [root@dom14npv101-infra-manager ~ vlabrc]# oc rsh -n openshift-storage pod/$CEPH_TOOL_POD ceph -s cluster: id: a2b0c334-ba7c-4ae1-b3f5-c6d514f19bec health: HEALTH_WARN 1 filesystem is degraded 11 osds down 3 hosts (23 osds) down Reduced data availability: 247 pgs inactive Degraded data redundancy: 5609127/17836029 objects degraded (31.448%), 811 pgs degraded, 839 pgs undersized 1597 slow ops, oldest one blocked for 24624 sec, daemons [osd.0,osd.10,osd.13,osd.19,osd.22,osd.27,osd.29,osd.3,osd.35,osd.36]... have slow ops. services: mon: 3 daemons, quorum h,j,k (age 6h) mgr: a(active, since 6h), standbys: b mds: 1/1 daemons up, 1 standby osd: 47 osds: 24 up (since 7h), 35 in (since 6h); 42 remapped pgs data: volumes: 0/1 healthy, 1 recovering pools: 12 pools, 1097 pgs objects: 5.95M objects, 20 TiB usage: 41 TiB used, 99 TiB / 140 TiB avail pgs: 3.829% pgs unknown 18.687% pgs not active 5609127/17836029 objects degraded (31.448%) 279094/17836029 objects misplaced (1.565%) 610 active+undersized+degraded 201 undersized+degraded+peered 190 active+clean 42 unknown 26 active+clean+remapped 16 active+undersized+remapped 8 active+undersized 4 undersized+peered [root@dom14npv101-infra-manager ~ vlabrc]# oc rsh -n openshift-storage pod/$CEPH_TOOL_POD ceph osd tree ID CLASS WEIGHT TYPE NAME STATUS REWEIGHT PRI-AFF -1 273.62927 root default -3 46.57520 host ncpvnpvlab1-storage-101-ncpvnpvlab1-pnwlab-nsn-rdnet-net 4 ssd 5.82190 osd.4 down 0 1.00000 9 ssd 5.82190 osd.9 down 1.00000 1.00000 14 ssd 5.82190 osd.14 down 0 1.00000 21 ssd 5.82190 osd.21 down 0 1.00000 28 ssd 5.82190 osd.28 down 1.00000 1.00000 34 ssd 5.82190 osd.34 down 1.00000 1.00000 41 ssd 5.82190 osd.41 down 1.00000 1.00000 44 ssd 5.82190 osd.44 down 1.00000 1.00000 -7 40.75330 host ncpvnpvlab1-storage-102-ncpvnpvlab1-pnwlab-nsn-rdnet-net 7 ssd 5.82190 osd.7 down 1.00000 1.00000 11 ssd 5.82190 osd.11 down 0 1.00000 17 ssd 5.82190 osd.17 down 0 1.00000 18 ssd 5.82190 osd.18 down 0 1.00000 24 ssd 5.82190 osd.24 down 0 1.00000 31 ssd 5.82190 osd.31 down 1.00000 1.00000 33 ssd 5.82190 osd.33 down 1.00000 1.00000 -9 46.57520 host ncpvnpvlab1-storage-103-ncpvnpvlab1-pnwlab-nsn-rdnet-net 0 ssd 5.82190 osd.0 up 1.00000 1.00000 5 ssd 5.82190 osd.5 up 1.00000 1.00000 10 ssd 5.82190 osd.10 up 1.00000 1.00000 15 ssd 5.82190 osd.15 up 1.00000 1.00000 23 ssd 5.82190 osd.23 up 1.00000 1.00000 25 ssd 5.82190 osd.25 up 1.00000 1.00000 35 ssd 5.82190 osd.35 up 1.00000 1.00000 36 ssd 5.82190 osd.36 up 1.00000 1.00000 -11 46.57520 host ncpvnpvlab1-storage-201-ncpvnpvlab1-pnwlab-nsn-rdnet-net 19 ssd 5.82190 osd.19 up 1.00000 1.00000 22 ssd 5.82190 osd.22 up 1.00000 1.00000 32 ssd 5.82190 osd.32 up 1.00000 1.00000 42 ssd 5.82190 osd.42 up 1.00000 1.00000 43 ssd 5.82190 osd.43 up 1.00000 1.00000 45 ssd 5.82190 osd.45 up 1.00000 1.00000 46 ssd 5.82190 osd.46 up 1.00000 1.00000 47 ssd 5.82190 osd.47 up 1.00000 1.00000 -13 46.57520 host ncpvnpvlab1-storage-202-ncpvnpvlab1-pnwlab-nsn-rdnet-net 3 ssd 5.82190 osd.3 up 1.00000 1.00000 8 ssd 5.82190 osd.8 up 1.00000 1.00000 13 ssd 5.82190 osd.13 up 1.00000 1.00000 20 ssd 5.82190 osd.20 up 1.00000 1.00000 27 ssd 5.82190 osd.27 up 1.00000 1.00000 29 ssd 5.82190 osd.29 up 1.00000 1.00000 38 ssd 5.82190 osd.38 up 1.00000 1.00000 39 ssd 5.82190 osd.39 up 1.00000 1.00000 -5 46.57520 host ncpvnpvlab1-storage-203-ncpvnpvlab1-pnwlab-nsn-rdnet-net 2 ssd 5.82190 osd.2 down 0 1.00000 6 ssd 5.82190 osd.6 down 0 1.00000 12 ssd 5.82190 osd.12 down 0 1.00000 16 ssd 5.82190 osd.16 down 0 1.00000 26 ssd 5.82190 osd.26 down 0 1.00000 30 ssd 5.82190 osd.30 down 1.00000 1.00000 37 ssd 5.82190 osd.37 down 1.00000 1.00000 40 ssd 5.82190 osd.40 down 1.00000 1.00000 [root@dom14npv101-infra-manager ~ vlabrc]# oc rsh -n openshift-storage pod/$CEPH_TOOL_POD ceph osd out 4,9 Error EINVAL: Expected option value to be integer, got '4,9'invalid osd id-1 command terminated with exit code 22 [root@dom14npv101-infra-manager ~ vlabrc]# oc rsh -n openshift-storage pod/$CEPH_TOOL_POD ceph osd out 4 osd.4 is already out. [root@dom14npv101-infra-manager ~ vlabrc]# oc rsh -n openshift-storage pod/$CEPH_TOOL_POD ceph osd out 9 marked out osd.9. [root@dom14npv101-infra-manager ~ vlabrc]# oc rsh -n openshift-storage pod/$CEPH_TOOL_POD ceph osd out 14 osd.14 is already out. [root@dom14npv101-infra-manager ~ vlabrc]# oc rsh -n openshift-storage pod/$CEPH_TOOL_POD ceph osd out ^C [root@dom14npv101-infra-manager ~ vlabrc]# [root@dom14npv101-infra-manager ~ vlabrc]# [root@dom14npv101-infra-manager ~ vlabrc]# [root@dom14npv101-infra-manager ~ vlabrc]# oc rsh -n openshift-storage pod/$CEPH_TOOL_POD ceph osd out 4 oc rsh -n openshift-storage pod/$CEPH_TOOL_POD ceph osd out 9 oc rsh -n openshift-storage pod/$CEPH_TOOL_POD ceph osd out 14 oc rsh -n openshift-storage pod/$CEPH_TOOL_POD ceph osd out 21 oc rsh -n openshift-storage pod/$CEPH_TOOL_POD ceph osd out 28 oc rsh -n openshift-storage pod/$CEPH_TOOL_POD ceph osd out 34 oc rsh -n openshift-storage pod/$CEPH_TOOL_POD ceph osd out 41 oc rsh -n openshift-storage pod/$CEPH_TOOL_POD ceph osd out 44 oc rsh -n openshift-storage pod/$CEPH_TOOL_POD ceph osd out 7 oc rsh -n openshift-storage pod/$CEPH_TOOL_POD ceph osd out 11 oc rsh -n openshift-storage pod/$CEPH_TOOL_POD ceph osd out 17 oc rsh -n openshift-storage pod/$CEPH_TOOL_POD ceph osd out 18 oc rsh -n openshift-storage pod/$CEPH_TOOL_POD ceph osd out 24 oc rsh -n openshift-storage pod/$CEPH_TOOL_POD ceph osd out 31 oc rsh -n openshift-storage pod/$CEPH_TOOL_POD ceph osd out 33 oc rsh -n openshift-storage pod/$CEPH_TOOL_POD ceph osd out 2 oc rsh -n openshift-storage pod/$CEPH_TOOL_POD ceph osd out 6 oc rsh -n openshift-storage pod/$CEPH_TOOL_POD ceph osd out 12 oc rsh -n openshift-storage pod/$CEPH_TOOL_POD ceph osd out 16 oc rsh -n openshift-storage pod/$CEPH_TOOL_POD ceph osd out 26 oc rsh -n openshift-storage pod/$CEPH_TOOL_POD ceph osd out 30 oc rsh -n openshift-storage pod/$CEPH_TOOL_POD ceph osd out 37 oc rsh -n openshift-storage pod/$CEPH_TOOL_POD ceph osd out 40 osd.4 is already out. osd.9 is already out. osd.14 is already out. osd.21 is already out. marked out osd.28. marked out osd.34. marked out osd.41. marked out osd.44. marked out osd.7. osd.11 is already out. osd.17 is already out. osd.18 is already out. osd.24 is already out. marked out osd.31. marked out osd.33. osd.2 is already out. osd.6 is already out. osd.12 is already out. osd.16 is already out. osd.26 is already out. marked out osd.30. marked out osd.37. marked out osd.40. [root@dom14npv101-infra-manager ~ vlabrc]# oc rsh -n openshift-storage pod/$CEPH_TOOL_POD ceph osd purge 4 --yes-i-really-mean-it oc rsh -n openshift-storage pod/$CEPH_TOOL_POD ceph osd purge 9 --yes-i-really-mean-it oc rsh -n openshift-storage pod/$CEPH_TOOL_POD ceph osd purge 14 --yes-i-really-mean-it oc rsh -n openshift-storage pod/$CEPH_TOOL_POD ceph osd purge 21 --yes-i-really-mean-it oc rsh -n openshift-storage pod/$CEPH_TOOL_POD ceph osd purge 28 --yes-i-really-mean-it oc rsh -n openshift-storage pod/$CEPH_TOOL_POD ceph osd purge 34 --yes-i-really-mean-it oc rsh -n openshift-storage pod/$CEPH_TOOL_POD ceph osd purge 41 --yes-i-really-mean-it oc rsh -n openshift-storage pod/$CEPH_TOOL_POD ceph osd purge 44 --yes-i-really-mean-it oc rsh -n openshift-storage pod/$CEPH_TOOL_POD ceph osd purge 7 --yes-i-really-mean-it oc rsh -n openshift-storage pod/$CEPH_TOOL_POD ceph osd purge 11 --yes-i-really-mean-it oc rsh -n openshift-storage pod/$CEPH_TOOL_POD ceph osd purge 17 --yes-i-really-mean-it oc rsh -n openshift-storage pod/$CEPH_TOOL_POD ceph osd purge 18 --yes-i-really-mean-it oc rsh -n openshift-storage pod/$CEPH_TOOL_POD ceph osd purge 24 --yes-i-really-mean-it oc rsh -n openshift-storage pod/$CEPH_TOOL_POD ceph osd purge 31 --yes-i-really-mean-it oc rsh -n openshift-storage pod/$CEPH_TOOL_POD ceph osd purge 33 --yes-i-really-mean-it purged osd.4 purged osd.9 purged osd.14 purged osd.21 purged osd.28 purged osd.34 purged osd.41 purged osd.44 purged osd.7 purged osd.11 purged osd.17 purged osd.18 purged osd.24 purged osd.31 purged osd.33 [root@dom14npv101-infra-manager ~ vlabrc]# [root@dom14npv101-infra-manager ~ vlabrc]# [root@dom14npv101-infra-manager ~ vlabrc]# [root@dom14npv101-infra-manager ~ vlabrc]# oc rsh -n openshift-storage pod/$CEPH_TOOL_POD ceph -s cluster: id: a2b0c334-ba7c-4ae1-b3f5-c6d514f19bec health: HEALTH_WARN 1 filesystem is degraded Reduced data availability: 203 pgs inactive Degraded data redundancy: 5434540/17836062 objects degraded (30.469%), 800 pgs degraded, 218 pgs undersized 62 slow ops, oldest one blocked for 24934 sec, daemons [osd.10,osd.13,osd.19,osd.22,osd.35,osd.36,osd.42,osd.45,osd.5] have slow ops. services: mon: 3 daemons, quorum h,j,k (age 6h) mgr: a(active, since 6h), standbys: b mds: 1/1 daemons up, 1 standby osd: 32 osds: 24 up (since 7h), 24 in (since 119s); 879 remapped pgs data: volumes: 0/1 healthy, 1 recovering pools: 12 pools, 1097 pgs objects: 5.95M objects, 20 TiB usage: 42 TiB used, 98 TiB / 140 TiB avail pgs: 3.829% pgs unknown 14.676% pgs not active 5434540/17836062 objects degraded (30.469%) 1857921/17836062 objects misplaced (10.417%) 639 active+undersized+degraded+remapped+backfill_wait 176 active+clean 157 undersized+degraded+remapped+backfill_wait+peered 79 active+remapped+backfill_wait 42 unknown 4 undersized+degraded+remapped+backfilling+peered io: client: 2.3 MiB/s wr, 0 op/s rd, 6 op/s wr recovery: 1.4 GiB/s, 0 keys/s, 455 objects/s [root@dom14npv101-infra-manager ~ vlabrc]# oc rsh -n openshift-storage pod/$CEPH_TOOL_POD ceph -s cluster: id: a2b0c334-ba7c-4ae1-b3f5-c6d514f19bec health: HEALTH_WARN 1 filesystem is degraded Reduced data availability: 203 pgs inactive Degraded data redundancy: 5429732/17836062 objects degraded (30.442%), 800 pgs degraded, 218 pgs undersized 62 slow ops, oldest one blocked for 24939 sec, daemons [osd.10,osd.13,osd.19,osd.22,osd.35,osd.36,osd.42,osd.45,osd.5] have slow ops. services: mon: 3 daemons, quorum h,j,k (age 6h) mgr: a(active, since 6h), standbys: b mds: 1/1 daemons up, 1 standby osd: 32 osds: 24 up (since 7h), 24 in (since 2m); 879 remapped pgs data: volumes: 0/1 healthy, 1 recovering pools: 12 pools, 1097 pgs objects: 5.95M objects, 20 TiB usage: 42 TiB used, 98 TiB / 140 TiB avail pgs: 3.829% pgs unknown 14.676% pgs not active 5429732/17836062 objects degraded (30.442%) 1857921/17836062 objects misplaced (10.417%) 639 active+undersized+degraded+remapped+backfill_wait 176 active+clean 157 undersized+degraded+remapped+backfill_wait+peered 79 active+remapped+backfill_wait 42 unknown 4 undersized+degraded+remapped+backfilling+peered io: client: 1.5 MiB/s wr, 0 op/s rd, 6 op/s wr recovery: 1.7 GiB/s, 0 keys/s, 507 objects/s [root@dom14npv101-infra-manager ~ vlabrc]#","title":"Ceph Rebalance Issue"},{"location":"openshift/storagemanagement/storage-node-replacement/","text":"Storage node replacement Delete the storage node Show the initial status of the storage nodes in the managed cluster (output of oc get nodes) and identify which node will be removed, e.g. storage-0. [root@dom14npv101-infra-manager ~ vlabrc]# oc get nodes |grep -i storage ncpvnpvlab1-storage-101.ncpvnpvlab1.pnwlab.nsn-rdnet.net Ready storage,worker 25d v1.29.10+67d3387 ncpvnpvlab1-storage-102.ncpvnpvlab1.pnwlab.nsn-rdnet.net Ready storage,worker 27d v1.29.10+67d3387 ncpvnpvlab1-storage-103.ncpvnpvlab1.pnwlab.nsn-rdnet.net Ready storage,worker 27d v1.29.10+67d3387 ncpvnpvlab1-storage-201.ncpvnpvlab1.pnwlab.nsn-rdnet.net Ready storage,worker 25d v1.29.10+67d3387 ncpvnpvlab1-storage-202.ncpvnpvlab1.pnwlab.nsn-rdnet.net Ready storage,worker 27d v1.29.10+67d3387 ncpvnpvlab1-storage-203.ncpvnpvlab1.pnwlab.nsn-rdnet.net Ready storage,worker 27d v1.29.10+67d3387 [root@dom14npv101-infra-manager ~ vlabrc]# Verify the ceph health status [root@dom14npv101-infra-manager ~ vlabrc]# oc exec -it $(oc get pod -n openshift-storage -l app=rook-ceph-operator -o name) -n openshift-storage -- ceph -s -c /var/lib/rook/openshift-storage/openshift-storage.config cluster: id: a2b0c334-ba7c-4ae1-b3f5-c6d514f19bec health: HEALTH_OK services: mon: 3 daemons, quorum f,g,h (age 21h) mgr: a(active, since 21h), standbys: b mds: 1/1 daemons up, 1 hot standby osd: 48 osds: 48 up (since 21h), 48 in (since 23h) rgw: 1 daemon active (1 hosts, 1 zones) data: volumes: 1/1 healthy pools: 12 pools, 1097 pgs objects: 190.32k objects, 393 GiB usage: 1.2 TiB used, 278 TiB / 279 TiB avail pgs: 1097 active+clean io: client: 8.7 KiB/s rd, 9.9 MiB/s wr, 11 op/s rd, 22 op/s wr [root@dom14npv101-infra-manager ~ vlabrc]# Identify the monitor pod (if any), and OSDs that are running in the node that you need to replace: [root@dom14npv101-infra-manager ~ vlabrc]# oc get pods -n openshift-storage -o wide | grep -i ncpvnpvlab1-storage-101.ncpvnpvlab1.pnwlab.nsn-rdnet.net |grep -i rook-ceph rook-ceph-crashcollector-73c0594e536089af81dd498574227f77-94vtt 1/1 Running 0 21h 172.28.18.41 ncpvnpvlab1-storage-101.ncpvnpvlab1.pnwlab.nsn-rdnet.net <none> <none> rook-ceph-exporter-73c0594e536089af81dd498574227f77-754b5866njj 1/1 Running 0 21h 172.28.18.42 ncpvnpvlab1-storage-101.ncpvnpvlab1.pnwlab.nsn-rdnet.net <none> <none> rook-ceph-mds-ocs-storagecluster-cephfilesystem-a-795996f7lvsqs 2/2 Running 0 22h 172.28.18.21 ncpvnpvlab1-storage-101.ncpvnpvlab1.pnwlab.nsn-rdnet.net <none> <none> rook-ceph-mds-ocs-storagecluster-cephfilesystem-b-78f7bbf8c2hhg 2/2 Running 0 22h 172.28.18.22 ncpvnpvlab1-storage-101.ncpvnpvlab1.pnwlab.nsn-rdnet.net <none> <none> rook-ceph-mgr-b-5468b7cf-fmwnp 4/4 Running 0 22h 172.28.18.7 ncpvnpvlab1-storage-101.ncpvnpvlab1.pnwlab.nsn-rdnet.net <none> <none> rook-ceph-mon-f-54d858f9cd-m5q76 2/2 Running 0 22h 172.28.18.8 ncpvnpvlab1-storage-101.ncpvnpvlab1.pnwlab.nsn-rdnet.net <none> <none> rook-ceph-operator-7bc4cf5ccd-4lxjr 1/1 Running 0 22h 172.28.18.39 ncpvnpvlab1-storage-101.ncpvnpvlab1.pnwlab.nsn-rdnet.net <none> <none> rook-ceph-osd-14-6df66b8b99-nmgvq 2/2 Running 0 22h 172.28.18.9 ncpvnpvlab1-storage-101.ncpvnpvlab1.pnwlab.nsn-rdnet.net <none> <none> rook-ceph-osd-21-86cd6b7f7f-498vh 2/2 Running 0 22h 172.28.18.12 ncpvnpvlab1-storage-101.ncpvnpvlab1.pnwlab.nsn-rdnet.net <none> <none> rook-ceph-osd-28-698bb96856-vmr8t 2/2 Running 0 22h 172.28.18.11 ncpvnpvlab1-storage-101.ncpvnpvlab1.pnwlab.nsn-rdnet.net <none> <none> rook-ceph-osd-34-5f49bdbb85-f528w 2/2 Running 0 22h 172.28.18.15 ncpvnpvlab1-storage-101.ncpvnpvlab1.pnwlab.nsn-rdnet.net <none> <none> rook-ceph-osd-4-7495d5f559-zccrg 2/2 Running 0 22h 172.28.18.34 ncpvnpvlab1-storage-101.ncpvnpvlab1.pnwlab.nsn-rdnet.net <none> <none> rook-ceph-osd-41-689699f766-clfzm 2/2 Running 0 22h 172.28.18.17 ncpvnpvlab1-storage-101.ncpvnpvlab1.pnwlab.nsn-rdnet.net <none> <none> rook-ceph-osd-44-94c7c6565-cz8wg 2/2 Running 0 22h 172.28.18.16 ncpvnpvlab1-storage-101.ncpvnpvlab1.pnwlab.nsn-rdnet.net <none> <none> rook-ceph-osd-9-6b966dc5db-28595 2/2 Running 0 22h 172.28.18.18 ncpvnpvlab1-storage-101.ncpvnpvlab1.pnwlab.nsn-rdnet.net <none> <none> rook-ceph-rgw-ocs-storagecluster-cephobjectstore-a-788d79bdrltz 2/2 Running 0 22h 172.28.18.38 ncpvnpvlab1-storage-101.ncpvnpvlab1.pnwlab.nsn-rdnet.net <none> <none> rook-ceph-tools-6f854c4bfc-wqhm7 1/1 Running 0 22h 172.28.18.30 ncpvnpvlab1-storage-101.ncpvnpvlab1.pnwlab.nsn-rdnet.net <none> <none> [root@dom14npv101-infra-manager ~ vlabrc]# Scale down the deployments of the pods identified in the previous step: (mon, osd, crashcollector) [root@dom14npv101-infra-manager ~ vlabrc]# oc -n openshift-storage scale deployment rook-ceph-crashcollector --replicas=0 oc -n openshift-storage scale deployment rook-ceph-mgr-b --replicas=0 oc -n openshift-storage scale deployment rook-ceph-mon-f --replicas=0 oc -n openshift-storage scale deployment rook-ceph-osd-14 --replicas=0 oc -n openshift-storage scale deployment rook-ceph-osd-21 --replicas=0 oc -n openshift-storage scale deployment rook-ceph-osd-28 --replicas=0 oc -n openshift-storage scale deployment rook-ceph-osd-34 --replicas=0 oc -n openshift-storage scale deployment rook-ceph-osd-4 --replicas=0 oc -n openshift-storage scale deployment rook-ceph-osd-41 --replicas=0 oc -n openshift-storage scale deployment rook-ceph-osd-44 --replicas=0 oc -n openshift-storage scale deployment rook-ceph-osd-9 --replicas=0 error: no objects passed to scale deployment.apps/rook-ceph-mgr-b scaled deployment.apps/rook-ceph-mon-f scaled deployment.apps/rook-ceph-osd-14 scaled deployment.apps/rook-ceph-osd-21 scaled deployment.apps/rook-ceph-osd-28 scaled deployment.apps/rook-ceph-osd-34 scaled deployment.apps/rook-ceph-osd-4 scaled deployment.apps/rook-ceph-osd-41 scaled deployment.apps/rook-ceph-osd-44 scaled deployment.apps/rook-ceph-osd-9 scaled [root@dom14npv101-infra-manager ~ vlabrc]# Add the following Annotation for node deletion in the siteconfig.yaml (add crsuppression and crannotation both) To initiate the automated deletion process, begin by deleting the BMH CR of the control plane node that has been previously annotated with the specific \u201ccrAnnotation\u201d. Add \u201ccrSuppression\u201d to SiteConfig so that node will be removed from the cluster. Note that you need to keep the \u201ccrAnnotation\u201d on the node. - hostName: \"ncpvnpvlab1-storage-101.ncpvnpvlab1.pnwlab.nsn-rdnet.net\" role: \"worker\" crSuppression: - BareMetalHost crAnnotations: add: BareMetalHost: bmac.agent-install.openshift.io/remove-agent-and-node-on-delete: true Git add/commit/push the SiteConfig.yaml, so that ArgoCD syncs the updated SiteConfig to the Hub Cluster a. The BMH on Hub cluster should start showing updated status that the node is being deprovisioning. This status change indicates that the node is undergoing the deprovisioning process, a necessary step before its complete removal. [root@dom14npv101-infra-manager ~ hubrc]# oc get bmh -n ncpvnpvlab1 |grep -i storage-101 ncpvnpvlab1-storage-101.ncpvnpvlab1.pnwlab.nsn-rdnet.net deprovisioning true 60m [root@dom14npv101-infra-manager ~ hubrc]# Cluster Administrators should wait for the BMH to finish deprovisioning and be fully deleted from the cluster environment. After ~10 minutes (this might take longer or shorter depending on your environment to complete the node clean up): a. The storage node \u201cstorage-0\u201d is powered off b. The BMH resource of the replaced node is deleted on the Hub Cluster. [root@dom14npv101-infra-manager ~ vlabrc]# oc get bmh -n ncpvnpvlab1 |grep -i storage ncpvnpvlab1-storage-102.ncpvnpvlab1.pnwlab.nsn-rdnet.net provisioned true 27d ncpvnpvlab1-storage-103.ncpvnpvlab1.pnwlab.nsn-rdnet.net provisioned true 27d ncpvnpvlab1-storage-201.ncpvnpvlab1.pnwlab.nsn-rdnet.net provisioned true 25d ncpvnpvlab1-storage-202.ncpvnpvlab1.pnwlab.nsn-rdnet.net provisioned true 27d ncpvnpvlab1-storage-203.ncpvnpvlab1.pnwlab.nsn-rdnet.net provisioned true 27d [root@dom14npv101-infra-manager ~ vlabrc]# \u201coc get node\u201d on cluster shows that the node \u201cstorage-101\u201d is no longer part of the cluster, only 2 storage nodes remain ncpvnpvlab1-storage-101.ncpvnpvlab1.pnwlab.nsn-rdnet.net ---> still part of it. [root@dom14npv101-infra-manager ~ vlabrc]# oc get nodes |grep -i storage ncpvnpvlab1-storage-101.ncpvnpvlab1.pnwlab.nsn-rdnet.net Ready storage,worker 25d v1.29.10+67d3387 ncpvnpvlab1-storage-102.ncpvnpvlab1.pnwlab.nsn-rdnet.net Ready storage,worker 27d v1.29.10+67d3387 ncpvnpvlab1-storage-103.ncpvnpvlab1.pnwlab.nsn-rdnet.net Ready storage,worker 27d v1.29.10+67d3387 ncpvnpvlab1-storage-201.ncpvnpvlab1.pnwlab.nsn-rdnet.net Ready storage,worker 25d v1.29.10+67d3387 ncpvnpvlab1-storage-202.ncpvnpvlab1.pnwlab.nsn-rdnet.net Ready storage,worker 27d v1.29.10+67d3387 ncpvnpvlab1-storage-203.ncpvnpvlab1.pnwlab.nsn-rdnet.net Ready storage,worker 27d v1.29.10+67d3387 [root@dom14npv101-infra-manager ~ vlabrc]#","title":"Storage Node Replacement"},{"location":"openshift/storagemanagement/storage-node-replacement/#storage-node-replacement","text":"","title":"Storage node replacement"},{"location":"openshift/storagemanagement/storage-node-replacement/#delete-the-storage-node","text":"Show the initial status of the storage nodes in the managed cluster (output of oc get nodes) and identify which node will be removed, e.g. storage-0. [root@dom14npv101-infra-manager ~ vlabrc]# oc get nodes |grep -i storage ncpvnpvlab1-storage-101.ncpvnpvlab1.pnwlab.nsn-rdnet.net Ready storage,worker 25d v1.29.10+67d3387 ncpvnpvlab1-storage-102.ncpvnpvlab1.pnwlab.nsn-rdnet.net Ready storage,worker 27d v1.29.10+67d3387 ncpvnpvlab1-storage-103.ncpvnpvlab1.pnwlab.nsn-rdnet.net Ready storage,worker 27d v1.29.10+67d3387 ncpvnpvlab1-storage-201.ncpvnpvlab1.pnwlab.nsn-rdnet.net Ready storage,worker 25d v1.29.10+67d3387 ncpvnpvlab1-storage-202.ncpvnpvlab1.pnwlab.nsn-rdnet.net Ready storage,worker 27d v1.29.10+67d3387 ncpvnpvlab1-storage-203.ncpvnpvlab1.pnwlab.nsn-rdnet.net Ready storage,worker 27d v1.29.10+67d3387 [root@dom14npv101-infra-manager ~ vlabrc]# Verify the ceph health status [root@dom14npv101-infra-manager ~ vlabrc]# oc exec -it $(oc get pod -n openshift-storage -l app=rook-ceph-operator -o name) -n openshift-storage -- ceph -s -c /var/lib/rook/openshift-storage/openshift-storage.config cluster: id: a2b0c334-ba7c-4ae1-b3f5-c6d514f19bec health: HEALTH_OK services: mon: 3 daemons, quorum f,g,h (age 21h) mgr: a(active, since 21h), standbys: b mds: 1/1 daemons up, 1 hot standby osd: 48 osds: 48 up (since 21h), 48 in (since 23h) rgw: 1 daemon active (1 hosts, 1 zones) data: volumes: 1/1 healthy pools: 12 pools, 1097 pgs objects: 190.32k objects, 393 GiB usage: 1.2 TiB used, 278 TiB / 279 TiB avail pgs: 1097 active+clean io: client: 8.7 KiB/s rd, 9.9 MiB/s wr, 11 op/s rd, 22 op/s wr [root@dom14npv101-infra-manager ~ vlabrc]# Identify the monitor pod (if any), and OSDs that are running in the node that you need to replace: [root@dom14npv101-infra-manager ~ vlabrc]# oc get pods -n openshift-storage -o wide | grep -i ncpvnpvlab1-storage-101.ncpvnpvlab1.pnwlab.nsn-rdnet.net |grep -i rook-ceph rook-ceph-crashcollector-73c0594e536089af81dd498574227f77-94vtt 1/1 Running 0 21h 172.28.18.41 ncpvnpvlab1-storage-101.ncpvnpvlab1.pnwlab.nsn-rdnet.net <none> <none> rook-ceph-exporter-73c0594e536089af81dd498574227f77-754b5866njj 1/1 Running 0 21h 172.28.18.42 ncpvnpvlab1-storage-101.ncpvnpvlab1.pnwlab.nsn-rdnet.net <none> <none> rook-ceph-mds-ocs-storagecluster-cephfilesystem-a-795996f7lvsqs 2/2 Running 0 22h 172.28.18.21 ncpvnpvlab1-storage-101.ncpvnpvlab1.pnwlab.nsn-rdnet.net <none> <none> rook-ceph-mds-ocs-storagecluster-cephfilesystem-b-78f7bbf8c2hhg 2/2 Running 0 22h 172.28.18.22 ncpvnpvlab1-storage-101.ncpvnpvlab1.pnwlab.nsn-rdnet.net <none> <none> rook-ceph-mgr-b-5468b7cf-fmwnp 4/4 Running 0 22h 172.28.18.7 ncpvnpvlab1-storage-101.ncpvnpvlab1.pnwlab.nsn-rdnet.net <none> <none> rook-ceph-mon-f-54d858f9cd-m5q76 2/2 Running 0 22h 172.28.18.8 ncpvnpvlab1-storage-101.ncpvnpvlab1.pnwlab.nsn-rdnet.net <none> <none> rook-ceph-operator-7bc4cf5ccd-4lxjr 1/1 Running 0 22h 172.28.18.39 ncpvnpvlab1-storage-101.ncpvnpvlab1.pnwlab.nsn-rdnet.net <none> <none> rook-ceph-osd-14-6df66b8b99-nmgvq 2/2 Running 0 22h 172.28.18.9 ncpvnpvlab1-storage-101.ncpvnpvlab1.pnwlab.nsn-rdnet.net <none> <none> rook-ceph-osd-21-86cd6b7f7f-498vh 2/2 Running 0 22h 172.28.18.12 ncpvnpvlab1-storage-101.ncpvnpvlab1.pnwlab.nsn-rdnet.net <none> <none> rook-ceph-osd-28-698bb96856-vmr8t 2/2 Running 0 22h 172.28.18.11 ncpvnpvlab1-storage-101.ncpvnpvlab1.pnwlab.nsn-rdnet.net <none> <none> rook-ceph-osd-34-5f49bdbb85-f528w 2/2 Running 0 22h 172.28.18.15 ncpvnpvlab1-storage-101.ncpvnpvlab1.pnwlab.nsn-rdnet.net <none> <none> rook-ceph-osd-4-7495d5f559-zccrg 2/2 Running 0 22h 172.28.18.34 ncpvnpvlab1-storage-101.ncpvnpvlab1.pnwlab.nsn-rdnet.net <none> <none> rook-ceph-osd-41-689699f766-clfzm 2/2 Running 0 22h 172.28.18.17 ncpvnpvlab1-storage-101.ncpvnpvlab1.pnwlab.nsn-rdnet.net <none> <none> rook-ceph-osd-44-94c7c6565-cz8wg 2/2 Running 0 22h 172.28.18.16 ncpvnpvlab1-storage-101.ncpvnpvlab1.pnwlab.nsn-rdnet.net <none> <none> rook-ceph-osd-9-6b966dc5db-28595 2/2 Running 0 22h 172.28.18.18 ncpvnpvlab1-storage-101.ncpvnpvlab1.pnwlab.nsn-rdnet.net <none> <none> rook-ceph-rgw-ocs-storagecluster-cephobjectstore-a-788d79bdrltz 2/2 Running 0 22h 172.28.18.38 ncpvnpvlab1-storage-101.ncpvnpvlab1.pnwlab.nsn-rdnet.net <none> <none> rook-ceph-tools-6f854c4bfc-wqhm7 1/1 Running 0 22h 172.28.18.30 ncpvnpvlab1-storage-101.ncpvnpvlab1.pnwlab.nsn-rdnet.net <none> <none> [root@dom14npv101-infra-manager ~ vlabrc]# Scale down the deployments of the pods identified in the previous step: (mon, osd, crashcollector) [root@dom14npv101-infra-manager ~ vlabrc]# oc -n openshift-storage scale deployment rook-ceph-crashcollector --replicas=0 oc -n openshift-storage scale deployment rook-ceph-mgr-b --replicas=0 oc -n openshift-storage scale deployment rook-ceph-mon-f --replicas=0 oc -n openshift-storage scale deployment rook-ceph-osd-14 --replicas=0 oc -n openshift-storage scale deployment rook-ceph-osd-21 --replicas=0 oc -n openshift-storage scale deployment rook-ceph-osd-28 --replicas=0 oc -n openshift-storage scale deployment rook-ceph-osd-34 --replicas=0 oc -n openshift-storage scale deployment rook-ceph-osd-4 --replicas=0 oc -n openshift-storage scale deployment rook-ceph-osd-41 --replicas=0 oc -n openshift-storage scale deployment rook-ceph-osd-44 --replicas=0 oc -n openshift-storage scale deployment rook-ceph-osd-9 --replicas=0 error: no objects passed to scale deployment.apps/rook-ceph-mgr-b scaled deployment.apps/rook-ceph-mon-f scaled deployment.apps/rook-ceph-osd-14 scaled deployment.apps/rook-ceph-osd-21 scaled deployment.apps/rook-ceph-osd-28 scaled deployment.apps/rook-ceph-osd-34 scaled deployment.apps/rook-ceph-osd-4 scaled deployment.apps/rook-ceph-osd-41 scaled deployment.apps/rook-ceph-osd-44 scaled deployment.apps/rook-ceph-osd-9 scaled [root@dom14npv101-infra-manager ~ vlabrc]# Add the following Annotation for node deletion in the siteconfig.yaml (add crsuppression and crannotation both) To initiate the automated deletion process, begin by deleting the BMH CR of the control plane node that has been previously annotated with the specific \u201ccrAnnotation\u201d. Add \u201ccrSuppression\u201d to SiteConfig so that node will be removed from the cluster. Note that you need to keep the \u201ccrAnnotation\u201d on the node. - hostName: \"ncpvnpvlab1-storage-101.ncpvnpvlab1.pnwlab.nsn-rdnet.net\" role: \"worker\" crSuppression: - BareMetalHost crAnnotations: add: BareMetalHost: bmac.agent-install.openshift.io/remove-agent-and-node-on-delete: true Git add/commit/push the SiteConfig.yaml, so that ArgoCD syncs the updated SiteConfig to the Hub Cluster a. The BMH on Hub cluster should start showing updated status that the node is being deprovisioning. This status change indicates that the node is undergoing the deprovisioning process, a necessary step before its complete removal. [root@dom14npv101-infra-manager ~ hubrc]# oc get bmh -n ncpvnpvlab1 |grep -i storage-101 ncpvnpvlab1-storage-101.ncpvnpvlab1.pnwlab.nsn-rdnet.net deprovisioning true 60m [root@dom14npv101-infra-manager ~ hubrc]# Cluster Administrators should wait for the BMH to finish deprovisioning and be fully deleted from the cluster environment. After ~10 minutes (this might take longer or shorter depending on your environment to complete the node clean up): a. The storage node \u201cstorage-0\u201d is powered off b. The BMH resource of the replaced node is deleted on the Hub Cluster. [root@dom14npv101-infra-manager ~ vlabrc]# oc get bmh -n ncpvnpvlab1 |grep -i storage ncpvnpvlab1-storage-102.ncpvnpvlab1.pnwlab.nsn-rdnet.net provisioned true 27d ncpvnpvlab1-storage-103.ncpvnpvlab1.pnwlab.nsn-rdnet.net provisioned true 27d ncpvnpvlab1-storage-201.ncpvnpvlab1.pnwlab.nsn-rdnet.net provisioned true 25d ncpvnpvlab1-storage-202.ncpvnpvlab1.pnwlab.nsn-rdnet.net provisioned true 27d ncpvnpvlab1-storage-203.ncpvnpvlab1.pnwlab.nsn-rdnet.net provisioned true 27d [root@dom14npv101-infra-manager ~ vlabrc]# \u201coc get node\u201d on cluster shows that the node \u201cstorage-101\u201d is no longer part of the cluster, only 2 storage nodes remain ncpvnpvlab1-storage-101.ncpvnpvlab1.pnwlab.nsn-rdnet.net ---> still part of it. [root@dom14npv101-infra-manager ~ vlabrc]# oc get nodes |grep -i storage ncpvnpvlab1-storage-101.ncpvnpvlab1.pnwlab.nsn-rdnet.net Ready storage,worker 25d v1.29.10+67d3387 ncpvnpvlab1-storage-102.ncpvnpvlab1.pnwlab.nsn-rdnet.net Ready storage,worker 27d v1.29.10+67d3387 ncpvnpvlab1-storage-103.ncpvnpvlab1.pnwlab.nsn-rdnet.net Ready storage,worker 27d v1.29.10+67d3387 ncpvnpvlab1-storage-201.ncpvnpvlab1.pnwlab.nsn-rdnet.net Ready storage,worker 25d v1.29.10+67d3387 ncpvnpvlab1-storage-202.ncpvnpvlab1.pnwlab.nsn-rdnet.net Ready storage,worker 27d v1.29.10+67d3387 ncpvnpvlab1-storage-203.ncpvnpvlab1.pnwlab.nsn-rdnet.net Ready storage,worker 27d v1.29.10+67d3387 [root@dom14npv101-infra-manager ~ vlabrc]#","title":"Delete the storage node"},{"location":"openshift/tools-management-ts/ts-tools/","text":"This documentation help to setup some troubleshooting tools installation on the openshift infra. TCPDUMP login to node via ssh or debug utitiy [root@ncputility ~ pancwl_rc]$ ssh core@gateway2.panclypcwl01.mnc020.mcc714 Red Hat Enterprise Linux CoreOS 416.94.202407081958-0 Part of OpenShift 4.16, RHCOS is a Kubernetes-native operating system managed by the Machine Config Operator (`clusteroperator/machine-config`). WARNING: Direct SSH access to machines is not recommended; instead, make configuration changes via `machineconfig` objects: https://docs.openshift.com/container-platform/4.16/architecture/architecture-rhcos.html --- [core@gateway2 ~]$ become root and update the toolbox rc file here. a. use your hub cluster quay to avoid ssl certificate trust error. [root@gateway2 ~]# cat /root/.toolboxrc #REGISTRY=ncputility.panclyphub01.mnc020.mcc714:8443/ocmirror/rhel9 REGISTRY=quay-registry.apps.panclyphub01.mnc020.mcc714/ocmirror/rhel9 IMAGE=support-tools:latest [root@gateway2 ~]# now trigger toolbox command to execute into a shell which contains all required tools like tcpdump, sosreport etc. [root@gateway2 ~]# toolbox .toolboxrc file detected, overriding defaults... Trying to pull quay-registry.apps.panclyphub01.mnc020.mcc714/ocmirror/rhel9/support-tools:latest... Getting image source signatures Copying blob f5e6502d2728 done | Copying blob ebc7dc32a098 done | Copying config affd08d3be done | Writing manifest to image destination affd08d3bead20c55f40f08270d477b1524d9d7a2db25235956c7858755ef5f3 Spawning a container 'toolbox-root' with image 'quay-registry.apps.panclyphub01.mnc020.mcc714/ocmirror/rhel9/support-tools:latest' Detected RUN label in the container image. Using that as the default... 6bdff24c2e5da044965e2cec8eea58c3d86668f3a5bbe1e2d34495e956fdf0d7 toolbox-root Container started successfully. To exit, type 'exit'. [root@gateway2 /]# now run tcpdump command against any linux network interface. [root@gateway2 /]# tcpdump -i vlan104 host 10.89.97.162 -n dropped privs to tcpdump tcpdump: verbose output suppressed, use -v[v]... for full protocol decode listening on vlan104, link-type EN10MB (Ethernet), snapshot length 262144 bytes 11:09:46.575071 ARP, Request who-has 10.89.97.167 tell 10.89.97.162, length 42 11:09:46.847283 IP 10.89.97.162.49152 > 10.89.97.165.bfd-control: BCM-LI-SHIM: direction unused, pkt-type unknown, pkt-subtype single VLAN tag, li-id 2097944 11:09:47.111266 ARP, Request who-has 10.89.97.166 tell 10.89.97.162, length 42 11:09:47.575002 ARP, Request who-has 10.89.97.167 tell 10.89.97.162, length 42 11:09:47.614353 IP 10.89.97.162.49152 > 10.89.97.165.bfd-control: BCM-LI-SHIM: direction unused, pkt-type unknown, pkt-subtype single VLAN tag, li-id 2097944 11:09:48.110974 ARP, Request who-has 10.89.97.166 tell 10.89.97.162, length 42 11:09:48.398615 IP 10.89.97.162.49152 > 10.89.97.165.bfd-control: BCM-LI-SHIM: direction unused, pkt-type unknown, pkt-subtype single VLAN tag, li-id 2097944 11:09:49.165561 IP 10.89.97.162.49152 > 10.89.97.165.bfd-control: BCM-LI-SHIM: direction unused, pkt-type unknown, pkt-subtype single VLAN tag, li-id 2097944 11:09:49.944591 IP 10.89.97.162.49152 > 10.89.97.165.bfd-control: BCM-LI-SHIM: direction unused, pkt-type unknown, pkt-subtype single VLAN tag, li-id 2097944 11:09:50.708761 IP 10.89.97.162.49152 > 10.89.97.165.bfd-control: BCM-LI-SHIM: direction unused, pkt-type unknown, pkt-subtype single VLAN tag, li-id 2097944 11:09:51.486902 IP 10.89.97.162.49152 > 10.89.97.165.bfd-control: BCM-LI-SHIM: direction unused, pkt-type unknown, pkt-subtype single VLAN tag, li-id 2097944 11:09:52.262277 IP 10.89.97.162.49152 > 10.89.97.165.bfd-control: BCM-LI-SHIM: direction unused, pkt-type unknown, pkt-subtype single VLAN tag, li-id 2097944 11:09:53.022279 IP 10.89.97.162.49152 > 10.89.97.165.bfd-control: BCM-LI-SHIM: direction unused, pkt-type unknown, pkt-subtype single VLAN tag, li-id 2097944 11:09:53.796498 IP 10.89.97.162.49152 > 10.89.97.165.bfd-control: BCM-LI-SHIM: direction unused, pkt-type unknown, pkt-subtype single VLAN tag, li-id 2097944 11:09:54.578752 IP 10.89.97.162.49152 > 10.89.97.165.bfd-control: BCM-LI-SHIM: direction unused, pkt-type unknown, pkt-subtype single VLAN tag, li-id 2097944 11:09:55.339790 IP 10.89.97.162.49152 > 10.89.97.165.bfd-control: BCM-LI-SHIM: direction unused, pkt-type unknown, pkt-subtype single VLAN tag, li-id 2097944 11:09:56.093946 IP 10.89.97.162.49152 > 10.89.97.165.bfd-control: BCM-LI-SHIM: direction unused, pkt-type unknown, pkt-subtype single VLAN tag, li-id 2097944 11:09:56.871023 IP 10.89.97.162.49152 > 10.89.97.165.bfd-control: BCM-LI-SHIM: direction unused, pkt-type unknown, pkt-subtype single VLAN tag, li-id 2097944 11:09:57.655185 IP 10.89.97.162.49152 > 10.89.97.165.bfd-control: BCM-LI-SHIM: direction unused, pkt-type unknown, pkt-subtype single VLAN tag, li-id 2097944 11:09:58.416364 IP 10.89.97.162.49152 > 10.89.97.165.bfd-control: BCM-LI-SHIM: direction unused, pkt-type unknown, pkt-subtype single VLAN tag, li-id 2097944 11:09:59.198508 IP 10.89.97.162.49152 > 10.89.97.165.bfd-control: BCM-LI-SHIM: direction unused, pkt-type unknown, pkt-subtype single VLAN tag, li-id 2097944 11:09:59.976760 IP 10.89.97.162.49152 > 10.89.97.165.bfd-control: BCM-LI-SHIM: direction unused, pkt-type unknown, pkt-subtype single VLAN tag, li-id 2097944 11:10:00.160910 IP 10.89.97.165.37680 > 10.89.97.162.bgp: Flags [P.], seq 207097212:207097231, ack 3177745402, win 64, options [nop,nop,TS val 4011175388 ecr 3700362321], length 19: BGP 11:10:00.162044 IP 10.89.97.162.bgp > 10.89.97.165.37680: Flags [.], ack 19, win 23411, options [nop,nop,TS val 3700389294 ecr 4011175388], length 0 11:10:00.738822 IP 10.89.97.162.49152 > 10.89.97.165.bfd-control: BCM-LI-SHIM: direction unused, pkt-type unknown, pkt-subtype single VLAN tag, li-id 2097944 11:10:01.497752 IP 10.89.97.162.49152 > 10.89.97.165.bfd-control: BCM-LI-SHIM: direction unused, pkt-type unknown, pkt-subtype single VLAN tag, li-id 2097944 11:10:02.254817 IP 10.89.97.162.49152 > 10.89.97.165.bfd-control: BCM-LI-SHIM: direction unused, pkt-type unknown, pkt-subtype single VLAN tag, li-id 2097944 11:10:03.028798 IP 10.89.97.162.49152 > 10.89.97.165.bfd-control: BCM-LI-SHIM: direction unused, pkt-type unknown, pkt-subtype single VLAN tag, li-id 2097944 11:10:03.209179 IP 10.89.97.162.bgp > 10.89.97.165.37680: Flags [P.], seq 1:20, ack 19, win 23411, options [nop,nop,TS val 3700392341 ecr 4011175388], length 19: BGP 11:10:03.209205 IP 10.89.97.165.37680 > 10.89.97.162.bgp: Flags [.], ack 20, win 64, options [nop,nop,TS val 4011178436 ecr 3700392341], length 0 11:10:03.798017 IP 10.89.97.162.49152 > 10.89.97.165.bfd-control: BCM-LI-SHIM: direction unused, pkt-type unknown, pkt-subtype single VLAN tag, li-id 2097944 11:10:04.086263 ARP, Request who-has 10.89.97.163 tell 10.89.97.162, length 42 11:10:04.564104 IP 10.89.97.162.49152 > 10.89.97.165.bfd-control: BCM-LI-SHIM: direction unused, pkt-type unknown, pkt-subtype single VLAN tag, li-id 2097944 11:10:05.086213 ARP, Request who-has 10.89.97.163 tell 10.89.97.162, length 42 11:10:05.343276 IP 10.89.97.162.49152 > 10.89.97.165.bfd-control: BCM-LI-SHIM: direction unused, pkt-type unknown, pkt-subtype single VLAN tag, li-id 2097944 11:10:06.086632 ARP, Request who-has 10.89.97.163 tell 10.89.97.162, length 42 11:10:06.110449 IP 10.89.97.162.49152 > 10.89.97.165.bfd-control: BCM-LI-SHIM: direction unused, pkt-type unknown, pkt-subtype single VLAN tag, li-id 2097944 11:10:06.906665 IP 10.89.97.162.49152 > 10.89.97.165.bfd-control: BCM-LI-SHIM: direction unused, pkt-type unknown, pkt-subtype single VLAN tag, li-id 2097944 11:10:07.686809 IP 10.89.97.162.49152 > 10.89.97.165.bfd-control: BCM-LI-SHIM: direction unused, pkt-type unknown, pkt-subtype single VLAN tag, li-id 2097944 11:10:08.455977 IP 10.89.97.162.49152 > 10.89.97.165.bfd-control: BCM-LI-SHIM: direction unused, pkt-type unknown, pkt-subtype single VLAN tag, li-id 2097944 11:10:09.231136 IP 10.89.97.162.49152 > 10.89.97.165.bfd-control: BCM-LI-SHIM: direction unused, pkt-type unknown, pkt-subtype single VLAN tag, li-id 2097944 11:10:10.002335 IP 10.89.97.162.49152 > 10.89.97.165.bfd-control: BCM-LI-SHIM: direction unused, pkt-type unknown, pkt-subtype single VLAN tag, li-id 2097944 11:10:10.792658 IP 10.89.97.162.49152 > 10.89.97.165.bfd-control: BCM-LI-SHIM: direction unused, pkt-type unknown, pkt-subtype single VLAN tag, li-id 2097944 11:10:11.561799 IP 10.89.97.162.49152 > 10.89.97.165.bfd-control: BCM-LI-SHIM: direction unused, pkt-type unknown, pkt-subtype single VLAN tag, li-id 2097944 ^C 44 packets captured 44 packets received by filter 0 packets dropped by kernel [root@gateway2 /]# sos report login to node via ssh or debug utitiy [root@ncputility ~ pancwl_rc]$ ssh core@gateway2.panclypcwl01.mnc020.mcc714 Red Hat Enterprise Linux CoreOS 416.94.202407081958-0 Part of OpenShift 4.16, RHCOS is a Kubernetes-native operating system managed by the Machine Config Operator (`clusteroperator/machine-config`). WARNING: Direct SSH access to machines is not recommended; instead, make configuration changes via `machineconfig` objects: https://docs.openshift.com/container-platform/4.16/architecture/architecture-rhcos.html --- [core@gateway2 ~]$ become root and update the toolbox rc file here. a. use your hub cluster quay to avoid ssl certificate trust error. [root@gateway2 ~]# cat /root/.toolboxrc #REGISTRY=ncputility.panclyphub01.mnc020.mcc714:8443/ocmirror/rhel9 REGISTRY=quay-registry.apps.panclyphub01.mnc020.mcc714/ocmirror/rhel9 IMAGE=support-tools:latest [root@gateway2 ~]# now trigger toolbox command to execute into a shell which contains all required tools like tcpdump, sosreport etc. [root@gateway2 ~]# toolbox .toolboxrc file detected, overriding defaults... Trying to pull quay-registry.apps.panclyphub01.mnc020.mcc714/ocmirror/rhel9/support-tools:latest... Getting image source signatures Copying blob f5e6502d2728 done | Copying blob ebc7dc32a098 done | Copying config affd08d3be done | Writing manifest to image destination affd08d3bead20c55f40f08270d477b1524d9d7a2db25235956c7858755ef5f3 Spawning a container 'toolbox-root' with image 'quay-registry.apps.panclyphub01.mnc020.mcc714/ocmirror/rhel9/support-tools:latest' Detected RUN label in the container image. Using that as the default... 6bdff24c2e5da044965e2cec8eea58c3d86668f3a5bbe1e2d34495e956fdf0d7 toolbox-root Container started successfully. To exit, type 'exit'. [root@gateway2 /]# now use sosreport from here sos report -k crio.all=on -k crio.logs=on -k podman.all=on -k podman.logs=on References Recovering a node that has lost all networking in OpenShift 4","title":"Tools Management TS"},{"location":"openshift/tools-management-ts/ts-tools/#this-documentation-help-to-setup-some-troubleshooting-tools-installation-on-the-openshift-infra","text":"","title":"This documentation help to setup some troubleshooting tools installation on the openshift infra."},{"location":"openshift/tools-management-ts/ts-tools/#tcpdump","text":"login to node via ssh or debug utitiy [root@ncputility ~ pancwl_rc]$ ssh core@gateway2.panclypcwl01.mnc020.mcc714 Red Hat Enterprise Linux CoreOS 416.94.202407081958-0 Part of OpenShift 4.16, RHCOS is a Kubernetes-native operating system managed by the Machine Config Operator (`clusteroperator/machine-config`). WARNING: Direct SSH access to machines is not recommended; instead, make configuration changes via `machineconfig` objects: https://docs.openshift.com/container-platform/4.16/architecture/architecture-rhcos.html --- [core@gateway2 ~]$ become root and update the toolbox rc file here. a. use your hub cluster quay to avoid ssl certificate trust error. [root@gateway2 ~]# cat /root/.toolboxrc #REGISTRY=ncputility.panclyphub01.mnc020.mcc714:8443/ocmirror/rhel9 REGISTRY=quay-registry.apps.panclyphub01.mnc020.mcc714/ocmirror/rhel9 IMAGE=support-tools:latest [root@gateway2 ~]# now trigger toolbox command to execute into a shell which contains all required tools like tcpdump, sosreport etc. [root@gateway2 ~]# toolbox .toolboxrc file detected, overriding defaults... Trying to pull quay-registry.apps.panclyphub01.mnc020.mcc714/ocmirror/rhel9/support-tools:latest... Getting image source signatures Copying blob f5e6502d2728 done | Copying blob ebc7dc32a098 done | Copying config affd08d3be done | Writing manifest to image destination affd08d3bead20c55f40f08270d477b1524d9d7a2db25235956c7858755ef5f3 Spawning a container 'toolbox-root' with image 'quay-registry.apps.panclyphub01.mnc020.mcc714/ocmirror/rhel9/support-tools:latest' Detected RUN label in the container image. Using that as the default... 6bdff24c2e5da044965e2cec8eea58c3d86668f3a5bbe1e2d34495e956fdf0d7 toolbox-root Container started successfully. To exit, type 'exit'. [root@gateway2 /]# now run tcpdump command against any linux network interface. [root@gateway2 /]# tcpdump -i vlan104 host 10.89.97.162 -n dropped privs to tcpdump tcpdump: verbose output suppressed, use -v[v]... for full protocol decode listening on vlan104, link-type EN10MB (Ethernet), snapshot length 262144 bytes 11:09:46.575071 ARP, Request who-has 10.89.97.167 tell 10.89.97.162, length 42 11:09:46.847283 IP 10.89.97.162.49152 > 10.89.97.165.bfd-control: BCM-LI-SHIM: direction unused, pkt-type unknown, pkt-subtype single VLAN tag, li-id 2097944 11:09:47.111266 ARP, Request who-has 10.89.97.166 tell 10.89.97.162, length 42 11:09:47.575002 ARP, Request who-has 10.89.97.167 tell 10.89.97.162, length 42 11:09:47.614353 IP 10.89.97.162.49152 > 10.89.97.165.bfd-control: BCM-LI-SHIM: direction unused, pkt-type unknown, pkt-subtype single VLAN tag, li-id 2097944 11:09:48.110974 ARP, Request who-has 10.89.97.166 tell 10.89.97.162, length 42 11:09:48.398615 IP 10.89.97.162.49152 > 10.89.97.165.bfd-control: BCM-LI-SHIM: direction unused, pkt-type unknown, pkt-subtype single VLAN tag, li-id 2097944 11:09:49.165561 IP 10.89.97.162.49152 > 10.89.97.165.bfd-control: BCM-LI-SHIM: direction unused, pkt-type unknown, pkt-subtype single VLAN tag, li-id 2097944 11:09:49.944591 IP 10.89.97.162.49152 > 10.89.97.165.bfd-control: BCM-LI-SHIM: direction unused, pkt-type unknown, pkt-subtype single VLAN tag, li-id 2097944 11:09:50.708761 IP 10.89.97.162.49152 > 10.89.97.165.bfd-control: BCM-LI-SHIM: direction unused, pkt-type unknown, pkt-subtype single VLAN tag, li-id 2097944 11:09:51.486902 IP 10.89.97.162.49152 > 10.89.97.165.bfd-control: BCM-LI-SHIM: direction unused, pkt-type unknown, pkt-subtype single VLAN tag, li-id 2097944 11:09:52.262277 IP 10.89.97.162.49152 > 10.89.97.165.bfd-control: BCM-LI-SHIM: direction unused, pkt-type unknown, pkt-subtype single VLAN tag, li-id 2097944 11:09:53.022279 IP 10.89.97.162.49152 > 10.89.97.165.bfd-control: BCM-LI-SHIM: direction unused, pkt-type unknown, pkt-subtype single VLAN tag, li-id 2097944 11:09:53.796498 IP 10.89.97.162.49152 > 10.89.97.165.bfd-control: BCM-LI-SHIM: direction unused, pkt-type unknown, pkt-subtype single VLAN tag, li-id 2097944 11:09:54.578752 IP 10.89.97.162.49152 > 10.89.97.165.bfd-control: BCM-LI-SHIM: direction unused, pkt-type unknown, pkt-subtype single VLAN tag, li-id 2097944 11:09:55.339790 IP 10.89.97.162.49152 > 10.89.97.165.bfd-control: BCM-LI-SHIM: direction unused, pkt-type unknown, pkt-subtype single VLAN tag, li-id 2097944 11:09:56.093946 IP 10.89.97.162.49152 > 10.89.97.165.bfd-control: BCM-LI-SHIM: direction unused, pkt-type unknown, pkt-subtype single VLAN tag, li-id 2097944 11:09:56.871023 IP 10.89.97.162.49152 > 10.89.97.165.bfd-control: BCM-LI-SHIM: direction unused, pkt-type unknown, pkt-subtype single VLAN tag, li-id 2097944 11:09:57.655185 IP 10.89.97.162.49152 > 10.89.97.165.bfd-control: BCM-LI-SHIM: direction unused, pkt-type unknown, pkt-subtype single VLAN tag, li-id 2097944 11:09:58.416364 IP 10.89.97.162.49152 > 10.89.97.165.bfd-control: BCM-LI-SHIM: direction unused, pkt-type unknown, pkt-subtype single VLAN tag, li-id 2097944 11:09:59.198508 IP 10.89.97.162.49152 > 10.89.97.165.bfd-control: BCM-LI-SHIM: direction unused, pkt-type unknown, pkt-subtype single VLAN tag, li-id 2097944 11:09:59.976760 IP 10.89.97.162.49152 > 10.89.97.165.bfd-control: BCM-LI-SHIM: direction unused, pkt-type unknown, pkt-subtype single VLAN tag, li-id 2097944 11:10:00.160910 IP 10.89.97.165.37680 > 10.89.97.162.bgp: Flags [P.], seq 207097212:207097231, ack 3177745402, win 64, options [nop,nop,TS val 4011175388 ecr 3700362321], length 19: BGP 11:10:00.162044 IP 10.89.97.162.bgp > 10.89.97.165.37680: Flags [.], ack 19, win 23411, options [nop,nop,TS val 3700389294 ecr 4011175388], length 0 11:10:00.738822 IP 10.89.97.162.49152 > 10.89.97.165.bfd-control: BCM-LI-SHIM: direction unused, pkt-type unknown, pkt-subtype single VLAN tag, li-id 2097944 11:10:01.497752 IP 10.89.97.162.49152 > 10.89.97.165.bfd-control: BCM-LI-SHIM: direction unused, pkt-type unknown, pkt-subtype single VLAN tag, li-id 2097944 11:10:02.254817 IP 10.89.97.162.49152 > 10.89.97.165.bfd-control: BCM-LI-SHIM: direction unused, pkt-type unknown, pkt-subtype single VLAN tag, li-id 2097944 11:10:03.028798 IP 10.89.97.162.49152 > 10.89.97.165.bfd-control: BCM-LI-SHIM: direction unused, pkt-type unknown, pkt-subtype single VLAN tag, li-id 2097944 11:10:03.209179 IP 10.89.97.162.bgp > 10.89.97.165.37680: Flags [P.], seq 1:20, ack 19, win 23411, options [nop,nop,TS val 3700392341 ecr 4011175388], length 19: BGP 11:10:03.209205 IP 10.89.97.165.37680 > 10.89.97.162.bgp: Flags [.], ack 20, win 64, options [nop,nop,TS val 4011178436 ecr 3700392341], length 0 11:10:03.798017 IP 10.89.97.162.49152 > 10.89.97.165.bfd-control: BCM-LI-SHIM: direction unused, pkt-type unknown, pkt-subtype single VLAN tag, li-id 2097944 11:10:04.086263 ARP, Request who-has 10.89.97.163 tell 10.89.97.162, length 42 11:10:04.564104 IP 10.89.97.162.49152 > 10.89.97.165.bfd-control: BCM-LI-SHIM: direction unused, pkt-type unknown, pkt-subtype single VLAN tag, li-id 2097944 11:10:05.086213 ARP, Request who-has 10.89.97.163 tell 10.89.97.162, length 42 11:10:05.343276 IP 10.89.97.162.49152 > 10.89.97.165.bfd-control: BCM-LI-SHIM: direction unused, pkt-type unknown, pkt-subtype single VLAN tag, li-id 2097944 11:10:06.086632 ARP, Request who-has 10.89.97.163 tell 10.89.97.162, length 42 11:10:06.110449 IP 10.89.97.162.49152 > 10.89.97.165.bfd-control: BCM-LI-SHIM: direction unused, pkt-type unknown, pkt-subtype single VLAN tag, li-id 2097944 11:10:06.906665 IP 10.89.97.162.49152 > 10.89.97.165.bfd-control: BCM-LI-SHIM: direction unused, pkt-type unknown, pkt-subtype single VLAN tag, li-id 2097944 11:10:07.686809 IP 10.89.97.162.49152 > 10.89.97.165.bfd-control: BCM-LI-SHIM: direction unused, pkt-type unknown, pkt-subtype single VLAN tag, li-id 2097944 11:10:08.455977 IP 10.89.97.162.49152 > 10.89.97.165.bfd-control: BCM-LI-SHIM: direction unused, pkt-type unknown, pkt-subtype single VLAN tag, li-id 2097944 11:10:09.231136 IP 10.89.97.162.49152 > 10.89.97.165.bfd-control: BCM-LI-SHIM: direction unused, pkt-type unknown, pkt-subtype single VLAN tag, li-id 2097944 11:10:10.002335 IP 10.89.97.162.49152 > 10.89.97.165.bfd-control: BCM-LI-SHIM: direction unused, pkt-type unknown, pkt-subtype single VLAN tag, li-id 2097944 11:10:10.792658 IP 10.89.97.162.49152 > 10.89.97.165.bfd-control: BCM-LI-SHIM: direction unused, pkt-type unknown, pkt-subtype single VLAN tag, li-id 2097944 11:10:11.561799 IP 10.89.97.162.49152 > 10.89.97.165.bfd-control: BCM-LI-SHIM: direction unused, pkt-type unknown, pkt-subtype single VLAN tag, li-id 2097944 ^C 44 packets captured 44 packets received by filter 0 packets dropped by kernel [root@gateway2 /]#","title":"TCPDUMP"},{"location":"openshift/tools-management-ts/ts-tools/#sos-report","text":"login to node via ssh or debug utitiy [root@ncputility ~ pancwl_rc]$ ssh core@gateway2.panclypcwl01.mnc020.mcc714 Red Hat Enterprise Linux CoreOS 416.94.202407081958-0 Part of OpenShift 4.16, RHCOS is a Kubernetes-native operating system managed by the Machine Config Operator (`clusteroperator/machine-config`). WARNING: Direct SSH access to machines is not recommended; instead, make configuration changes via `machineconfig` objects: https://docs.openshift.com/container-platform/4.16/architecture/architecture-rhcos.html --- [core@gateway2 ~]$ become root and update the toolbox rc file here. a. use your hub cluster quay to avoid ssl certificate trust error. [root@gateway2 ~]# cat /root/.toolboxrc #REGISTRY=ncputility.panclyphub01.mnc020.mcc714:8443/ocmirror/rhel9 REGISTRY=quay-registry.apps.panclyphub01.mnc020.mcc714/ocmirror/rhel9 IMAGE=support-tools:latest [root@gateway2 ~]# now trigger toolbox command to execute into a shell which contains all required tools like tcpdump, sosreport etc. [root@gateway2 ~]# toolbox .toolboxrc file detected, overriding defaults... Trying to pull quay-registry.apps.panclyphub01.mnc020.mcc714/ocmirror/rhel9/support-tools:latest... Getting image source signatures Copying blob f5e6502d2728 done | Copying blob ebc7dc32a098 done | Copying config affd08d3be done | Writing manifest to image destination affd08d3bead20c55f40f08270d477b1524d9d7a2db25235956c7858755ef5f3 Spawning a container 'toolbox-root' with image 'quay-registry.apps.panclyphub01.mnc020.mcc714/ocmirror/rhel9/support-tools:latest' Detected RUN label in the container image. Using that as the default... 6bdff24c2e5da044965e2cec8eea58c3d86668f3a5bbe1e2d34495e956fdf0d7 toolbox-root Container started successfully. To exit, type 'exit'. [root@gateway2 /]# now use sosreport from here sos report -k crio.all=on -k crio.logs=on -k podman.all=on -k podman.logs=on","title":"sos report"},{"location":"openshift/tools-management-ts/ts-tools/#references","text":"Recovering a node that has lost all networking in OpenShift 4","title":"References"},{"location":"openshift/troubleshooting/nsenter/","text":"Method to use nsenter on the OCP Please find the method to use nsenter, so that you will not struggle during your deployment. Login to the ocp cluster with cluster admin role and find the pod name which you want to login inside the container using nsenter. ncom01pan-caas-plugin-9bd7755bb-bb5fs is selected. [root@ncputility ~ pancwl_rc]$ source /root/pancwlrc WARNING: Using insecure TLS client config. Setting this option is not supported! Login successful. You have access to 119 projects, the list has been suppressed. You can list all projects with 'oc projects' Using project \"ncom01pan\". [root@ncputility ~ pancwl_rc]$ oc get pods -A -o wide |grep -i ^C [root@ncputility ~ pancwl_rc]$ oc get pods -n ncom01pan -o wide |grep -i caas ncom01pan-caas-plugin-9bd7755bb-bb5fs 1/1 Running 0 3h15m 172.17.18.34 appworker23.panclypcwl01.mnc020.mcc714 <none> <none> ncom01pan-caas-plugin-9bd7755bb-cwzmm 1/1 Running 0 3h15m 172.18.8.78 appworker16.panclypcwl01.mnc020.mcc714 <none> <none> [root@ncputility ~ pancwl_rc]$ execute to that node where your pod hosted, and this will be indentified from the previous command. [root@ncputility ~ pancwl_rc]$ oc debug -t node/appworker0.panclypcwl01.mnc020.mcc714 Temporary namespace openshift-debug-vz9qc is created for debugging node... Starting pod/appworker0panclypcwl01mnc020mcc714-debug-87f7l ... To use host binaries, run `chroot /host` Pod IP: 10.89.96.26 If you don't see a command prompt, try pressing enter. sh-5.1# chroot /host sh-5.1# now find out the container id using crictl command here sh-5.1# crictl ps |grep -i ncom01pan-caas-plugin-7654b86fdb-mz5r7 2b61910d5eb23 quay-registry.apps.panclyphub01.mnc020.mcc714/ncom01pan/ncom/caas-plugin@sha256:d6d9506d14d756ecafe7d93debcb9eeb498cc805506fb1480002713d17ce64d6 19 minutes ago Running cjee-wildfly 0 8ca17869e45fa ncom01pan-caas-plugin-7654b86fdb-mz5r7 find out the pid of the container using inspect command. sh-5.1# crictl inpsect 2b61910d5eb23 |grep -i pid No help topic for 'inpsect' sh-5.1# crictl inspect 2b61910d5eb23 |grep -i pid \"pid\": 60545, \"pids\": { \"type\": \"pid\" \"getpid\", \"getppid\", \"pidfd_getfd\", \"pidfd_open\", \"pidfd_send_signal\", \"waitpid\", now use the toolbox command, since tcpdump is not configured on the host os level. sh-5.1# toolbox .toolboxrc file detected, overriding defaults... Checking if there is a newer version of quay-registry.apps.panclyphub01.mnc020.mcc714/ocmirror/rhel9/support-tools:latest available... Container 'toolbox-root' already exists. Trying to start... (To remove the container and start with a fresh toolbox, run: sudo podman rm 'toolbox-root') toolbox-root Container started successfully. To exit, type 'exit'. [root@appworker0 /]# nsenter -t 60545 -n ip a 1: lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 inet 127.0.0.1/8 scope host lo valid_lft forever preferred_lft forever inet6 ::1/128 scope host valid_lft forever preferred_lft forever 2: gre0@NONE: <NOARP> mtu 1476 qdisc noop state DOWN group default qlen 1000 link/gre 0.0.0.0 brd 0.0.0.0 3: gretap0@NONE: <BROADCAST,MULTICAST> mtu 1462 qdisc noop state DOWN group default qlen 1000 link/ether 00:00:00:00:00:00 brd ff:ff:ff:ff:ff:ff 4: erspan0@NONE: <BROADCAST,MULTICAST> mtu 1450 qdisc noop state DOWN group default qlen 1000 link/ether 00:00:00:00:00:00 brd ff:ff:ff:ff:ff:ff 5: ip6tnl0@NONE: <NOARP> mtu 1452 qdisc noop state DOWN group default qlen 1000 link/tunnel6 :: brd :: permaddr 3eac:e266:df07:: 6: ip6gre0@NONE: <NOARP> mtu 1448 qdisc noop state DOWN group default qlen 1000 link/gre6 :: brd :: permaddr 6679:afbc:3648:: 7: eth0@if609: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 8900 qdisc noqueue state UP group default link/ether 0a:58:ac:10:04:0f brd ff:ff:ff:ff:ff:ff link-netns d00bdece-6c79-4840-87d1-10d3103ecdd7 inet 172.16.4.15/23 brd 172.16.5.255 scope global eth0 valid_lft forever preferred_lft forever inet6 fe80::858:acff:fe10:40f/64 scope link valid_lft forever preferred_lft forever [root@appworker0 /]# nsenter -t 60545 -n ping quay-registry.apps.panclyphub01.mnc020.mcc714 PING quay-registry.apps.panclyphub01.mnc020.mcc714 (10.89.97.143) 56(84) bytes of data. From 10.89.97.168 (10.89.97.168) icmp_seq=1 Destination Host Unreachable From 10.89.97.168 (10.89.97.168) icmp_seq=2 Destination Host Unreachable From 10.89.97.168 (10.89.97.168) icmp_seq=3 Destination Host Unreachable From 10.89.97.168 (10.89.97.168) icmp_seq=4 Destination Host Unreachable ^C --- quay-registry.apps.panclyphub01.mnc020.mcc714 ping statistics --- 5 packets transmitted, 0 received, +4 errors, 100% packet loss, time 4089ms pipe 4 [root@appworker0 /]# nsenter -t 60545 -n ping 10.89.97.143 PING 10.89.97.143 (10.89.97.143) 56(84) bytes of data. From 10.89.97.168 icmp_seq=1 Destination Host Unreachable From 10.89.97.168 icmp_seq=2 Destination Host Unreachable From 10.89.97.168 icmp_seq=3 Destination Host Unreachable From 10.89.97.168 icmp_seq=4 Destination Host Unreachable ^C --- 10.89.97.143 ping statistics --- 4 packets transmitted, 0 received, +4 errors, 100% packet loss, time 3088ms pipe 4 [root@appworker0 /]# nsenter -t 60545 -n tracepath 10.89.97.143 1?: [LOCALHOST] pmtu 8900 1: *.apps.panclyphub01.mnc020.mcc714 1.683ms asymm 2 1: *.apps.panclyphub01.mnc020.mcc714 0.878ms asymm 2 2: 100.88.0.7 1.903ms asymm 3 3: 172.17.2.2 2.086ms 4: no reply 4: 10.89.97.168 3087.371ms !H Resume: pmtu 8900 [root@appworker0 /]#","title":"Nsenter Usage"},{"location":"openshift/troubleshooting/nsenter/#method-to-use-nsenter-on-the-ocp","text":"Please find the method to use nsenter, so that you will not struggle during your deployment. Login to the ocp cluster with cluster admin role and find the pod name which you want to login inside the container using nsenter. ncom01pan-caas-plugin-9bd7755bb-bb5fs is selected. [root@ncputility ~ pancwl_rc]$ source /root/pancwlrc WARNING: Using insecure TLS client config. Setting this option is not supported! Login successful. You have access to 119 projects, the list has been suppressed. You can list all projects with 'oc projects' Using project \"ncom01pan\". [root@ncputility ~ pancwl_rc]$ oc get pods -A -o wide |grep -i ^C [root@ncputility ~ pancwl_rc]$ oc get pods -n ncom01pan -o wide |grep -i caas ncom01pan-caas-plugin-9bd7755bb-bb5fs 1/1 Running 0 3h15m 172.17.18.34 appworker23.panclypcwl01.mnc020.mcc714 <none> <none> ncom01pan-caas-plugin-9bd7755bb-cwzmm 1/1 Running 0 3h15m 172.18.8.78 appworker16.panclypcwl01.mnc020.mcc714 <none> <none> [root@ncputility ~ pancwl_rc]$ execute to that node where your pod hosted, and this will be indentified from the previous command. [root@ncputility ~ pancwl_rc]$ oc debug -t node/appworker0.panclypcwl01.mnc020.mcc714 Temporary namespace openshift-debug-vz9qc is created for debugging node... Starting pod/appworker0panclypcwl01mnc020mcc714-debug-87f7l ... To use host binaries, run `chroot /host` Pod IP: 10.89.96.26 If you don't see a command prompt, try pressing enter. sh-5.1# chroot /host sh-5.1# now find out the container id using crictl command here sh-5.1# crictl ps |grep -i ncom01pan-caas-plugin-7654b86fdb-mz5r7 2b61910d5eb23 quay-registry.apps.panclyphub01.mnc020.mcc714/ncom01pan/ncom/caas-plugin@sha256:d6d9506d14d756ecafe7d93debcb9eeb498cc805506fb1480002713d17ce64d6 19 minutes ago Running cjee-wildfly 0 8ca17869e45fa ncom01pan-caas-plugin-7654b86fdb-mz5r7 find out the pid of the container using inspect command. sh-5.1# crictl inpsect 2b61910d5eb23 |grep -i pid No help topic for 'inpsect' sh-5.1# crictl inspect 2b61910d5eb23 |grep -i pid \"pid\": 60545, \"pids\": { \"type\": \"pid\" \"getpid\", \"getppid\", \"pidfd_getfd\", \"pidfd_open\", \"pidfd_send_signal\", \"waitpid\", now use the toolbox command, since tcpdump is not configured on the host os level. sh-5.1# toolbox .toolboxrc file detected, overriding defaults... Checking if there is a newer version of quay-registry.apps.panclyphub01.mnc020.mcc714/ocmirror/rhel9/support-tools:latest available... Container 'toolbox-root' already exists. Trying to start... (To remove the container and start with a fresh toolbox, run: sudo podman rm 'toolbox-root') toolbox-root Container started successfully. To exit, type 'exit'. [root@appworker0 /]# nsenter -t 60545 -n ip a 1: lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 inet 127.0.0.1/8 scope host lo valid_lft forever preferred_lft forever inet6 ::1/128 scope host valid_lft forever preferred_lft forever 2: gre0@NONE: <NOARP> mtu 1476 qdisc noop state DOWN group default qlen 1000 link/gre 0.0.0.0 brd 0.0.0.0 3: gretap0@NONE: <BROADCAST,MULTICAST> mtu 1462 qdisc noop state DOWN group default qlen 1000 link/ether 00:00:00:00:00:00 brd ff:ff:ff:ff:ff:ff 4: erspan0@NONE: <BROADCAST,MULTICAST> mtu 1450 qdisc noop state DOWN group default qlen 1000 link/ether 00:00:00:00:00:00 brd ff:ff:ff:ff:ff:ff 5: ip6tnl0@NONE: <NOARP> mtu 1452 qdisc noop state DOWN group default qlen 1000 link/tunnel6 :: brd :: permaddr 3eac:e266:df07:: 6: ip6gre0@NONE: <NOARP> mtu 1448 qdisc noop state DOWN group default qlen 1000 link/gre6 :: brd :: permaddr 6679:afbc:3648:: 7: eth0@if609: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 8900 qdisc noqueue state UP group default link/ether 0a:58:ac:10:04:0f brd ff:ff:ff:ff:ff:ff link-netns d00bdece-6c79-4840-87d1-10d3103ecdd7 inet 172.16.4.15/23 brd 172.16.5.255 scope global eth0 valid_lft forever preferred_lft forever inet6 fe80::858:acff:fe10:40f/64 scope link valid_lft forever preferred_lft forever [root@appworker0 /]# nsenter -t 60545 -n ping quay-registry.apps.panclyphub01.mnc020.mcc714 PING quay-registry.apps.panclyphub01.mnc020.mcc714 (10.89.97.143) 56(84) bytes of data. From 10.89.97.168 (10.89.97.168) icmp_seq=1 Destination Host Unreachable From 10.89.97.168 (10.89.97.168) icmp_seq=2 Destination Host Unreachable From 10.89.97.168 (10.89.97.168) icmp_seq=3 Destination Host Unreachable From 10.89.97.168 (10.89.97.168) icmp_seq=4 Destination Host Unreachable ^C --- quay-registry.apps.panclyphub01.mnc020.mcc714 ping statistics --- 5 packets transmitted, 0 received, +4 errors, 100% packet loss, time 4089ms pipe 4 [root@appworker0 /]# nsenter -t 60545 -n ping 10.89.97.143 PING 10.89.97.143 (10.89.97.143) 56(84) bytes of data. From 10.89.97.168 icmp_seq=1 Destination Host Unreachable From 10.89.97.168 icmp_seq=2 Destination Host Unreachable From 10.89.97.168 icmp_seq=3 Destination Host Unreachable From 10.89.97.168 icmp_seq=4 Destination Host Unreachable ^C --- 10.89.97.143 ping statistics --- 4 packets transmitted, 0 received, +4 errors, 100% packet loss, time 3088ms pipe 4 [root@appworker0 /]# nsenter -t 60545 -n tracepath 10.89.97.143 1?: [LOCALHOST] pmtu 8900 1: *.apps.panclyphub01.mnc020.mcc714 1.683ms asymm 2 1: *.apps.panclyphub01.mnc020.mcc714 0.878ms asymm 2 2: 100.88.0.7 1.903ms asymm 3 3: 172.17.2.2 2.086ms 4: no reply 4: 10.89.97.168 3087.371ms !H Resume: pmtu 8900 [root@appworker0 /]#","title":"Method to use nsenter on the OCP"},{"location":"openshift/usermanagement/remove-kubeadmin/","text":"Removing the kubeadmin user The kubeadmin user OpenShift Container Platform creates a cluster administrator, kubeadmin, after the installation process completes. This user has the cluster-admin role automatically applied and is treated as the root user for the cluster. The password is dynamically generated and unique to your OpenShift Container Platform environment. After installation completes the password is provided in the installation program\u2019s output. For example: INFO Install complete! INFO Run 'export KUBECONFIG=<your working directory>/auth/kubeconfig' to manage the cluster with 'oc', the OpenShift CLI. INFO The cluster is ready when 'oc login -u kubeadmin -p <provided>' succeeds (wait a few minutes). INFO Access the OpenShift web-console here: https://console-openshift-console.apps.demo1.openshift4-beta-abcorp.com INFO Login to the console with user: kubeadmin, password: <provided> Removing the kubeadmin user After you define an identity provider and create a new cluster-admin user, you can remove the kubeadmin to improve cluster security. Warning If you follow this procedure before another user is a cluster-admin, then OpenShift Container Platform must be reinstalled. It is not possible to undo this command. Prerequisites You must have configured at least one identity provider. You must have added the cluster-admin role to a user. You must be logged in as an administrator. Procedure Remove the kubeadmin secrets: oc delete secrets kubeadmin -n kube-system References Remove kubeadmin id","title":"Remove Kubeadmin"},{"location":"openshift/usermanagement/remove-kubeadmin/#removing-the-kubeadmin-user","text":"","title":"Removing the kubeadmin user"},{"location":"openshift/usermanagement/remove-kubeadmin/#the-kubeadmin-user","text":"OpenShift Container Platform creates a cluster administrator, kubeadmin, after the installation process completes. This user has the cluster-admin role automatically applied and is treated as the root user for the cluster. The password is dynamically generated and unique to your OpenShift Container Platform environment. After installation completes the password is provided in the installation program\u2019s output. For example: INFO Install complete! INFO Run 'export KUBECONFIG=<your working directory>/auth/kubeconfig' to manage the cluster with 'oc', the OpenShift CLI. INFO The cluster is ready when 'oc login -u kubeadmin -p <provided>' succeeds (wait a few minutes). INFO Access the OpenShift web-console here: https://console-openshift-console.apps.demo1.openshift4-beta-abcorp.com INFO Login to the console with user: kubeadmin, password: <provided>","title":"The kubeadmin user"},{"location":"openshift/usermanagement/remove-kubeadmin/#removing-the-kubeadmin-user_1","text":"After you define an identity provider and create a new cluster-admin user, you can remove the kubeadmin to improve cluster security.","title":"Removing the kubeadmin user"},{"location":"openshift/usermanagement/remove-kubeadmin/#warning","text":"If you follow this procedure before another user is a cluster-admin, then OpenShift Container Platform must be reinstalled. It is not possible to undo this command.","title":"Warning"},{"location":"openshift/usermanagement/remove-kubeadmin/#prerequisites","text":"You must have configured at least one identity provider. You must have added the cluster-admin role to a user. You must be logged in as an administrator.","title":"Prerequisites"},{"location":"openshift/usermanagement/remove-kubeadmin/#procedure","text":"Remove the kubeadmin secrets: oc delete secrets kubeadmin -n kube-system","title":"Procedure"},{"location":"openshift/usermanagement/remove-kubeadmin/#references","text":"Remove kubeadmin id","title":"References"},{"location":"openshift/usermanagement/user-management/","text":"OCP User management using htpassword Configure HTPASSWD as an identity provider for OCP Add, update, remove users for OCP using htpasswd as indentity provider. Procedure to Add an additional users 1) Retrieve the htpasswd file from the htpass-secret Secret object and save the file to your file system: [root@dom14npv101-infra-manager ~ hub]# oc get secret htpass-secret -ojsonpath={.data.htpasswd} -n openshift-config | base64 --decode > users.htpasswd [root@dom14npv101-infra-manager ~ hub]# cat users.htpasswd ncpadmin:$2y$05$DYlpXiwFzfyRioO4hRNq1.ZdFLO3yMz3Pl3gs7.yUpEUKeOGoHX9K [root@dom14npv101-infra-manager ~ hub]# 2) Add or remove users from the users.htpasswd file. [root@dom14npv101-infra-manager ~ hub]# htpasswd -bB users.htpasswd nokia nokia@123 Adding password for user nokia [root@dom14npv101-infra-manager ~ hub]# 3) Replace the htpass-secret Secret object with the updated users in the users.htpasswd file: [root@dom14npv101-infra-manager ~ hub]# oc create secret generic htpass-secret --from-file=htpasswd=users.htpasswd --dry-run=client -o yaml -n openshift-config | oc replace -f - secret/htpass-secret replaced [root@dom14npv101-infra-manager ~ hub]# 4) Wait for all these pods to be restarted [root@dom14npv101-infra-manager ~ hub]# oc get pods -n openshift-authentication -o wide |grep -i oauth oauth-openshift-f446bd5b-58cps 1/1 Running 0 82s 172.20.2.190 ncpvnpvhub-hubmaster-101.ncpvnpvhub.pnwlab.nsn-rdnet.net <none> <none> oauth-openshift-f446bd5b-k8dqx 0/1 Running 0 27s 172.21.0.241 ncpvnpvhub-hubmaster-103.ncpvnpvhub.pnwlab.nsn-rdnet.net <none> <none> oauth-openshift-f446bd5b-v4n6m 1/1 Running 0 55s 172.20.0.134 ncpvnpvhub-hubmaster-102.ncpvnpvhub.pnwlab.nsn-rdnet.net <none> <none> [root@dom14npv101-infra-manager ~ hub]# 5) Validate the login now. [root@dom14npv101-infra-manager ~ hub]# oc login -u nokia -p nokia@123 WARNING: Using insecure TLS client config. Setting this option is not supported! Login successful. You don't have any projects. You can try to create a new project, by running oc new-project <projectname> [root@dom14npv101-infra-manager ~ hub]# oc whoami nokia [root@dom14npv101-infra-manager ~ hub]# Procedure to update the password of an existing users 1) Retrieve the htpasswd file from the htpass-secret Secret object and save the file to your file system: [root@dom14npv101-infra-manager ~ hub]# oc get secret htpass-secret -ojsonpath={.data.htpasswd} -n openshift-config | base64 --decode > users.htpasswd [root@dom14npv101-infra-manager ~ hub]# cat users.htpasswd ncpadmin:$2y$05$DYlpXiwFzfyRioO4hRNq1.ZdFLO3yMz3Pl3gs7.yUpEUKeOGoHX9K nokia:$2y$05$SLpgZb3AE.fE7WQEGIMOXesoZdY9pV9OHVpXb5CuITdZO5RVId8Ye [root@dom14npv101-infra-manager ~ hub]# 2) update the users password from the users.htpasswd file. [root@dom14npv101-infra-manager ~ hub]# htpasswd -bB users.htpasswd nokia nokia@1234 Updating password for user nokia [root@dom14npv101-infra-manager ~ hub]# 3) Replace the htpass-secret Secret object with the updated users in the users.htpasswd file: [root@dom14npv101-infra-manager ~ hub]# oc create secret generic htpass-secret --from-file=htpasswd=users.htpasswd --dry-run=client -o yaml -n openshift-config | oc replace -f - secret/htpass-secret replaced [root@dom14npv101-infra-manager ~ hub]# 4) Wait for all these pods to be restarted [root@dom14npv101-infra-manager ~ hub]# oc get pods -n openshift-authentication -o wide -w NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES oauth-openshift-f446bd5b-58cps 1/1 Running 0 6m48s 172.20.2.190 ncpvnpvhub-hubmaster-101.ncpvnpvhub.pnwlab.nsn-rdnet.net <none> <none> oauth-openshift-f446bd5b-k8dqx 1/1 Running 0 5m53s 172.21.0.241 ncpvnpvhub-hubmaster-103.ncpvnpvhub.pnwlab.nsn-rdnet.net <none> <none> oauth-openshift-f446bd5b-v4n6m 1/1 Running 0 6m21s 172.20.0.134 ncpvnpvhub-hubmaster-102.ncpvnpvhub.pnwlab.nsn-rdnet.net <none> <none> oauth-openshift-f446bd5b-v4n6m 1/1 Terminating 0 6m24s 172.20.0.134 ncpvnpvhub-hubmaster-102.ncpvnpvhub.pnwlab.nsn-rdnet.net <none> <none> oauth-openshift-6497ccb4f5-d5wxd 0/1 Pending 0 0s <none> <none> <none> <none> oauth-openshift-6497ccb4f5-d5wxd 0/1 Pending 0 0s <none> <none> <none> <none> 5) Validate the login now. [root@dom14npv101-infra-manager ~ hub]# oc login -u nokia -p nokia@123 WARNING: Using insecure TLS client config. Setting this option is not supported! Login failed (401 Unauthorized) Verify you have provided the correct credentials. [root@dom14npv101-infra-manager ~ hub]# oc login -u nokia -p nokia@1234 WARNING: Using insecure TLS client config. Setting this option is not supported! Login successful. You don't have any projects. You can try to create a new project, by running oc new-project <projectname> [root@dom14npv101-infra-manager ~ hub]# Procedure to delete an user completely 1) Retrieve the htpasswd file from the htpass-secret Secret object and save the file to your file system: [root@dom14npv101-infra-manager ~ hub]# oc get secret htpass-secret -ojsonpath={.data.htpasswd} -n openshift-config | base64 --decode > users.htpasswd [root@dom14npv101-infra-manager ~ hub]# cat users.htpasswd ncpadmin:$2y$05$DYlpXiwFzfyRioO4hRNq1.ZdFLO3yMz3Pl3gs7.yUpEUKeOGoHX9K nokia:$2y$05$SLpgZb3AE.fE7WQEGIMOXesoZdY9pV9OHVpXb5CuITdZO5RVId8Ye [root@dom14npv101-infra-manager ~ hub]# 2) remove users from the users.htpasswd file. [root@dom14npv101-infra-manager ~ hub]# htpasswd -D users.htpasswd nokia Deleting password for user nokia [root@dom14npv101-infra-manager ~ hub]# 3) Replace the htpass-secret Secret object with the updated users in the users.htpasswd file: [root@dom14npv101-infra-manager ~ hub]# oc create secret generic htpass-secret --from-file=htpasswd=users.htpasswd --dry-run=client -o yaml -n openshift-config | oc replace -f - secret/htpass-secret replaced [root@dom14npv101-infra-manager ~ hub]# 4) Wait for all these pods to be restarted [root@dom14npv101-infra-manager ~ hub]# oc get pods -n openshift-authentication -o wide -w NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES oauth-openshift-f446bd5b-58cps 1/1 Running 0 6m48s 172.20.2.190 ncpvnpvhub-hubmaster-101.ncpvnpvhub.pnwlab.nsn-rdnet.net <none> <none> oauth-openshift-f446bd5b-k8dqx 1/1 Running 0 5m53s 172.21.0.241 ncpvnpvhub-hubmaster-103.ncpvnpvhub.pnwlab.nsn-rdnet.net <none> <none> oauth-openshift-f446bd5b-v4n6m 1/1 Running 0 6m21s 172.20.0.134 ncpvnpvhub-hubmaster-102.ncpvnpvhub.pnwlab.nsn-rdnet.net <none> <none> oauth-openshift-f446bd5b-v4n6m 1/1 Terminating 0 6m24s 172.20.0.134 ncpvnpvhub-hubmaster-102.ncpvnpvhub.pnwlab.nsn-rdnet.net <none> <none> oauth-openshift-6497ccb4f5-d5wxd 0/1 Pending 0 0s <none> <none> <none> <none> oauth-openshift-6497ccb4f5-d5wxd 0/1 Pending 0 0s <none> <none> <none> <none> 5) If you removed one or more users, you must additionally remove existing resources for each user. a) Delete the User object: ``` [root@dom14npv101-infra-manager ~ hub]# oc get user NAME UID FULL NAME IDENTITIES ncpadmin 8e728883-a17f-4bde-8b0f-2eab78ccc6c3 my_htpasswd_provider:ncpadmin nokia 7e44ba3a-8d91-435f-800f-380a8e87f0d1 my_htpasswd_provider:nokia [root@dom14npv101-infra-manager ~ hub]# oc delete user nokia user.user.openshift.io \"nokia\" deleted [root@dom14npv101-infra-manager ~ hub]# ``` b) Be sure to remove the user, otherwise the user can continue using their token as long as it has not expired. ``` [root@dom14npv101-infra-manager ~ hub]# oc get identity NAME IDP NAME IDP USER NAME USER NAME USER UID my_htpasswd_provider:ncpadmin my_htpasswd_provider ncpadmin ncpadmin 8e728883-a17f-4bde-8b0f-2eab78ccc6c3 my_htpasswd_provider:nokia my_htpasswd_provider nokia nokia 7e44ba3a-8d91-435f-800f-380a8e87f0d1 [root@dom14npv101-infra-manager ~ hub]# oc delete identity my_htpasswd_provider:nokia identity.user.openshift.io \"my_htpasswd_provider:nokia\" deleted [root@dom14npv101-infra-manager ~ hub]# ``` 5) Validate the login and it should not work. [root@dom14npv101-infra-manager ~ hub]# oc login -u nokia -p nokia@1234 WARNING: Using insecure TLS client config. Setting this option is not supported! Login failed (401 Unauthorized) Verify you have provided the correct credentials. [root@dom14npv101-infra-manager ~ hub]# Disclaimer All these procedures are collected from redhat link given below. References Configuring an htpasswd identity provider To configure an HTPasswd identity provider in OpenShift 4 Remove kubeadmin id","title":"User Management"},{"location":"openshift/usermanagement/user-management/#ocp-user-management-using-htpassword","text":"","title":"OCP User management using htpassword"},{"location":"openshift/usermanagement/user-management/#configure-htpasswd-as-an-identity-provider-for-ocp","text":"","title":"Configure HTPASSWD as an identity provider for OCP"},{"location":"openshift/usermanagement/user-management/#add-update-remove-users-for-ocp-using-htpasswd-as-indentity-provider","text":"","title":"Add, update, remove users for OCP using htpasswd as indentity provider."},{"location":"openshift/usermanagement/user-management/#procedure-to-add-an-additional-users","text":"1) Retrieve the htpasswd file from the htpass-secret Secret object and save the file to your file system: [root@dom14npv101-infra-manager ~ hub]# oc get secret htpass-secret -ojsonpath={.data.htpasswd} -n openshift-config | base64 --decode > users.htpasswd [root@dom14npv101-infra-manager ~ hub]# cat users.htpasswd ncpadmin:$2y$05$DYlpXiwFzfyRioO4hRNq1.ZdFLO3yMz3Pl3gs7.yUpEUKeOGoHX9K [root@dom14npv101-infra-manager ~ hub]# 2) Add or remove users from the users.htpasswd file. [root@dom14npv101-infra-manager ~ hub]# htpasswd -bB users.htpasswd nokia nokia@123 Adding password for user nokia [root@dom14npv101-infra-manager ~ hub]# 3) Replace the htpass-secret Secret object with the updated users in the users.htpasswd file: [root@dom14npv101-infra-manager ~ hub]# oc create secret generic htpass-secret --from-file=htpasswd=users.htpasswd --dry-run=client -o yaml -n openshift-config | oc replace -f - secret/htpass-secret replaced [root@dom14npv101-infra-manager ~ hub]# 4) Wait for all these pods to be restarted [root@dom14npv101-infra-manager ~ hub]# oc get pods -n openshift-authentication -o wide |grep -i oauth oauth-openshift-f446bd5b-58cps 1/1 Running 0 82s 172.20.2.190 ncpvnpvhub-hubmaster-101.ncpvnpvhub.pnwlab.nsn-rdnet.net <none> <none> oauth-openshift-f446bd5b-k8dqx 0/1 Running 0 27s 172.21.0.241 ncpvnpvhub-hubmaster-103.ncpvnpvhub.pnwlab.nsn-rdnet.net <none> <none> oauth-openshift-f446bd5b-v4n6m 1/1 Running 0 55s 172.20.0.134 ncpvnpvhub-hubmaster-102.ncpvnpvhub.pnwlab.nsn-rdnet.net <none> <none> [root@dom14npv101-infra-manager ~ hub]# 5) Validate the login now. [root@dom14npv101-infra-manager ~ hub]# oc login -u nokia -p nokia@123 WARNING: Using insecure TLS client config. Setting this option is not supported! Login successful. You don't have any projects. You can try to create a new project, by running oc new-project <projectname> [root@dom14npv101-infra-manager ~ hub]# oc whoami nokia [root@dom14npv101-infra-manager ~ hub]#","title":"Procedure to Add an additional users"},{"location":"openshift/usermanagement/user-management/#procedure-to-update-the-password-of-an-existing-users","text":"1) Retrieve the htpasswd file from the htpass-secret Secret object and save the file to your file system: [root@dom14npv101-infra-manager ~ hub]# oc get secret htpass-secret -ojsonpath={.data.htpasswd} -n openshift-config | base64 --decode > users.htpasswd [root@dom14npv101-infra-manager ~ hub]# cat users.htpasswd ncpadmin:$2y$05$DYlpXiwFzfyRioO4hRNq1.ZdFLO3yMz3Pl3gs7.yUpEUKeOGoHX9K nokia:$2y$05$SLpgZb3AE.fE7WQEGIMOXesoZdY9pV9OHVpXb5CuITdZO5RVId8Ye [root@dom14npv101-infra-manager ~ hub]# 2) update the users password from the users.htpasswd file. [root@dom14npv101-infra-manager ~ hub]# htpasswd -bB users.htpasswd nokia nokia@1234 Updating password for user nokia [root@dom14npv101-infra-manager ~ hub]# 3) Replace the htpass-secret Secret object with the updated users in the users.htpasswd file: [root@dom14npv101-infra-manager ~ hub]# oc create secret generic htpass-secret --from-file=htpasswd=users.htpasswd --dry-run=client -o yaml -n openshift-config | oc replace -f - secret/htpass-secret replaced [root@dom14npv101-infra-manager ~ hub]# 4) Wait for all these pods to be restarted [root@dom14npv101-infra-manager ~ hub]# oc get pods -n openshift-authentication -o wide -w NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES oauth-openshift-f446bd5b-58cps 1/1 Running 0 6m48s 172.20.2.190 ncpvnpvhub-hubmaster-101.ncpvnpvhub.pnwlab.nsn-rdnet.net <none> <none> oauth-openshift-f446bd5b-k8dqx 1/1 Running 0 5m53s 172.21.0.241 ncpvnpvhub-hubmaster-103.ncpvnpvhub.pnwlab.nsn-rdnet.net <none> <none> oauth-openshift-f446bd5b-v4n6m 1/1 Running 0 6m21s 172.20.0.134 ncpvnpvhub-hubmaster-102.ncpvnpvhub.pnwlab.nsn-rdnet.net <none> <none> oauth-openshift-f446bd5b-v4n6m 1/1 Terminating 0 6m24s 172.20.0.134 ncpvnpvhub-hubmaster-102.ncpvnpvhub.pnwlab.nsn-rdnet.net <none> <none> oauth-openshift-6497ccb4f5-d5wxd 0/1 Pending 0 0s <none> <none> <none> <none> oauth-openshift-6497ccb4f5-d5wxd 0/1 Pending 0 0s <none> <none> <none> <none> 5) Validate the login now. [root@dom14npv101-infra-manager ~ hub]# oc login -u nokia -p nokia@123 WARNING: Using insecure TLS client config. Setting this option is not supported! Login failed (401 Unauthorized) Verify you have provided the correct credentials. [root@dom14npv101-infra-manager ~ hub]# oc login -u nokia -p nokia@1234 WARNING: Using insecure TLS client config. Setting this option is not supported! Login successful. You don't have any projects. You can try to create a new project, by running oc new-project <projectname> [root@dom14npv101-infra-manager ~ hub]#","title":"Procedure to update the password of an existing users"},{"location":"openshift/usermanagement/user-management/#procedure-to-delete-an-user-completely","text":"1) Retrieve the htpasswd file from the htpass-secret Secret object and save the file to your file system: [root@dom14npv101-infra-manager ~ hub]# oc get secret htpass-secret -ojsonpath={.data.htpasswd} -n openshift-config | base64 --decode > users.htpasswd [root@dom14npv101-infra-manager ~ hub]# cat users.htpasswd ncpadmin:$2y$05$DYlpXiwFzfyRioO4hRNq1.ZdFLO3yMz3Pl3gs7.yUpEUKeOGoHX9K nokia:$2y$05$SLpgZb3AE.fE7WQEGIMOXesoZdY9pV9OHVpXb5CuITdZO5RVId8Ye [root@dom14npv101-infra-manager ~ hub]# 2) remove users from the users.htpasswd file. [root@dom14npv101-infra-manager ~ hub]# htpasswd -D users.htpasswd nokia Deleting password for user nokia [root@dom14npv101-infra-manager ~ hub]# 3) Replace the htpass-secret Secret object with the updated users in the users.htpasswd file: [root@dom14npv101-infra-manager ~ hub]# oc create secret generic htpass-secret --from-file=htpasswd=users.htpasswd --dry-run=client -o yaml -n openshift-config | oc replace -f - secret/htpass-secret replaced [root@dom14npv101-infra-manager ~ hub]# 4) Wait for all these pods to be restarted [root@dom14npv101-infra-manager ~ hub]# oc get pods -n openshift-authentication -o wide -w NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES oauth-openshift-f446bd5b-58cps 1/1 Running 0 6m48s 172.20.2.190 ncpvnpvhub-hubmaster-101.ncpvnpvhub.pnwlab.nsn-rdnet.net <none> <none> oauth-openshift-f446bd5b-k8dqx 1/1 Running 0 5m53s 172.21.0.241 ncpvnpvhub-hubmaster-103.ncpvnpvhub.pnwlab.nsn-rdnet.net <none> <none> oauth-openshift-f446bd5b-v4n6m 1/1 Running 0 6m21s 172.20.0.134 ncpvnpvhub-hubmaster-102.ncpvnpvhub.pnwlab.nsn-rdnet.net <none> <none> oauth-openshift-f446bd5b-v4n6m 1/1 Terminating 0 6m24s 172.20.0.134 ncpvnpvhub-hubmaster-102.ncpvnpvhub.pnwlab.nsn-rdnet.net <none> <none> oauth-openshift-6497ccb4f5-d5wxd 0/1 Pending 0 0s <none> <none> <none> <none> oauth-openshift-6497ccb4f5-d5wxd 0/1 Pending 0 0s <none> <none> <none> <none> 5) If you removed one or more users, you must additionally remove existing resources for each user. a) Delete the User object: ``` [root@dom14npv101-infra-manager ~ hub]# oc get user NAME UID FULL NAME IDENTITIES ncpadmin 8e728883-a17f-4bde-8b0f-2eab78ccc6c3 my_htpasswd_provider:ncpadmin nokia 7e44ba3a-8d91-435f-800f-380a8e87f0d1 my_htpasswd_provider:nokia [root@dom14npv101-infra-manager ~ hub]# oc delete user nokia user.user.openshift.io \"nokia\" deleted [root@dom14npv101-infra-manager ~ hub]# ``` b) Be sure to remove the user, otherwise the user can continue using their token as long as it has not expired. ``` [root@dom14npv101-infra-manager ~ hub]# oc get identity NAME IDP NAME IDP USER NAME USER NAME USER UID my_htpasswd_provider:ncpadmin my_htpasswd_provider ncpadmin ncpadmin 8e728883-a17f-4bde-8b0f-2eab78ccc6c3 my_htpasswd_provider:nokia my_htpasswd_provider nokia nokia 7e44ba3a-8d91-435f-800f-380a8e87f0d1 [root@dom14npv101-infra-manager ~ hub]# oc delete identity my_htpasswd_provider:nokia identity.user.openshift.io \"my_htpasswd_provider:nokia\" deleted [root@dom14npv101-infra-manager ~ hub]# ``` 5) Validate the login and it should not work. [root@dom14npv101-infra-manager ~ hub]# oc login -u nokia -p nokia@1234 WARNING: Using insecure TLS client config. Setting this option is not supported! Login failed (401 Unauthorized) Verify you have provided the correct credentials. [root@dom14npv101-infra-manager ~ hub]#","title":"Procedure to delete an user completely"},{"location":"openshift/usermanagement/user-management/#disclaimer","text":"All these procedures are collected from redhat link given below.","title":"Disclaimer"},{"location":"openshift/usermanagement/user-management/#references","text":"Configuring an htpasswd identity provider To configure an HTPasswd identity provider in OpenShift 4 Remove kubeadmin id","title":"References"}]}