{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to NCP OpenShift Notes","text":"<p>Your comprehensive troubleshooting guide to OpenShift installation, backup, storage, troubleshooting, and more.</p> <p></p>"},{"location":"#documentation-overview","title":"\ud83d\udcda Documentation Overview","text":"<ul> <li>Git Helper</li> <li>Git Deployment</li> <li>Project Management</li> <li>Deployment Guides</li> <li>User Management</li> <li>OpenShift Backup &amp; Restore</li> <li>CNF Onboarding Support</li> <li>Intergrations</li> <li>Maintenance Procedures</li> <li>Networking Configurations</li> <li>Storage Management</li> <li>Troubleshooting and Workarounds</li> <li>Disaster Management</li> <li>HUB Master Replacement</li> </ul> <p>   Logout </p>"},{"location":"access-denied/","title":"Access Denied","text":"<p>You do not have permission to view this site.</p> <p>If you believe this is a mistake, please contact the administrator at venkatapathiraj.ravichandran.ext@nokia.com.</p>"},{"location":"git-helper/ncd-git-backup/","title":"Backing up Nokia Git Server","text":"<p>this procedure, works for NCD 24.9 mp1 pp1. </p>"},{"location":"git-helper/ncd-git-backup/#instructions-on-backing-up-nokia-git-server","title":"Instructions on backing up Nokia Git Server","text":""},{"location":"git-helper/ncd-git-backup/#backup-utility-will-generate-the-following-outputs","title":"backup utility will generate the following outputs","text":"<ul> <li>database.sql file containing the PostgreSQL database content</li> <li>Git repository data</li> <li>Kubernetes secrets required for the upgrade</li> <li> <p>backup_information.yml generated by the backup tool</p> </li> <li> <p>Example of backup_information.yml</p> </li> </ul> <pre><code>:db_version: 20240508085441\n:backup_created_at: 2024-06-13 13:57:08 +0000\n:gitlab_version: 17.0.1\n:tar_version: tar (GNU tar) 1.34\n:installation_type: gitlab-helm-chart\n:skipped: builds,pages,registry,uploads,artifacts,lfs,packages,\nexternal_diffs,terraform_state,pages,ci_secure_files\n:repositories_server_side: false\n</code></pre>"},{"location":"git-helper/ncd-git-backup/#prerequisites","title":"Prerequisites","text":"<ul> <li>To back up Nokia Git Server and the necessary Helm plugins for Backup and Recovery (CBUR), CBUR must be installed on your cluster.</li> <li>CBUR must be enabled in the Nokia Git Server installation.</li> </ul>"},{"location":"git-helper/ncd-git-backup/#remove-plugins-for-helm-backup-and-restore","title":"remove plugins for helm (backup and restore)","text":"<ol> <li>login to bastion host and login to cluster here .</li> </ol> <pre><code>[root@ncputility ~]# source /root/panhubrc\nWARNING: Using insecure TLS client config. Setting this option is not supported!\n\nLogin successful.\n\nYou have access to 101 projects, the list has been suppressed. You can list all projects with 'oc projects'\n\nUsing project \"default\".\n[root@ncputility ~ panhub_rc]$\n</code></pre> <ol> <li>list of plugins installed on helm here.</li> </ol> <pre><code>[root@ncputility ~ panhub_rc]$ helm plugin list\nNAME    VERSION DESCRIPTION\nbackup  0.1.2   backup/restore releases in a namespace to/from a file\n[root@ncputility ~ panhub_rc]$\n</code></pre> <ol> <li>remove using following command on helm <code>#helm plugin remove backup</code></li> </ol> <pre><code>[root@ncputility ~ panhub_rc]$ helm plugin remove backup\nUninstalled plugin: backup\n[root@ncputility ~ panhub_rc]$\n</code></pre>"},{"location":"git-helper/ncd-git-backup/#install-plugins-for-helm-backup-and-restore","title":"Install plugins for helm (backup and restore)","text":"<ol> <li>login to bastion host and login to cluster here .</li> </ol> <pre><code>[root@ncputility ~]# source /root/panhubrc\nWARNING: Using insecure TLS client config. Setting this option is not supported!\n\nLogin successful.\n\nYou have access to 101 projects, the list has been suppressed. You can list all projects with 'oc projects'\n\nUsing project \"default\".\n[root@ncputility ~ panhub_rc]$\n</code></pre> <ol> <li>list of plugins installed on helm here.</li> </ol> <pre><code>[root@ncputility ~ panhub_rc]$ helm plugin list\nNAME    VERSION DESCRIPTION\n[root@ncputility ~ panhub_rc]$\n</code></pre> <ol> <li>Install the latest version of the Helm backup and restore plugins.</li> </ol> <p>For the Helm backup and restore plugins, use the version that is compatible with the respective Backup and Recovery chart release.</p> <pre><code>[root@ncputility ~ panhub_rc]$ export HELM_HOME=$HOME/.helm\n[root@ncputility ~ panhub_rc]$ cd\n[root@ncputility ~ panhub_rc]$ WORK_DIR=`mktemp -d`\n[root@ncputility ~ panhub_rc]$ mkdir -p $WORK_DIR/backup\n[root@ncputility ~ panhub_rc]$ mkdir -p $WORK_DIR/restore\n[root@ncputility ~ panhub_rc]$ tar xf /root/ncd/NCD_24.9_Git_Server_ORB-RC/ncd-git-server-product/INSTALL_MEDIA/backup-3.7.4.tgz -C $WORK_DIR/backup\n[root@ncputility ~ panhub_rc]$ tar xf /root/ncd/NCD_24.9_Git_Server_ORB-RC/ncd-git-server-product/INSTALL_MEDIA/restore-3.7.4.tgz -C $WORK_DIR/restore\n[root@ncputility ~ panhub_rc]$ helm plugin install $WORK_DIR/backup\nInstalled plugin: backup\n[root@ncputility ~ panhub_rc]$ helm plugin install $WORK_DIR/restore\nInstalled plugin: restore\n[root@ncputility ~ panhub_rc]$ rm -rf $WORK_DIR\n[root@ncputility ~ panhub_rc]$\n</code></pre> <pre><code>a. check the status of installed helm plugins.\n\n```\n[root@ncputility ~ panhub_rc]$ helm plugin list\nNAME    VERSION DESCRIPTION\nbackup  3.7.4   Plugin responsible for the backup of helm releases and k8s namespaces with CBUR.\nrestore 3.7.4   Plugin responsible for restoring helm releases and k8s namespaces with CBUR.\n[root@ncputility ~ panhub_rc]$ \n```\n</code></pre> <ol> <li>Configure RBAC for CRDs. (only when you deploy using tenant credentials)</li> </ol> <p>For tenants only using Backup and Recovery, permission to read, create, modify and delete BrPolicy and BrHook is sufficient. However, tenants installing their own namespaces for Backup and Recovery also need permission to read brpolices/status.</p> <pre><code>kind: ClusterRole\napiVersion: rbac.authorization.k8s.io/v1\nmetadata:\n  name: admin:brpolices\n  labels:\n    rbac.authorization.k8s.io/aggregate-to-admin: \"true\"\n    rbac.authorization.k8s.io/aggregate-to-edit: \"true\"\nrules:\n- apiGroups: [\"cbur.csf.nokia.com\"]\n  resources: [\"brpolices\", \"brhooks\"]\n  verbs: [\"get\", \"list\", \"watch\", \"create\", \"update\", \"patch\", \"delete\"]\n- apiGroups: [\"cbur.csf.nokia.com\"]\n  resources: [\"brpolices/status\"]\n  verbs: [\"get\", \"list\", \"watch\", \"update\", \"patch\"]\n---\nkind: ClusterRole\napiVersion: rbac.authorization.k8s.io/v1\nmetadata:\n  name: view:brpolices\n  labels:\n    rbac.authorization.k8s.io/aggregate-to-view: \"true\"\nrules:\n- apiGroups: [\"cbur.csf.nokia.com\"]\n  resources: [\"brpolices\", \"brhooks\", \"brpolices/status\"]\n  verbs: [\"get\", \"list\", \"watch\"]\n</code></pre>"},{"location":"git-helper/ncd-git-backup/#backup-nokia-git-server","title":"Backup Nokia Git Server","text":"<p>Notes:</p> <pre><code>The CBUR master service (CBUR_MASTER_SERVICE) must be specified in either of the following formats:\nhttp(s)://dns_name:port\nhttp(s)://ipv4:port\nhttp(s)://[ipv6]:port\nhttp(s)://dns_name\nhttp(s)://ingress/ingresspath\n</code></pre> <p>here is the command to run the backup of git server </p> <pre><code>helm backup -t ncd-git -a none -n paclypancdgit01 -x http://172.20.8.113:80\n\nhelm backup -n paclypancdgit01 -t ncd-git -a none -x http://172.20.8.113:80\n</code></pre> <ol> <li>login to bastion host and login to cluster here .</li> </ol> <pre><code>[root@ncputility ~]# source /root/panhubrc\nWARNING: Using insecure TLS client config. Setting this option is not supported!\n\nLogin successful.\n\nYou have access to 101 projects, the list has been suppressed. You can list all projects with 'oc projects'\n\nUsing project \"default\".\n[root@ncputility ~ panhub_rc]$\n</code></pre> <ol> <li>Find the <code>namespace</code> and <code>release</code> name of git server and cbut details from here 'helm list -A '</li> </ol> <pre><code>[root@ncputility ~ panhub_rc]$ helm list  -A\nNAME            NAMESPACE               REVISION        UPDATED                                 STATUS          CHART                                   APP VERSION\ncbur-crds       paclypancdcbur01        1               2025-02-27 14:18:04.313208995 -0500 EST deployed        cbur-crds-2.6.0                         2.6.0\nncd-cbur        paclypancdcbur01        1               2025-02-27 14:24:56.578242699 -0500 EST deployed        cbur-1.18.1                             1.13.1\nncd-git         paclypancdgit01         1               2025-02-27 14:45:25.489107042 -0500 EST deployed        ncd-git-server-24.9.1-7.g30f1acf        17.3.3\nncd-postgresql  paclypancddb01          1               2025-02-27 14:30:27.65639258 -0500 EST  deployed        postgresql-ha-24.9.1-1009.g19e2a92      24.9.1-1009.g19e2a92\nncd-redis       paclypancddb01          1               2025-02-27 14:37:11.651840036 -0500 EST deployed        ncd-redis-24.9.1-1009.g19e2a92          24.9.1-1009.g19e2a92\n[root@ncputility ~ panhub_rc]$\n</code></pre> <ol> <li>trigger the backup job using following command here </li> </ol> <p>Uri should be having <code>cbur</code> structure name in it </p> <pre><code>[root@ncputility ~ panhub_rc]$ helm backup -n paclypancdgit01 -a none -t ncd-git -x http://cbur.apps.panclyphub01.mnc020.mcc714/cbur\n{\n  \"kind\": \"Status\",\n  \"apiVersion\": \"v1\",\n  \"metadata\": {},\n  \"status\": \"Success\",\n  \"message\": \"backupHelmRelease task = a80ed466-254c-4644-8d19-9a0cdf813fbe is on!\",\n  \"reason\": \"\",\n  \"details\": {\n    \"a80ed466-254c-4644-8d19-9a0cdf813fbe\": {\n      \"name\": \"ncd-git\",\n      \"namespace\": \"paclypancdgit01\",\n      \"timestamp\": \"20250402104554\",\n      \"backup_data\": {},\n      \"helm_version\": 3,\n      \"request\": \"backupHelmRelease\"\n    }\n  },\n  \"code\": 202\n}\n[root@ncputility ~ panhub_rc]$ \n</code></pre> <ol> <li>here is an example with <code>--verbose</code> mode  (optinal for troubleshooting)</li> </ol> <pre><code>[root@ncputility ~ panhub_rc]$ helm backup -n paclypancdgit01 -a none -t ncd-git -x http://cbur.apps.panclyphub01.mnc020.mcc714/cbur --verbose\n*******\n+ exit_func 0\n+ '[' -d /tmp/certs.1974896 ']'\n+ rm -rf /tmp/certs.1974896\n+ exit 0\n[root@ncputility ~ panhub_rc]$\n</code></pre> <ol> <li>check the status of the backup using cbur br policy status</li> </ol> <pre><code>[root@ncputility ~ panhub_rc]$ kubectl -n paclypancdgit01 get brpolices.cbur.csf.nokia.com ncd-git-toolbox\nNAME              AGE\nncd-git-toolbox   33d\n[root@ncputility ~ panhub_rc]$ kubectl -n paclypancdgit01 get brpolices.cbur.csf.nokia.com ncd-git-toolbox -o yaml\n[root@ncputility ~ panhub_rc]$\n</code></pre> <ol> <li>look for backup files exported. , right now backup files are saved locally on the gitserver(paclypancdgit01) <code>namespace</code> within <code>ncd-git-toolbox</code> pod. </li> </ol> <p>but we need this backup files to be saved outside the cluster, like sftp server or something, so that we can restore it on new git server.</p> <pre><code>[root@ncputility ~ panhub_rc]$ oc -n paclypancdgit01 exec -it ncd-git-toolbox-56b88dd4c7-27hb5 -- bash\nDefaulted container \"toolbox\" out of: toolbox, cbura-sidecar, certificates (init), configure (init)\ngit@ncd-git-toolbox-56b88dd4c7-27hb5:/$ cd /srv/gitlab/tmp/backups\ngit@ncd-git-toolbox-56b88dd4c7-27hb5:/srv/gitlab/tmp/backups$ ls\nbackup_information.yml  db  repositories\ngit@ncd-git-toolbox-56b88dd4c7-27hb5:/srv/gitlab/tmp/backups$ ls -la\ntotal 4\ndrwxr-sr-x. 4 git  git  66 Apr  2 10:46 .\ndrwxrwsrwx. 3 root git  21 Apr  2 10:46 ..\n-rw-r--r--. 1 git  git 318 Apr  2 10:46 backup_information.yml\ndrwxr-sr-x. 2 git  git  29 Apr  2 10:46 db\ndrwx--S---. 4 git  git  38 Apr  2 10:46 repositories\ngit@ncd-git-toolbox-56b88dd4c7-27hb5:/srv/gitlab/tmp/backups$\n</code></pre>"},{"location":"git-helper/ncd-git-backup/#creating-user-for-remote-backup","title":"Creating user for remote backup","text":"<ol> <li>login to the <code>linux</code> host and used <code>useradd</code> and <code>passwd</code> commands to create <code>user and password</code> here </li> </ol> <pre><code>[root@ncputility ~ panhub_rc]$ useradd gitserverbackup\n[root@ncputility ~ panhub_rc]$ passwd gitserverbackup\nChanging password for user gitserverbackup.\nNew password:\nBAD PASSWORD: The password is shorter than 8 characters\nRetype new password:\npasswd: all authentication tokens updated successfully.\n[root@ncputility ~ panhub_rc]$\n</code></pre> <ol> <li>create a public key on the new user authrazation file.</li> </ol> <pre><code>[root@ncputility ~ pancwl_rc]$ su - gitserverbackup\n[gitserverbackup@ncputility ~]$ pwd\n/home/gitserverbackup\n[gitserverbackup@ncputility ~]$ \n[gitserverbackup@ncputility ~]$ ssh-keygen -t rsa\nGenerating public/private rsa key pair.\nEnter file in which to save the key (/home/gitserverbackup/.ssh/id_rsa):\nCreated directory '/home/gitserverbackup/.ssh'.\nEnter passphrase (empty for no passphrase):\nEnter same passphrase again:\nYour identification has been saved in /home/gitserverbackup/.ssh/id_rsa\nYour public key has been saved in /home/gitserverbackup/.ssh/id_rsa.pub\nThe key fingerprint is:\nSHA256:oF5K3Ky1tLDmb5ALID4FXyLPnlTtX97ZpJyFE0r3Z2A gitserverbackup@ncputility.panclyphub01.mnc020.mcc714\nThe key's randomart image is:\n+---[RSA 3072]----+\n|      .          |\n| o . o .   . oE  |\n|  * + o   . o.+. |\n|o  B + o   o o +o|\n|o.+ *.* S o o O..|\n| o.=oO o . . * . |\n|  ..*oo          |\n|   o. .          |\n|    .o.          |\n+----[SHA256]-----+\n[gitserverbackup@ncputility ~]$\n</code></pre> <ol> <li>take copy of the cbur public using following oc command</li> </ol> <pre><code>[root@ncputility ~ panhub_rc]$ helm list -A\nNAME            NAMESPACE               REVISION        UPDATED                                 STATUS          CHART                                   APP VERSION\ncbur-crds       paclypancdcbur01        1               2025-02-27 14:18:04.313208995 -0500 EST deployed        cbur-crds-2.6.0                         2.6.0\nncd-cbur        paclypancdcbur01        2               2025-04-02 04:17:30.755392541 -0500 EST deployed        cbur-1.18.1                             1.13.1\nncd-git         paclypancdgit01         1               2025-02-27 14:45:25.489107042 -0500 EST deployed        ncd-git-server-24.9.1-7.g30f1acf        17.3.3\nncd-postgresql  paclypancddb01          1               2025-02-27 14:30:27.65639258 -0500 EST  deployed        postgresql-ha-24.9.1-1009.g19e2a92      24.9.1-1009.g19e2a92\nncd-redis       paclypancddb01          1               2025-02-27 14:37:11.651840036 -0500 EST deployed        ncd-redis-24.9.1-1009.g19e2a92          24.9.1-1009.g19e2a92\n[root@ncputility ~ panhub_rc]$\n[root@ncputility ~ panhub_rc]$ oc get secret -n paclypancdcbur01 cburm-ssh-public-key -o jsonpath={.data.ssh_public_key} | base64 -d\nssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQDUH73f7kf8ey2UEkgeN/IJbEBDJPgricRlsD7d7pB0RAQGLx/fNZT15VRX2qOvLydsTbtcxz28Uzryy5zAmAB9z0zGuYKaSo80bS7bXjIsKc71fGD6NvvfSBBLQ1GCk0mFIjn06XmkRJOgqtgOvq66HQEcGSJQ6jq3NQzGERe+VrCk1VWbyfr0vtqqasmKChgr0dAh+0f07lUdpbR9XzEnOG20LNCAcffEBPXXccSEz/huPHOV0Kjfe7rtaKj5ZoIkFlFETPTz4HoKPlZcfxp/s94yICXk++TiI9+mF2SVYuEUWqx00p1DTa79dmUncTaz1c5nshaq4bNdJcPkoDdp[root@ncputility ~ panhub_rc]$\n</code></pre> <ol> <li>here i am using ssh-copy-id command to auto create an authzation file </li> </ol> <pre><code>[gitserverbackup@ncputility ~]$ ssh-copy-id localhost\n/usr/bin/ssh-copy-id: INFO: Source of key(s) to be installed: \"/home/gitserverbackup/.ssh/id_rsa.pub\"\nThe authenticity of host 'localhost (::1)' can't be established.\nED25519 key fingerprint is SHA256:KXMGNVqIYHOtvcd+VqGq/5d/t5UWWGKxPVkuffCOD9I.\nThis key is not known by any other names\nAre you sure you want to continue connecting (yes/no/[fingerprint])? yes\n/usr/bin/ssh-copy-id: INFO: attempting to log in with the new key(s), to filter out any that are already installed\n/usr/bin/ssh-copy-id: INFO: 1 key(s) remain to be installed -- if you are prompted now it is to install the new keys\ngitserverbackup@localhost's password:\n\nNumber of key(s) added: 1\n\nNow try logging into the machine, with:   \"ssh 'localhost'\"\nand check to make sure that only the key(s) you wanted were added.\n\n[gitserverbackup@ncputility ~]$ cd .ssh/\n[gitserverbackup@ncputility .ssh]$ ll\ntotal 20\n-rw-------. 1 gitserverbackup gitserverbackup  607 Apr  2 08:14 authorized_keys\n-rw-------. 1 gitserverbackup gitserverbackup 2655 Apr  2 08:14 id_rsa\n-rw-r--r--. 1 gitserverbackup gitserverbackup  607 Apr  2 08:14 id_rsa.pub\n-rw-------. 1 gitserverbackup gitserverbackup  825 Apr  2 08:14 known_hosts\n-rw-r--r--. 1 gitserverbackup gitserverbackup   91 Apr  2 08:14 known_hosts.old\n[gitserverbackup@ncputility .ssh]$ vi authorized_keys   #&lt;- this updated based on step.3 output&gt;\n[gitserverbackup@ncputility .ssh]$\n</code></pre>"},{"location":"git-helper/ncd-git-backup/#creating-remote-backup-on-ncd-git","title":"Creating remote backup on NCD git","text":"<ol> <li>Create an git server backup on a remote server, so create a secret with sftp details. </li> </ol> <pre><code>[root@ncputility ~ panhub_rc]$ oc create secret generic bastionhostpan \\\n  --namespace=paclypancdcbur01 \\\n  --from-literal=port=\"22\" \\\n  --from-literal=host=\"10.89.100.66\" \\\n  --from-literal=mode=\"sftp\" \\\n  --from-literal=username=\"gitserverbackup\" \\\n  --from-literal=path=\"/home/gitserverbackup\" \\\n  --from-literal=strictHostKeyChecking=\"no\" \\\n  --from-literal=hostKey=\"\"\nsecret/bastionhostpan created\n[root@ncputility ~ panhub_rc]$\n</code></pre> <ol> <li>make sure, it's avaiable via get command. </li> </ol> <pre><code>[root@ncputility ~ panhub_rc]$ oc  get secret -n paclypancdcbur01\nNAME                              TYPE                             DATA   AGE\nbastionhostpan                    Opaque                           7      10s\ncbur-redis                        Opaque                           1      33d\ncburm-ssh-public-key              Opaque                           1      33d\nmy-pull-secret                    kubernetes.io/dockerconfigjson   1      33d\nsh.helm.release.v1.cbur-crds.v1   helm.sh/release.v1               1      33d\nsh.helm.release.v1.ncd-cbur.v1    helm.sh/release.v1               1      33d\nsh.helm.release.v1.ncd-cbur.v2    helm.sh/release.v1               1      4h14m\nsh.helm.release.v1.ncd-cbur.v3    helm.sh/release.v1               1      14m\n[root@ncputility ~ panhub_rc]$\n</code></pre>"},{"location":"git-helper/ncd-git-backup/#update-on-the-git-cbur-side","title":"update on the git cbur side.","text":"<ol> <li>get the backup of cbur repo values file. and update the ssh:credentialName </li> </ol> <pre><code>[root@ncputility ~ panhub_rc]$ helm get values ncd-cbur -n paclypancdcbur01 &gt;  rr.yaml\n[root@ncputility ~ panhub_rc]$\n\nSSH:\n  credentialName: bastionhostpan\n\n[root@ncputility ~ panhub_rc]$ helm upgrade ncd-cbur -n paclypancdcbur01 /root/ncd/NCD_24.9_Git_Server_ORB-RC/ncd-git-server-product/helmcharts/cbur-1.18.1.tgz -f rr.yaml --debug\n</code></pre> <ol> <li>Similar to git cbur modification, need to update the git=server chart as well. </li> </ol> <pre><code>[root@ncputility ~ panhub_rc]$ helm get values ncd-git -n paclypancdgit01 &gt; rr.yaml\n[root@ncputility ~ panhub_rc]$\n\nupdate:\n\ncbur:\n  brPolicy:\n    apiVersion: cbur.csf.nokia.com/v1\n    spec:\n      autoEnableCron: false\n      autoUpdateCron: false\n      backend:\n        mode: sftp\n      cronSpec: 0 0 * * *\n      dataEncryption:\n        enable: false\n      ignoreFileChanged: false\n      maxiCopy: 3\n      weight: 5\n\n[root@ncputility ~ panhub_rc]$  helm upgrade ncd-git -n paclypancdgit01 /root/ncd/NCD_24.9_Git_Server_ORB-RC/ncd-git-server-product/helmcharts/ncd-git-server-24.9.1-7.g30f1acf.tgz -f rr.yaml --debug --timeout 20m\n</code></pre> <ol> <li>validate there is no pods are in pending or crashloop error here .</li> </ol>"},{"location":"git-helper/ncd-git-backup/#trigger-the-backup-on-remote-desination","title":"Trigger the backup on remote desination","text":"<ol> <li>using follow method to trigger  the backup </li> </ol> <pre><code>[root@ncputility ~ panhub_rc]$ helm backup -n paclypancdgit01 -a none -t ncd-git -x http://cbur.apps.panclyphub01.mnc020.mcc714/cbur --backend sftp --verbose\n\n[root@ncputility ~ panhub_rc]$\n</code></pre> <ol> <li>login to that SFTP server and check for backup files.</li> </ol> <pre><code>[gitserverbackup@ncputility 20250402134019_SFTP_paclypancdgit01_ncd-git-toolbox]$ pwd\n/home/gitserverbackup/BACKUP/sftp/paclypancdgit01/DEPLOYMENT_ncd-git-toolbox/20250402134019_SFTP_paclypancdgit01_ncd-git-toolbox\n[gitserverbackup@ncputility 20250402134019_SFTP_paclypancdgit01_ncd-git-toolbox]$ ll\ntotal 2476\n-rw-r--r--. 1 gitserverbackup gitserverbackup    7846 Apr  2 08:40 20250402134019_SFTP_paclypancdgit01_ncd-git-toolbox_Secrets.tar.gz\n-rw-r--r--. 1 gitserverbackup gitserverbackup 2523279 Apr  2 08:41 20250402134019_SFTP_paclypancdgit01_ncd-git-toolbox_volume.tar.gz\n[gitserverbackup@ncputility 20250402134019_SFTP_paclypancdgit01_ncd-git-toolbox]$\n</code></pre>"},{"location":"git-helper/ncd-git-backup/#troubleshooting","title":"Troubleshooting","text":""},{"location":"git-helper/ncd-git-backup/#certificate-error","title":"Certificate error","text":"<ol> <li>copy crt file used during installation.</li> </ol> <pre><code>[root@ncputility denmark]# cp -rp ca.crt /etc/pki/ca-trust/source/anchors/\n[root@ncputility denmark]# sudo update-ca-trust\n[root@ncputility denmark]#\n</code></pre>"},{"location":"git-helper/ncd-git-backup/#backup-job-failure","title":"backup job failure","text":"<ol> <li>if Error looks like this, then problem on the namespace spoce. </li> </ol> <pre><code>[root@ncputility ~ hn_hub_rc]$ helm backup -n hnevocdgit01 -a none -t ncd-git -x http://cbur.apps.hnevocphub01.mnc002.mcc708/cbur\n{\n  \"kind\": \"Status\",\n  \"apiVersion\": \"v1\",\n  \"metadata\": {},\n  \"status\": \"Failure\",\n  \"message\": \"Failed to get helm release ncd-git in namespace hnevocdgit01\",\n  \"reason\": \"\",\n  \"details\": {},\n  \"code\": 500\n}\n[root@ncputility ~ hn_hub_rc]$ \n</code></pre> <ol> <li>to resolve this issue, get the values file and update the namespace scope list.  include git-server namespace too. </li> </ol> <pre><code>[root@ncputility ~ hn_hub_rc]$ helm get values ncd-cbur  -n hnevocdcbur01 &gt; rr.yaml\n</code></pre> <ol> <li>retry it, it will work now</li> </ol> <pre><code>[root@ncputility ~ hn_hub_rc]$ helm backup -n hnevocdgit01 -a none -t ncd-git -x http://cbur.apps.hnevocphub01.mnc002.mcc708/cbur\n{\n  \"kind\": \"Status\",\n  \"apiVersion\": \"v1\",\n  \"metadata\": {},\n  \"status\": \"Success\",\n  \"message\": \"backupHelmRelease task = cc857932-d83d-4e2d-9493-1252595da3f2 is on!\",\n  \"reason\": \"\",\n  \"details\": {\n    \"cc857932-d83d-4e2d-9493-1252595da3f2\": {\n      \"name\": \"ncd-git\",\n      \"namespace\": \"hnevocdgit01\",\n      \"timestamp\": \"20250403005800\",\n      \"backup_data\": {},\n      \"helm_version\": 3,\n      \"request\": \"backupHelmRelease\"\n    }\n  },\n  \"code\": 202\n}\n[root@ncputility ~ hn_hub_rc]$\n</code></pre>"},{"location":"git-helper/ncd-git-backup/#reference","title":"Reference","text":"<ol> <li>NCDFM tickets</li> <li>https://jiradc2.ext.net.nokia.com/browse/NCDFM-3023</li> </ol>"},{"location":"git-helper/ncd-git-restore/","title":"Backing up Nokia Git Server","text":"<p>this procedure, works for NCD 24.9 mp1 pp1. </p>"},{"location":"git-helper/ncd-git-restore/#instructions-on-backing-up-nokia-git-server","title":"Instructions on backing up Nokia Git Server","text":""},{"location":"git-helper/readme/","title":"This document to help pushing you git code without any issues","text":"<ol> <li>Set Your Name and Email Globally (One-Time Setup)     <code>If the issue is related to your identity (name and email), configure it using:</code></li> </ol> <pre><code># git config --global user.name \"venkatapathiraj Ravichandran\"\n\n# git config --global user.email \"rajurraju400@gmail.com\"\n</code></pre> <p>This will apply to all repositories on your system.</p> <ol> <li> <p>Enable Git Credential Caching</p> <p>a. If the issue is related to authentication (username/password), you can cache your credentials using Git's credential helper. Here's how:     For a Temporary Cache (Default 15 Minutes)</p> </li> </ol> <pre><code>#git config --global credential.helper cache\n</code></pre> <pre><code>b. Git will store your credentials in memory for 15 minutes.\n\nc. You can adjust the cache timeout (in seconds) like this:\n</code></pre> <pre><code>git config --global credential.helper 'cache --timeout=3600'\n</code></pre> <ol> <li> <p>For a Persistent Cache</p> <p>a. To store your credentials securely and never be prompted again:</p> </li> </ol> <pre><code>git config --global credential.helper store\n</code></pre> <pre><code>b. Your credentials will be stored in a plain-text file (~/.git-credentials).\n\nc. Be cautious with this method if you're using a shared machine.\n</code></pre> <ol> <li>Configure Git to Skip SSL Verification (Not Recommended for Production)</li> </ol> <p>If you're in a controlled environment where you trust the source, and you want to bypass SSL verification temporarily, you can configure Git to skip SSL verification:</p> <pre><code>git config --global http.sslVerify false\n</code></pre>"},{"location":"git-helper/Deployment/git-server-deployment/","title":"Git Server Deployment Guide","text":"<p>This document outlines the steps required to deploy the Git Server and its associated services on an OpenShift-based infrastructure.</p>"},{"location":"git-helper/Deployment/git-server-deployment/#1-create-required-namespaces","title":"1. Create Required Namespaces","text":"<p>Git required to create following three namespace/project</p> <pre><code>oc new-project paclypancdgit01\noc new-project paclypancddb01\noc new-project paclypancdcbur01\n</code></pre>"},{"location":"git-helper/Deployment/git-server-deployment/#2-set-security-context-constraints-scc","title":"2. Set Security Context Constraints (SCC)","text":"<p>this step is added only for the git project.</p> <pre><code>oc adm policy add-scc-to-group anyuid system:serviceaccounts:paclypancdgit01\n</code></pre>"},{"location":"git-helper/Deployment/git-server-deployment/#3-apply-pod-security-labels","title":"3. Apply Pod Security Labels","text":"<pre><code>oc label --overwrite ns paclypancdgit01 pod-security.kubernetes.io/enforce=baseline\noc label --overwrite ns paclypancddb01 pod-security.kubernetes.io/enforce=restricted\noc label --overwrite ns paclypancdcbur01 pod-security.kubernetes.io/enforce=restricted\n</code></pre>"},{"location":"git-helper/Deployment/git-server-deployment/#4-create-image-pull-secrets","title":"4. Create Image Pull Secrets","text":"<p>here the username and password should be used on the git hub registry.</p> <pre><code># Git Namespace\noc create secret -n paclypancdgit01 docker-registry my-pull-secret   --docker-server=quay-registry.apps.panclyphub01.mnc020.mcc714   --docker-username=ncd01pan --docker-password=ncd01pan\n\n# DB Namespace\noc create secret -n paclypancddb01 docker-registry my-pull-secret   --docker-server=quay-registry.apps.panclyphub01.mnc020.mcc714   --docker-username=ncd01pan --docker-password=ncd01pan\n\n# CBUR Namespace\noc create secret -n paclypancdcbur01 docker-registry my-pull-secret   --docker-server=quay-registry.apps.panclyphub01.mnc020.mcc714   --docker-username=ncd01pan --docker-password=ncd01pan\n</code></pre>"},{"location":"git-helper/Deployment/git-server-deployment/#5-generate-tls-certificate-for-gitlab","title":"5. Generate TLS Certificate for GitLab","text":"<p>you can define the actual git url as /CN field.  example <code>gitlab.apps.panclyphub01.mnc020.mcc714</code></p> <pre><code>openssl genrsa -out ca.key 2048\n\nopenssl req -x509 -new -nodes -key ca.key -days 3650   -reqexts v3_req -subj \"/CN=gitlab.apps.panclyphub01.mnc020.mcc714\"   -extensions v3_ca -out ca.crt\n</code></pre>"},{"location":"git-helper/Deployment/git-server-deployment/#6-deploy-helm-charts","title":"6. Deploy Helm Charts","text":""},{"location":"git-helper/Deployment/git-server-deployment/#61-cbur-crds","title":"6.1 CBUR CRDs","text":"<pre><code>helm install cbur-crds -n paclypancdcbur01   /root/ncd/NCD_24.9_Git_Server_ORB-RC/ncd-git-server-product/helmcharts/cbur-crds-2.6.0.tgz   -f cbur-crd.yaml\n</code></pre>"},{"location":"git-helper/Deployment/git-server-deployment/#62-cbur-product","title":"6.2 CBUR Product","text":"<pre><code>helm install ncd-cbur -n paclypancdcbur01   /root/ncd/NCD_24.9_Git_Server_ORB-RC/ncd-git-server-product/helmcharts/cbur-1.18.1.tgz   -f cbur.yaml --debug\n</code></pre>"},{"location":"git-helper/Deployment/git-server-deployment/#63-postgresql-ha","title":"6.3 PostgreSQL HA","text":"<pre><code>helm install ncd-postgresql -n paclypancddb01   /root/ncd/NCD_24.9_Git_Server_ORB-RC/ncd-git-server-product/helmcharts/postgresql-ha-24.9.1-1009.g19e2a92.tgz   -f post.yaml --debug --timeout 20m\n</code></pre>"},{"location":"git-helper/Deployment/git-server-deployment/#64-redis","title":"6.4 Redis","text":"<pre><code>helm install ncd-redis -n paclypancddb01   /root/ncd/NCD_24.9_Git_Server_ORB-RC/ncd-git-server-product/helmcharts/ncd-redis-24.9.1-1009.g19e2a92.tgz   -f redis.yaml --debug --timeout 20m\n</code></pre>"},{"location":"git-helper/Deployment/git-server-deployment/#7-validate-pod-scc-annotations","title":"7. Validate Pod SCC Annotations","text":"<pre><code>oc get pods -n paclypancddb01   -o jsonpath='{range .items[*]}{.metadata.name}{\"\\t\"}{.metadata.annotations.openshift\\.io/scc}{\"\\n\"}{end}'\n</code></pre>"},{"location":"git-helper/Deployment/git-server-deployment/#8-deploy-git-server","title":"8. Deploy Git Server","text":"<pre><code>helm install ncd-git -n paclypancdgit01   /root/ncd/NCD_24.9_Git_Server_ORB-RC/ncd-git-server-product/helmcharts/ncd-git-server-24.9.1-7.g30f1acf.tgz   -f git.yaml --debug --timeout 20m\n</code></pre>"},{"location":"git-helper/Deployment/git-server-deployment/#notes","title":"Notes","text":"<ul> <li>No changes are required in the values files: <code>cbur-crd.yaml</code>.</li> <li>update are improtant on  <code>cbur.yaml</code>, <code>post.yaml</code>, <code>redis.yaml</code>, and <code>git.yaml</code>.</li> <li>Ensure the registry, certificates, hosts, redis and postgress service hostnames are properly configured before deployment.</li> </ul>"},{"location":"git-helper/Deployment/git-server-deployment/#download-the-sample-files","title":"Download the sample files","text":"<p>\ud83d\udcc4 Click here to download sample</p>"},{"location":"openshift/CNF-onboarding-support/Failtoconfigure_nncp/","title":"Failed to configure or Degraded","text":""},{"location":"openshift/CNF-onboarding-support/Failtoconfigure_nncp/#troubleshooting-failed-to-configure-nncp-error-in-ocp","title":"Troubleshooting failed to configure nncp Error in OCP","text":"<p>When encountering a <code>failed to configure nncp</code> error in OpenShift, it typically indicates an issue with applying the Node Network Configuration Policy (NNCP) using the Kubernetes NMState Operator. also this will make application installation failed due to missing master interface. </p>"},{"location":"openshift/CNF-onboarding-support/Failtoconfigure_nncp/#common-causes","title":"Common Causes","text":""},{"location":"openshift/CNF-onboarding-support/Failtoconfigure_nncp/#invalid-nncp-yaml","title":"Invalid NNCP YAML","text":"<ul> <li>Syntax errors</li> <li>Incorrect interface names or missing fields</li> </ul>"},{"location":"openshift/CNF-onboarding-support/Failtoconfigure_nncp/#nmstate-operator-issues","title":"NMState Operator Issues","text":"<ul> <li>Operator not installed</li> <li>Operator pods in failed states</li> </ul>"},{"location":"openshift/CNF-onboarding-support/Failtoconfigure_nncp/#node-labeling-mismatch","title":"Node Labeling Mismatch","text":"<ul> <li>NNCP uses nodeSelector that doesn\u2019t match any node</li> </ul>"},{"location":"openshift/CNF-onboarding-support/Failtoconfigure_nncp/#conflicting-policies","title":"Conflicting Policies","text":"<ul> <li>Multiple NNCPs modifying the same interface</li> </ul>"},{"location":"openshift/CNF-onboarding-support/Failtoconfigure_nncp/#insufficient-permissions","title":"Insufficient Permissions","text":"<ul> <li>NMState may lack required privileges (not applicable for CWL, since we are using blueprinted templates.)</li> </ul>"},{"location":"openshift/CNF-onboarding-support/Failtoconfigure_nncp/#node-issues","title":"Node Issues","text":"<ul> <li>Node is <code>NotReady</code> or cordoned</li> </ul>"},{"location":"openshift/CNF-onboarding-support/Failtoconfigure_nncp/#troubleshooting-steps","title":"Troubleshooting Steps","text":"<p>1) Check NNCP Status</p> <pre><code>oc get nncp\noc describe nncp &lt;nncp-name&gt;\n</code></pre>"},{"location":"openshift/CNF-onboarding-support/NCP-default-usersandrole/","title":"NCP  user's and role's as delivery","text":"<p>This document outlines the list of approved user IDs and their associated access levels as permitted by management. It serves as a reference for the NCP engineering team to ensure that user and role configurations are strictly within the defined scope. No actions should be taken beyond what is specified in this document.</p> <p>Note: All user IDs and privileges must be provisioned exactly as outlined above. Any deviations or additional access requests require explicit approval from management.</p>"},{"location":"openshift/CNF-onboarding-support/NCP-default-usersandrole/#users-and-role-management-on-hub-cluster","title":"Users and Role management on HUB cluster","text":"<p>For <code>Administrator</code> level privileges</p> <ul> <li> <p><code>ncpadmin\u2013</code> <code>cluster-admin</code> role \u2013 for the NCP team (our team).</p> </li> <li> <p><code>ncdadmin</code> \u2013 <code>cluster-admin</code> role \u2013 for the NCD team. This user will be used for NCD git installation (our team)and shared with the NCD team for ongoing lifecycle management.</p> </li> </ul> <p>Note 1: if your <code>HUB</code> cluster does not planned to install <code>ncd git server</code>, you can ignore those respective user id creation.</p>"},{"location":"openshift/CNF-onboarding-support/NCP-default-usersandrole/#users-and-role-management-on-nmcnwc-clusters","title":"Users and Role management on NMC/NWC clusters","text":"<p>For <code>Administrator</code> level privileges</p> <ul> <li> <p><code>ncpadmin</code> \u2013 <code>cluster-admin</code> role \u2013 for the NCP team (our team).</p> </li> <li> <p><code>ncdadmin</code> \u2013 <code>cluster-admin</code> role \u2013 for the NCD team. Used for NCD application installation by NCD team.</p> </li> <li> <p><code>ncomadmin</code> \u2013 <code>cluster-admin</code> role \u2013 for the NCOM team. To be used exclusively for NCOM installation.</p> </li> <li> <p><code>ncom-sa</code> - (<code>cluster-admin</code> role) Service Account - for NCOM Application is used via CaaS registration. We will create it for them.  </p> </li> </ul> <p>Note 1: if your <code>NMC/NWC</code> clusters does not planned to install <code>ncd</code> or <code>ncom</code>, you can ignore those respective user id creation.</p>"},{"location":"openshift/CNF-onboarding-support/NCP-default-usersandrole/#cluster-admin-role-implementation","title":"Cluster-Admin Role Implementation","text":"<p>No additional setup required. Clusterrole <code>cluster-admin</code> mapped to user.()</p> <pre><code>oc adm policy add-cluster-role-to-user cluster-admin ncpadmin\n</code></pre>"},{"location":"openshift/CNF-onboarding-support/NCP-default-usersandrole/#role-management-for-cnf-users-on-nmcnwc","title":"Role Management for CNF users on NMC/NWC","text":"<p>For Non-Administrator users.</p> <p>By default, all CNF application users are assigned the following four roles:</p> <ul> <li> <p><code>Admin</code> role: Full access to the entire namespace (tenant-admin role for their specific namespace/project).</p> </li> <li> <p><code>ncd-cbur-role</code>: Access at the namespace level, allowing users to create, update, delete, and schedule backup jobs.</p> </li> <li> <p><code>net-attach-def-cluster-role</code>: Access at the namespace level to create, update, delete network attachment definitions within their namespace.</p> </li> <li> <p><code>ncp-default-cnf-role</code>: Access at the cluster level to list, view, watch, and get information for nodes, SCC, NNCP, Metallb IP pool, static routes, backward routes, egress routes, CRDs, profiles etc.</p> </li> <li> <p><code>redisenterpriseclusters-role</code> :  Access at namespace level to create, update,delete,edit,patch,lig  rec based objects. </p> </li> </ul> <p>Note 1: No need to add above four role to any of these users (ncpadmin, ncdadmin,ncomadmin and ncom-sa)</p> <p>Note 2: Clarity - CNF id act as read-only for cluster level resources and read-write for tenant level resources.</p>"},{"location":"openshift/CNF-onboarding-support/NCP-default-usersandrole/#admin-role-implementation","title":"Admin Role Implementation","text":"<p>No additional setup required. Role <code>admin</code> mapped to user and namespace:</p> <pre><code>oc policy add-role-to-user admin npcvzr1np1 -n npcvzr1np1\n</code></pre>"},{"location":"openshift/CNF-onboarding-support/NCP-default-usersandrole/#network-attachment-role-implementation","title":"Network Attachment Role Implementation","text":"<p>1) Create <code>ClusterRole</code> for network attachment:</p> <pre><code>apiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRole\nmetadata:\n  name: net-attach-def-cluster-role\nrules:\n  - apiGroups: [\"k8s.cni.cncf.io\"]\n    resources: [\"network-attachment-definitions\"]\n    verbs: [\"create\", \"get\", \"list\", \"watch\", \"update\", \"patch\", \"delete\"]\n</code></pre> <p>2) Bind role to user in their namespace:</p> <pre><code>oc create rolebinding net-attach-def-rolebinding \\\n  --clusterrole=net-attach-def-cluster-role \\\n  --user=paclypamrf01 \\\n  --namespace=paclypamrf01\n</code></pre> <p>3) Validate access:</p> <pre><code>oc auth can-i create network-attachment-definitions.k8s.cni.cncf.io --as=paclypamrf01 -n paclypamrf01\n</code></pre>"},{"location":"openshift/CNF-onboarding-support/NCP-default-usersandrole/#cbur-role-implementation","title":"CBUR Role Implementation","text":"<p>1) Create <code>ClusterRole</code>:</p> <pre><code>apiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRole\nmetadata:\n  name: ncd-cbur-role\nrules:\n  - apiGroups: [\"cbur.bcmt.local\"]\n    resources: [\"brpolices\"]\n    verbs: [\"create\", \"get\", \"list\", \"watch\", \"update\", \"patch\"]\n  - apiGroups: [\"cbur.csf.nokia.com\"]\n    resources: [\"brhooks\", \"brpolices\"]\n    verbs: [\"create\", \"get\", \"list\", \"watch\", \"update\", \"patch\"]\n</code></pre> <p>2) Apply role and bind:</p> <pre><code>oc apply -f cburrole.yaml\noc create rolebinding ncd-cbur-role-binding \\\n  --clusterrole=ncd-cbur-role \\\n  --user=nokia \\\n  --namespace=test01\n</code></pre> <p>3) Validate access:</p> <pre><code>oc auth can-i create brpolices --as=nokia -n test01\n</code></pre>"},{"location":"openshift/CNF-onboarding-support/NCP-default-usersandrole/#redisenterpriseclusters-role-implementation","title":"RedisEnterpriseClusters Role Implementation","text":"<p>1) Create <code>ClusterRole</code>:</p> <pre><code>apiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRole\nmetadata:\n  name: redisenterpriseclusters-role\nrules:\n  - apiGroups: [\"app.redislabs.com\"]\n    resources: [\"redisenterpriseclusters\"]\n    verbs: [\"get\", \"list\", \"create\", \"delete\", \"update\", \"patch\"]\n</code></pre> <p>2) Apply role and bind:</p> <pre><code>oc apply -f redisenterpriseclusters-role.yaml\noc create redisenterpriseclusters-role-binding \\\n  --clusterrole=redisenterpriseclusters-role \\\n  --user=nokia \\\n  --namespace=test01\n</code></pre> <p>3) Validate access:</p> <pre><code>oc auth can-i create redisenterpriseclusters --as=nokia -n test01\n</code></pre>"},{"location":"openshift/CNF-onboarding-support/NCP-default-usersandrole/#read-only-cluster-infra-role-implementation","title":"Read-Only Cluster Infra Role Implementation","text":"<p>1) Create <code>ClusterRole</code> for infra read access:</p> <pre><code>apiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRole\nmetadata:\n  name: ncp-default-cnf-role\nrules:\n  - apiGroups: [\"security.openshift.io\"]\n    resources: [\"securitycontextconstraints\"]\n    verbs: [\"get\", \"list\"]\n  - apiGroups: [\"\"]\n    resources: [\"nodes\"]\n    verbs: [\"get\", \"list\"]\n  - apiGroups: [\"apiextensions.k8s.io\"]\n    resources: [\"customresourcedefinitions\"]\n    verbs: [\"get\", \"list\"]\n  - apiGroups: [\"compliance.openshift.io\"]\n    resources: [\"profiles\"]\n    verbs: [\"get\", \"list\"]\n  - apiGroups: [\"nmstate.io\"]\n    resources: [\"nodenetworkconfigurationpolicies\"]\n    verbs: [\"get\", \"list\", \"watch\"]\n  - apiGroups: [\"metallb.io\"]\n    resources: [\"ipaddresspools\"]\n    verbs: [\"get\", \"list\"]\n  - apiGroups: [\"k8s.ovn.org\"]\n    resources: [\"egressips\"]\n    verbs: [\"get\", \"list\", \"watch\"]\n  - apiGroups: [\"k8s.cni.cncf.io\"]\n    resources: [\"network-attachment-definitions\"]\n    verbs: [\"get\", \"list\", \"watch\"]\n  - apiGroups: [\"topology.node.k8s.io\"]\n    resources: [\"noderesourcetopologies\"]\n    verbs: [\"get\", \"list\", \"watch\"]\n  - apiGroups: [\"sriovnetwork.openshift.io\"]\n    resources: [\"sriovnetworknodepolicies\"]\n    verbs: [\"get\", \"list\", \"watch\"]\n  - apiGroups: [\"\"]\n    resources: [\"persistentvolumes\"]\n    verbs: [\"get\", \"list\", \"watch\"]\n</code></pre> <p>2) Bind the role:</p> <pre><code>oc create clusterrolebinding ncp-default-cnf-role-ppaaa01-binding \\\n  --clusterrole=ncp-default-cnf-role \\\n  --user=ppaaa01\n</code></pre> <p>3) Validate access:</p> <pre><code>oc auth can-i get nodes --as=ppaaa01\noc auth can-i get scc --as=ppaaa01\noc auth can-i get crds --as=ppaaa01\noc login -u ppaaa01 -p redhat123\noc get nodes\n</code></pre>"},{"location":"openshift/CNF-onboarding-support/image-tls-issue/","title":"Container Image download issue due to TLS problem.","text":""},{"location":"openshift/CNF-onboarding-support/image-tls-issue/#creating-tls-certificate-on-the-ocp-cluster-as-additional-tls-newly","title":"Creating tls certificate on the OCP cluster as additional tls <code>newly</code>","text":""},{"location":"openshift/CNF-onboarding-support/image-tls-issue/#this-step-will-resolve-issue-on-linux-os-levelbastion-host-level","title":"This step will resolve issue on Linux OS level(bastion host level)","text":"<p>In order to be able to push images (for example) from the infra node to this Quay, the rootCA certificate shall be put to the trusted list of the client. The Quay is exposed using the OCP\u2019s default ingress controller, route, so its rootCA shall be fetched (if it was not swapped already).</p> <p>1) from bastion host, login to respective cluster and find the respective CWL cluster quay url. </p> <pre><code>[root@dom14npv101-infra-manager ~ vlabrc]# oc get route -A |grep -i quay\nquay-registry              quay-registry-quay                          quay-registry.apps.ncpvnpvlab1.pnwlab.nsn-rdnet.net                                                             quay-registry-quay-app                             http         edge/Redirect          None\nquay-registry              quay-registry-quay-builder                  quay-registry-quay-builder-quay-registry.apps.ncpvnpvlab1.pnwlab.nsn-rdnet.net                                  quay-registry-quay-app                             grpc         edge/Redirect          None\n[root@dom14npv101-infra-manager ~ vlabrc]# \n</code></pre> <p>2) first locally resolve it, so make dir and download the cert (this is resolve on that particular linux server.)</p> <pre><code>[root@dom14npv101-infra-manager ~ vlabrc]# sudo mkdir -p /etc/containers/certs.d/quay-registry.apps.ncpvnpvlab1.pnwlab.nsn-rdnet.net\n[root@dom14npv101-infra-manager ~ vlabrc]#\n</code></pre> <p>3) create the ingress certificate to local host </p> <pre><code>[root@dom14npv101-infra-manager ~ vlabrc]# oc get secrets -n openshift-ingress-operator router-ca \\\n-o jsonpath=\"{.data['tls\\.crt']}\" | base64 -d | \\\nsudo tee -a \\\n/etc/containers/certs.d/quay-registry.apps.ncpvnpvlab1.pnwlab.nsn-rdnet.net/ca.crt\n-----BEGIN CERTIFICATE-----\nMIIDDDCCAfSgAwIBAgIBATANBgkqhkiG9w0BAQsFADAmMSQwIgYDVQQDDBtpbmdy\nZXNzLW9wZXJhdG9yQDE3NDE2MjI1OTkwHhcNMjUwMzEwMTYwMzE4WhcNMjcwMzEw\nMTYwMzE5WjAmMSQwIgYDVQQDDBtpbmdyZXNzLW9wZXJhdG9yQDE3NDE2MjI1OTkw\nggEiMA0GCSqGSIb3DQEBAQUAA4IBDwAwggEKAoIBAQCtDCDqA6Cx+MvulZtYNleK\nT3e4OVlQrR8zytgIQIvlLpeGoqsoNWEe/ZukJ35fco6LA42RcIiI+F5aI6zZ4+F8\n81ZEpsSRBx8VUMsZCdkb5mA8tl5vJjV+GC1tRlohk6KWqYoR5VJVLbggFer95efv\nxyny/BCYXmU2CSHSmRQRnwAI6cX0K7QgEB0kMHaFjEta16UnwzKdhNbaj5rn0aTm\nhLSGYLvPMx9RVZswjqOqrju0Aovfv9ZzzE++e6+KH9/jZr2HepK62ZZdGznbmnzu\nAl0D+ILaVj8DiFpcwUIaSaRxUVlphAUmm530GLbKrBdQGSsWcJTo4ixf8R2wYo69\nAgMBAAGjRTBDMA4GA1UdDwEB/wQEAwICpDASBgNVHRMBAf8ECDAGAQH/AgEAMB0G\nA1UdDgQWBBQMuq1odXMd4OlIU4vG8Kfu/aLltjANBgkqhkiG9w0BAQsFAAOCAQEA\nBVerN+fxay+kk9uei+bQIpryakFstJ5ApuB1wDKgLY3LucwbzXhaE48i9TEOoNlB\n32ugNpShYQoOyVMMAvQNQG69HNu0KDJHYGDMAs4seGIsMwqityS6Zgv8T3xo176g\nmR0y74yiK1ImtnUAaAPt7NNFflhpZafzhY24k4L3AVNEjMKI9B2SgUJAscmXkNIZ\nDri+EILpba6MzmeLdE3sVTaOIRberr6yTKbZQskaci+twaO7r83hD3E3xwGJB823\nZu+B2i/txKbBrFeKUpppfrg7zCsyqM1UwFtenuj1yj3qECJVwe1Lr8SctrzpJc+J\nryyB1JeEPQWwewI1j7QXqg==\n-----END CERTIFICATE-----\n[root@dom14npv101-infra-manager ~ vlabrc]#\n</code></pre>"},{"location":"openshift/CNF-onboarding-support/image-tls-issue/#on-the-ocp-cluster-level-changes-tls-creation","title":"On the OCP cluster level changes, TLS Creation","text":"<p>1) Similarly if images would be pulled on the HUB cluster, its ingress\u2019 rootCA shall be put into the image.config.openshift.io/cluster CR, as the registry is using self-signed certificate by default and treated as an insecure registry, more precisely the OCP\u2019s ingress (if it is not swapped yet). In order to overcome issues the following commands:</p> <pre><code> [root@dom14npv101-infra-manager ~ vlabrc]# oc get secrets -n openshift-ingress-operator router-ca \\\n-o jsonpath=\"{.data['tls\\.crt']}\" | base64 -d &gt; ingress_ca.crt\n[root@dom14npv101-infra-manager ~ vlabrc]# \n</code></pre> <p>2) create an cm to enforce the certificate here.</p> <pre><code>[root@dom14npv101-infra-manager ~ vlabrc]# oc create configmap registry-cas -n openshift-config \\\n--from-file=quay-registry.apps.ncpvnpvlab1.pnwlab.nsn-rdnet.net=ingress_ca.crt\nconfigmap/registry-cas created\n[root@dom14npv101-infra-manager ~ vlabrc]# \n</code></pre> <p>3) now create an image patch to cluster iamge config. </p> <pre><code>[root@dom14npv101-infra-manager ~ vlabrc]# oc patch image.config.openshift.io/cluster --patch '{\"spec\":\n{\"additionalTrustedCA\":{\"name\":\"registry-cas\"}}}' \\\n--type=merge\nimage.config.openshift.io/cluster patched\n[root@dom14npv101-infra-manager ~ vlabrc]#\n</code></pre>"},{"location":"openshift/CNF-onboarding-support/image-tls-issue/#creating-tls-certificate-on-the-ocp-cluster-as-additional-tls-add","title":"Creating tls certificate on the OCP cluster as additional tls <code>ADD</code>","text":"<p>1) oc apply is the preferred way to update an existing resource, including ConfigMaps. This command will update the ConfigMap with the new data while keeping existing data intact. If you don\u2019t have a YAML file, you can first export the current ConfigMap to a file, edit it, and then apply the changes:</p> <pre><code>oc get configmap registry-cas -n openshift-config -o yaml &gt; registry-cas.yaml\n</code></pre> <p>2) now try to prepare file for new certificate with correct file intension using <code>--dry-run</code></p> <pre><code>oc create configmap registry-cas -n openshift-config \\\n--from-file=harbor.ncdvnpv.ncpvnpvmgt.pnwlab.nsn-rdnet.net=ingress_ca.crt \\\n--dry-run=client -o yaml \n</code></pre> <p>3) Now try to copy the harbor and it's certificate line's and then add it as second list of the certificate on the first yaml file and later apply it. </p> <pre><code>vi registry-cas.yaml\n\noc apply -f registry-cas.yaml\n</code></pre>"},{"location":"openshift/CNF-onboarding-support/istio-for-cnf/","title":"Configure the Redhat ISTIO for CNF","text":""},{"location":"openshift/CNF-onboarding-support/istio-for-cnf/#creating-meshroll-and-allow-cnf-namespace-access-istio","title":"Creating meshroll and allow cnf namespace access istio.","text":"<p>1) </p> <p>[root@longb92bst01 ~ cwlcluster]# cat servicemeshmemberroll.yaml apiVersion: maistra.io/v1 kind: ServiceMeshMemberRoll metadata:   name: default   namespace: istio-system spec:   members:     - longb92ncc01 # your application ns name. [root@longb92bst01 ~ cwlcluster]# [root@longb92bst01 ~ cwlcluster]# [root@longb92bst01 ~ cwlcluster]# [root@longb92bst01 ~ cwlcluster]# oc apply -f servicemeshmemberroll.yaml servicemeshmemberroll.maistra.io/default created [root@longb92bst01 ~ cwlcluster]# [root@longb92bst01 ~ cwlcluster]#</p> <p>istio-injection=disabled -&gt; adding this label.</p>"},{"location":"openshift/CNF-onboarding-support/kubeconfig/","title":"Kubeconfig overlapping","text":""},{"location":"openshift/CNF-onboarding-support/kubeconfig/#method-to-avoid-ocp-user-profile-overlapping-issue","title":"Method to avoid OCP user profile overlapping issue.","text":"<p>this can resolved by create an rc file each cluster/user access. and source before running oc command can prevant. secret will be kubeconfig file be exported to uniq file each new rc files. </p>"},{"location":"openshift/CNF-onboarding-support/kubeconfig/#problem-describe","title":"Problem describe","text":"<ul> <li>when two user or two different shell try to login with two different users or two different clusters, most recent login will be overlapped with old one. example:  both screen will be having same login to user/cluster.</li> </ul>"},{"location":"openshift/CNF-onboarding-support/kubeconfig/#create-an-rc-for-hub-cluster","title":"Create an RC for hub cluster.","text":"<p>1) login to linux machine and create a rc file for hub cluster </p> <ul> <li><code>KUBECONFIG</code> variable should unique between the clusters. so that token will be saved on that particular sheel profile.</li> <li><code>PS1</code> this variable should be unique name and can be used as shell reference.</li> </ul> <pre><code>[root@ncputility ~ pancwl_rc]$ cat /root/panhubrc\nexport KUBECONFIG=~/.kube/hubconfig\n\noc login -u kubeadmin -p NdnkM-SWpwe-SE4Ba-rNSbR https://api.panclyphub01.mnc020.mcc714:6443\nPS1=\"[\\u@\\h ~ panhub_rc]$ \"\n[root@ncputility ~ pancwl_rc]$\n</code></pre> <p>2) To login to this cluster try </p> <pre><code>[root@ncputility ~ pancwl_rc]$ source /root/panhubrc\nWARNING: Using insecure TLS client config. Setting this option is not supported!\n\nLogin successful.\n\nYou have access to 103 projects, the list has been suppressed. You can list all projects with 'oc projects'\n\nUsing project \"panclypcwl01\".\n[root@ncputility ~ panhub_rc]$\n</code></pre>"},{"location":"openshift/CNF-onboarding-support/kubeconfig/#create-an-rc-for-cwl-cluster","title":"Create an RC for CWL cluster.","text":"<p>1) login to linux machine and create a rc file for CWL cluster </p> <ul> <li><code>KUBECONFIG</code> variable should unique between the clusters. so that token will be saved on that particular shell profile.</li> <li><code>PS1</code> this variable should be unique name and can be used as shell reference.</li> </ul> <pre><code>[root@ncputility ~ pancwl_rc]$ cat /root/pancwlrc\nexport KUBECONFIG=~/.kube/cwlconfig\n\noc login -u kubeadmin -p Y59eh-d8Poa-sSdyp-9zBvL https://api.panclypcwl01.mnc020.mcc714:6443\nPS1=\"[\\u@\\h ~ pancwl_rc]$ \"\n\n[root@ncputility ~ pancwl_rc]$\n</code></pre> <p>2) to login to this cluster try </p> <pre><code>[root@ncputility ~ pancwl_rc]$ source /root/pancwlrc\nWARNING: Using insecure TLS client config. Setting this option is not supported!\n\nLogin successful.\n\nYou have access to 117 projects, the list has been suppressed. You can list all projects with 'oc projects'\n\nUsing project \"default\".\n[root@ncputility ~ pancwl_rc]$\n</code></pre>"},{"location":"openshift/CNF-onboarding-support/oclogin-tls-issue/","title":"Oc login ssl error on OCP","text":"<p>To login to NWC cluster, oauth-serving-cert (second certificate) to be copied to /etc/pki/ca-trust/source/anchors/ and then run sudo update-ca-trust extract command. It should be done for each NWC cluster.</p> <p>1) To get the certificate, execute below command on NWC cluster.</p> <pre><code>[root@ncputility ~ panhub_rc]$ oc get configmaps -n openshift-config-managed oauth-serving-cert -o yaml\napiVersion: v1\ndata:\n  ca-bundle.crt: |2\n\n    -----BEGIN CERTIFICATE-----\n    MIIDeTCCAmGgAwIBAgIIItt373u4MZMwDQYJKoZIhvcNAQELBQAwJjEkMCIGA1UE\n    AwwbaW5ncmVzcy1vcGVyYXRvckAxNzQwMDc0NTQ0MB4XDTI1MDIyMDE4MDIyM1oX\n    DTI3MDIyMDE4MDIyNFowLDEqMCgGA1UEAwwhKi5hcHBzLnBhbmNseXBodWIwMS5t\n    bmMwMjAubWNjNzE0MIIBIjANBgkqhkiG9w0BAQEFAAOCAQ8AMIIBCgKCAQEAoNne\n    91mbAMQEjKAyOBWp0wGYpELbNKgYtgUGqL0zteOA3WI+opnJnpTPztlqr4Xqddyw\n    EAuPS7gjUqplbwJROO0lQ+sDSVqfOCC7wvrkd6pI/0jxWPK9WgnSZt1lmiJ9L0Rs\n    s7H5iVdUq8hvIfI9ZUzvr2BUGi9StdABRFoxk1R+BF6yRRiQnxhyqhYjPOuzV4GM\n    blfDAvo3yqFoMOHo0DTZQXcRLQnbt2a3ApPHcsLgyBjTmOMlPilRSHtVFivQP2Pd\n    VRhZGSsAANk7aJyCvHZ+oMo0DLUqmBgikHpgm9TAv6M+oX0kbfdqfMci8sEF7Vqj\n    9fK5l19t+zZaXTnH3QIDAQABo4GkMIGhMA4GA1UdDwEB/wQEAwIFoDATBgNVHSUE\n    DDAKBggrBgEFBQcDATAMBgNVHRMBAf8EAjAAMB0GA1UdDgQWBBTsyQRlbeyo2H/V\n    f887YPhF8jVixDAfBgNVHSMEGDAWgBTZS/SzkBfiHQ5/Gy+b4g1XJOHkojAsBgNV\n    HREEJTAjgiEqLmFwcHMucGFuY2x5cGh1YjAxLm1uYzAyMC5tY2M3MTQwDQYJKoZI\n    hvcNAQELBQADggEBAE6kBjPoA3RJI09pYfUzQlEyKQrnudNTu+O61ZspCPvafp4s\n    4py8hyS/pzkp7611KfmJnzXjiBjw6qzcE5lye4coO5vwplYDbZUTCn9bz30+2g1O\n    wpA1ZOLLTHet11+i0FG0m4AJq4OXEjHuA1K2+AyfzG0TsogT88WstndoNPtGrYWJ\n    pj1kQYbVqwBtCU/jhKbXycEQ+UdeICRuNp5FbBcJ/ZrxJRmJa/zUkT6tHWMvzlsI\n    IPrTpL7BEkoRtWPYhcW4gL70XgkmahuX2bssG7C2IJxN2DNTvmFsMSWHzQNc7AaD\n    ND+v4E+mn2zKhzdQyOB4Mx6H4LH5cEHZsLfVal8=\n    -----END CERTIFICATE-----\n\n    -----BEGIN CERTIFICATE-----\n    MIIDDDCCAfSgAwIBAgIBATANBgkqhkiG9w0BAQsFADAmMSQwIgYDVQQDDBtpbmdy\n    ZXNzLW9wZXJhdG9yQDE3NDAwNzQ1NDQwHhcNMjUwMjIwMTgwMjIzWhcNMjcwMjIw\n    MTgwMjI0WjAmMSQwIgYDVQQDDBtpbmdyZXNzLW9wZXJhdG9yQDE3NDAwNzQ1NDQw\n    ggEiMA0GCSqGSIb3DQEBAQUAA4IBDwAwggEKAoIBAQDjXWFSVPshwihivZhaTrB7\n    0boUOw2j3Ut/J6eSm+JA+xVl05L4XD8+C8VSst+f32Pe42Lso2hrovY1dT7IBX3g\n    6S9Nnd2iRc9vC/qHcQkGQ6krIYSQ48aCH7UuahCpTqxEp+MwmhCTQngN2maTJBe3\n    0E6K2IL4zXSi0Iuj08BOnH/w4pJxeWhyngXDf2SeA88EmU3juHLshHrAND84uou2\n    /sOStIAhFwU+o887dSgx7iERy+6nczSDB9Qvq11cUSS4RPC82bNnxcc+BrynDwZ/\n    eggs0OEaj/1cHu/svKZHX9gUKrqz80wF8YGZLLgI2oPAf2VJorvtkJAErzgttroF\n    AgMBAAGjRTBDMA4GA1UdDwEB/wQEAwICpDASBgNVHRMBAf8ECDAGAQH/AgEAMB0G\n    A1UdDgQWBBTZS/SzkBfiHQ5/Gy+b4g1XJOHkojANBgkqhkiG9w0BAQsFAAOCAQEA\n    UwPhAbzTWZIlBsMHAL+8jvxM8qxc6HDhayAD4gbCE65vHYgSizost02vRfpOPQq1\n    D6HM8JjifS3KHd6E6chdTbrHI0W8pMJJPon5akCJf/uGeGDl+2wKfmVC6UoV7hC3\n    pcUzm3JKwsNJbjS5rxL8f5a8bNdIFfLQKuyRpnVX2CsNHvh+WJzynQ+PUJ6zCa7y\n    x5AJxca2PTnBKRoVTAyumT1suluI9f4GRYnxTE/qIKRZRs+uT3kIl/N9VX+GbjGb\n    pPszJ+p6N6Arl1BqJP1DdLin2IFGZL39pTyifm5GP+Vou2aHPuHZDVoCdxsFKup+\n    gUY2KeKz0UManwubPQNnKA==\n    -----END CERTIFICATE-----\nkind: ConfigMap\nmetadata:\n  annotations:\n    openshift.io/owning-component: apiserver-auth\n  creationTimestamp: \"2025-02-20T18:02:31Z\"\n  name: oauth-serving-cert\n  namespace: openshift-config-managed\n  resourceVersion: \"73869405\"\n  uid: a08c0137-033d-41d0-9c79-8f345662140c\n[root@ncputility ~ panhub_rc]$\n</code></pre> <p>2) Then copy second certificate and put it in a file. Move this file to <code>/etc/pki/ca-trust/source/anchors/</code>Post that, run <code>update-ca-trust extrace</code>command.</p> <p>3) Create separate linux user and do oc login kubeadmin, password and api url (find from the kubeconfig) file.</p>"},{"location":"openshift/CNF-onboarding-support/proxy-cache-pod/","title":"Proxy-cache quay based pod creation","text":""},{"location":"openshift/CNF-onboarding-support/proxy-cache-pod/#proxy-cache-tls-issue","title":"proxy-cache TLS issue.","text":"<p>0) Login in to CWL cluster and make sure mcp and nodes are looks good at this point, if any issue, need to be resolved first. </p> <pre><code>[root@ncputility ~ pancwl_rc]$ oc get nodes\nNAME                                     STATUS   ROLES                              AGE   VERSION\nappworker0.panclypcwl01.mnc020.mcc714    Ready    appworker,appworker-mcp-a,worker   38d   v1.29.6+aba1e8d\nappworker1.panclypcwl01.mnc020.mcc714    Ready    appworker,appworker-mcp-a,worker   38d   v1.29.6+aba1e8d\nappworker10.panclypcwl01.mnc020.mcc714   Ready    appworker,appworker-mcp-b,worker   37d   v1.29.6+aba1e8d\nappworker11.panclypcwl01.mnc020.mcc714   Ready    appworker,appworker-mcp-b,worker   37d   v1.29.6+aba1e8d\nappworker12.panclypcwl01.mnc020.mcc714   Ready    appworker,appworker-mcp-b,worker   37d   v1.29.6+aba1e8d\nappworker13.panclypcwl01.mnc020.mcc714   Ready    appworker,appworker-mcp-b,worker   37d   v1.29.6+aba1e8d\nappworker14.panclypcwl01.mnc020.mcc714   Ready    appworker,appworker-mcp-b,worker   37d   v1.29.6+aba1e8d\nappworker15.panclypcwl01.mnc020.mcc714   Ready    appworker,appworker-mcp-b,worker   37d   v1.29.6+aba1e8d\nappworker16.panclypcwl01.mnc020.mcc714   Ready    appworker,appworker-mcp-b,worker   37d   v1.29.6+aba1e8d\nappworker17.panclypcwl01.mnc020.mcc714   Ready    appworker,appworker-mcp-b,worker   37d   v1.29.6+aba1e8d\nappworker19.panclypcwl01.mnc020.mcc714   Ready    appworker,appworker-mcp-c,worker   37d   v1.29.6+aba1e8d\nappworker2.panclypcwl01.mnc020.mcc714    Ready    appworker,appworker-mcp-a,worker   30d   v1.29.6+aba1e8d\nappworker20.panclypcwl01.mnc020.mcc714   Ready    appworker,appworker-mcp-c,worker   37d   v1.29.6+aba1e8d\nappworker21.panclypcwl01.mnc020.mcc714   Ready    appworker,appworker-mcp-c,worker   37d   v1.29.6+aba1e8d\nappworker22.panclypcwl01.mnc020.mcc714   Ready    appworker,appworker-mcp-c,worker   37d   v1.29.6+aba1e8d\nappworker23.panclypcwl01.mnc020.mcc714   Ready    appworker,appworker-mcp-c,worker   37d   v1.29.6+aba1e8d\nappworker24.panclypcwl01.mnc020.mcc714   Ready    appworker,appworker-mcp-c,worker   37d   v1.29.6+aba1e8d\nappworker25.panclypcwl01.mnc020.mcc714   Ready    appworker,appworker-mcp-c,worker   37d   v1.29.6+aba1e8d\nappworker26.panclypcwl01.mnc020.mcc714   Ready    appworker,appworker-mcp-c,worker   37d   v1.29.6+aba1e8d\nappworker27.panclypcwl01.mnc020.mcc714   Ready    appworker,appworker-mcp-d,worker   37d   v1.29.6+aba1e8d\nappworker28.panclypcwl01.mnc020.mcc714   Ready    appworker,appworker-mcp-d,worker   37d   v1.29.6+aba1e8d\nappworker29.panclypcwl01.mnc020.mcc714   Ready    appworker,appworker-mcp-d,worker   37d   v1.29.6+aba1e8d\nappworker3.panclypcwl01.mnc020.mcc714    Ready    appworker,appworker-mcp-a,worker   38d   v1.29.6+aba1e8d\nappworker30.panclypcwl01.mnc020.mcc714   Ready    appworker,appworker-mcp-d,worker   37d   v1.29.6+aba1e8d\nappworker31.panclypcwl01.mnc020.mcc714   Ready    appworker,appworker-mcp-d,worker   37d   v1.29.6+aba1e8d\nappworker32.panclypcwl01.mnc020.mcc714   Ready    appworker,appworker-mcp-d,worker   37d   v1.29.6+aba1e8d\nappworker33.panclypcwl01.mnc020.mcc714   Ready    appworker,appworker-mcp-d,worker   37d   v1.29.6+aba1e8d\nappworker34.panclypcwl01.mnc020.mcc714   Ready    appworker,appworker-mcp-d,worker   37d   v1.29.6+aba1e8d\nappworker4.panclypcwl01.mnc020.mcc714    Ready    appworker,appworker-mcp-a,worker   38d   v1.29.6+aba1e8d\nappworker5.panclypcwl01.mnc020.mcc714    Ready    appworker,appworker-mcp-a,worker   38d   v1.29.6+aba1e8d\nappworker6.panclypcwl01.mnc020.mcc714    Ready    appworker,appworker-mcp-a,worker   38d   v1.29.6+aba1e8d\nappworker7.panclypcwl01.mnc020.mcc714    Ready    appworker,appworker-mcp-a,worker   37d   v1.29.6+aba1e8d\nappworker9.panclypcwl01.mnc020.mcc714    Ready    appworker,appworker-mcp-b,worker   37d   v1.29.6+aba1e8d\ngateway1.panclypcwl01.mnc020.mcc714      Ready    gateway,gateway-mcp-a,worker       38d   v1.29.6+aba1e8d\ngateway2.panclypcwl01.mnc020.mcc714      Ready    gateway,gateway-mcp-a,worker       38d   v1.29.6+aba1e8d\ngateway3.panclypcwl01.mnc020.mcc714      Ready    gateway,gateway-mcp-b,worker       38d   v1.29.6+aba1e8d\ngateway4.panclypcwl01.mnc020.mcc714      Ready    gateway,gateway-mcp-b,worker       38d   v1.29.6+aba1e8d\nmaster0.panclypcwl01.mnc020.mcc714       Ready    control-plane,master,monitor       38d   v1.29.6+aba1e8d\nmaster1.panclypcwl01.mnc020.mcc714       Ready    control-plane,master,monitor       38d   v1.29.6+aba1e8d\nmaster2.panclypcwl01.mnc020.mcc714       Ready    control-plane,master,monitor       38d   v1.29.6+aba1e8d\nstorage0.panclypcwl01.mnc020.mcc714      Ready    storage,worker                     38d   v1.29.6+aba1e8d\nstorage1.panclypcwl01.mnc020.mcc714      Ready    storage,worker                     38d   v1.29.6+aba1e8d\nstorage2.panclypcwl01.mnc020.mcc714      Ready    storage,worker                     38d   v1.29.6+aba1e8d\nstorage3.panclypcwl01.mnc020.mcc714      Ready    storage,worker                     38d   v1.29.6+aba1e8d\nstorage4.panclypcwl01.mnc020.mcc714      Ready    storage,worker                     38d   v1.29.6+aba1e8d\n[root@ncputility ~ pancwl_rc]$ oc get mcp\nNAME              CONFIG                                                      UPDATED   UPDATING   DEGRADED   MACHINECOUNT   READYMACHINECOUNT   UPDATEDMACHINECOUNT   DEGRADEDMACHINECOUNT   AGE\nappworker-mcp-a   rendered-appworker-mcp-a-0e9dd6df593dcd5016bbe7d601119bf4   True      False      False      8              8                   8                     0                      37d\nappworker-mcp-b   rendered-appworker-mcp-b-0e9dd6df593dcd5016bbe7d601119bf4   True      False      False      9              9                   9                     0                      37d\nappworker-mcp-c   rendered-appworker-mcp-c-5afb864664d3b10530b54b3153a1a61e   True      False      False      8              8                   8                     0                      29h\nappworker-mcp-d   rendered-appworker-mcp-d-5afb864664d3b10530b54b3153a1a61e   True      False      False      8              8                   8                     0                      29h\ngateway-mcp-a     rendered-gateway-mcp-a-c81254a16575de9053ae543c4f1ba3fb     True      False      False      2              2                   2                     0                      37d\ngateway-mcp-b     rendered-gateway-mcp-b-3be41ecbbe09004c35ca04a4309cabf0     True      False      False      2              2                   2                     0                      29h\nmaster            rendered-master-114f60e6be691323222ea11e72de0bcf            True      False      False      3              3                   3                     0                      38d\nstorage           rendered-storage-dc2d8a34080bce1400e11bb1fb098693           True      False      False      5              5                   5                     0                      37d\nworker            rendered-worker-68cb1df39185f7ad80fda7915e4c5a42            True      False      False      0              0                   0                     0                      38d\n[root@ncputility ~ pancwl_rc]$\n</code></pre> <p>1) login to hub cluster and run this command </p> <pre><code>[root@ncputility ~ pancwl_rc]$ source /root/panhubrc\nWARNING: Using insecure TLS client config. Setting this option is not supported!\n\nLogin successful.\n\nYou have access to 105 projects, the list has been suppressed. You can list all projects with 'oc projects'\n\nUsing project \"default\".\n[root@ncputility ~ panhub_rc]$ oc get secret -n openshift-ingress-operator router-ca -o \"jsonpath={.data['tls\\.crt']}\" | base64 -d\n-----BEGIN CERTIFICATE-----\nMIIDDDCCAfSgAwIBAgIBATANBgkqhkiG9w0BAQsFADAmMSQwIgYDVQQDDBtpbmdy\nZXNzLW9wZXJhdG9yQDE3NDAwNzQ1NDQwHhcNMjUwMjIwMTgwMjIzWhcNMjcwMjIw\nMTgwMjI0WjAmMSQwIgYDVQQDDBtpbmdyZXNzLW9wZXJhdG9yQDE3NDAwNzQ1NDQw\nggEiMA0GCSqGSIb3DQEBAQUAA4IBDwAwggEKAoIBAQDjXWFSVPshwihivZhaTrB7\n0boUOw2j3Ut/J6eSm+JA+xVl05L4XD8+C8VSst+f32Pe42Lso2hrovY1dT7IBX3g\n6S9Nnd2iRc9vC/qHcQkGQ6krIYSQ48aCH7UuahCpTqxEp+MwmhCTQngN2maTJBe3\n0E6K2IL4zXSi0Iuj08BOnH/w4pJxeWhyngXDf2SeA88EmU3juHLshHrAND84uou2\n/sOStIAhFwU+o887dSgx7iERy+6nczSDB9Qvq11cUSS4RPC82bNnxcc+BrynDwZ/\neggs0OEaj/1cHu/svKZHX9gUKrqz80wF8YGZLLgI2oPAf2VJorvtkJAErzgttroF\nAgMBAAGjRTBDMA4GA1UdDwEB/wQEAwICpDASBgNVHRMBAf8ECDAGAQH/AgEAMB0G\nA1UdDgQWBBTZS/SzkBfiHQ5/Gy+b4g1XJOHkojANBgkqhkiG9w0BAQsFAAOCAQEA\nUwPhAbzTWZIlBsMHAL+8jvxM8qxc6HDhayAD4gbCE65vHYgSizost02vRfpOPQq1\nD6HM8JjifS3KHd6E6chdTbrHI0W8pMJJPon5akCJf/uGeGDl+2wKfmVC6UoV7hC3\npcUzm3JKwsNJbjS5rxL8f5a8bNdIFfLQKuyRpnVX2CsNHvh+WJzynQ+PUJ6zCa7y\nx5AJxca2PTnBKRoVTAyumT1suluI9f4GRYnxTE/qIKRZRs+uT3kIl/N9VX+GbjGb\npPszJ+p6N6Arl1BqJP1DdLin2IFGZL39pTyifm5GP+Vou2aHPuHZDVoCdxsFKup+\ngUY2KeKz0UManwubPQNnKA==\n-----END CERTIFICATE-----\n[root@ncputility ~ panhub_rc]$\n</code></pre> <p>2) login to workload cluster and run this command </p> <pre><code>[root@ncputility ~ pancwl_rc]$ source /root/pancwlrc\nWARNING: Using insecure TLS client config. Setting this option is not supported!\n\nLogin successful.\n\nYou have access to 115 projects, the list has been suppressed. You can list all projects with 'oc projects'\n\nUsing project \"nokia\".\n[root@ncputility ~ pancwl_rc]$ oc get cm -n openshift-config user-ca-bundle -o yaml\napiVersion: v1\ndata:\n  ca-bundle.crt: |\n    -----BEGIN CERTIFICATE-----\n    MIIDDDCCAfSgAwIBAgIBATANBgkqhkiG9w0BAQsFADAmMSQwIgYDVQQDDBtpbmdy\n    ZXNzLW9wZXJhdG9yQDE3NDAwNzQ1NDQwHhcNMjUwMjIwMTgwMjIzWhcNMjcwMjIw\n    MTgwMjI0WjAmMSQwIgYDVQQDDBtpbmdyZXNzLW9wZXJhdG9yQDE3NDAwNzQ1NDQw\n    ggEiMA0GCSqGSIb3DQEBAQUAA4IBDwAwggEKAoIBAQDjXWFSVPshwihivZhaTrB7\n    0boUOw2j3Ut/J6eSm+JA+xVl05L4XD8+C8VSst+f32Pe42Lso2hrovY1dT7IBX3g\n    6S9Nnd2iRc9vC/qHcQkGQ6krIYSQ48aCH7UuahCpTqxEp+MwmhCTQngN2maTJBe3\n    0E6K2IL4zXSi0Iuj08BOnH/w4pJxeWhyngXDf2SeA88EmU3juHLshHrAND84uou2\n    /sOStIAhFwU+o887dSgx7iERy+6nczSDB9Qvq11cUSS4RPC82bNnxcc+BrynDwZ/\n    eggs0OEaj/1cHu/svKZHX9gUKrqz80wF8YGZLLgI2oPAf2VJorvtkJAErzgttroF\n    AgMBAAGjRTBDMA4GA1UdDwEB/wQEAwICpDASBgNVHRMBAf8ECDAGAQH/AgEAMB0G\n    A1UdDgQWBBTZS/SzkBfiHQ5/Gy+b4g1XJOHkojANBgkqhkiG9w0BAQsFAAOCAQEA\n    UwPhAbzTWZIlBsMHAL+8jvxM8qxc6HDhayAD4gbCE65vHYgSizost02vRfpOPQq1\n    D6HM8JjifS3KHd6E6chdTbrHI0W8pMJJPon5akCJf/uGeGDl+2wKfmVC6UoV7hC3\n    pcUzm3JKwsNJbjS5rxL8f5a8bNdIFfLQKuyRpnVX2CsNHvh+WJzynQ+PUJ6zCa7y\n    x5AJxca2PTnBKRoVTAyumT1suluI9f4GRYnxTE/qIKRZRs+uT3kIl/N9VX+GbjGb\n    pPszJ+p6N6Arl1BqJP1DdLin2IFGZL39pTyifm5GP+Vou2aHPuHZDVoCdxsFKup+\n    gUY2KeKz0UManwubPQNnKA==\n    -----END CERTIFICATE-----\nkind: ConfigMap\nmetadata:\n  annotations:\n    openshift.io/owning-component: End User\n  creationTimestamp: \"2025-03-03T08:56:26Z\"\n  name: user-ca-bundle\n  namespace: openshift-config\n  resourceVersion: \"43318895\"\n  uid: 6624e5ad-93b5-418e-8c1e-7a91c724a760\n[root@ncputility ~ pancwl_rc]$ oc get proxy cluster -o yaml\napiVersion: config.openshift.io/v1\nkind: Proxy\nmetadata:\n  creationTimestamp: \"2025-03-03T08:54:59Z\"\n  generation: 2\n  name: cluster\n  resourceVersion: \"48312416\"\n  uid: ceaebe02-9bd3-4361-847e-1b880ebb85de\nspec:\n  trustedCA:\n    name: user-ca-bundle   &lt;---------------------------- this should be patched. if missing. \nstatus: {}\n[root@ncputility ~ pancwl_rc]$\n</code></pre> <p>3) login to workload cluster add tls of hub to cwl. </p> <pre><code>[root@ncputility ~ pancwl_rc]$ oc patch proxy cluster --patch '{\"spec\":{\"trustedCA\":{\"name\":\"user-ca-bundle\"}}}' --type=merge\n[root@ncputility ~ pancwl_rc]$ oc get proxy cluster -o yaml\napiVersion: config.openshift.io/v1\nkind: Proxy\nmetadata:\n  creationTimestamp: \"2025-03-03T08:54:59Z\"\n  generation: 2\n  name: cluster\n  resourceVersion: \"48312416\"\n  uid: ceaebe02-9bd3-4361-847e-1b880ebb85de\nspec:\n  trustedCA:\n    name: user-ca-bundle\nstatus: {}\n[root@ncputility ~ pancwl_rc]$\n</code></pre>"},{"location":"openshift/CNF-onboarding-support/proxy-cache-pod/#configure-the-proxy-cache-on-the-registry-level","title":"Configure the proxy cache on the registry level.","text":""},{"location":"openshift/CNF-onboarding-support/proxy-cache-pod/#hub-quay-configiration","title":"hub quay configiration","text":"<p>1) Open up the hub quay url </p>"},{"location":"openshift/CNF-onboarding-support/proxy-cache-pod/#cwl-quay-configuration","title":"CWL quay configuration","text":"<p>1) Open CWL quay login via super account and created an user called cnfowners also fix the passwd.</p> <p> </p> <p>2) Create an org as same as hub quay. </p> <p></p> <p>3) Set the proxy cache configuration for the organization (in this step adding <code>/org-name</code> too, example: <code>quay-registry.apps.panclyphub01.mnc020.mcc714/cnfimagesnew</code> )</p> <p></p> <p>4) create robot account and default permission</p> <p></p> <p>5) Create a new team for image pull user</p> <p></p> <p>6) Set default permission for the pull user (optional)</p> <p> \u2192 Default Permissions \u2192 + Create Default Permission <p>7) Extend the global image pull secret</p> <p>During the Managed cluster installation, the global pull secret is configured. If the 2nd Hub Quay account and the cache account are not prepared in advance, these accounts need to be added. In case of mirrored registries, only the global pull secret can be used. It is not possible to add project specific pull secrets. For more information, see chapter Image configuration resources in document Images, available in OpenShift Container Platform Product documentation.</p>"},{"location":"openshift/CNF-onboarding-support/proxy-cache-pod/#testing-pod-creation-using-proxy-cache-quay","title":"Testing pod creation using <code>proxy-cache</code> quay.","text":"<p>1) Login the namespace with cluster admin access to grand rights for an scc. </p> <pre><code>[root@ncputility ~ pancwl_rc]$ source  /root/pancwlrc\nWARNING: Using insecure TLS client config. Setting this option is not supported!\n\nLogin successful.\n\nYou have access to 116 projects, the list has been suppressed. You can list all projects with 'oc projects'\n\nUsing project \"default\".\n[root@ncputility ~ pancwl_rc]$ \n</code></pre> <p>2) Grand admin rights to project. if missed during project creation phase. </p> <pre><code>[root@ncputility ~ pancwl_rc]$ oc policy add-role-to-user admin  nokia  -n nokia\nclusterrole.rbac.authorization.k8s.io/admin added: \"nokia\"\n[root@ncputility ~ pancwl_rc]$ \n</code></pre> <p>3) Also grand scc role to default service account via anyuid. </p> <pre><code>[root@ncputility ~ pancwl_rc]$ oc adm policy add-scc-to-user anyuid -z default -n nokia\nclusterrole.rbac.authorization.k8s.io/system:openshift:scc:anyuid added: \"default\"\n[root@ncputility ~ pancwl_rc]$ \n</code></pre> <p>4) login to cnf tenant here .</p> <pre><code>[root@ncputility ~ pancwl_rc]$ oc login -u nokia -p nokia@123\nWARNING: Using insecure TLS client config. Setting this option is not supported!\n\nLogin successful.\n\nYou have one project on this server: \"nokia\"\n\nUsing project \"nokia\".\n[root@ncputility ~ pancwl_rc]$ \n</code></pre> <p>5) run an pod using proxy-cache url </p> <pre><code>[root@ncputility ~ pancwl_rc]$  oc run podpingtest3 --image=ephemeral.url/cnfimages/testimage01:latest --restart=Never  -- tail -f /dev/null\npod/podpingtest3 created\n[root@ncputility ~ pancwl_rc]$ oc get pods\nNAME           READY   STATUS              RESTARTS   AGE\npodpingtest3   0/1     ContainerCreating   0          4s\n[root@ncputility ~ pancwl_rc]$ oc get pods\nNAME           READY   STATUS              RESTARTS   AGE\npodpingtest3   0/1     ContainerCreating   0          6s\n[root@ncputility ~ pancwl_rc]$ oc get pods\nNAME           READY   STATUS              RESTARTS   AGE\npodpingtest3   0/1     ContainerCreating   0          8s\n[root@ncputility ~ pancwl_rc]$ oc get pods\nNAME           READY   STATUS              RESTARTS   AGE\npodpingtest3   0/1     ContainerCreating   0          9s\n[root@ncputility ~ pancwl_rc]$ oc get pods\nNAME           READY   STATUS              RESTARTS   AGE\npodpingtest3   0/1     ContainerCreating   0          11s\n[root@ncputility ~ pancwl_rc]$ oc get pods\nNAME           READY   STATUS              RESTARTS   AGE\npodpingtest3   0/1     ContainerCreating   0          13s\n[root@ncputility ~ pancwl_rc]$ \n</code></pre> <p>6) validate the pod status and make sure it's getting the image via proxy-cache. </p> <pre><code>[root@ncputility ~ pancwl_rc]$ oc get pods\nNAME           READY   STATUS    RESTARTS   AGE\npodpingtest3   1/1     Running   0          5m1s\n[root@ncputility ~ pancwl_rc]$\n\n\n[root@ncputility ~ pancwl_rc]$ oc describe pod podpingtest3\nName:             podpingtest3\nNamespace:        nokia\nPriority:         0\nService Account:  default\nNode:             appworker9.panclypcwl01.mnc020.mcc714/10.89.96.35\nStart Time:       Fri, 04 Apr 2025 08:04:47 -0500\nLabels:           run=podpingtest3\nAnnotations:      k8s.ovn.org/pod-networks:\n                    {\"default\":{\"ip_addresses\":[\"172.19.21.252/23\"],\"mac_address\":\"0a:58:ac:13:15:fc\",\"gateway_ips\":[\"172.19.20.1\"],\"routes\":[{\"dest\":\"172.16....\n                  k8s.v1.cni.cncf.io/network-status:\n                    [{\n                        \"name\": \"ovn-kubernetes\",\n                        \"interface\": \"eth0\",\n                        \"ips\": [\n                            \"172.19.21.252\"\n                        ],\n                        \"mac\": \"0a:58:ac:13:15:fc\",\n                        \"default\": true,\n                        \"dns\": {}\n                    }]\n                  openshift.io/scc: anyuid\nStatus:           Running\nIP:               172.19.21.252\nIPs:\n  IP:  172.19.21.252\nContainers:\n  podpingtest3:\n    Container ID:  cri-o://749055ef608c6f30be42248c63889fd85377928389dae2e29eed50919cc2ee79\n    Image:         ephemeral.url/cnfimages/testimage01:latest\n    Image ID:      ephemeral.url/cnfimages/testimage01@sha256:32666e0234f88377a91de56bb78f2d4f8df45b4f99c1c2dc9ee1d134c84f4753\n    Port:          &lt;none&gt;\n    Host Port:     &lt;none&gt;\n    Args:\n      tail\n      -f\n      /dev/null\n    State:          Running\n      Started:      Fri, 04 Apr 2025 08:05:05 -0500\n    Ready:          True\n    Restart Count:  0\n    Environment:    &lt;none&gt;\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-mp46w (ro)\nConditions:\n  Type                        Status\n  PodReadyToStartContainers   True\n  Initialized                 True\n  Ready                       True\n  ContainersReady             True\n  PodScheduled                True\nVolumes:\n  kube-api-access-mp46w:\n    Type:                    Projected (a volume that contains injected data from multiple sources)\n    TokenExpirationSeconds:  3607\n    ConfigMapName:           kube-root-ca.crt\n    ConfigMapOptional:       &lt;nil&gt;\n    DownwardAPI:             true\n    ConfigMapName:           openshift-service-ca.crt\n    ConfigMapOptional:       &lt;nil&gt;\nQoS Class:                   BestEffort\nNode-Selectors:              &lt;none&gt;\nTolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 60s\n                             node.kubernetes.io/unreachable:NoExecute op=Exists for 60s\nEvents:\n  Type    Reason          Age    From               Message\n  ----    ------          ----   ----               -------\n  Normal  Scheduled       6m5s   default-scheduler  Successfully assigned nokia/podpingtest3 to appworker9.panclypcwl01.mnc020.mcc714\n  Normal  AddedInterface  6m4s   multus             Add eth0 [172.19.21.252/23] from ovn-kubernetes\n  Normal  Pulling         6m4s   kubelet            Pulling image \"ephemeral.url/cnfimages/testimage01:latest\"\n  Normal  Pulled          5m47s  kubelet            Successfully pulled image \"ephemeral.url/cnfimages/testimage01:latest\" in 17.521s (17.521s including waiting)\n  Normal  Created         5m47s  kubelet            Created container podpingtest3\n  Normal  Started         5m47s  kubelet            Started container podpingtest3\n[root@ncputility ~ pancwl_rc]$\n</code></pre> <p></p>"},{"location":"openshift/CNF-onboarding-support/proxy-cache-pod/#cnf-image-upload-using-pod-command","title":"CNF image upload using pod command","text":"<p>1) login to hub quay using cnfowners and org as cnfimages. </p> <pre><code>[root@ncputility ~ pancwl_rc]$ podman login quay-registry.apps.panclyphub01.mnc020.mcc714 -u cnfowners -p cnfowners\nLogin Succeeded!\n[root@ncputility ~ pancwl_rc]$\n</code></pre> <p>2) load the container images to log registry </p> <pre><code>[root@ncputility ~ pancwl_rc]$ podman load -i &lt;filename&gt;.tar^C\n[root@ncputility ~ pancwl_rc]$\n</code></pre> <p>3) tag the image to your registry here </p> <pre><code>[root@ncputility ~ pancwl_rc]$ podman tag quay-registry.apps.panclyphub01.mnc020.mcc714/cnfimages/testimage01 quay-registry.apps.panclyphub01.mnc020.mcc714/cnfimagesnew/testimage01:latest\n[root@ncputility ~ pancwl_rc]$\n</code></pre> <p>4) push the image to remore registry using podman push command here .</p> <pre><code>[root@ncputility ~ pancwl_rc]$ podman push quay-registry.apps.panclyphub01.mnc020.mcc714/cnfimagesnew/testimage01:latest\nGetting image source signatures\nCopying blob 1af69dabfc93 done   |\nCopying blob 53f86715cdba done   |\nCopying blob b6361360b38a done   |\nCopying config d39b33df22 done   |\nWriting manifest to image destination\n[root@ncputility ~ pancwl_rc]$\n</code></pre>"},{"location":"openshift/CNF-onboarding-support/rolebinding/","title":"Role Allocation for CNF user","text":""},{"location":"openshift/CNF-onboarding-support/rolebinding/#openshift-container-platform-ocp-role-defination","title":"OpenShift Container Platform (OCP) role defination","text":"<p>In OpenShift Container Platform (OCP), roles define a set of permissions and are a core part of Role-Based Access Control (RBAC). OCP uses Kubernetes RBAC with some OpenShift-specific enhancements. You can define different types of roles depending on your access and security needs.</p>"},{"location":"openshift/CNF-onboarding-support/rolebinding/#types-of-roles-in-ocp","title":"Types of Roles in OCP:","text":""},{"location":"openshift/CNF-onboarding-support/rolebinding/#clusterrole","title":"ClusterRole","text":"<ul> <li> <p>Scope: Cluster-wide.</p> </li> <li> <p>Purpose: Grants permissions across the entire cluster or to non-namespaced resources (e.g., nodes, persistent volumes).</p> </li> <li> <p>Use Cases:     Granting administrators full access across all projects.     Giving access to cluster-wide resources like nodes or storage classes.</p> </li> </ul>"},{"location":"openshift/CNF-onboarding-support/rolebinding/#if-you-use-a-clusterrole","title":"If You Use a ClusterRole:","text":"<p>Scope: Cluster-wide, but can still be bound to specific namespaces via RoleBinding.</p>"},{"location":"openshift/CNF-onboarding-support/rolebinding/#implication","title":"Implication:","text":"<ul> <li> <p>You can reuse the same ClusterRole and bind it to multiple users and namespaces.</p> </li> <li> <p>Just create separate RoleBindings in each namespace for each user.</p> </li> </ul>"},{"location":"openshift/CNF-onboarding-support/rolebinding/#role","title":"Role","text":"<ul> <li> <p>Scope: Namespaced.</p> </li> <li> <p>Purpose: Grants permissions only within a specific namespace (project).</p> </li> <li> <p>Use Cases:</p> <p>Application developers working within a single namespace. CI/CD pipelines running in isolated namespaces.</p> </li> </ul>"},{"location":"openshift/CNF-onboarding-support/rolebinding/#if-you-use-a-role-namespaced","title":"If You Use a Role (namespaced):","text":"<p>Scope: One namespace only.</p>"},{"location":"openshift/CNF-onboarding-support/rolebinding/#implication_1","title":"Implication:","text":"<ul> <li> <p>You can\u2019t reuse the same Role in another namespace.</p> </li> <li> <p>You\u2019d have to create a new Role (with the same rules) in each namespace where you want the same access.</p> </li> </ul>"},{"location":"openshift/CNF-onboarding-support/rolebinding/#binding-roles","title":"Binding Roles","text":"<p>You assign roles to users or groups through bindings:</p> <ul> <li> <p>RoleBinding: Assigns a Role to a user/group/service account within a specific namespace.</p> </li> <li> <p>ClusterRoleBinding: Assigns a ClusterRole to a user/group/service account cluster-wide or within a namespace (via a RoleBinding referencing a ClusterRole).</p> </li> </ul>"},{"location":"openshift/CNF-onboarding-support/rolebinding/#implementation-for-network-attachement-role","title":"Implementation for Network attachement role:","text":"<p>1) login to cluster with admin privilage and then create a new role (clusterrole) with granding access to network attachements.</p> <p>in case need it as yaml file. (optional step)</p> <pre><code>---\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRole\nmetadata:\n  name: net-attach-def-cluster-role\nrules:\n  - apiGroups: [\"k8s.cni.cncf.io\"]\n    resources: [\"network-attachment-definitions\"]\n    verbs: [\"create\", \"get\", \"list\", \"watch\", \"update\", \"patch\", \"delete\"]\n</code></pre> <p>(or)</p> <pre><code>[root@ncputility ~ pancwl_rc]$ oc create clusterrole net-attach-def-cluster-role \\\n  --verb=create,get,list,watch,update,patch,delete \\\n  --resource=network-attachment-definitions.k8s.cni.cncf.io\n\nclusterrole.rbac.authorization.k8s.io/net-attach-def-cluster-role created\n</code></pre> <p>2) Run this step for below cnf users requesting for access to network attachment definition and assign to their NS.</p> <pre><code>[root@ncputility ~ pancwl_rc]$ oc create rolebinding net-attach-def-rolebinding \\\n  --clusterrole=net-attach-def-cluster-role \\\n  --user=paclypamrf01 \\\n  --namespace=paclypamrf01\nrolebinding.rbac.authorization.k8s.io/net-attach-def-rolebinding created\n</code></pre> <p>3) Then validate is that user having access to it. </p> <pre><code>[root@ncputility ~ pancwl_rc]$ oc auth can-i create network-attachment-definitions.k8s.cni.cncf.io --as=paclypamrf01 -n paclypamrf01\nyes\n[root@ncputility ~ pancwl_rc]$\n</code></pre>"},{"location":"openshift/CNF-onboarding-support/rolebinding/#implementation-for-cbur-role","title":"Implementation for Cbur role:","text":"<p>1) login to cluster with admin privilage and then create a new role (clusterrole) with granding access to cbur br polices.</p> <p>code snippet: <code># cat  &gt; cburrole.yaml</code></p> <pre><code>apiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRole\nmetadata:\n  name: ncd-cbur-role\nrules:\n  - apiGroups: [\"cbur.bcmt.local\"]\n    resources: [\"brpolices\"]\n    verbs: [\"create\", \"get\", \"list\", \"watch\", \"update\", \"patch\"]\n  - apiGroups: [\"cbur.csf.nokia.com\"]\n    resources: [\"brhooks\", \"brpolices\"]\n    verbs: [\"create\", \"get\", \"list\", \"watch\", \"update\", \"patch\"]\n</code></pre> <p>1.1) here is the command executed on this platform. just give ctrl+c at last. right after that just apply it. </p> <pre><code>[root@ncputility ~ panhub_rc]$ cat  &gt; cburrole.yaml\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRole\nmetadata:\n  name: ncd-cbur-role\nrules:\n  - apiGroups: [\"cbur.bcmt.local\"]\n    resources: [\"brpolices\"]\n    verbs: [\"create\", \"get\", \"list\", \"watch\", \"update\", \"patch\"]\n  - apiGroups: [\"cbur.csf.nokia.com\"]\n    resources: [\"brhooks\", \"brpolices\"]\n    verbs: [\"create\", \"get\", \"list\", \"watch\", \"update\", \"patch\"]\n^C\n[root@ncputility ~ panhub_rc]$ oc apply  -f cburrole.yaml\nclusterrole.rbac.authorization.k8s.io/ncd-cbur-role created\n[root@ncputility ~ panhub_rc]$\n</code></pre> <p>2) Run this step for below cnf users requesting for access to cbur br polices and assign to their NS.</p> <pre><code>[root@ncputility ~ panhub_rc]$ oc create rolebinding ncd-cbur-role-binding \\\n  --clusterrole=ncd-cbur-role \\\n  --user=nokia \\\n  --namespace=test01\nrolebinding.rbac.authorization.k8s.io/ncd-cbur-role-binding created\n[root@ncputility ~ panhub_rc]$\n</code></pre> <p>3) Then validate is that user having access to it. </p> <pre><code>[root@ncputility ~ panhub_rc]$ oc auth can-i create brpolices --as=nokia -n test01\nyes\n[root@ncputility ~ panhub_rc]$\n</code></pre>"},{"location":"openshift/CNF-onboarding-support/rolebinding/#implementation-for-securitynodescustomresourcedefinitionsetc-role","title":"Implementation for security,nodes,customresourcedefinitions,etc role:","text":"<p>1) login to cluster with admin privilage and then create a new role (clusterrole) with granding access to nodes, scc, crds, etc.</p> <p>code snippet: <code># cat  &gt; read-cluster-infra-info.yaml</code></p> <pre><code>apiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRole\nmetadata:\n  name: read-cluster-infra-info\nrules:\n  - apiGroups: [\"security.openshift.io\"]\n    resources: [\"securitycontextconstraints\"]\n    verbs: [\"get\", \"list\"]\n  - apiGroups: [\"\"]\n    resources: [\"nodes\"]\n    verbs: [\"get\", \"list\"]\n  - apiGroups: [\"apiextensions.k8s.io\"]\n    resources: [\"customresourcedefinitions\"]\n    verbs: [\"get\", \"list\"]\n  - apiGroups: [\"compliance.openshift.io\"]\n    resources: [\"profiles\"]\n    verbs: [\"get\", \"list\"]\n  - apiGroups: [\"nmstate.io\"]\n    resources: [\"nodenetworkconfigurationpolicies\"]\n    verbs: [\"get\", \"list\", \"watch\"]\n  - apiGroups: [\"metallb.io\"]\n    resources: [\"ipaddresspools\"]\n    verbs: [\"get\", \"list\"]\n</code></pre> <p>1.1) here is the command executed on this platform. just give ctrl+c at last. right after that just apply it. </p> <pre><code>[root@ncputility ~ panhub_rc]$ cat  &gt; ncp-default-cnf-role.yaml\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRole\nmetadata:\n  name: ncp-default-cnf-role\nrules:\n  - apiGroups: [\"security.openshift.io\"]\n    resources: [\"securitycontextconstraints\"]\n    verbs: [\"get\", \"list\"]\n  - apiGroups: [\"\"]\n    resources: [\"nodes\"]\n    verbs: [\"get\", \"list\"]\n  - apiGroups: [\"apiextensions.k8s.io\"]\n    resources: [\"customresourcedefinitions\"]\n    verbs: [\"get\", \"list\"]\n  - apiGroups: [\"compliance.openshift.io\"]\n    resources: [\"profiles\"]\n    verbs: [\"get\", \"list\"]\n  - apiGroups: [\"nmstate.io\"]\n    resources: [\"nodenetworkconfigurationpolicies\"]\n    verbs: [\"get\", \"list\", \"watch\"]\n  - apiGroups: [\"metallb.io\"]\n    resources: [\"ipaddresspools\"]\n    verbs: [\"get\", \"list\"]\n\n^C\n[root@ncputility ~ panhub_rc]$ oc apply  -f ncp-default-cnf-role.yaml\nclusterrole.rbac.authorization.k8s.io/ncp-default-cnf-role created\n[root@ncputility ~ panhub_rc]$\n</code></pre> <p>2) Run this step for below cnf users requesting for access to <code>nodes,scc,etc</code> polices and assign to their user id.</p> <pre><code>[root@ncputility ~ panhub_rc]$  oc create clusterrolebinding ncp-default-cnf-role-pppcf01-binding --clusterrole=ncp-default-cnf-role --user=pppcf01 \nclusterrolebinding.rbac.authorization.k8s.io/ncp-default-cnf-role-pppcf01-binding created\n[root@ncputility ~ panhub_rc]$\n</code></pre> <p>3) Then validate is that user having access to it. </p> <pre><code>[root@ncputility ~ nmcrc]$ oc auth can-i get nodes --as=ppaaa01\nWarning: resource 'nodes' is not namespace scoped\n\nyes\n[root@ncputility ~ nmcrc]$ oc auth can-i get scc --as=ppaaa01\nWarning: resource 'securitycontextconstraints' is not namespace scoped in group 'security.openshift.io'\n\nyes\n[root@ncputility ~ nmcrc]$ oc auth can-i get crds --as=ppaaa01\nWarning: resource 'customresourcedefinitions' is not namespace scoped in group 'apiextensions.k8s.io'\n\nyes\n[root@ncputility ~ nmcrc]$ oc login -u ppaaa01 -p redhat123\nWARNING: Using insecure TLS client config. Setting this option is not supported!\n\nLogin successful.\n\nYou have one project on this server: \"ppaaa01\"\n\nUsing project \"ppaaa01\".\n[root@ncputility ~ nmcrc]$ oc get nodes\nNAME                                            STATUS   ROLES                                         AGE    VERSION\nappworker1-0.ppwncp01.infra.mobi.eastlink.ca    Ready    appworker,appworker-mcp-a,appworker1,worker   10d    v1.29.10+67d3387\nappworker1-1.ppwncp01.infra.mobi.eastlink.ca    Ready    appworker,appworker-mcp-a,appworker1,worker   10d    v1.29.10+67d3387\nappworker1-10.ppwncp01.infra.mobi.eastlink.ca   Ready    appworker,appworker-mcp-b,appworker1,worker   10d    v1.29.10+67d3387\nappworker1-11.ppwncp01.infra.mobi.eastlink.ca   Ready    appworker,appworker-mcp-b,appworker1,worker   10d    v1.29.10+67d3387\nappworker1-12.ppwncp01.infra.mobi.eastlink.ca   Ready    appworker,appworker-mcp-b,appworker1,worker   10d    v1.29.10+67d3387\nappworker1-13.ppwncp01.infra.mobi.eastlink.ca   Ready    appworker,appworker-mcp-b,appworker1,worker   10d    v1.29.10+67d3387\nappworker1-14.ppwncp01.infra.mobi.eastlink.ca   Ready    appworker,appworker-mcp-b,appworker1,worker   10d    v1.29.10+67d3387\nappworker1-15.ppwncp01.infra.mobi.eastlink.ca   Ready    appworker,appworker-mcp-b,appworker1,worker   10d    v1.29.10+67d3387\n</code></pre>"},{"location":"openshift/CNF-onboarding-support/ts-tools/","title":"MACD Guide: Tcpdump and Diagnostic Tools in OpenShift Clusters","text":"<p>This documentation assists the MACD team in running <code>tcpdump</code>, <code>sosreport</code>, and <code>must-gather</code> on the OpenShift cluster.</p>"},{"location":"openshift/CNF-onboarding-support/ts-tools/#guidelines-for-macd-team","title":"Guidelines for MACD Team","text":"<ul> <li> <p>Inform the application teams that they are responsible for their own trace collection.  </p> <p>Running <code>tcpdump</code> or trace collection is additional validation/troubleshooting initiated by the application teams. They will instruct what needs to be executed.</p> </li> <li> <p>CNF teams must specify where they want traces to be collected.  </p> <p>MACD should not guide CNF teams. Simply run the <code>toolbox</code> and follow instructions provided by them.</p> </li> <li> <p>Share screen during the session.  </p> <p>Screen sharing ensures CNF teams are aware of what is being executed.</p> </li> <li> <p>Cleanup after session.  </p> <p>After transferring the <code>.pcap</code> file to the infra-manager node, delete it using <code>rm -f &lt;filename&gt;</code> to prevent disk pressure on the host OS.</p> </li> <li> <p><code>tcpdump</code> should be run on physical or VLAN-based interfaces.  </p> <p>No capture filters should be applied during the <code>tcpdump</code> by MACD engineers. All Filters should be applied by CNF owners while reading it from wireshark or equivalent tools. </p> </li> </ul>"},{"location":"openshift/CNF-onboarding-support/ts-tools/#tcpdump-collection","title":"Tcpdump collection","text":"<p><code>Note 1</code>: Collection as much information as possible before joing the call. [ \"Name of the cluster\", \"Application NS name\", \"Application pod name\", \"Pod hosted compute name\", \"vlan name to capture the trace\" etc.]</p> <p>1) Login to node via ssh or debug utitiy </p> <pre><code>[root@ncputility ~ pancwl_rc]$ ssh core@appworker2-5.ppwncp01.infra.mobi.eastlink.ca\nRed Hat Enterprise Linux CoreOS 416.94.202407081958-0\n  Part of OpenShift 4.16, RHCOS is a Kubernetes-native operating system\n  managed by the Machine Config Operator (`clusteroperator/machine-config`).\n\nWARNING: Direct SSH access to machines is not recommended; instead,\nmake configuration changes via `machineconfig` objects:\n  https://docs.openshift.com/container-platform/4.16/architecture/architecture-rhcos.html\n\n---\n[core@appworker2-5 ~]$\n\nor \n\n[root@ncputility ~ cwl_rc]$oc debug -t node/appworker2-5.ppwncp01.infra.mobi.eastlink.ca\nTemporary namespace openshift-debug-99xtp is created for debugging node...\nStarting pod/appworker2-5ppwncp01inframobieastlinkca-debug-fm9xk ...\nTo use host binaries, run `chroot /host`\nPod IP: 10.236.97.53\nIf you don't see a command prompt, try pressing enter.\nsh-5.1# chroot /host\nsh-5.1# \n</code></pre> <p>2) Trigger toolbox command to execute into a shell which contains all required tools like tcpdump, sosreport etc. </p> <pre><code>sh-5.1# toolbox\nTrying to pull registry.redhat.io/rhel9/support-tools:latest...\nGetting image source signatures\nCopying blob a0e56de801f5 done   |\nCopying blob ec465ce79861 done   |\nCopying blob facf1e7dd3e0 done   |\nCopying blob cbea42b25984 done   |\nCopying config a627accb68 done   |\nWriting manifest to image destination\na627accb682adb407580be0d7d707afbcb90abf2f407a0b0519bacafa15dd409\nSpawning a container 'toolbox-root' with image 'registry.redhat.io/rhel9/support-tools'\nDetected RUN label in the container image. Using that as the default...\nb8a833e8ed0aa428271acb952cfb0f870eea66c0465a62fd23e917b0e2217d45\ntoolbox-root\nContainer started successfully. To exit, type 'exit'.\n[root@appworker2-5 /]#\n</code></pre> <p>4) Run <code>tcpdump</code> command against any linux network interface. </p> <pre><code>[root@appworker2-5 /]# tcpdump -i br-ex -w br-ex.pcap\ndropped privs to tcpdump\ntcpdump: listening on br-ex, link-type EN10MB (Ethernet), snapshot length 262144 bytes\n^C26702 packets captured\n27154 packets received by filter\n0 packets dropped by kernel\n[root@appworker2-5 /]# file br-ex.pcap\nbr-ex.pcap: pcap capture file, microsecond ts (little-endian) - version 2.4 (Ethernet, capture length 262144)\n[root@appworker2-5 /]# exit\nexit\n</code></pre> <p>5) Locate the file from host OS level for fast transfer, note toolbox files are saved on <code>/var/lib/containers/storage/</code> so we will do ing the find command to locate the file easly. </p> <pre><code>sh-5.1# find /var/lib/containers/storage/ -name br-ex.pcap -print\n/var/lib/containers/storage/overlay/b645fb6d5a034493f332f9794cf47967ac50bb8a8e92f26e2c13da18697a5387/diff/br-ex.pcap\nsh-5.1#\n</code></pre> <p>6) Scp to infra-manager node using scp command. if you have a <code>dedicated infra-manager</code> ask the application teams where to upload the file.</p> <p><code>Note 1</code>: Dont share file via <code>dedicated infra-manager</code> node.  its not a file sharing server. </p> <pre><code>scp -rp /var/lib/containers/storage/overlay/b645fb6d5a034493f332f9794cf47967ac50bb8a8e92f26e2c13da18697a5387/diff/br-ex.pcap root@infra-manager:/tmp/ \n</code></pre> <p>7) Delete the file from the node, so prevent causing an disk pressure issue. </p> <pre><code>toolbox\nrm -fr br-ex.pcap\n</code></pre> <p><code>Note 1</code>: It is our responsibility to <code>delete trace files from the OCP node</code> level. However, removing the trace file after download is <code>Nokia\u2019s responsibility</code> (from shared infra-manager node or Jump server).</p> <p><code>Note 2</code>: If you copy the trace file to a shared <code>infra-manager node</code>, it is still the <code>responsibility of the application team to delete</code> the trace file after downloading it.</p> <p><code>Note 3</code>: if you have a <code>dedicated infra-manager</code> node, Don't share or use this node as file sharing server. Application team should be providing you the IP, user/passwd to where to upload the trace file. </p>"},{"location":"openshift/CNF-onboarding-support/ts-tools/#sos-report-collection","title":"Sos report collection","text":"<p>1) Login to node via ssh or debug utitiy </p> <pre><code>[root@ncputility ~ pancwl_rc]$ ssh core@gateway2.panclypcwl01.mnc020.mcc714\nRed Hat Enterprise Linux CoreOS 416.94.202407081958-0\n  Part of OpenShift 4.16, RHCOS is a Kubernetes-native operating system\n  managed by the Machine Config Operator (`clusteroperator/machine-config`).\n\nWARNING: Direct SSH access to machines is not recommended; instead,\nmake configuration changes via `machineconfig` objects:\n  https://docs.openshift.com/container-platform/4.16/architecture/architecture-rhcos.html\n\n---\n[core@gateway2 ~]$\n</code></pre> <p>2) Trigger toolbox command to execute into a shell which contains all required tools like tcpdump, sosreport etc. </p> <pre><code>[root@gateway2 ~]# toolbox\n.toolboxrc file detected, overriding defaults...\nTrying to pull quay-registry.apps.panclyphub01.mnc020.mcc714/ocmirror/rhel9/support-tools:latest...\nGetting image source signatures\nCopying blob f5e6502d2728 done   |\nCopying blob ebc7dc32a098 done   |\nCopying config affd08d3be done   |\nWriting manifest to image destination\naffd08d3bead20c55f40f08270d477b1524d9d7a2db25235956c7858755ef5f3\nSpawning a container 'toolbox-root' with image 'quay-registry.apps.panclyphub01.mnc020.mcc714/ocmirror/rhel9/support-tools:latest'\nDetected RUN label in the container image. Using that as the default...\n6bdff24c2e5da044965e2cec8eea58c3d86668f3a5bbe1e2d34495e956fdf0d7\ntoolbox-root\nContainer started successfully. To exit, type 'exit'.\n[root@gateway2 /]#\n</code></pre> <p>3) now use sosreport from here </p> <pre><code>sos report -k crio.all=on -k crio.logs=on  -k podman.all=on -k podman.logs=on\n</code></pre>"},{"location":"openshift/CNF-onboarding-support/ts-tools/#must-gather-collection","title":"Must-Gather collection","text":""},{"location":"openshift/CNF-onboarding-support/ts-tools/#references","title":"References","text":"<ul> <li>Recovering a node that has lost all networking in OpenShift 4</li> </ul>"},{"location":"openshift/Intergration/syslog/","title":"Cluster logging operator","text":""},{"location":"openshift/Intergration/syslog/#limitations","title":"Limitations","text":"<ul> <li>Currently hub cluster cant intergrated with external log receiving server. </li> <li>This procedure tested only with the syslog based log server. not work with splunk etc.</li> <li>local forwarding or external forwarding either one only works. not both. </li> </ul>"},{"location":"openshift/Intergration/syslog/#reference","title":"Reference","text":"<ul> <li>Documented some limitations via NCPFM-1425</li> </ul>"},{"location":"openshift/Intergration/syslog/#configuring-logging-operator","title":"Configuring logging operator","text":"<p>Prerequisites Cluster Logging Operator was deployed via \u201ecommon\u201d <code>PolicyGenTemplate</code> and it has to be configured. For more details on cluster logging, see About Logging in document Logging, available in OpenShift Container Platform Product Documentation.</p> <pre><code>The YAML files can be created directly on the cluster or via GitOps pipeline. In the first option, always apply the files, in the second option, always trigger the `ClusterGroupUpgrade` object with the correct policy name to apply the files.\nHowever, it is recommended to use the GitOps based option.\n</code></pre> <p>1) Create the ClusterLogging object</p> <p>clusterloggingobject_loki.yaml</p> <pre><code>[root@ncputility ~ hn_cwl_rc]$ cat  clusterloggingobject_loki.yaml\napiVersion: logging.openshift.io/v1\nkind: ClusterLogging\nmetadata:\n  name: instance\n  namespace: openshift-logging\nspec:\n  managementState: Managed\n  collection:\n    type: vector\n    tolerations:\n    - key: node.ocs.openshift.io/storage\n      operator: Equal\n      value: 'true'\n      effect: NoSchedule\n  visualization:\n    type: ocp-console\n    ocpConsole: null\n  logStore:\n    type: lokistack\n    lokistack:\n      name: logging-loki\n[root@ncputility ~ hn_cwl_rc]$ oc get ClusterLogging instance  -n openshift-logging\nNAME       MANAGEMENT STATE\ninstance   Managed\n[root@ncputility ~ hn_cwl_rc]$\n</code></pre> <p>2) After the file was applied, the objects are created in the <code>openshift-logging</code> namespace:</p> <pre><code>[root@ncputility ~ hn_cwl_rc]$ oc get pods -n openshift-logging -o wide\nNAME                                          READY   STATUS    RESTARTS   AGE   IP              NODE                                    NOMINATED NODE   READINESS GATES\ncluster-logging-operator-5fd7f999cc-c74nw     1/1     Running   0          10d   172.18.12.35    appworker15.hnevopcwl01.mnc002.mcc708   &lt;none&gt;           &lt;none&gt;\ncollector-2p5hc                               1/1     Running   3          14d   172.16.12.51    appworker12.hnevopcwl01.mnc002.mcc708   &lt;none&gt;           &lt;none&gt;\ncollector-4482t                               1/1     Running   2          14d   172.19.0.22     storage1.hnevopcwl01.mnc002.mcc708      &lt;none&gt;           &lt;none&gt;\ncollector-556t8                               1/1     Running   1          14d   172.19.8.71     appworker8.hnevopcwl01.mnc002.mcc708    &lt;none&gt;           &lt;none&gt;\ncollector-5k4vt                               1/1     Running   1          14d   172.18.16.50    appworker20.hnevopcwl01.mnc002.mcc708   &lt;none&gt;           &lt;none&gt;\ncollector-69f88                               1/1     Running   1          14d   172.16.14.80    appworker13.hnevopcwl01.mnc002.mcc708   &lt;none&gt;           &lt;none&gt;\ncollector-6ccv9                               1/1     Running   1          14d   172.19.16.78    appworker23.hnevopcwl01.mnc002.mcc708   &lt;none&gt;           &lt;none&gt;\ncollector-8nkzz                               1/1     Running   1          14d   172.18.20.105   appworker32.hnevopcwl01.mnc002.mcc708   &lt;none&gt;           &lt;none&gt;\ncollector-8nndr                               1/1     Running   3          14d   172.17.0.41     master2.hnevopcwl01.mnc002.mcc708       &lt;none&gt;           &lt;none&gt;\ncollector-8wgtd                               1/1     Running   2          14d   172.19.2.3      gateway2.hnevopcwl01.mnc002.mcc708      &lt;none&gt;           &lt;none&gt;\ncollector-985sr                               1/1     Running   2          14d   172.18.6.3      gateway4.hnevopcwl01.mnc002.mcc708      &lt;none&gt;           &lt;none&gt;\ncollector-9n7zn                               1/1     Running   2          14d   172.19.12.61    appworker16.hnevopcwl01.mnc002.mcc708   &lt;none&gt;           &lt;none&gt;\ncollector-9qsmg                               1/1     Running   2          14d   172.16.4.69     appworker5.hnevopcwl01.mnc002.mcc708    &lt;none&gt;           &lt;none&gt;\ncollector-9qv85                               1/1     Running   3          14d   172.16.8.65     master0.hnevopcwl01.mnc002.mcc708       &lt;none&gt;           &lt;none&gt;\ncollector-9slfx                               1/1     Running   1          14d   172.16.20.63    appworker26.hnevopcwl01.mnc002.mcc708   &lt;none&gt;           &lt;none&gt;\ncollector-bhhmt                               1/1     Running   2          14d   172.17.4.20     storage4.hnevopcwl01.mnc002.mcc708      &lt;none&gt;           &lt;none&gt;\ncollector-c8r5v                               1/1     Running   2          14d   172.18.18.48    appworker28.hnevopcwl01.mnc002.mcc708   &lt;none&gt;           &lt;none&gt;\ncollector-cs4rp                               1/1     Running   3          14d   172.17.18.77    appworker27.hnevopcwl01.mnc002.mcc708   &lt;none&gt;           &lt;none&gt;\ncollector-ggmdf                               1/1     Running   2          14d   172.17.6.80     appworker0.hnevopcwl01.mnc002.mcc708    &lt;none&gt;           &lt;none&gt;\ncollector-gkxmx                               1/1     Running   2          14d   172.17.14.53    appworker22.hnevopcwl01.mnc002.mcc708   &lt;none&gt;           &lt;none&gt;\ncollector-gn7c4                               1/1     Running   3          14d   172.18.22.100   appworker34.hnevopcwl01.mnc002.mcc708   &lt;none&gt;           &lt;none&gt;\ncollector-h5xw6                               1/1     Running   1          14d   172.16.16.75    appworker19.hnevopcwl01.mnc002.mcc708   &lt;none&gt;           &lt;none&gt;\ncollector-hfhf8                               1/1     Running   1          14d   172.16.10.106   appworker2.hnevopcwl01.mnc002.mcc708    &lt;none&gt;           &lt;none&gt;\ncollector-hh4rp                               1/1     Running   1          14d   172.17.22.62    appworker33.hnevopcwl01.mnc002.mcc708   &lt;none&gt;           &lt;none&gt;\ncollector-hmrkh                               1/1     Running   2          14d   172.16.6.67     appworker3.hnevopcwl01.mnc002.mcc708    &lt;none&gt;           &lt;none&gt;\ncollector-hpv64                               1/1     Running   2          14d   172.19.14.44    appworker18.hnevopcwl01.mnc002.mcc708   &lt;none&gt;           &lt;none&gt;\ncollector-jcgs4                               1/1     Running   2          14d   172.17.2.3      gateway1.hnevopcwl01.mnc002.mcc708      &lt;none&gt;           &lt;none&gt;\ncollector-jr7zw                               1/1     Running   1          14d   172.17.20.40    appworker31.hnevopcwl01.mnc002.mcc708   &lt;none&gt;           &lt;none&gt;\ncollector-k7nn9                               1/1     Running   2          14d   172.16.2.25     storage0.hnevopcwl01.mnc002.mcc708      &lt;none&gt;           &lt;none&gt;\ncollector-kwpqh                               1/1     Running   2          14d   172.18.2.18     storage2.hnevopcwl01.mnc002.mcc708      &lt;none&gt;           &lt;none&gt;\ncollector-l2jzn                               1/1     Running   2          14d   172.18.4.88     appworker1.hnevopcwl01.mnc002.mcc708    &lt;none&gt;           &lt;none&gt;\ncollector-lq8hx                               1/1     Running   3          14d   172.16.0.60     master1.hnevopcwl01.mnc002.mcc708       &lt;none&gt;           &lt;none&gt;\ncollector-lzgrn                               1/1     Running   1          14d   172.18.14.61    appworker17.hnevopcwl01.mnc002.mcc708   &lt;none&gt;           &lt;none&gt;\ncollector-nbs6z                               1/1     Running   2          14d   172.19.4.60     appworker4.hnevopcwl01.mnc002.mcc708    &lt;none&gt;           &lt;none&gt;\ncollector-pr7g7                               1/1     Running   2          14d   172.17.8.50     appworker6.hnevopcwl01.mnc002.mcc708    &lt;none&gt;           &lt;none&gt;\ncollector-pt9hv                               1/1     Running   2          14d   172.19.6.31     storage3.hnevopcwl01.mnc002.mcc708      &lt;none&gt;           &lt;none&gt;\ncollector-stzf4                               1/1     Running   2          14d   172.19.10.109   appworker14.hnevopcwl01.mnc002.mcc708   &lt;none&gt;           &lt;none&gt;\ncollector-szptx                               1/1     Running   2          14d   172.18.8.189    appworker7.hnevopcwl01.mnc002.mcc708    &lt;none&gt;           &lt;none&gt;\ncollector-t9zh7                               1/1     Running   2          14d   172.18.0.3      gateway3.hnevopcwl01.mnc002.mcc708      &lt;none&gt;           &lt;none&gt;\ncollector-td9sw                               1/1     Running   1          14d   172.16.22.46    appworker25.hnevopcwl01.mnc002.mcc708   &lt;none&gt;           &lt;none&gt;\ncollector-tmzq6                               1/1     Running   1          14d   172.19.20.66    appworker30.hnevopcwl01.mnc002.mcc708   &lt;none&gt;           &lt;none&gt;\ncollector-tvl9n                               1/1     Running   2          14d   172.18.12.45    appworker15.hnevopcwl01.mnc002.mcc708   &lt;none&gt;           &lt;none&gt;\ncollector-vtqc2                               1/1     Running   2          14d   172.17.12.66    appworker11.hnevopcwl01.mnc002.mcc708   &lt;none&gt;           &lt;none&gt;\ncollector-wfv2n                               1/1     Running   1          14d   172.16.18.52    appworker24.hnevopcwl01.mnc002.mcc708   &lt;none&gt;           &lt;none&gt;\ncollector-wgfql                               1/1     Running   1          14d   172.19.18.48    appworker29.hnevopcwl01.mnc002.mcc708   &lt;none&gt;           &lt;none&gt;\ncollector-xdtlz                               1/1     Running   2          14d   172.17.10.65    appworker9.hnevopcwl01.mnc002.mcc708    &lt;none&gt;           &lt;none&gt;\ncollector-z5r8p                               1/1     Running   2          14d   172.18.10.59    appworker10.hnevopcwl01.mnc002.mcc708   &lt;none&gt;           &lt;none&gt;\ncollector-z9rpj                               1/1     Running   1          14d   172.17.16.50    appworker21.hnevopcwl01.mnc002.mcc708   &lt;none&gt;           &lt;none&gt;\nlogging-loki-compactor-0                      1/1     Running   0          10d   172.16.22.3     appworker25.hnevopcwl01.mnc002.mcc708   &lt;none&gt;           &lt;none&gt;\nlogging-loki-distributor-78d44b7496-c7kjr     1/1     Running   0          10d   172.19.16.40    appworker23.hnevopcwl01.mnc002.mcc708   &lt;none&gt;           &lt;none&gt;\nlogging-loki-distributor-78d44b7496-nc9zt     1/1     Running   0          10d   172.16.10.31    appworker2.hnevopcwl01.mnc002.mcc708    &lt;none&gt;           &lt;none&gt;\nlogging-loki-gateway-6558fdbc65-lmqzq         2/2     Running   0          10d   172.16.10.46    appworker2.hnevopcwl01.mnc002.mcc708    &lt;none&gt;           &lt;none&gt;\nlogging-loki-gateway-6558fdbc65-sg45d         2/2     Running   0          10d   172.18.20.18    appworker32.hnevopcwl01.mnc002.mcc708   &lt;none&gt;           &lt;none&gt;\nlogging-loki-index-gateway-0                  1/1     Running   0          10d   172.18.20.48    appworker32.hnevopcwl01.mnc002.mcc708   &lt;none&gt;           &lt;none&gt;\nlogging-loki-index-gateway-1                  1/1     Running   0          10d   172.16.20.31    appworker26.hnevopcwl01.mnc002.mcc708   &lt;none&gt;           &lt;none&gt;\nlogging-loki-ingester-0                       1/1     Running   0          10d   172.19.16.34    appworker23.hnevopcwl01.mnc002.mcc708   &lt;none&gt;           &lt;none&gt;\nlogging-loki-ingester-1                       1/1     Running   0          10d   172.18.16.58    appworker20.hnevopcwl01.mnc002.mcc708   &lt;none&gt;           &lt;none&gt;\nlogging-loki-querier-79b54c8cf5-tzxst         1/1     Running   0          10d   172.16.12.4     appworker12.hnevopcwl01.mnc002.mcc708   &lt;none&gt;           &lt;none&gt;\nlogging-loki-querier-79b54c8cf5-wqpgm         1/1     Running   0          10d   172.16.22.41    appworker25.hnevopcwl01.mnc002.mcc708   &lt;none&gt;           &lt;none&gt;\nlogging-loki-query-frontend-8d8785885-r7kfr   1/1     Running   0          10d   172.16.22.42    appworker25.hnevopcwl01.mnc002.mcc708   &lt;none&gt;           &lt;none&gt;\nlogging-loki-query-frontend-8d8785885-rblkm   1/1     Running   0          10d   172.18.20.52    appworker32.hnevopcwl01.mnc002.mcc708   &lt;none&gt;           &lt;none&gt;\nlogging-view-plugin-566757957-nlp2n           1/1     Running   0          10d   172.19.12.23    appworker16.hnevopcwl01.mnc002.mcc708   &lt;none&gt;           &lt;none&gt;\n[root@ncputility ~ hn_cwl_rc]$\n</code></pre>"},{"location":"openshift/Intergration/syslog/#local-forwarding-the-logs-make-logs-visiable-on-locally","title":"Local forwarding the logs (make logs visiable on locally)","text":"<p>1) Create the <code>ClusterLogForwarder</code> object to all log types to make them locally visible</p> <p>CWL cluster can support either local forwarding or external forwarding not both. </p> <pre><code>[root@ncputility ~ hn_cwl_rc]$ oc apply -f local_clusterlogforwarder.yaml\napplied.\n[root@ncputility ~ hn_cwl_rc]$ cat  local_clusterlogforwarder.yaml\napiVersion: logging.openshift.io/v1\nkind: ClusterLogForwarder\nmetadata:\n  name: instance\n  namespace: openshift-logging\nspec:\n  pipelines:\n  - name: all-to-default\n    inputRefs:\n    - infrastructure\n    - application\n    - audit\n    outputRefs:\n    - default\n[root@ncputility ~ hn_cwl_rc]$\n</code></pre> <p>2) continue this section <code>Redhat Cluster logging operator plugin enable from UI.</code></p>"},{"location":"openshift/Intergration/syslog/#sending-logs-to-external-syslog-server","title":"Sending logs to external syslog server","text":"<p>For a detailed description, see Forwarding logs using the syslog protocol in document Logging, available in OpenShift Container Platform Product Documentation.</p> <p>1) To send logs to the external source, ClusterLogForwarder object has to be modified, as shown in the following example.</p> <p>All logs and external syslog server can be found at Kibana with the above options. url: udp://:514 should be updated based on your infra. <pre><code>[root@ncputility ~ hn_cwl_rc]$ cat  external-syslog_clusterlogforwarder.yaml\napiVersion: logging.openshift.io/v1\nkind: ClusterLogForwarder\nmetadata:\n  name: instance\n  namespace: openshift-logging\nspec:\n  outputs:\n  - name: infra-node\n    syslog:\n      appName: myapp\n      facility: user\n      msgID: mymsg\n      procID: myproc\n      rfc: RFC5424\n      severity: informational\n    type: syslog\n    url: udp://10.89.27.71:13000\n  pipelines:\n  - inputRefs:\n    - infrastructure\n    - application\n    - audit\n    name: all-to-default\n    outputRefs:\n    - default\n  - inputRefs:\n    - infrastructure\n    - application\n    - audit\n    name: infra-node-rsyslog\n    outputRefs:\n    - infra-node\n[root@ncputility ~ hn_cwl_rc]$ oc apply  -f external-syslog_clusterlogforwarder.yaml\nclusterlogforwarder.logging.openshift.io/instance created\n[root@ncputility ~ hn_cwl_rc]$\n</code></pre> <p>Just for testing, i applied it manually. but it should done via PGT process. </p>"},{"location":"openshift/Intergration/syslog/#redhat-cluster-logging-operator-plugin-enable-from-ui","title":"Redhat Cluster logging operator plugin enable from UI.","text":"<p>1) Go to Operators &gt; Red Hat Openshift Logging to view the collected logs in the webUI</p> <p>In the Details page, click \"Disabled\" for the \"Console plugin\" option In the \"Console plugin\" enablement dialog, select \"Enable\" Click on \"Save\" Verify that the \"Console plugin\" option now shows \"Enabled\"</p> <p>2) The web console displays a pop-up window when changes have been applied. The window prompts to reload the web console. Refresh the browser when you see the pop-up window to apply the changes. After the browser is refreshed to apply the changes, the logs can be check under Objects &gt; Logs.</p>"},{"location":"openshift/Intergration/syslog/#pgt-process","title":"PGT process","text":"<p>1) make sure site-specific.yaml policy file has been called for external log forwarding. </p> <pre><code>[root@ncputility ~ hn_cwl_rc]$ cat  site-hnevopcwl01-247mp1-config.yaml |tail -10\n   # Cluster Logging Operator\n     - fileName: clusterlogging/clusterloggingobject_loki.yaml\n       policyName: config-policies-2nd-wave\n         #Uncommnent the necessary method of collecting logs before apply this PolicyGenTemplate\n         #External syslog collection\n     - fileName: clusterlogging/external-syslog_clusterlogforwarder.yaml\n       policyName: config-policies-2nd-wave\n         #Local log collection only\n    # - fileName: clusterlogging/local_clusterlogforwarder.yaml\n    #   policyName: config-policies-2nd-wave\n[root@ncputility ~ hn_cwl_rc]$\n</code></pre> <p>2) update it on the git and commit it.</p> <pre><code>[root@ncputility ~ hn_cwl_rc]$ git add  .\n[root@ncputility ~ hn_cwl_rc]$ git commit -m \"external-syslog_clusterlogforwarder\"\n[main 65680b3] external-syslog_clusterlogforwarder\n Committer: root &lt;root@ncputility.panclyphub01.mnc020.mcc714&gt;\nYour name and email address were configured automatically based\non your username and hostname. Please check that they are accurate.\nYou can suppress this message by setting them explicitly:\n\n    git config --global user.name \"Your Name\"\n    git config --global user.email you@example.com\n\nAfter doing this, you may fix the identity used for this commit with:\n\n    git commit --amend --reset-author\n\n 4 files changed, 39 insertions(+), 3 deletions(-)\n create mode 100644 CWL_CLUSTER/site-policies/sites/hub/source-crs/metallb/ncp-metallb-oam-pa-pa-bgp-peer-loopback.yaml\n create mode 100644 CWL_CLUSTER/site-policies/sites/hub/source-crs/metallb/ncp-metallb-static-routes-bgpadvertisement-loopback.yaml\n[root@ncputility ~ hn_cwl_rc]$ git push\nUsername for 'https://gitlab.apps.panclyphub01.mnc020.mcc714': ncpadmin\nPassword for 'https://ncpadmin@gitlab.apps.panclyphub01.mnc020.mcc714':\nEnumerating objects: 23, done.\nCounting objects: 100% (23/23), done.\nDelta compression using up to 128 threads\nCompressing objects: 100% (13/13), done.\nWriting objects: 100% (13/13), 1.70 KiB | 1.70 MiB/s, done.\nTotal 13 (delta 8), reused 0 (delta 0), pack-reused 0\nTo https://gitlab.apps.panclyphub01.mnc020.mcc714/ncpadmin/hnevopcwl01.git\n   abbe4fa..65680b3  main -&gt; main\n[root@ncputility ~ hn_cwl_rc]$\n</code></pre> <p>3) Access the argocd console and sync it. </p> <p>4) apply the CGU on the hub cluster and wait for it complete. </p>"},{"location":"openshift/backup-restore/ACM-localbackup/","title":"ACM Local Backup","text":""},{"location":"openshift/backup-restore/ACM-localbackup/#backup-hub-clusters-with-red-hat-advanced-cluster-management-for-kubernetes","title":"Backup Hub Clusters with Red Hat Advanced Cluster Management for Kubernetes","text":""},{"location":"openshift/backup-restore/ACM-localbackup/#redhat-acm-introduction","title":"Redhat ACM introduction","text":"<p>Red Hat Advanced Cluster Management for Kubernetes (RHACM) defines two main types of clusters: hub clusters and managed clusters. </p> <ul> <li>The hub cluster is the main cluster with RHACM installed on it. You can create, manage, and monitor other Kubernetes clusters with the hub cluster.</li> <li>The managed clusters are Kubernetes clusters that are managed by the hub cluster. You can create some clusters by using the RHACM hub cluster, and you can also import existing clusters to be managed by the hub cluster.</li> </ul>"},{"location":"openshift/backup-restore/ACM-localbackup/#prerequisites","title":"Prerequisites","text":"<p>MultiClusterHub resource is created and displays the status of Running; the MultiClusterHub resource is automatically created when you install the RHACM Operator</p> <p>The cluster backup and restore operator chart is not installed automatically. Enable the cluster-backup operator on the hub cluster. Edit the MultiClusterHub resource and set the cluster-backup to true. This installs the OADP operator in the same namespace with the backup chart. </p>"},{"location":"openshift/backup-restore/ACM-localbackup/#how-it-works","title":"How it works","text":"<p>The cluster backup and restore operator runs on the hub cluster and depends on the OADP Operator to create a connection to a backup storage location on the hub cluster. The OADP Operator also installs Velero, which is the component used to backup and restore user created hub resources.</p> <p>The cluster backup and restore operator is installed using the cluster-backup-chart file. The cluster backup and restore operator chart is not installed automatically. Starting with RHACM version 2.5, the cluster backup and restore operator chart is installed by setting the cluster-backup option to true on the MultiClusterHub resource.</p> <p>The cluster backup and restore operator chart automatically installs the OADP Operator in the same namespace with the backup chart. If you have previously installed and used the OADP Operator on your hub cluster, you should uninstall the version since the backup chart works now with the operator that is installed in the chart namespace. This should not affect your old backups and previous work. Just use the same storage location for the DataProtectionApplication resource, which is owned by the OADP Operator and installed with the backup chart; you should have access to the same backup data as the previous operator. The only difference is that Velero backup resources are now loaded in the new OADP Operator namespace on your hub cluster.</p>"},{"location":"openshift/backup-restore/ACM-localbackup/#implementation","title":"Implementation","text":"<p>1) You need to be logged in with a user who has cluster-admin privileges:</p> <pre><code>[root@ncputility ~ pancwl_rc]$ source /root/panhubrc\nWARNING: Using insecure TLS client config. Setting this option is not supported!\n\nLogin successful.\n\nYou have access to 103 projects, the list has been suppressed. You can list all projects with 'oc projects'\n\nUsing project \"panclypcwl01\".\n[root@ncputility ~ panhub_rc]$\n</code></pre> <p>2) Add a new annotation to the <code>multiclusterhub</code> CR and enable <code>cluster-backup</code>for triggering to deploy the OADP</p> <p>Active hub</p> <pre><code>[root@ncputility ~ panhub_rc]$ oc edit multiclusterhubs.operator.open-cluster-management.io -n open-cluster-management multiclusterhub\nmulticlusterhub.operator.open-cluster-management.io/multiclusterhub edited\n[root@ncputility ~ panhub_rc]$ \n</code></pre> <p>2.1) sample syntax are attached here</p> <pre><code>oc edit multiclusterhubs.operator.open-cluster-management.io -n open-cluster-management multiclusterhub\n\napiVersion: operator.open-cluster-management.io/v1\nkind: MultiClusterHub\nmetadata:\n  annotations:\n    installer.open-cluster-management.io/oadp-subscription-spec: '{\"source\": \"cs-redhat-operator-index-acm-oadp-1-4-0\"}'\n\ncluster-backup\nenabled: true\n</code></pre> <p>On the Hub cluster, the value of source has to be the name of the catalogsource of the Infra manager node's Quay that is pointing to the organization where the operators of 24.7 were mirrored.</p> <p>3) Create BucketClass</p> <p>passive cluster or CWL cluster. </p> <pre><code>[root@ncputility ~ pancwl_rc]$ cat bucketclass-noobaa-default-backing-store.yaml\napiVersion: noobaa.io/v1alpha1\nkind: BucketClass\nmetadata:\n  name: bucketclass-noobaa-default-backing-store\n  namespace: openshift-storage\nspec:\n  placementPolicy:\n    tiers:\n    - backingStores:\n      - noobaa-default-backing-store\n      placement: Spread\n[root@ncputility ~ pancwl_rc]$\n</code></pre> <p>oc apply -f bucketclass-noobaa-default-backing-store.yaml</p> <p>4) Create Object bucket</p> <p>passive cluster or CWL cluster. </p> <pre><code>[root@ncputility ~ pancwl_rc]$ cat ObjectBucketClaim.yaml\napiVersion: objectbucket.io/v1alpha1\nkind: ObjectBucketClaim\nmetadata:\n  name: acm-backups\n  namespace: openshift-storage\nspec:\n  generateBucketName: acm-backups\n  storageClassName: openshift-storage.noobaa.io\n  additionalConfig:\n      bucketclass: bucketclass-noobaa-default-backing-store\n[root@ncputility ~ pancwl_rc]$\n\n# spec refers to the BucketClass created earlier.\n</code></pre> <p>oc apply -f ObjectBucketClaim.yaml</p> <p>a) Use the following commands to gather values for later steps</p> <pre><code>BUCKET_NAME=$(oc get -n openshift-storage configmap acm-backups -o jsonpath='{.data.BUCKET_NAME}')\nACCESS_KEY_ID=$(oc get -n openshift-storage secret acm-backups -o jsonpath='{.data.AWS_ACCESS_KEY_ID}' | base64 -d)\nSECRET_ACCESS_KEY=$(oc get -n openshift-storage secret acm-backups -o jsonpath='{.data.AWS_SECRET_ACCESS_KEY}' | base64 -d)\n\necho \"BUCKET_NAME=$BUCKET_NAME\"\necho \"ACCESS_KEY_ID=$ACCESS_KEY_ID\"\necho \"SECRET_ACCESS_KEY=$SECRET_ACCESS_KEY\"\n</code></pre> <p>5) Create secret.txt file</p> <pre><code>[root@ncputility ~ pancwl_rc]$ cat secret.txt\n[default]\naws_access_key_id = hOJGxRsZDKOaBgRRXqf5\naws_secret_access_key = KuOAAqyTDM434if+EM16Ajp7Le66MN1+KirdzXKt\n[root@ncputility ~ pancwl_rc]$\n\n#aws_access_key_id and aws_secret_access_key provide access to Object bucket.\n</code></pre> <p>6) Apply the following command on secret.txt</p> <pre><code>base64 -w 0 secret.txt\n</code></pre> <p>Expected output: </p> <pre><code>W2RlZmF1bHRdCmF3c19hY2Nlc3Nfa2V5X2lkID0gWkQ3YnJrRWRUa0lheTBwVUphVlMKYXdzX3NlY3JldF9hY2Nlc3Nfa2V5ID0gZmlvZ0JYek40elpES1lBNVR6cHcrWUhTM3FEeWYwa0tJWlNyekt0agoK\n</code></pre> <p>7) Create secret.yaml file</p> <p>both clusters: </p> <pre><code>[root@ncputility ~ pancwl_rc]$ cat secret.yaml\napiVersion: v1\ndata:\n  cloud: W2RlZmF1bHRdCmF3c19hY2Nlc3Nfa2V5X2lkID0gaE9KR3hSc1pES09hQmdSUlhxZjUKYXdzX3NlY3JldF9hY2Nlc3Nfa2V5ID0gS3VPQUFxeVRETTQzNGlmK0VNMTZBanA3TGU2Nk1OMStLaXJkelhLdAo=\nkind: Secret\nmetadata:\n  name: cloud-credentials\n  namespace: open-cluster-management-backup\ntype: Opaque\n[root@ncputility ~ pancwl_rc]$\n\n\n#The value of cloud under data block is the base64 encoded output of the secret.txt.\n</code></pre> <p>8) Apply secret.yaml </p> <pre><code>[root@dom14npv101-infra-manager ~ hubrc]# oc apply -f secret.yaml\nsecret/cloud-credentials created\n[root@dom14npv101-infra-manager ~ hubrc]#\n</code></pre> <p>9) Create DataProtectionApplication custom resource</p> <p>9.1) s3 interface can be accessed on the standby cluster via s3Url. The value of s3Url can be checked with the following command:</p> <p>oc get routes -n openshift-storage | grep s3-openshift-storage | awk '{print $2}'</p> <p>9.2) caCert is a root certificate in base64 format of the standby hub cluster's default ingress controller which is used for exposing s3 service. The value of caCert can be checked with the following command:</p> <p>oc get secrets -n openshift-ingress-operator router-ca -o jsonpath=\"{.data['tls.crt']}\"</p> <p>9.3) The value of credential is the name of the secret which was created in secret.yaml. The value of key is the name of the value created insecret.yaml under data block.</p> <pre><code>[root@ncputility ~ pancwl_rc]$ cat DataProtectionApplication.yaml\nkind: DataProtectionApplication\napiVersion: oadp.openshift.io/v1alpha1\nmetadata:\n  name: velero-acm-backup\n  namespace: open-cluster-management-backup\nspec:\n  backupLocations:\n    - velero:\n        config:\n          profile: default\n          region: none\n          s3ForcePathStyle: \"true\"\n          s3Url: \"https://s3-openshift-storage.apps.hnevocphub01.mnc002.mcc708\"\n        default: true\n        provider: aws\n        objectStorage:\n          bucket: acm-backups-3885ebaa-8c8b-4985-b9c7-0c25d631de14\n          prefix: velero\n          caCert: \"LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSURERENDQWZTZ0F3SUJBZ0lCQVRBTkJna3Foa2lHOXcwQkFRc0ZBREFtTVNRd0lnWURWUVFEREJ0cGJtZHkKWlhOekxXOXdaWEpoZEc5eVFERTNORFV4TlRNM09UZ3dIaGNOTWpVd05ESXdNVEkxTmpNM1doY05NamN3TkRJdwpNVEkxTmpNNFdqQW1NU1F3SWdZRFZRUUREQnRwYm1keVpYTnpMVzl3WlhKaGRHOXlRREUzTkRVeE5UTTNPVGd3CmdnRWlNQTBHQ1NxR1NJYjNEUUVCQVFVQUE0SUJEd0F3Z2dFS0FvSUJBUUM4aXhWMDZnM0ozZ01lUE04d0JEVTEKRFdMNWhyVW9DeXh5SThkTFNob2trSDNtdTF1RFNCeEJ2blBQYnZhRUs1aFR1RFhjQ3dSaUVNYmZPcFFjUnZzdApQT2dVSU05T3RhY3lwWVZTeGVhSUNQZnNpMlBzd0VaNElWNWMwZEM1SVVSc2hTTTFXaksxckpvanZYQmE3c0c3CjhpQXRCMnJ0K1FwUkhJRzVSYVpnZVdXQVhrMXVPdWlTazBSLzJ3ajZsbDJmZ01NYklOSGluNXdseU1YQldjOHYKaHJTc3VjR2wrL2pXRVl2QWdXKzFrRW5YRDgvclpqUTN6bExVVTRpdzNvNlBMRFlrKzhjYm5DQnNMQjh2RWlqWQppbFRzQ1N3TjdwUk5CVGNqcWxLYVpqWVZ2Mk9QYi80WjF4TG1aekFxWGRhc2pSRW5XNXcyZGtrZy9RNlgxWmZGCkFnTUJBQUdqUlRCRE1BNEdBMVVkRHdFQi93UUVBd0lDcERBU0JnTlZIUk1CQWY4RUNEQUdBUUgvQWdFQU1CMEcKQTFVZERnUVdCQlRsdFFLZ2NjYjZ4bE5BZUt5SFpjY1BZT1A1b0RBTkJna3Foa2lHOXcwQkFRc0ZBQU9DQVFFQQpnZDYyWVJkRHhNTm5HcCtDWStRaXBZRlMxMkNRMUVwVXcwMEVIUno0b2k0UG5PQzcrazlGbHVZeW9vWWNZa1QyCkNGTGp4K3hOODE2bkFVRWFMQ25Ia2NjK2JCSlhuNmU0LzFqWUZGdFFjRnhJeEZVdXByb3dNY21ZeGxBbHlBQ1oKZU5ac1V1Tk4xWVh1aDl2Z3MrRVE2a1VNSjFYSVNPeVp3RDNIbVZSaW16RHN6MUo4MDRLSlhqVnRURlNEbXR1WQpBbGxLWnF2czdqWGJWTUlxY1VtUmFQclluRm8xRDBXcFFGMnFLRGtUL1RDdnFuS3ROR2x5dHVCZmp1YittL0U0Ck1yOExTdXFjTGlhcGhFQS9BM3pwNHNaVWxVbmVCaksrRnkzTkwwdnprcjZxZGVjbG1YSHU5bzdtMnV4UlBKR2UKTm9vb240cmdYU25SSnB5VForcEhyUT09Ci0tLS0tRU5EIENFUlRJRklDQVRFLS0tLS0K\"\n        credential:\n          key: cloud\n          name: cloud-credentials\n  configuration:\n    nodeAgent:\n      enable: true\n      uploaderType: restic\n    velero:\n      featureFlags:\n        - EnableCSI\n      defaultPlugins:\n        - openshift\n        - aws\n        - csi\n[root@ncputility ~ pancwl_rc]$\n\n\n#bucket is the value checked earlier with BUCKET_NAME command.\n</code></pre> <p><code>[root@dom14npv101-infra-manager ~ hubrc]# oc apply -f DataProtectionApplication.yaml dataprotectionapplication.oadp.openshift.io/velero-acm-backup created [root@dom14npv101-infra-manager ~ hubrc]#</code></p> <p>10) validate that resouce creation is completed successfully. </p> <pre><code>[root@dom14npv101-infra-manager ~ hubrc]# oc get backupstoragelocations.velero.io -n open-cluster-management-backup\nNAME                  PHASE       LAST VALIDATED   AGE   DEFAULT\nvelero-acm-backup-1   Available   22s              33s   true\n[root@dom14npv101-infra-manager ~ hubrc]#\n</code></pre> <p>11) Check the resources that are application projects from GitOps</p> <p>hub cluster: </p> <pre><code>[root@dom14npv101-infra-manager ~ hubrc]# oc get appprojects.argoproj.io -n openshift-gitops | grep -v default\nNAME                  AGE\nsite-config-project   59d\nsite-policy-project   59d\n</code></pre> <p>12) Add label velero.io/exclude-from-backup: \"true\" manually on both Hub clusters</p> <pre><code>[root@dom14npv101-infra-manager ~ hubrc]# oc get appprojects.argoproj.io -n openshift-gitops -l velero.io/exclude-from-backup\nNAME                  AGE\nsite-config-project   59d\nsite-policy-project   59d\n</code></pre> <p>13) Add label velero.io/exclude-from-backup: \"true\" manually on both Hub clusters</p> <pre><code>[root@dom14npv101-infra-manager ~ hubrc]# oc get applications.argoproj.io -n openshift-gitops -l velero.io/exclude-from-backup\nNAME                        SYNC STATUS   HEALTH STATUS\nncpvnpvlab1-site-configs    Synced        Healthy\nncpvnpvlab1-site-policies   Synced        Healthy\nncpvnpvmgt-site-configs     Synced        Healthy\nncpvnpvmgt-site-policies    Synced        Healthy\n[root@dom14npv101-infra-manager ~ hubrc]#\n</code></pre> <p>14) A backup is made everyday at 10 PM.</p> <pre><code>[root@dom14npv101-infra-manager ~ hubrc]# cat &gt; backup.yaml\napiVersion: cluster.open-cluster-management.io/v1beta1\nkind: BackupSchedule\nmetadata:\n  name: schedule-acm\n  namespace: open-cluster-management-backup\nspec:\n  veleroSchedule: 0 22 * * *\n  veleroTtl: 120h\n^C\n[root@dom14npv101-infra-manager ~ hubrc]# date\nThu May  1 06:04:49 PM UTC 2025\n[root@dom14npv101-infra-manager ~ hubrc]# vi backup.yaml\n[root@dom14npv101-infra-manager ~ hubrc]# oc apply  -f  backup.yaml\nbackupschedule.cluster.open-cluster-management.io/schedule-acm created\n[root@dom14npv101-infra-manager ~ hubrc]# \n</code></pre>"},{"location":"openshift/backup-restore/ACM-localbackup/#status-of-backup-job-here","title":"Status of backup job here:","text":"<p>1) checking the status of the backup </p> <pre><code>[root@dom14npv101-infra-manager ~ hubrc]# oc get backupstoragelocations.velero.io -A\nNAMESPACE                        NAME                  PHASE       LAST VALIDATED   AGE     DEFAULT\nopen-cluster-management-backup   velero-acm-backup-1   Available   4s               5m15s   true\n[root@dom14npv101-infra-manager ~ hubrc]# \n</code></pre> <p>2) describe the status of the output job </p> <pre><code>[root@dom14npv101-infra-manager ~ hubrc]#  oc describe backupstoragelocations.velero.io velero-acm-backup-1 -n open-cluster-management-backup\nName:         velero-acm-backup-1\nNamespace:    open-cluster-management-backup\nLabels:       app.kubernetes.io/component=bsl\n              app.kubernetes.io/instance=velero-acm-backup-1\n              app.kubernetes.io/managed-by=oadp-operator\n              app.kubernetes.io/name=oadp-operator-velero\n              openshift.io/oadp=True\n              openshift.io/oadp-registry=True\nAnnotations:  &lt;none&gt;\nAPI Version:  velero.io/v1\nKind:         BackupStorageLocation\nMetadata:\n  Creation Timestamp:  2025-05-01T18:02:31Z\n  Generation:          13\n  Owner References:\n    API Version:           oadp.openshift.io/v1alpha1\n    Block Owner Deletion:  true\n    Controller:            true\n    Kind:                  DataProtectionApplication\n    Name:                  velero-acm-backup\n    UID:                   89734e12-0144-4376-954d-cb13cde17515\n  Resource Version:        142419914\n  UID:                     52b6b341-5e36-4e02-bc98-15622be65673\nSpec:\n  Config:\n    Checksum Algorithm:\n    Profile:             default\n    Region:              none\n    s3ForcePathStyle:    true\n    s3Url:               https://s3-openshift-storage.apps.ncpvnpvmgt.pnwlab.nsn-rdnet.net\n  Credential:\n    Key:    cloud\n    Name:   cloud-credentials\n  Default:  true\n  Object Storage:\n    Bucket:   acm-backups-bcf1990d-0846-4a6d-8518-67db81ebee63\n    Ca Cert:  LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSURERENDQWZTZ0F3SUJBZ0lCQVRBTkJna3Foa2lHOXcwQkFRc0ZBREFtTVNRd0lnWURWUVFEREJ0cGJtZHkKWlhOekxXOXdaWEpoZEc5eVFERTNOREV4TXpJek16RXdIaGNOTWpVd016QTBNak0xTWpF        d1doY05NamN3TXpBMApNak0xTWpFeFdqQW1NU1F3SWdZRFZRUUREQnRwYm1keVpYTnpMVzl3WlhKaGRHOXlRREUzTkRFeE16SXpNekV3CmdnRWlNQTBHQ1NxR1NJYjNEUUVCQVFVQUE0SUJEd0F3Z2dFS0FvSUJBUURLOUhrUkd4VWNPRnZjcHVZc1dLdWQKd2ltMFBWc3B3VU        1LNXgyejhBQWJ1eGhuOTRFWThWbk9tSkJUVkhqRnIvbmI0SjRzYldzN3JBbXZINWpjbTdzbApMYW1wWU1uUWlSamtrdE5FSVV3Rm9LVm9VU3ZRSWtiTHJlZk1hcjJYSENOOEU0dHVHeXA3U1c1YjBMRjc4cUY5CktIY0ttMXBDVi9NR3lyU1RtMkx0SmtBcnM5d0lsL0ZPYmox        UEcvUmsvQThtRHZhalBmSUVGbU8yMHduWEQ5bWcKZDVIVk1ZVzkyWWRWVDZPR0FWMEZUNCtJNzNibEdyK3pqQUJzMklxTnUzQ3h0cHlucXMvVVV6RGFodGRvc2ZxSgpWeHhaTUFaZlRENTk0UUtzMFZhamR2aTU1Z1pPejVBQXRLSG96Rm85TWk5dklXblpwR3Z0T2xINnIyVE        ZmSytmCkFnTUJBQUdqUlRCRE1BNEdBMVVkRHdFQi93UUVBd0lDcERBU0JnTlZIUk1CQWY4RUNEQUdBUUgvQWdFQU1CMEcKQTFVZERnUVdCQlRJNVRqeVp2aUJ1NE0wNzd6Mk9PY0lDRmkwMURBTkJna3Foa2lHOXcwQkFRc0ZBQU9DQVFFQQpuNG96eWZha0sySUFqb3dFSlZE        bFNMMlp4YVJuWmJFcjVLanhJbjhiQ0tjaUdBK0h2UURuY0UzK1BzSTJDZGxpCjhXQlR4ZnJ3aFoxTVZ2YjVySmlsTXpZUklQamJaanUrbitaNlB0SGJYMEZDb2Q0elpaYkZBOFQvNlZXUkJSSmUKL3VMaXU3VVRENjJQRDgydVlNSmJFTDNTa1V6b1U5T2NXMSt1S1R3UG56NG        VFblVvNzVnbVlUWnBybkhKSEttNAowUVljTlF6RndHR3JnQnNuTy8wRW94Z2Roa09keENlWFBqRUpURnZXRTdpRjhYWTlKQVZKMVpCcStuZUNoMmRCCkowRGRkaGZjMDhtUnpHbStkVXRyeCtZMnRoSXBxWVUreTN1WDZaM09TcDVNeFIvQzMvelRGR2pqK3pKUmNIOTIKUmtH        c2V5bGNCYXpYbmVvWXgzdEVxUT09Ci0tLS0tRU5EIENFUlRJRklDQVRFLS0tLS0K\n    Prefix:   velero\n  Provider:   aws\nStatus:\n  Last Synced Time:      2025-05-01T18:07:42Z\n  Last Validation Time:  2025-05-01T18:07:42Z\n  Phase:                 Available\nEvents:\n  Type    Reason                           Age    From            Message\n  ----    ------                           ----   ----            -------\n  Normal  BackupStorageLocationReconciled  5m33s  DPA-controller  performed created on backupstoragelocation open-cluster-management-backup/velero-acm-backup-1\n</code></pre>"},{"location":"openshift/backup-restore/ACM-localbackup/#access-the-backup-jobs","title":"Access the backup jobs","text":"<p>1) status of open-cluster-manager pods here</p> <pre><code>[root@dom14npv101-infra-manager ~ hubrc]# oc get pods -n  open-cluster-management-backup\nNAME                                                 READY   STATUS    RESTARTS   AGE\ncluster-backup-chart-clusterbackup-698656f7f-cqxxj   1/1     Running   0          24d\ncluster-backup-chart-clusterbackup-698656f7f-dvdz8   1/1     Running   0          24d\nnode-agent-brq4q                                     1/1     Running   0          6m6s\nnode-agent-j9xm8                                     1/1     Running   0          6m6s\nnode-agent-mlrwk                                     1/1     Running   0          6m6s\nnode-agent-nkb7v                                     1/1     Running   0          6m6s\nnode-agent-sgqxw                                     1/1     Running   0          6m6s\nopenshift-adp-controller-manager-74f799649f-v2slw    1/1     Running   0          24d\nvelero-549fbfb95d-s966g                              1/1     Running   0          6m6s\n[root@dom14npv101-infra-manager ~ hubrc]# \n</code></pre> <p>2) status of job from velero binary</p> <pre><code>[root@dom14npv101-infra-manager ~ hubrc]# oc -n open-cluster-management-backup exec -it velero-549fbfb95d-s966g -- ./velero backup-location get\nDefaulted container \"velero\" out of: velero, openshift-velero-plugin (init), velero-plugin-for-aws (init)\nNAME                  PROVIDER   BUCKET/PREFIX                                             PHASE       LAST VALIDATED                  ACCESS MODE   DEFAULT\nvelero-acm-backup-1   aws        acm-backups-bcf1990d-0846-4a6d-8518-67db81ebee63/velero   Available   2025-05-01 18:08:42 +0000 UTC   ReadWrite     true\n[root@dom14npv101-infra-manager ~ hubrc]#\n</code></pre> <p>3) Look at the final status of the backup jobs</p> <pre><code> [root@dom14npv101-infra-manager ~ hubrc]# oc -n open-cluster-management-backup exec -it velero-549fbfb95d-s966g -- ./velero backup get\nDefaulted container \"velero\" out of: velero, openshift-velero-plugin (init), velero-plugin-for-aws (init)\nNAME                                            STATUS      ERRORS   WARNINGS   CREATED                         EXPIRES   STORAGE LOCATION      SELECTOR\nacm-credentials-schedule-20250501180616         Completed   0        0          2025-05-01 18:06:17 +0000 UTC   4d        velero-acm-backup-1   &lt;none&gt;\nacm-managed-clusters-schedule-20250501180616    Completed   0        0          2025-05-01 18:06:18 +0000 UTC   4d        velero-acm-backup-1   &lt;none&gt;\nacm-resources-generic-schedule-20250501180616   Completed   0        0          2025-05-01 18:06:20 +0000 UTC   4d        velero-acm-backup-1   cluster.open-cluster-management.io/backup\nacm-resources-schedule-20250501180616           Completed   0        0          2025-05-01 18:06:29 +0000 UTC   4d        velero-acm-backup-1   !cluster.open-cluster-management.io/backup,!policy.open-cluste        r-management.io/root-policy\nacm-validation-policy-schedule-20250501180616   Completed   0        0          2025-05-01 18:06:30 +0000 UTC   1d        velero-acm-backup-1   &lt;none&gt;\n[root@dom14npv101-infra-manager ~ hubrc]# date\nThu May  1 06:09:39 PM UTC 2025\n[root@dom14npv101-infra-manager ~ hubrc]# \n</code></pre>"},{"location":"openshift/backup-restore/AWS-S3/","title":"AWS S3 Backup","text":"<p>[root@dom14npv101-infra-manager ~ hubrc]# oc -n open-cluster-management-backup exec -it velero-549fbfb95d-s966g -- ./velero backup-location get Defaulted container \"velero\" out of: velero, openshift-velero-plugin (init), velero-plugin-for-aws (init) NAME                  PROVIDER   BUCKET/PREFIX                                             PHASE       LAST VALIDATED                  ACCESS MODE   DEFAULT velero-acm-backup-1   aws        acm-backups-bcf1990d-0846-4a6d-8518-67db81ebee63/velero   Available   2025-05-01 18:20:42 +0000 UTC   ReadWrite     true [root@dom14npv101-infra-manager ~ hubrc]# oc -n open-cluster-management-backup exec -it velero-549fbfb95d-s966g -- ./velero backup get Defaulted container \"velero\" out of: velero, openshift-velero-plugin (init), velero-plugin-for-aws (init) NAME                                            STATUS      ERRORS   WARNINGS   CREATED                         EXPIRES   STORAGE LOCATION      SELECTOR acm-credentials-schedule-20250501180616         Completed   0        0          2025-05-01 18:06:17 +0000 UTC   4d        velero-acm-backup-1    acm-managed-clusters-schedule-20250501180616    Completed   0        0          2025-05-01 18:06:18 +0000 UTC   4d        velero-acm-backup-1    acm-resources-generic-schedule-20250501180616   Completed   0        0          2025-05-01 18:06:20 +0000 UTC   4d        velero-acm-backup-1   cluster.open-cluster-management.io/backup acm-resources-schedule-20250501180616           Completed   0        0          2025-05-01 18:06:29 +0000 UTC   4d        velero-acm-backup-1   !cluster.open-cluster-management.io/backup,!policy.open-cluste        r-management.io/root-policy acm-validation-policy-schedule-20250501180616   Completed   0        0          2025-05-01 18:06:30 +0000 UTC   23h       velero-acm-backup-1    [root@dom14npv101-infra-manager ~ hubrc]# oc -n open-cluster-management-backup exec -it velero-549fbfb95d-s966g -- ./velero backup get Defaulted container \"velero\" out of: velero, openshift-velero-plugin (init), velero-plugin-for-aws (init) NAME                                            STATUS      ERRORS   WARNINGS   CREATED                         EXPIRES   STORAGE LOCATION      SELECTOR acm-credentials-schedule-20250501180616         Completed   0        0          2025-05-01 18:06:17 +0000 UTC   4d        velero-acm-backup-1    acm-managed-clusters-schedule-20250501180616    Completed   0        0          2025-05-01 18:06:18 +0000 UTC   4d        velero-acm-backup-1    acm-resources-generic-schedule-20250501180616   Completed   0        0          2025-05-01 18:06:20 +0000 UTC   4d        velero-acm-backup-1   cluster.open-cluster-management.io/backup acm-resources-schedule-20250501180616           Completed   0        0          2025-05-01 18:06:29 +0000 UTC   4d        velero-acm-backup-1   !cluster.open-cluster-management.io/backup,!policy.open-cluste        r-management.io/root-policy acm-validation-policy-schedule-20250501180616   Completed   0        0          2025-05-01 18:06:30 +0000 UTC   22h       velero-acm-backup-1    [root@dom14npv101-infra-manager ~ hubrc]# curl \"https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip\" -o \"awscliv2.zip\"   % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current                                  Dload  Upload   Total   Spent    Left  Speed 100 67.2M  100 67.2M    0     0  20.2M      0  0:00:03  0:00:03 --:--:-- 20.2M [root@dom14npv101-infra-manager ~ hubrc]# unzip awscliv2.zip Archive:  awscliv2.zip    creating: aws/    creating: aws/dist/   inflating: aws/install   inflating: aws/README.md   inflating: aws/THIRD_PARTY_LICENSES    creating: aws/dist/awscli/    creating: aws/dist/cryptography/    creating: aws/dist/docutils/    creating: aws/dist/lib-dynload/   inflating: aws/dist/aws   inflating: aws/dist/aws_completer   inflating: aws/dist/libpython3.13.so.1.0   inflating: aws/dist/_awscrt.abi3.so   inflating: aws/dist/_cffi_backend.cpython-313-x86_64-linux-gnu.so   inflating: aws/dist/_ruamel_yaml.cpython-313-x86_64-linux-gnu.so   inflating: aws/dist/libz.so.1   inflating: aws/dist/liblzma.so.5   inflating: aws/dist/libbz2.so.1   inflating: aws/dist/libffi.so.6   inflating: aws/dist/libuuid.so.1 <p>[root@dom14npv101-infra-manager ~ hubrc]# sudo ./aws/install You can now run: /usr/local/bin/aws --version [root@dom14npv101-infra-manager ~ hubrc]# aws --version aws-cli/2.27.6 Python/3.13.2 Linux/5.14.0-427.13.1.el9_4.x86_64 exe/x86_64.rhel.9 [root@dom14npv101-infra-manager ~ hubrc]# </p> <p>root@dom14npv101-infra-manager ~ hubrc]# aws configure --profile oadp AWS Access Key ID [None]: I35Rzin4xFf58GaCkJlC AWS Secret Access Key [None]: vaimKLwBnO5glSe+f4AKgwJTtYEMOPMnfon540LZ Default region name [None]: Default output format [None]: [root@dom14npv101-infra-manager ~ hubrc]# aws --endpoint-url $AWS_ENDPOINT_URL s3 ls s3://acm-backups-bcf1990d-0846-4a6d-8518-67db81ebee63/velero/</p> <p>SSL validation failed for https://s3-openshift-storage.apps.ncpvnpvmgt.pnwlab.nsn-rdnet.net:443/acm-backups-bcf1990d-0846-4a6d-8518-67db81ebee63?list-type=2&amp;prefix=velero%2F&amp;delimiter=%2F&amp;encoding-type=url         [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: self signed certificate in certificate chain (_ssl.c:1028)</p> <p>[root@dom14npv101-infra-manager ~ hubrc]# [root@dom14npv101-infra-manager ~ hubrc]# aws --endpoint-url \"$AWS_ENDPOINT_URL\" --no-verify-ssl s3 ls s3://acm-backups-bcf1990d-0846-4a6d-8518-67db81ebee63/velero/ urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 's3-openshift-storage.apps.ncpvnpvmgt.pnwlab.nsn-rdnet.net'. Adding certificate verification is strongl        y advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings                            PRE backups/ [root@dom14npv101-infra-manager ~ hubrc]# aws --endpoint-url \"$AWS_ENDPOINT_URL\" --no-verify-ssl s3 ls s3://acm-backups-bcf1990d-0846-4a6d-8518-67db81ebee63/velero/backups/ urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 's3-openshift-storage.apps.ncpvnpvmgt.pnwlab.nsn-rdnet.net'. Adding certificate verification is strongl        y advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings                            PRE acm-credentials-schedule-20250501180616/                            PRE acm-managed-clusters-schedule-20250501180616/                            PRE acm-resources-generic-schedule-20250501180616/                            PRE acm-resources-schedule-20250501180616/                            PRE acm-validation-policy-schedule-20250501180616/ [root@dom14npv101-infra-manager ~ hubrc]# aws --endpoint-url \"$AWS_ENDPOINT_URL\" --no-verify-ssl s3 ls s3://acm-backups-bcf1990d-0846-4a6d-8518-67db81ebee63/velero/backups/acm-credentials-schedule-202505011        80616/ urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 's3-openshift-storage.apps.ncpvnpvmgt.pnwlab.nsn-rdnet.net'. Adding certificate verification is strongl        y advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings 2025-05-01 18:06:18         29 acm-credentials-schedule-20250501180616-csi-volumesnapshotclasses.json.gz 2025-05-01 18:06:18         27 acm-credentials-schedule-20250501180616-csi-volumesnapshotcontents.json.gz 2025-05-01 18:06:18         29 acm-credentials-schedule-20250501180616-csi-volumesnapshots.json.gz 2025-05-01 18:06:18         27 acm-credentials-schedule-20250501180616-itemoperations.json.gz 2025-05-01 18:06:18      11604 acm-credentials-schedule-20250501180616-logs.gz 2025-05-01 18:06:18         29 acm-credentials-schedule-20250501180616-podvolumebackups.json.gz 2025-05-01 18:06:18        327 acm-credentials-schedule-20250501180616-resource-list.json.gz 2025-05-01 18:06:18         49 acm-credentials-schedule-20250501180616-results.gz 2025-05-01 18:06:18         27 acm-credentials-schedule-20250501180616-volumeinfo.json.gz 2025-05-01 18:06:18         29 acm-credentials-schedule-20250501180616-volumesnapshots.json.gz 2025-05-01 18:06:18      46601 acm-credentials-schedule-20250501180616.tar.gz 2025-05-01 18:06:18       4428 velero-backup.json [root@dom14npv101-infra-manager ~ hubrc]#</p>"},{"location":"openshift/backup-restore/etcd-backup/","title":"Control plane backup","text":""},{"location":"openshift/backup-restore/etcd-backup/#backing-up-etcd","title":"Backing up etcd","text":"<p>etcd is the key-value store for OpenShift Container Platform, which persists the state of all resource objects.</p> <p>Back up your cluster\u2019s etcd data regularly and store in a secure location ideally outside the OpenShift Container Platform environment. Do not take an etcd backup before the first certificate rotation completes, which occurs 24 hours after installation, otherwise the backup will contain expired certificates. It is also recommended to take etcd backups during non-peak usage hours because the etcd snapshot has a high I/O cost.</p> <p>Be sure to take an etcd backup after you upgrade your cluster. This is important because when you restore your cluster, you must use an etcd backup that was taken from the same z-stream release. For example, an OpenShift Container Platform 4.y.z cluster must use an etcd backup that was taken from 4.y.z.</p>"},{"location":"openshift/backup-restore/etcd-backup/#backing-up-etcd-data","title":"Backing up etcd data","text":""},{"location":"openshift/backup-restore/etcd-backup/#prerequisites","title":"Prerequisites","text":"<ul> <li>You have access to the cluster as a user with the cluster-admin role.</li> <li>You have checked whether the cluster-wide proxy is enabled.</li> </ul>"},{"location":"openshift/backup-restore/etcd-backup/#procedure","title":"Procedure","text":"<ol> <li>Start a debug session for a control plane node:</li> </ol> <pre><code>oc debug node/&lt;node_name&gt;\n</code></pre> <ol> <li>Change your root directory to /host:</li> </ol> <pre><code>chroot /host\n</code></pre> <ol> <li> <p>If the cluster-wide proxy is enabled, be sure that you have exported the NO_PROXY, HTTP_PROXY, and HTTPS_PROXY environment variables. (optional)</p> </li> <li> <p>Run the cluster-backup.sh script and pass in the location to save the backup to.</p> </li> </ol> <pre><code>sh-4.4# /usr/local/bin/cluster-backup.sh /home/core/assets/backup\n</code></pre> <p><code>Example script output</code></p> <pre><code>found latest kube-apiserver: /etc/kubernetes/static-pod-resources/kube-apiserver-pod-6\nfound latest kube-controller-manager: /etc/kubernetes/static-pod-resources/kube-controller-manager-pod-7\nfound latest kube-scheduler: /etc/kubernetes/static-pod-resources/kube-scheduler-pod-6\nfound latest etcd: /etc/kubernetes/static-pod-resources/etcd-pod-3\nede95fe6b88b87ba86a03c15e669fb4aa5bf0991c180d3c6895ce72eaade54a1\netcdctl version: 3.4.14\nAPI version: 3.4\n{\"level\":\"info\",\"ts\":1624647639.0188997,\"caller\":\"snapshot/v3_snapshot.go:119\",\"msg\":\"created temporary db file\",\"path\":\"/home/core/assets/backup/snapshot_2021-06-25_190035.db.part\"}\n{\"level\":\"info\",\"ts\":\"2021-06-25T19:00:39.030Z\",\"caller\":\"clientv3/maintenance.go:200\",\"msg\":\"opened snapshot stream; downloading\"}\n{\"level\":\"info\",\"ts\":1624647639.0301006,\"caller\":\"snapshot/v3_snapshot.go:127\",\"msg\":\"fetching snapshot\",\"endpoint\":\"https://10.0.0.5:2379\"}\n{\"level\":\"info\",\"ts\":\"2021-06-25T19:00:40.215Z\",\"caller\":\"clientv3/maintenance.go:208\",\"msg\":\"completed snapshot read; closing\"}\n{\"level\":\"info\",\"ts\":1624647640.6032252,\"caller\":\"snapshot/v3_snapshot.go:142\",\"msg\":\"fetched snapshot\",\"endpoint\":\"https://10.0.0.5:2379\",\"size\":\"114 MB\",\"took\":1.584090459}\n{\"level\":\"info\",\"ts\":1624647640.6047094,\"caller\":\"snapshot/v3_snapshot.go:152\",\"msg\":\"saved\",\"path\":\"/home/core/assets/backup/snapshot_2021-06-25_190035.db\"}\nSnapshot saved at /home/core/assets/backup/snapshot_2021-06-25_190035.db\n{\"hash\":3866667823,\"revision\":31407,\"totalKey\":12828,\"totalSize\":114446336}\nsnapshot db and kube resources are successfully saved to /home/core/assets/backup\n</code></pre>"},{"location":"openshift/backup-restore/etcd-backup/#automated-steps-to-run-and-copy-the-file-locally","title":"automated steps to run and copy the file locally","text":"<pre><code>backupdir=/tmp/backup-etcd/\n\nservername=$(oc get nodes |grep -i master|awk '{print $1}'|head -1)\n\n[ -d $backupdir/${servername} ] || mkdir -p $backupdir/${servername}\n\nexecout=$(oc debug -t node/$servername -- chroot /host bash -c '/usr/local/bin/cluster-backup.sh /home/core/assets/backup') \n# filename=$(echo $execout | awk -F'path\":\"' '{print $2}' | awk -F'\"' '{print $1}')   #&lt;--- not working\nsleep 20\n\noc debug -t node/$servername -- chroot /host bash -c 'chown core:core /home/core/assets/backup/*.db'\n\nscp -rp core@$servername:${filename} $backupdir/${servername}/\n</code></pre>"},{"location":"openshift/backup-restore/etcd-backup/#reference","title":"Reference","text":"<ul> <li>ETCD backup and restore</li> </ul>"},{"location":"openshift/backup-restore/etcd-restore/","title":"Replacing an unhealthy etcd member","text":""},{"location":"openshift/backup-restore/etcd-restore/#reference","title":"Reference","text":"<ul> <li>ETCD backup and restore</li> </ul>"},{"location":"openshift/deployment/readme/","title":"Readme","text":"<p>update</p>"},{"location":"openshift/deployment/automation/nokia-site-config-fixes/","title":"Nokia Automation Tool - Site Config Template Fixes","text":"<p>This document captures the issues and required corrections for the Nokia automation tool used to generate and manage OpenShift site-config templates.</p> <p>Please download the automation 24.7 from NOLS NCP package. this tool is only for cwl cluster based site-config files. </p>"},{"location":"openshift/deployment/automation/nokia-site-config-fixes/#1-site-configresourcekustomizationyaml","title":"1. <code>site-config/resource/kustomization.yaml</code>","text":""},{"location":"openshift/deployment/automation/nokia-site-config-fixes/#issue","title":"Issue:","text":"<p>Incorrect resource reference in the file.</p>"},{"location":"openshift/deployment/automation/nokia-site-config-fixes/#correction","title":"Correction:","text":"<p>Update the resource name to:</p> <pre><code>resources:\n  - ncp-24-7-mp1-release.yaml\n</code></pre>"},{"location":"openshift/deployment/automation/nokia-site-config-fixes/#2-site-configresourcencp-24-7-mp1-releaseyaml","title":"2. <code>site-config/resource/ncp-24-7-mp1-release.yaml</code>","text":""},{"location":"openshift/deployment/automation/nokia-site-config-fixes/#issue_1","title":"Issue:","text":"<p>Incorrect or missing ClusterImageSet name.</p>"},{"location":"openshift/deployment/automation/nokia-site-config-fixes/#correction_1","title":"Correction:","text":"<p>Ensure the following value is set:</p> <pre><code>spec:\n  clusterImageSetRef:\n    name: ncp-24-7-mp1-release\n</code></pre>"},{"location":"openshift/deployment/automation/nokia-site-config-fixes/#3-site-configpre-reqsbmc-credentials-sitenameyaml","title":"3. <code>site-config/pre-reqs/bmc-credentials-sitename.yaml</code>","text":""},{"location":"openshift/deployment/automation/nokia-site-config-fixes/#issue_2","title":"Issue:","text":"<p>base64 encrypt username/password field will be empty.</p>"},{"location":"openshift/deployment/automation/nokia-site-config-fixes/#correction_2","title":"Correction:","text":"<p>Base64 encode the BMC credentials:</p> <pre><code>username: &lt;base64 encoded username&gt;\npassword: &lt;base64 encoded password&gt;\n</code></pre> <p>Use the following command to encode:</p> <pre><code>echo -n 'username' | base64\necho -n 'password' | base64\n</code></pre>"},{"location":"openshift/deployment/automation/nokia-site-config-fixes/#4-site-configpre-reqspull-secret-sitenameyaml","title":"4. <code>site-config/pre-reqs/pull-secret-sitename.yaml</code>","text":""},{"location":"openshift/deployment/automation/nokia-site-config-fixes/#issue_3","title":"Issue:","text":"<p>Hub Quay credentials are not encoded.</p>"},{"location":"openshift/deployment/automation/nokia-site-config-fixes/#correction_3","title":"Correction:","text":"<p>Base64 encode only for user/passwd and the entire pull-secret string text:</p> <pre><code>.data:\n  .dockerconfigjson: &lt;base64 encoded pull secret&gt;\n</code></pre>"},{"location":"openshift/deployment/automation/nokia-site-config-fixes/#5-site-configsite-namesite-configyaml","title":"5. <code>site-config/&lt;site-name&gt;/site-config.yaml</code>","text":""},{"location":"openshift/deployment/automation/nokia-site-config-fixes/#issues","title":"Issues:","text":"<ul> <li>Missing DNS search domain on master nodes, but validate for all other host's too. </li> <li>Third NTP server not configured.</li> <li>Incorrect hostnames. </li> <li><code>tenant-bond-2</code> port mapping incorrect</li> <li>Master subnet incorrectly set to <code>/26</code></li> </ul>"},{"location":"openshift/deployment/automation/nokia-site-config-fixes/#corrections","title":"Corrections:","text":"<ul> <li>Ensure DNS search domain is defined under <code>host manifest at last.</code></li> <li>Add third NTP server if required under <code>ntpServers</code></li> <li>Validate and correct all hostnames to match inventory defined on the LLD. </li> <li>Correct <code>tenant-bond-2</code> interfaces/port assignments</li> <li>Define proper subnet, e.g. <code>/24</code> instead of <code>/26</code> for masters</li> </ul>"},{"location":"openshift/deployment/automation/nokia-site-config-fixes/#6-site-configsite-namestorage-configyaml-and-worker-configyaml","title":"6. <code>site-config/&lt;site-name&gt;/storage-config.yaml</code> and <code>worker-config.yaml</code>","text":""},{"location":"openshift/deployment/automation/nokia-site-config-fixes/#issue_4","title":"Issue:","text":"<p>Configuration not aligned with product line template. if miss to do this step, your deployment will fail inigition file issue.</p>"},{"location":"openshift/deployment/automation/nokia-site-config-fixes/#workaround","title":"Workaround:","text":"<p>Manually copy-paste the file contents from the official product line template.</p>"},{"location":"openshift/deployment/automation/nokia-site-config-fixes/#7-site-configsite-nameextra-manifestschrony-update-masteryaml-chrony-update-workeryaml","title":"7. <code>site-config/&lt;site-name&gt;/extra-manifests/chrony-update-master.yaml</code> &amp; <code>chrony-update-worker.yaml</code>","text":""},{"location":"openshift/deployment/automation/nokia-site-config-fixes/#issue_5","title":"Issue:","text":"<p>Missing or incomplete NTP server list</p>"},{"location":"openshift/deployment/automation/nokia-site-config-fixes/#correction_4","title":"Correction:","text":"<ul> <li>Add the third NTP server IP to the chrony config</li> <li>Base64 encode the full <code>chrony.conf</code> content</li> </ul> <p>Example:</p> <pre><code>spec:\n  config:\n    data:\n      chrony.conf: |\n        &lt;base64 encoded chrony.conf with 3 NTP entries&gt;\n</code></pre> <p>Use the following command to encode:</p> <pre><code>cat chrony.conf | base64 -w0\n</code></pre>"},{"location":"openshift/deployment/automation/nokia-site-config-fixes/#summary-table","title":"Summary Table","text":"File Path Issue Action <code>resource/kustomization.yaml</code> Wrong resource name Use <code>ncp-24-7-mp1-release.yaml</code> <code>ncp-24-7-mp1-release.yaml</code> Missing ClusterImageSet Set <code>ncp-24-7-mp1-release</code> <code>bmc-credentials-sitename.yaml</code> Unencrypted credentials Base64 encode values <code>pull-secret-sitename.yaml</code> Unencrypted secret Base64 encode full JSON <code>site-config.yaml</code> Multiple infra/network issues Fix DNS, NTP, hostnames, subnet, ports <code>storage-config.yaml</code> &amp; <code>worker-config.yaml</code> Config mismatch Copy from official template <code>chrony-update-master.yaml</code>, <code>chrony-update-worker.yaml</code> Missing NTP Add 3rd IP and base64 encode <p>Note: Always validate all YAML files and value before you start the installation.</p>"},{"location":"openshift/disaster-management/hub-cluster-redeployment/","title":"Index","text":""},{"location":"openshift/disaster-management/hub-cluster-redeployment/#backup-hub-clusters-with-red-hat-advanced-cluster-management-for-kubernetes","title":"Backup Hub Clusters with Red Hat Advanced Cluster Management for Kubernetes","text":""},{"location":"openshift/disaster-management/hub-cluster-redeployment/#redhat-acm-introduction","title":"Redhat ACM introduction","text":"<p>Red Hat Advanced Cluster Management for Kubernetes (RHACM) defines two main types of clusters: hub clusters and managed clusters. </p> <ul> <li>The hub cluster is the main cluster with RHACM installed on it. You can create, manage, and monitor other Kubernetes clusters with the hub cluster.</li> <li>The managed clusters are Kubernetes clusters that are managed by the hub cluster. You can create some clusters by using the RHACM hub cluster, and you can also import existing clusters to be managed by the hub cluster.</li> </ul>"},{"location":"openshift/disaster-management/hub-cluster-redeployment/#prerequisites","title":"Prerequisites","text":"<p>MultiClusterHub resource is created and displays the status of Running; the MultiClusterHub resource is automatically created when you install the RHACM Operator</p> <p>The cluster backup and restore operator chart is not installed automatically. Enable the cluster-backup operator on the hub cluster. Edit the MultiClusterHub resource and set the cluster-backup to true. This installs the OADP operator in the same namespace with the backup chart. </p>"},{"location":"openshift/disaster-management/hub-cluster-redeployment/#how-it-works","title":"How it works","text":"<p>The cluster backup and restore operator runs on the hub cluster and depends on the OADP Operator to create a connection to a backup storage location on the hub cluster. The OADP Operator also installs Velero, which is the component used to backup and restore user created hub resources.</p> <p>The cluster backup and restore operator is installed using the cluster-backup-chart file. The cluster backup and restore operator chart is not installed automatically. Starting with RHACM version 2.5, the cluster backup and restore operator chart is installed by setting the cluster-backup option to true on the MultiClusterHub resource.</p> <p>The cluster backup and restore operator chart automatically installs the OADP Operator in the same namespace with the backup chart. If you have previously installed and used the OADP Operator on your hub cluster, you should uninstall the version since the backup chart works now with the operator that is installed in the chart namespace. This should not affect your old backups and previous work. Just use the same storage location for the DataProtectionApplication resource, which is owned by the OADP Operator and installed with the backup chart; you should have access to the same backup data as the previous operator. The only difference is that Velero backup resources are now loaded in the new OADP Operator namespace on your hub cluster.</p>"},{"location":"openshift/disaster-management/hub-cluster-redeployment/#highlevel-steps","title":"Highlevel steps","text":""},{"location":"openshift/disaster-management/hub-cluster-redeployment/#backup-preparation","title":"Backup Preparation:","text":"<p>ACM Backup: Configure ACM backup of the hub cluster to the CWL cluster via S3 bucket. However, we need Red Hat to confirm the recommended validation steps to ensure the backup is complete and sufficient for restoration.</p> <p>Argo CD Configuration: Back up Argo CD configurations, including Application Projects and Application tiles. We're currently relying on YAML-based backups\u2014can Red Hat confirm if this approach is sufficient or recommend any additional steps?</p> <p>Git Server: Backup of Git server repositories and configurations will be handled by Nokia. We\u2019ll follow up on this separately.</p> <p>All Operator installation Manifests and Configs: We have saved all operator installation and configuration YAML files and plan to reuse them during redeployment. Would appreciate any feedback or recommendations from Red Hat on this.</p> <p>Quay Registry: We'll document and retain all relevant details including organization structure, usernames, and passwords.</p> <p>Ingress Certificates: We believe no additional action is needed for ingress certificate backup but would like Red Hat to confirm if there's anything required here.</p>"},{"location":"openshift/disaster-management/hub-cluster-redeployment/#redeployment-steps","title":"Redeployment Steps:","text":"<p>Cleanup the system by format all OS and ceph OSD drives before shutdown.</p> <p>Redeploy the hub cluster from scratch.</p> <p>Install and configure all required operators. Using old yaml files.</p> <p>Upload and configure Quay, recreating the organization name, username, and password as per the previous setup.</p> <p>Rebuild the Git server and restore the repositories. (Restoration, will be taken care by nokia.)</p> <p>Restore ACM backup to the new hub cluster. Since Nokia has not provided the steps for this, we would appreciate guidance from Red Hat.</p> <p>Restore GitOps (Argo CD) configurations from backup, including Application Projects and tiles for site-config and site-policies.</p> <p>We assume this will not trigger automatic reinstallation of CWL clusters : Redhat to confirm.</p>"},{"location":"openshift/disaster-management/hub-cluster-redeployment/#post-restoration-validation","title":"Post-Restoration Validation:","text":"<p>Verify cluster and application health across all environments. Make changes to site policies to validate propagation and reconciliation. Perform scale-in/scale-out tests to ensure proper functionality.</p>"},{"location":"openshift/disaster-management/hub-cluster-redeployment/#implementation","title":"Implementation","text":"<p>1) You need to be logged in with a user who has cluster-admin privileges:</p> <pre><code>[root@ncputility ~ pancwl_rc]$ source /root/panhubrc\nWARNING: Using insecure TLS client config. Setting this option is not supported!\n\nLogin successful.\n\nYou have access to 103 projects, the list has been suppressed. You can list all projects with 'oc projects'\n\nUsing project \"panclypcwl01\".\n[root@ncputility ~ panhub_rc]$\n</code></pre> <p>2) Add a new annotation to the <code>multiclusterhub</code> CR and enable <code>cluster-backup</code>for triggering to deploy the OADP</p> <p>Active hub</p> <pre><code>[root@ncputility ~ panhub_rc]$ oc edit multiclusterhubs.operator.open-cluster-management.io -n open-cluster-management multiclusterhub\nmulticlusterhub.operator.open-cluster-management.io/multiclusterhub edited\n[root@ncputility ~ panhub_rc]$ \n</code></pre> <p>2.1) sample syntax are attached here</p> <pre><code>oc edit multiclusterhubs.operator.open-cluster-management.io -n open-cluster-management multiclusterhub\n\napiVersion: operator.open-cluster-management.io/v1\nkind: MultiClusterHub\nmetadata:\n  annotations:\n    installer.open-cluster-management.io/oadp-subscription-spec: '{\"source\": \"cs-redhat-operator-index-acm-oadp-1-4-0\"}'\n\ncluster-backup\nenabled: true\n</code></pre> <p>On the Hub cluster, the value of source has to be the name of the catalogsource of the Infra manager node's Quay that is pointing to the organization where the operators of 24.7 were mirrored.</p> <p>3) Create BucketClass</p> <p>passive cluster or CWL cluster. </p> <pre><code>[root@ncputility ~ pancwl_rc]$ cat bucketclass-noobaa-default-backing-store.yaml\napiVersion: noobaa.io/v1alpha1\nkind: BucketClass\nmetadata:\n  name: bucketclass-noobaa-default-backing-store\n  namespace: openshift-storage\nspec:\n  placementPolicy:\n    tiers:\n    - backingStores:\n      - noobaa-default-backing-store\n      placement: Spread\n[root@ncputility ~ pancwl_rc]$\n</code></pre> <p>oc apply -f bucketclass-noobaa-default-backing-store.yaml</p> <p>4) Create Object bucket</p> <p>passive cluster or CWL cluster. </p> <pre><code>[root@ncputility ~ pancwl_rc]$ cat ObjectBucketClaim.yaml\napiVersion: objectbucket.io/v1alpha1\nkind: ObjectBucketClaim\nmetadata:\n  name: acm-backups\n  namespace: openshift-storage\nspec:\n  generateBucketName: acm-backups\n  storageClassName: openshift-storage.noobaa.io\n  additionalConfig:\n      bucketclass: bucketclass-noobaa-default-backing-store\n[root@ncputility ~ pancwl_rc]$\n\n# spec refers to the BucketClass created earlier.\n</code></pre> <p>oc apply -f ObjectBucketClaim.yaml</p> <p>a) Use the following commands to gather values for later steps</p> <pre><code>BUCKET_NAME=$(oc get -n openshift-storage configmap acm-backups -o jsonpath='{.data.BUCKET_NAME}')\nACCESS_KEY_ID=$(oc get -n openshift-storage secret acm-backups -o jsonpath='{.data.AWS_ACCESS_KEY_ID}' | base64 -d)\nSECRET_ACCESS_KEY=$(oc get -n openshift-storage secret acm-backups -o jsonpath='{.data.AWS_SECRET_ACCESS_KEY}' | base64 -d)\n\necho \"BUCKET_NAME=$BUCKET_NAME\"\necho \"ACCESS_KEY_ID=$ACCESS_KEY_ID\"\necho \"SECRET_ACCESS_KEY=$SECRET_ACCESS_KEY\"\n</code></pre> <p>5) Create secret.txt file</p> <pre><code>[root@ncputility ~ pancwl_rc]$ cat secret.txt\n[default]\naws_access_key_id = hOJGxRsZDKOaBgRRXqf5\naws_secret_access_key = KuOAAqyTDM434if+EM16Ajp7Le66MN1+KirdzXKt\n[root@ncputility ~ pancwl_rc]$\n\n#aws_access_key_id and aws_secret_access_key provide access to Object bucket.\n</code></pre> <p>6) Apply the following command on secret.txt</p> <pre><code>base64 -w 0 secret.txt\n</code></pre> <p>Expected output: </p> <pre><code>W2RlZmF1bHRdCmF3c19hY2Nlc3Nfa2V5X2lkID0gWkQ3YnJrRWRUa0lheTBwVUphVlMKYXdzX3NlY3JldF9hY2Nlc3Nfa2V5ID0gZmlvZ0JYek40elpES1lBNVR6cHcrWUhTM3FEeWYwa0tJWlNyekt0agoK\n</code></pre> <p>7) Create secret.yaml file</p> <p>both clusters: </p> <pre><code>[root@ncputility ~ pancwl_rc]$ cat secret.yaml\napiVersion: v1\ndata:\n  cloud: W2RlZmF1bHRdCmF3c19hY2Nlc3Nfa2V5X2lkID0gaE9KR3hSc1pES09hQmdSUlhxZjUKYXdzX3NlY3JldF9hY2Nlc3Nfa2V5ID0gS3VPQUFxeVRETTQzNGlmK0VNMTZBanA3TGU2Nk1OMStLaXJkelhLdAo=\nkind: Secret\nmetadata:\n  name: cloud-credentials\n  namespace: open-cluster-management-backup\ntype: Opaque\n[root@ncputility ~ pancwl_rc]$\n\n\n#The value of cloud under data block is the base64 encoded output of the secret.txt.\n</code></pre> <p>8) Apply secret.yaml </p> <pre><code>[root@dom14npv101-infra-manager ~ hubrc]# oc apply -f secret.yaml\nsecret/cloud-credentials created\n[root@dom14npv101-infra-manager ~ hubrc]#\n</code></pre> <p>9) Create DataProtectionApplication custom resource</p> <p>9.1) s3 interface can be accessed on the standby cluster via s3Url. The value of s3Url can be checked with the following command:</p> <p>oc get routes -n openshift-storage | grep s3-openshift-storage | awk '{print $2}'</p> <p>9.2) caCert is a root certificate in base64 format of the standby hub cluster's default ingress controller which is used for exposing s3 service. The value of caCert can be checked with the following command:</p> <p>oc get secrets -n openshift-ingress-operator router-ca -o jsonpath=\"{.data['tls.crt']}\"</p> <p>9.3) The value of credential is the name of the secret which was created in secret.yaml. The value of key is the name of the value created insecret.yaml under data block.</p> <pre><code>[root@ncputility ~ pancwl_rc]$ cat DataProtectionApplication.yaml\nkind: DataProtectionApplication\napiVersion: oadp.openshift.io/v1alpha1\nmetadata:\n  name: velero-acm-backup\n  namespace: open-cluster-management-backup\nspec:\n  backupLocations:\n    - velero:\n        config:\n          profile: default\n          region: none\n          s3ForcePathStyle: \"true\"\n          s3Url: \"https://s3-openshift-storage.apps.hnevocphub01.mnc002.mcc708\"\n        default: true\n        provider: aws\n        objectStorage:\n          bucket: acm-backups-3885ebaa-8c8b-4985-b9c7-0c25d631de14\n          prefix: velero\n          caCert: \"LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSURERENDQWZTZ0F3SUJBZ0lCQVRBTkJna3Foa2lHOXcwQkFRc0ZBREFtTVNRd0lnWURWUVFEREJ0cGJtZHkKWlhOekxXOXdaWEpoZEc5eVFERTNORFV4TlRNM09UZ3dIaGNOTWpVd05ESXdNVEkxTmpNM1doY05NamN3TkRJdwpNVEkxTmpNNFdqQW1NU1F3SWdZRFZRUUREQnRwYm1keVpYTnpMVzl3WlhKaGRHOXlRREUzTkRVeE5UTTNPVGd3CmdnRWlNQTBHQ1NxR1NJYjNEUUVCQVFVQUE0SUJEd0F3Z2dFS0FvSUJBUUM4aXhWMDZnM0ozZ01lUE04d0JEVTEKRFdMNWhyVW9DeXh5SThkTFNob2trSDNtdTF1RFNCeEJ2blBQYnZhRUs1aFR1RFhjQ3dSaUVNYmZPcFFjUnZzdApQT2dVSU05T3RhY3lwWVZTeGVhSUNQZnNpMlBzd0VaNElWNWMwZEM1SVVSc2hTTTFXaksxckpvanZYQmE3c0c3CjhpQXRCMnJ0K1FwUkhJRzVSYVpnZVdXQVhrMXVPdWlTazBSLzJ3ajZsbDJmZ01NYklOSGluNXdseU1YQldjOHYKaHJTc3VjR2wrL2pXRVl2QWdXKzFrRW5YRDgvclpqUTN6bExVVTRpdzNvNlBMRFlrKzhjYm5DQnNMQjh2RWlqWQppbFRzQ1N3TjdwUk5CVGNqcWxLYVpqWVZ2Mk9QYi80WjF4TG1aekFxWGRhc2pSRW5XNXcyZGtrZy9RNlgxWmZGCkFnTUJBQUdqUlRCRE1BNEdBMVVkRHdFQi93UUVBd0lDcERBU0JnTlZIUk1CQWY4RUNEQUdBUUgvQWdFQU1CMEcKQTFVZERnUVdCQlRsdFFLZ2NjYjZ4bE5BZUt5SFpjY1BZT1A1b0RBTkJna3Foa2lHOXcwQkFRc0ZBQU9DQVFFQQpnZDYyWVJkRHhNTm5HcCtDWStRaXBZRlMxMkNRMUVwVXcwMEVIUno0b2k0UG5PQzcrazlGbHVZeW9vWWNZa1QyCkNGTGp4K3hOODE2bkFVRWFMQ25Ia2NjK2JCSlhuNmU0LzFqWUZGdFFjRnhJeEZVdXByb3dNY21ZeGxBbHlBQ1oKZU5ac1V1Tk4xWVh1aDl2Z3MrRVE2a1VNSjFYSVNPeVp3RDNIbVZSaW16RHN6MUo4MDRLSlhqVnRURlNEbXR1WQpBbGxLWnF2czdqWGJWTUlxY1VtUmFQclluRm8xRDBXcFFGMnFLRGtUL1RDdnFuS3ROR2x5dHVCZmp1YittL0U0Ck1yOExTdXFjTGlhcGhFQS9BM3pwNHNaVWxVbmVCaksrRnkzTkwwdnprcjZxZGVjbG1YSHU5bzdtMnV4UlBKR2UKTm9vb240cmdYU25SSnB5VForcEhyUT09Ci0tLS0tRU5EIENFUlRJRklDQVRFLS0tLS0K\"\n        credential:\n          key: cloud\n          name: cloud-credentials\n  configuration:\n    nodeAgent:\n      enable: true\n      uploaderType: restic\n    velero:\n      featureFlags:\n        - EnableCSI\n      defaultPlugins:\n        - openshift\n        - aws\n        - csi\n[root@ncputility ~ pancwl_rc]$\n\n\n#bucket is the value checked earlier with BUCKET_NAME command.\n</code></pre> <p><code>[root@dom14npv101-infra-manager ~ hubrc]# oc apply -f DataProtectionApplication.yaml dataprotectionapplication.oadp.openshift.io/velero-acm-backup created [root@dom14npv101-infra-manager ~ hubrc]#</code></p> <p>10) validate that resouce creation is completed successfully. </p> <pre><code>[root@dom14npv101-infra-manager ~ hubrc]# oc get backupstoragelocations.velero.io -n open-cluster-management-backup\nNAME                  PHASE       LAST VALIDATED   AGE   DEFAULT\nvelero-acm-backup-1   Available   22s              33s   true\n[root@dom14npv101-infra-manager ~ hubrc]#\n</code></pre> <p>11) Check the resources that are application projects from GitOps</p> <p>hub cluster: </p> <pre><code>[root@dom14npv101-infra-manager ~ hubrc]# oc get appprojects.argoproj.io -n openshift-gitops | grep -v default\nNAME                  AGE\nsite-config-project   59d\nsite-policy-project   59d\n</code></pre> <p>12) Add label velero.io/exclude-from-backup: \"true\" manually on both Hub clusters</p> <pre><code>[root@dom14npv101-infra-manager ~ hubrc]# oc get appprojects.argoproj.io -n openshift-gitops -l velero.io/exclude-from-backup\nNAME                  AGE\nsite-config-project   59d\nsite-policy-project   59d\n</code></pre> <p>13) Add label velero.io/exclude-from-backup: \"true\" manually on both Hub clusters</p> <pre><code>[root@dom14npv101-infra-manager ~ hubrc]# oc get applications.argoproj.io -n openshift-gitops -l velero.io/exclude-from-backup\nNAME                        SYNC STATUS   HEALTH STATUS\nncpvnpvlab1-site-configs    Synced        Healthy\nncpvnpvlab1-site-policies   Synced        Healthy\nncpvnpvmgt-site-configs     Synced        Healthy\nncpvnpvmgt-site-policies    Synced        Healthy\n[root@dom14npv101-infra-manager ~ hubrc]#\n</code></pre> <p>14) A backup is made everyday at 10 PM.</p> <pre><code>[root@dom14npv101-infra-manager ~ hubrc]# cat &gt; backup.yaml\napiVersion: cluster.open-cluster-management.io/v1beta1\nkind: BackupSchedule\nmetadata:\n  name: schedule-acm\n  namespace: open-cluster-management-backup\nspec:\n  veleroSchedule: 0 22 * * *\n  veleroTtl: 120h\n^C\n[root@dom14npv101-infra-manager ~ hubrc]# date\nThu May  1 06:04:49 PM UTC 2025\n[root@dom14npv101-infra-manager ~ hubrc]# vi backup.yaml\n[root@dom14npv101-infra-manager ~ hubrc]# oc apply  -f  backup.yaml\nbackupschedule.cluster.open-cluster-management.io/schedule-acm created\n[root@dom14npv101-infra-manager ~ hubrc]# \n</code></pre>"},{"location":"openshift/disaster-management/hub-cluster-redeployment/#status-of-backup-job-here","title":"Status of backup job here:","text":"<p>1) checking the status of the backup </p> <pre><code>[root@dom14npv101-infra-manager ~ hubrc]# oc get backupstoragelocations.velero.io -A\nNAMESPACE                        NAME                  PHASE       LAST VALIDATED   AGE     DEFAULT\nopen-cluster-management-backup   velero-acm-backup-1   Available   4s               5m15s   true\n[root@dom14npv101-infra-manager ~ hubrc]# \n</code></pre> <p>2) describe the status of the output job </p> <pre><code>[root@dom14npv101-infra-manager ~ hubrc]#  oc describe backupstoragelocations.velero.io velero-acm-backup-1 -n open-cluster-management-backup\nName:         velero-acm-backup-1\nNamespace:    open-cluster-management-backup\nLabels:       app.kubernetes.io/component=bsl\n              app.kubernetes.io/instance=velero-acm-backup-1\n              app.kubernetes.io/managed-by=oadp-operator\n              app.kubernetes.io/name=oadp-operator-velero\n              openshift.io/oadp=True\n              openshift.io/oadp-registry=True\nAnnotations:  &lt;none&gt;\nAPI Version:  velero.io/v1\nKind:         BackupStorageLocation\nMetadata:\n  Creation Timestamp:  2025-05-01T18:02:31Z\n  Generation:          13\n  Owner References:\n    API Version:           oadp.openshift.io/v1alpha1\n    Block Owner Deletion:  true\n    Controller:            true\n    Kind:                  DataProtectionApplication\n    Name:                  velero-acm-backup\n    UID:                   89734e12-0144-4376-954d-cb13cde17515\n  Resource Version:        142419914\n  UID:                     52b6b341-5e36-4e02-bc98-15622be65673\nSpec:\n  Config:\n    Checksum Algorithm:\n    Profile:             default\n    Region:              none\n    s3ForcePathStyle:    true\n    s3Url:               https://s3-openshift-storage.apps.ncpvnpvmgt.pnwlab.nsn-rdnet.net\n  Credential:\n    Key:    cloud\n    Name:   cloud-credentials\n  Default:  true\n  Object Storage:\n    Bucket:   acm-backups-bcf1990d-0846-4a6d-8518-67db81ebee63\n    Ca Cert:  LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSURERENDQWZTZ0F3SUJBZ0lCQVRBTkJna3Foa2lHOXcwQkFRc0ZBREFtTVNRd0lnWURWUVFEREJ0cGJtZHkKWlhOekxXOXdaWEpoZEc5eVFERTNOREV4TXpJek16RXdIaGNOTWpVd016QTBNak0xTWpF        d1doY05NamN3TXpBMApNak0xTWpFeFdqQW1NU1F3SWdZRFZRUUREQnRwYm1keVpYTnpMVzl3WlhKaGRHOXlRREUzTkRFeE16SXpNekV3CmdnRWlNQTBHQ1NxR1NJYjNEUUVCQVFVQUE0SUJEd0F3Z2dFS0FvSUJBUURLOUhrUkd4VWNPRnZjcHVZc1dLdWQKd2ltMFBWc3B3VU        1LNXgyejhBQWJ1eGhuOTRFWThWbk9tSkJUVkhqRnIvbmI0SjRzYldzN3JBbXZINWpjbTdzbApMYW1wWU1uUWlSamtrdE5FSVV3Rm9LVm9VU3ZRSWtiTHJlZk1hcjJYSENOOEU0dHVHeXA3U1c1YjBMRjc4cUY5CktIY0ttMXBDVi9NR3lyU1RtMkx0SmtBcnM5d0lsL0ZPYmox        UEcvUmsvQThtRHZhalBmSUVGbU8yMHduWEQ5bWcKZDVIVk1ZVzkyWWRWVDZPR0FWMEZUNCtJNzNibEdyK3pqQUJzMklxTnUzQ3h0cHlucXMvVVV6RGFodGRvc2ZxSgpWeHhaTUFaZlRENTk0UUtzMFZhamR2aTU1Z1pPejVBQXRLSG96Rm85TWk5dklXblpwR3Z0T2xINnIyVE        ZmSytmCkFnTUJBQUdqUlRCRE1BNEdBMVVkRHdFQi93UUVBd0lDcERBU0JnTlZIUk1CQWY4RUNEQUdBUUgvQWdFQU1CMEcKQTFVZERnUVdCQlRJNVRqeVp2aUJ1NE0wNzd6Mk9PY0lDRmkwMURBTkJna3Foa2lHOXcwQkFRc0ZBQU9DQVFFQQpuNG96eWZha0sySUFqb3dFSlZE        bFNMMlp4YVJuWmJFcjVLanhJbjhiQ0tjaUdBK0h2UURuY0UzK1BzSTJDZGxpCjhXQlR4ZnJ3aFoxTVZ2YjVySmlsTXpZUklQamJaanUrbitaNlB0SGJYMEZDb2Q0elpaYkZBOFQvNlZXUkJSSmUKL3VMaXU3VVRENjJQRDgydVlNSmJFTDNTa1V6b1U5T2NXMSt1S1R3UG56NG        VFblVvNzVnbVlUWnBybkhKSEttNAowUVljTlF6RndHR3JnQnNuTy8wRW94Z2Roa09keENlWFBqRUpURnZXRTdpRjhYWTlKQVZKMVpCcStuZUNoMmRCCkowRGRkaGZjMDhtUnpHbStkVXRyeCtZMnRoSXBxWVUreTN1WDZaM09TcDVNeFIvQzMvelRGR2pqK3pKUmNIOTIKUmtH        c2V5bGNCYXpYbmVvWXgzdEVxUT09Ci0tLS0tRU5EIENFUlRJRklDQVRFLS0tLS0K\n    Prefix:   velero\n  Provider:   aws\nStatus:\n  Last Synced Time:      2025-05-01T18:07:42Z\n  Last Validation Time:  2025-05-01T18:07:42Z\n  Phase:                 Available\nEvents:\n  Type    Reason                           Age    From            Message\n  ----    ------                           ----   ----            -------\n  Normal  BackupStorageLocationReconciled  5m33s  DPA-controller  performed created on backupstoragelocation open-cluster-management-backup/velero-acm-backup-1\n</code></pre>"},{"location":"openshift/disaster-management/hub-cluster-redeployment/#access-the-backup-jobs","title":"Access the backup jobs","text":"<p>1) status of open-cluster-manager pods here</p> <pre><code>[root@dom14npv101-infra-manager ~ hubrc]# oc get pods -n  open-cluster-management-backup\nNAME                                                 READY   STATUS    RESTARTS   AGE\ncluster-backup-chart-clusterbackup-698656f7f-cqxxj   1/1     Running   0          24d\ncluster-backup-chart-clusterbackup-698656f7f-dvdz8   1/1     Running   0          24d\nnode-agent-brq4q                                     1/1     Running   0          6m6s\nnode-agent-j9xm8                                     1/1     Running   0          6m6s\nnode-agent-mlrwk                                     1/1     Running   0          6m6s\nnode-agent-nkb7v                                     1/1     Running   0          6m6s\nnode-agent-sgqxw                                     1/1     Running   0          6m6s\nopenshift-adp-controller-manager-74f799649f-v2slw    1/1     Running   0          24d\nvelero-549fbfb95d-s966g                              1/1     Running   0          6m6s\n[root@dom14npv101-infra-manager ~ hubrc]# \n</code></pre> <p>2) status of job from velero binary</p> <pre><code>[root@dom14npv101-infra-manager ~ hubrc]# oc -n open-cluster-management-backup exec -it velero-549fbfb95d-s966g -- ./velero backup-location get\nDefaulted container \"velero\" out of: velero, openshift-velero-plugin (init), velero-plugin-for-aws (init)\nNAME                  PROVIDER   BUCKET/PREFIX                                             PHASE       LAST VALIDATED                  ACCESS MODE   DEFAULT\nvelero-acm-backup-1   aws        acm-backups-bcf1990d-0846-4a6d-8518-67db81ebee63/velero   Available   2025-05-01 18:08:42 +0000 UTC   ReadWrite     true\n[root@dom14npv101-infra-manager ~ hubrc]#\n</code></pre> <p>3) Look at the final status of the backup jobs</p> <pre><code> [root@dom14npv101-infra-manager ~ hubrc]# oc -n open-cluster-management-backup exec -it velero-549fbfb95d-s966g -- ./velero backup get\nDefaulted container \"velero\" out of: velero, openshift-velero-plugin (init), velero-plugin-for-aws (init)\nNAME                                            STATUS      ERRORS   WARNINGS   CREATED                         EXPIRES   STORAGE LOCATION      SELECTOR\nacm-credentials-schedule-20250501180616         Completed   0        0          2025-05-01 18:06:17 +0000 UTC   4d        velero-acm-backup-1   &lt;none&gt;\nacm-managed-clusters-schedule-20250501180616    Completed   0        0          2025-05-01 18:06:18 +0000 UTC   4d        velero-acm-backup-1   &lt;none&gt;\nacm-resources-generic-schedule-20250501180616   Completed   0        0          2025-05-01 18:06:20 +0000 UTC   4d        velero-acm-backup-1   cluster.open-cluster-management.io/backup\nacm-resources-schedule-20250501180616           Completed   0        0          2025-05-01 18:06:29 +0000 UTC   4d        velero-acm-backup-1   !cluster.open-cluster-management.io/backup,!policy.open-cluste        r-management.io/root-policy\nacm-validation-policy-schedule-20250501180616   Completed   0        0          2025-05-01 18:06:30 +0000 UTC   1d        velero-acm-backup-1   &lt;none&gt;\n[root@dom14npv101-infra-manager ~ hubrc]# date\nThu May  1 06:09:39 PM UTC 2025\n[root@dom14npv101-infra-manager ~ hubrc]# \n</code></pre>"},{"location":"openshift/disaster-management/hub-cluster-redeployment/backmaster-replacment%20copy/","title":"hub master replacement","text":""},{"location":"openshift/disaster-management/hub-cluster-redeployment/backmaster-replacment%20copy/#highlevel-steps","title":"Highlevel Steps","text":"<ul> <li>(Removing the failed node from the cluster)[]</li> <li></li> </ul>"},{"location":"openshift/disaster-management/hub-cluster-redeployment/backmaster-replacment%20copy/#removing-the-failed-node-from-the-cluster","title":"Removing the failed node from the cluster","text":"<p>1) Identifying the failed control plane node</p> <p>First identify which node is the failed one, e.g. which is in NotReady state, using the <code>oc get nodes</code> command.</p> <pre><code>[root@dom16hub101-infra-manager ~]# oc get nodes\nNAME                                               STATUS   ROLES                                 AGE   VERSION\nncpvblvhub-hubmaster-101.ncpvblvhub.t-mobile.lab   Ready    control-plane,master,monitor,worker   74d   v1.29.10+67d3387\nncpvblvhub-hubmaster-102.ncpvblvhub.t-mobile.lab   Ready    control-plane,master,monitor,worker   74d   v1.29.10+67d3387\nncpvblvhub-hubmaster-103.ncpvblvhub.t-mobile.lab   Ready    control-plane,master,monitor,worker   74d   v1.29.10+67d3387\nncpvblvhub-hubworker-101.ncpvblvhub.t-mobile.lab   Ready    gateway,worker                        74d   v1.29.10+67d3387\nncpvblvhub-hubworker-102.ncpvblvhub.t-mobile.lab   Ready    gateway,worker                        74d   v1.29.10+67d3387\n[root@dom16hub101-infra-manager ~]#\n</code></pre> <p>In the above example disaster, it turned out, that accidentally all the logical volumes on top of RAID10 were removed, and the RAID10 setting was also deleted. (just an example).</p> <p>[root@dom16hub101-infra-manager ~]# oc debug node/ncpvblvhub-hubmaster-103.ncpvblvhub.t-mobile.lab --chroot /host ip a s | grep 10. error: unknown flag: --chroot See 'oc debug --help' for usage. [root@dom16hub101-infra-manager ~]# oc debug node/ncpvblvhub-hubmaster-103.ncpvblvhub.t-mobile.lab -- chroot /host ip a s | grep 10. Starting pod/ncpvblvhub-hubmaster-103ncpvblvhubt-mobilelab-debug-pvf4x ... To use host binaries, run <code>chroot /host</code> 1: lo:  mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000 2: enp0s20f0u10u3:  mtu 1500 qdisc fq_codel state UNKNOWN group default qlen 1000     inet 169.254.0.2/24 brd 169.254.0.255 scope link dynamic noprefixroute enp0s20f0u10u3 3: ens1f0np0:  mtu 9126 qdisc mq master tenant-bond-1 state UP group default qlen 1000 4: ens1f1np1:  mtu 9126 qdisc mq master tenant-bond-1 state UP group default qlen 1000 5: eno12399np0:  mtu 9126 qdisc mq master infra-bond state UP group default qlen 1000 6: eno12409np1:  mtu 9126 qdisc mq master infra-bond state UP group default qlen 1000 7: ens3f0np0:  mtu 1500 qdisc mq state DOWN group default qlen 1000 8: ens3f1np1:  mtu 1500 qdisc mq state DOWN group default qlen 1000 9: ovs-system:  mtu 1500 qdisc noop state DOWN group default qlen 1000 10: ovn-k8s-mp0:  mtu 9026 qdisc noqueue state UNKNOWN group default qlen 1000 11: genev_sys_6081:  mtu 65000 qdisc noqueue master ovs-system state UNKNOWN group default qlen 1000 12: br-int:  mtu 9026 qdisc noop state DOWN group default qlen 1000 13: tenant-bond-1:  mtu 9126 qdisc noqueue state UP group default qlen 1000 14: ten1-vlan.202@tenant-bond-1:  mtu 9126 qdisc noqueue state UP group default qlen 1000 15: infra-bond:  mtu 9126 qdisc noqueue state UP group default qlen 1000 16: infra-bond.200@infra-bond:  mtu 9126 qdisc noqueue master ovs-system state UP group default qlen 1000 17: tenant-bond-2:  mtu 9126 qdisc noqueue state DOWN group default qlen 1000 18: br-ex:  mtu 9126 qdisc noqueue state UNKNOWN group default qlen 1000     inet 10.145.151.94/26 brd 10.145.151.127 scope global noprefixroute br-ex     inet 10.145.151.74/32 scope global vip     inet 10.145.151.70/32 scope global vip     link/ether 46:10:38:6c:e1:1f brd ff:ff:ff:ff:ff:ff link-netns 6d81cd37-2550-4215-bb6a-de1bd8538f00     inet6 fe80::4410:38ff:fe6c:e11f/64 scope link 36: c010da31da27dc9@if2:  mtu 9026 qdisc noqueue master ovs-system state UP group default 1604: f5a50bc28106ffb@if2:  mtu 9026 qdisc noqueue master ovs-system state UP group default     link/ether 86:93:38:1a:8a:08 brd ff:ff:ff:ff:ff:ff link-netns 9734c9ca-4369-429e-9581-9f4d1d10fe65     link/ether 7a:3b:77:b5:7d:9f brd ff:ff:ff:ff:ff:ff link-netns db960360-9a29-4af5-87cb-8a282d10cad1 910: d89dbe96556e3c4@if2:  mtu 9026 qdisc noqueue master ovs-system state UP group default     link/ether 7e:71:15:80:ca:f8 brd ff:ff:ff:ff:ff:ff link-netns 101e0f1f-2969-48f7-9d6c-aa75e2472245 409: e5e151095b59925@if2:  mtu 9026 qdisc noqueue master ovs-system state UP group default 410: 8355c386d4a777c@if2:  mtu 9026 qdisc noqueue master ovs-system state UP group default     link/ether f2:e3:c9:ef:a5:28 brd ff:ff:ff:ff:ff:ff link-netns fb8f84e7-7101-445d-8f3c-815a82fdac97 715: 21a55a1021a0098@if2:  mtu 9026 qdisc noqueue master ovs-system state UP group default 722: 406bac110a6d77f@if2:  mtu 9026 qdisc noqueue master ovs-system state UP group default <p>Removing debug pod ... [root@dom16hub101-infra-manager ~]# [root@dom16hub101-infra-manager ~]# oc debug node/ncpvblvhub-hubmaster-102.ncpvblvhub.t-mobile.lab -- chroot /host ip a s | grep 10. Starting pod/ncpvblvhub-hubmaster-102ncpvblvhubt-mobilelab-debug-vlgfh ... To use host binaries, run <code>chroot /host</code> 1: lo:  mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000 2: enp0s20f0u10u3:  mtu 1500 qdisc fq_codel state UNKNOWN group default qlen 1000     inet 169.254.0.2/24 brd 169.254.0.255 scope link dynamic noprefixroute enp0s20f0u10u3 3: ens1f0np0:  mtu 9126 qdisc mq master tenant-bond-1 state UP group default qlen 1000 4: ens1f1np1:  mtu 9126 qdisc mq master tenant-bond-1 state UP group default qlen 1000 5: eno12399np0:  mtu 9126 qdisc mq master infra-bond state UP group default qlen 1000 6: eno12409np1:  mtu 9126 qdisc mq master infra-bond state UP group default qlen 1000 7: ens3f0np0:  mtu 1500 qdisc mq state DOWN group default qlen 1000 8: ens3f1np1:  mtu 1500 qdisc mq state DOWN group default qlen 1000 9: ovs-system:  mtu 1500 qdisc noop state DOWN group default qlen 1000 10: genev_sys_6081:  mtu 65000 qdisc noqueue master ovs-system state UNKNOWN group default qlen 1000 11: ovn-k8s-mp0:  mtu 9026 qdisc noqueue state UNKNOWN group default qlen 1000 12: br-int:  mtu 9026 qdisc noop state DOWN group default qlen 1000 13: tenant-bond-1:  mtu 9126 qdisc noqueue state UP group default qlen 1000 14: ten1-vlan.202@tenant-bond-1:  mtu 9126 qdisc noqueue state UP group default qlen 1000 15: infra-bond:  mtu 9126 qdisc noqueue state UP group default qlen 1000 16: infra-bond.200@infra-bond:  mtu 9126 qdisc noqueue master ovs-system state UP group default qlen 1000 17: tenant-bond-2:  mtu 9126 qdisc noqueue state DOWN group default qlen 1000 18: br-ex:  mtu 9126 qdisc noqueue state UNKNOWN group default qlen 1000     inet 10.145.151.93/26 brd 10.145.151.127 scope global noprefixroute br-ex 1236: 3aa4c1068d4ca9f@if2:  mtu 9026 qdisc noqueue master ovs-system state UP group default     link/ether 56:d9:4f:3d:39:e3 brd ff:ff:ff:ff:ff:ff link-netns 5e6ae102-a8ce-4964-ad55-02b2c25bff03     inet6 fe80::14c4:58ff:fed6:610c/64 scope link     link/ether 72:46:09:32:30:4e brd ff:ff:ff:ff:ff:ff link-netns e61097e2-dfe2-4b83-aad3-fac770a17b39     link/ether a6:2c:42:05:97:1e brd ff:ff:ff:ff:ff:ff link-netns 46658f16-9c06-4604-9565-9c16117b1038     inet6 fe80::104d:4aff:fe12:7a3b/64 scope link     inet6 fe80::50ba:fbff:fe9a:8106/64 scope link     link/ether 4e:59:53:2e:fe:78 brd ff:ff:ff:ff:ff:ff link-netns 31054be2-161e-479c-a847-bc1101aa3745     link/ether 72:b2:cb:33:34:4d brd ff:ff:ff:ff:ff:ff link-netns 201f8aab-07ab-48ef-ad50-21017880f001     inet6 fe80::1027:eeff:fec3:2e17/64 scope link 1279: a4320d3b10a7b81@if2:  mtu 9026 qdisc noqueue master ovs-system state UP group default <p>Removing debug pod ... [root@dom16hub101-infra-manager ~]# [root@dom16hub101-infra-manager ~]# [root@dom16hub101-infra-manager ~]# oc get nodes | grep ncpvblvhub-hubmaster-102.ncpvblvhub.t-mobile.lab ncpvblvhub-hubmaster-102.ncpvblvhub.t-mobile.lab   Ready    control-plane,master,monitor,worker   74d   v1.29.10+67d3387 [root@dom16hub101-infra-manager ~]# oc get nodes --show-labels | grep ncpvblvhub-hubmaster-102.ncpvblvhub.t-mobile.lab ncpvblvhub-hubmaster-102.ncpvblvhub.t-mobile.lab   Ready    control-plane,master,monitor,worker   74d   v1.29.10+67d3387   beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,cluster.ocs.openshift.io/openshift-storage=,kubernetes.io/arch=amd64,kubernetes.io/hostname=ncpvblvhub-hubmaster-102.ncpvblvhub.t-mobile.lab,kubernetes.io/os=linux,node-role.kubernetes.io/control-plane=,node-role.kubernetes.io/master=,node-role.kubernetes.io/monitor=,node-role.kubernetes.io/worker=,node.openshift.io/os_id=rhcos [root@dom16hub101-infra-manager ~]# oc get pods -n openshift-storage -o wide | grep -i ncpvblvhub-hubmaster-102.ncpvblvhub.t-mobile.lab csi-cephfsplugin-rnkrk                                            2/2     Running   6              71d   10.145.151.93    ncpvblvhub-hubmaster-102.ncpvblvhub.t-mobile.lab     csi-rbdplugin-provisioner-646d95bdd9-496ng                        6/6     Running   0              15d   172.21.1.43      ncpvblvhub-hubmaster-102.ncpvblvhub.t-mobile.lab     csi-rbdplugin-zg88h                                               3/3     Running   9              71d   10.145.151.93    ncpvblvhub-hubmaster-102.ncpvblvhub.t-mobile.lab     rook-ceph-crashcollector-11cef195e99cf42211bc5b21918ec486-b8jpz   1/1     Running   0              26d   172.21.0.19      ncpvblvhub-hubmaster-102.ncpvblvhub.t-mobile.lab     rook-ceph-exporter-11cef195e99cf42211bc5b21918ec486-6f8c85r84bf   1/1     Running   0              26d   172.21.0.20      ncpvblvhub-hubmaster-102.ncpvblvhub.t-mobile.lab     rook-ceph-mon-a-66bcddd94-wbfs4                                   2/2     Running   0              26d   172.21.0.9       ncpvblvhub-hubmaster-102.ncpvblvhub.t-mobile.lab     rook-ceph-osd-0-54d5b7dd6b-vjf4f                                  2/2     Running   0              27d   172.21.0.11      ncpvblvhub-hubmaster-102.ncpvblvhub.t-mobile.lab     rook-ceph-osd-5-79fb5f7965-wpgpq                                  2/2     Running   0              27d   172.21.0.17      ncpvblvhub-hubmaster-102.ncpvblvhub.t-mobile.lab     [root@dom16hub101-infra-manager ~]# oc -n openshift-etcd get pods -l k8s-app=etcd -o wide NAME                                                    READY   STATUS    RESTARTS   AGE   IP              NODE                                               NOMINATED NODE   READINESS GATES etcd-ncpvblvhub-hubmaster-101.ncpvblvhub.t-mobile.lab   4/4     Running   12         74d   10.145.151.92   ncpvblvhub-hubmaster-101.ncpvblvhub.t-mobile.lab     etcd-ncpvblvhub-hubmaster-102.ncpvblvhub.t-mobile.lab   4/4     Running   12         74d   10.145.151.93   ncpvblvhub-hubmaster-102.ncpvblvhub.t-mobile.lab     etcd-ncpvblvhub-hubmaster-103.ncpvblvhub.t-mobile.lab   4/4     Running   8          74d   10.145.151.94   ncpvblvhub-hubmaster-103.ncpvblvhub.t-mobile.lab     [root@dom16hub101-infra-manager ~]# oc get pods -n openshift-storage -o wide | grep -i ncpvblvhub-hubmaster-102.ncpvblvhub.t-mobile.lab csi-cephfsplugin-rnkrk                                            2/2     Running   6              71d   10.145.151.93    ncpvblvhub-hubmaster-102.ncpvblvhub.t-mobile.lab     csi-rbdplugin-provisioner-646d95bdd9-496ng                        6/6     Running   0              15d   172.21.1.43      ncpvblvhub-hubmaster-102.ncpvblvhub.t-mobile.lab     csi-rbdplugin-zg88h                                               3/3     Running   9              71d   10.145.151.93    ncpvblvhub-hubmaster-102.ncpvblvhub.t-mobile.lab     rook-ceph-crashcollector-11cef195e99cf42211bc5b21918ec486-b8jpz   1/1     Running   0              26d   172.21.0.19      ncpvblvhub-hubmaster-102.ncpvblvhub.t-mobile.lab     rook-ceph-exporter-11cef195e99cf42211bc5b21918ec486-6f8c85r84bf   1/1     Running   0              26d   172.21.0.20      ncpvblvhub-hubmaster-102.ncpvblvhub.t-mobile.lab     rook-ceph-mon-a-66bcddd94-wbfs4                                   2/2     Running   0              26d   172.21.0.9       ncpvblvhub-hubmaster-102.ncpvblvhub.t-mobile.lab     rook-ceph-osd-0-54d5b7dd6b-vjf4f                                  2/2     Running   0              27d   172.21.0.11      ncpvblvhub-hubmaster-102.ncpvblvhub.t-mobile.lab     rook-ceph-osd-5-79fb5f7965-wpgpq                                  2/2     Running   0              27d   172.21.0.17      ncpvblvhub-hubmaster-102.ncpvblvhub.t-mobile.lab     [root@dom16hub101-infra-manager ~]# oc scale deployment rook-ceph-mon-a --replicas=0 -n openshift-storage deployment.apps/rook-ceph-mon-a scaled [root@dom16hub101-infra-manager ~]# oc scale deployment rook-ceph-osd-0 --replicas=0 -n openshift-storage deployment.apps/rook-ceph-osd-0 scaled [root@dom16hub101-infra-manager ~]# oc scale deployment rook-ceph-osd-0 --replicas=5 -n openshift-storage deployment.apps/rook-ceph-osd-0 scaled [root@dom16hub101-infra-manager ~]# oc get pods -n openshift-storage -o wide | grep -i ncpvblvhub-hubmaster-102.ncpvblvhub.t-mobile.lab csi-cephfsplugin-rnkrk                                            2/2     Running    6              71d   10.145.151.93    ncpvblvhub-hubmaster-102.ncpvblvhub.t-mobile.lab     csi-rbdplugin-provisioner-646d95bdd9-496ng                        6/6     Running    0              15d   172.21.1.43      ncpvblvhub-hubmaster-102.ncpvblvhub.t-mobile.lab     csi-rbdplugin-zg88h                                               3/3     Running    9              71d   10.145.151.93    ncpvblvhub-hubmaster-102.ncpvblvhub.t-mobile.lab     rook-ceph-crashcollector-11cef195e99cf42211bc5b21918ec486-b8jpz   1/1     Running    0              26d   172.21.0.19      ncpvblvhub-hubmaster-102.ncpvblvhub.t-mobile.lab     rook-ceph-exporter-11cef195e99cf42211bc5b21918ec486-6f8c85r84bf   1/1     Running    0              26d   172.21.0.20      ncpvblvhub-hubmaster-102.ncpvblvhub.t-mobile.lab     rook-ceph-osd-0-54d5b7dd6b-2lnw5                                  0/2     Init:2/4   0              4s    172.21.1.124     ncpvblvhub-hubmaster-102.ncpvblvhub.t-mobile.lab     rook-ceph-osd-0-54d5b7dd6b-9vm85                                  0/2     Init:2/4   0              4s    172.21.1.125     ncpvblvhub-hubmaster-102.ncpvblvhub.t-mobile.lab     rook-ceph-osd-0-54d5b7dd6b-mk7tl                                  0/2     Init:1/4   0              4s    172.21.1.122     ncpvblvhub-hubmaster-102.ncpvblvhub.t-mobile.lab     rook-ceph-osd-0-54d5b7dd6b-pnn7x                                  0/2     Init:2/4   0              4s    172.21.1.123     ncpvblvhub-hubmaster-102.ncpvblvhub.t-mobile.lab     rook-ceph-osd-0-54d5b7dd6b-vb5c2                                  0/2     Init:2/4   0              4s    172.21.1.121     ncpvblvhub-hubmaster-102.ncpvblvhub.t-mobile.lab     rook-ceph-osd-5-79fb5f7965-wpgpq                                  2/2     Running    0              27d   172.21.0.17      ncpvblvhub-hubmaster-102.ncpvblvhub.t-mobile.lab     [root@dom16hub101-infra-manager ~]# <p>[root@dom16hub101-infra-manager ~]# oc adm cordon ncpvblvhub-hubmaster-102.ncpvblvhub.t-mobile.lab node/ncpvblvhub-hubmaster-102.ncpvblvhub.t-mobile.lab cordoned [root@dom16hub101-infra-manager ~]# oc scale deployment rook-ceph-osd-0 --replicas=0 -n openshift-storage deployment.apps/rook-ceph-osd-0 scaled [root@dom16hub101-infra-manager ~]# [root@dom16hub101-infra-manager ~]# oc scale deployment rook-ceph-osd-5 --replicas=0 -n openshift-storage deployment.apps/rook-ceph-osd-5 scaled [root@dom16hub101-infra-manager ~]# oc get pods -n openshift-storage -o wide | grep -i ncpvblvhub-hubmaster-102.ncpvblvhub.t-mobile.lab csi-cephfsplugin-rnkrk                                            2/2     Running       6              71d   10.145.151.93    ncpvblvhub-hubmaster-102.ncpvblvhub.t-mobile.lab     csi-rbdplugin-provisioner-646d95bdd9-496ng                        6/6     Running       0              15d   172.21.1.43      ncpvblvhub-hubmaster-102.ncpvblvhub.t-mobile.lab     csi-rbdplugin-zg88h                                               3/3     Running       9              71d   10.145.151.93    ncpvblvhub-hubmaster-102.ncpvblvhub.t-mobile.lab     rook-ceph-exporter-11cef195e99cf42211bc5b21918ec486-6f8c85r84bf   0/1     Terminating   0              26d   172.21.0.20      ncpvblvhub-hubmaster-102.ncpvblvhub.t-mobile.lab     [root@dom16hub101-infra-manager ~]# oc scale deployment --selector=app=rook-ceph-crashcollector,node_name=ncpvblvhub-hubmaster-102.ncpvblvhub.t-mobile.lab  --replicas=0 -n openshift-storage error: no objects passed to scale [root@dom16hub101-infra-manager ~]# oc get pods -n openshift-storage -o wide | grep -i ncpvblvhub-hubmaster-102.ncpvblvhub.t-mobile.lab                csi-cephfsplugin-rnkrk                                            2/2     Running   6              71d   10.145.151.93    ncpvblvhub-hubmaster-102.ncpvblvhub.t-mobile.lab     csi-rbdplugin-provisioner-646d95bdd9-496ng                        6/6     Running   0              15d   172.21.1.43      ncpvblvhub-hubmaster-102.ncpvblvhub.t-mobile.lab     csi-rbdplugin-zg88h                                               3/3     Running   9              71d   10.145.151.93    ncpvblvhub-hubmaster-102.ncpvblvhub.t-mobile.lab     [root@dom16hub101-infra-manager ~]# oc -n openshift-etcd get pods -l k8s-app=etcd -o wide NAME                                                    READY   STATUS    RESTARTS   AGE   IP              NODE                                               NOMINATED NODE   READINESS GATES etcd-ncpvblvhub-hubmaster-101.ncpvblvhub.t-mobile.lab   4/4     Running   12         74d   10.145.151.92   ncpvblvhub-hubmaster-101.ncpvblvhub.t-mobile.lab     etcd-ncpvblvhub-hubmaster-102.ncpvblvhub.t-mobile.lab   4/4     Running   12         74d   10.145.151.93   ncpvblvhub-hubmaster-102.ncpvblvhub.t-mobile.lab     etcd-ncpvblvhub-hubmaster-103.ncpvblvhub.t-mobile.lab   4/4     Running   8          74d   10.145.151.94   ncpvblvhub-hubmaster-103.ncpvblvhub.t-mobile.lab     [root@dom16hub101-infra-manager ~]# [root@dom16hub101-infra-manager ~]# [root@dom16hub101-infra-manager ~]# oc get nodes NAME                                               STATUS                     ROLES                                 AGE   VERSION ncpvblvhub-hubmaster-101.ncpvblvhub.t-mobile.lab   Ready                      control-plane,master,monitor,worker   74d   v1.29.10+67d3387 ncpvblvhub-hubmaster-102.ncpvblvhub.t-mobile.lab   Ready,SchedulingDisabled   control-plane,master,monitor,worker   74d   v1.29.10+67d3387 ncpvblvhub-hubmaster-103.ncpvblvhub.t-mobile.lab   Ready                      control-plane,master,monitor,worker   74d   v1.29.10+67d3387 ncpvblvhub-hubworker-101.ncpvblvhub.t-mobile.lab   Ready                      gateway,worker                        74d   v1.29.10+67d3387 ncpvblvhub-hubworker-102.ncpvblvhub.t-mobile.lab   Ready                      gateway,worker                        74d   v1.29.10+67d3387 [root@dom16hub101-infra-manager ~]# oc adm drain (reverse-i-search)`drain': oc adm drain ncpvblvhub-hubworker-102.ncpvblvhub.t-mobile.lab ncpvblvhub-hubworker-102.ncpvblvhub.t-mobile.lab [root@dom16hub101-infra-manager ~]# oc adm drain ncpvblvhub-hubmaster-102.ncpvblvhub.t-mobile.lab --ignore-daemonsets --delete-emptydir-data --force node/ncpvblvhub-hubmaster-102.ncpvblvhub.t-mobile.lab already cordoned Warning: ignoring DaemonSet-managed Pods: open-cluster-management-backup/node-agent-lvlpm, openshift-cluster-node-tuning-operator/tuned-2xqzr, openshift-dns/dns-default-x7x7p, openshift-dns/node-resolver-sd5qh, openshift-image-registry/node-ca-5nkn2, openshift-ingress-canary/ingress-canary-rngvh, openshift-local-storage/diskmaker-discovery-4snb5, openshift-local-storage/diskmaker-manager-vkkxk, openshift-machine-api/ironic-proxy-24kth, openshift-machine-config-operator/machine-config-daemon-9j45d, openshift-machine-config-operator/machine-config-server-ww4tz, openshift-monitoring/node-exporter-9xshs, openshift-multus/multus-additional-cni-plugins-hkcdv, openshift-multus/multus-dt7qt, openshift-multus/network-metrics-daemon-4j5fx, openshift-multus/whereabouts-reconciler-bv24v, openshift-network-diagnostics/network-check-target-btgf8, openshift-network-node-identity/network-node-identity-td874, openshift-network-operator/iptables-alerter-75rfw, openshift-nmstate/nmstate-handler-tngrh, openshift-ovn-kubernetes/ovnkube-node-svz9x, openshift-storage/csi-cephfsplugin-rnkrk, openshift-storage/csi-rbdplugin-zg88h; deleting Pods that declare no controller: openshift-etcd/etcd-guard-ncpvblvhub-hubmaster-102.ncpvblvhub.t-mobile.lab, openshift-kube-apiserver/kube-apiserver-guard-ncpvblvhub-hubmaster-102.ncpvblvhub.t-mobile.lab, openshift-kube-controller-manager/kube-controller-manager-guard-ncpvblvhub-hubmaster-102.ncpvblvhub.t-mobile.lab, openshift-kube-scheduler/openshift-kube-scheduler-guard-ncpvblvhub-hubmaster-102.ncpvblvhub.t-mobile.lab evicting pod multicluster-engine/clusterclaims-controller-7d56ff6fc9-555pz evicting pod ncd-git/ncd-git-gitlab-shell-55ccf645ff-ks77d evicting pod multicluster-engine/assisted-image-service-0 evicting pod multicluster-engine/assisted-service-96cf94d9c-wg9zh evicting pod ncd-db/ncd-postgresql-postgresql-ha-sentinel-8556d466c6-kd52n evicting pod openshift-logging/logging-loki-gateway-7687c66f64-fxd4d evicting pod ncd-db/ncd-postgresql-postgresql-ha-proxy-54c69f6cc-csqbz evicting pod open-cluster-management-observability/observability-thanos-rule-1 evicting pod openshift-machine-api/cluster-baremetal-operator-75fbb58cc5-lpqds evicting pod open-cluster-management-hub/cluster-manager-registration-controller-7bcbcd64c5-s5p8l evicting pod open-cluster-management-observability/observability-grafana-85c6896fd4-7vxzp evicting pod openshift-machine-api/control-plane-machine-set-operator-65fbf4bd7-j2l2r evicting pod openshift-kube-apiserver/installer-56-ncpvblvhub-hubmaster-102.ncpvblvhub.t-mobile.lab evicting pod openshift-apiserver/apiserver-776b5f87d7-6t6vz evicting pod openshift-logging/logging-loki-distributor-647986c9d4-6v25d evicting pod open-cluster-management-observability/observability-thanos-query-75d9d7758c-b9ndn evicting pod openshift-monitoring/thanos-querier-86bd945c74-5ptmn evicting pod openshift-logging/cluster-logging-operator-5fd7f999cc-hn5tw evicting pod multicluster-engine/console-mce-console-57b6b4968-jfl4x evicting pod openshift-kube-scheduler/openshift-kube-scheduler-guard-ncpvblvhub-hubmaster-102.ncpvblvhub.t-mobile.lab evicting pod open-cluster-management-hub/cluster-manager-addon-manager-controller-7ff747d9c4-gvlvl evicting pod openshift-kube-apiserver/installer-55-ncpvblvhub-hubmaster-102.ncpvblvhub.t-mobile.lab evicting pod openshift-kube-apiserver/revision-pruner-57-ncpvblvhub-hubmaster-102.ncpvblvhub.t-mobile.lab evicting pod ncd-git/ncd-git-praefect-0 evicting pod openshift-etcd/revision-pruner-9-ncpvblvhub-hubmaster-102.ncpvblvhub.t-mobile.lab evicting pod openshift-kube-apiserver/revision-pruner-58-ncpvblvhub-hubmaster-102.ncpvblvhub.t-mobile.lab evicting pod open-cluster-management/cluster-permission-66c848c957-f7x8t evicting pod ncd-git/ncd-git-gitaly-default-0 evicting pod ncd-git/ncd-git-webservice-default-69c6749944-fb4vb evicting pod open-cluster-management-observability/observability-thanos-receive-controller-56dc887bbf-g2xl4 evicting pod ncd-db/ncd-postgresql-postgresql-ha-keeper-2 evicting pod openshift-kube-apiserver/revision-pruner-56-ncpvblvhub-hubmaster-102.ncpvblvhub.t-mobile.lab evicting pod openshift-logging/logging-loki-querier-67f785c56c-hdqpl evicting pod openshift-kube-apiserver/revision-pruner-59-ncpvblvhub-hubmaster-102.ncpvblvhub.t-mobile.lab evicting pod open-cluster-management-agent/klusterlet-566f969d78-gt5ld evicting pod open-cluster-management-hub/cluster-manager-registration-webhook-9c6456956-k62pb evicting pod openshift-kube-apiserver/installer-59-ncpvblvhub-hubmaster-102.ncpvblvhub.t-mobile.lab evicting pod openshift-route-controller-manager/route-controller-manager-5fcc66b66c-4gf4h evicting pod open-cluster-management-agent/klusterlet-agent-58cfcbdd89-glgjb evicting pod open-cluster-management-observability/observability-rbac-query-proxy-559d84cdb8-r8rbz evicting pod open-cluster-management/multiclusterhub-operator-b8594bb5b-8kdt6 evicting pod openshift-kube-controller-manager/kube-controller-manager-guard-ncpvblvhub-hubmaster-102.ncpvblvhub.t-mobile.lab evicting pod openshift-console/console-7c7f65d4d9-dlxdv evicting pod open-cluster-management/grc-policy-propagator-8798d56f9-hjrpv evicting pod ncd-db/ncd-redis-crdb-redisio-admin-6c86857d5c-m5x22 evicting pod open-cluster-management/klusterlet-addon-controller-v2-c6b9f6f4-zlv9s evicting pod open-cluster-management-backup/velero-549fbfb95d-q78p4 evicting pod open-cluster-management-backup/cluster-backup-chart-clusterbackup-698656f7f-q9q5s evicting pod openshift-operators/quay-operator.v3.12.4-7c5644d54b-n9wp9 evicting pod open-cluster-management/multicluster-integrations-6fdb6554f4-rcgpf evicting pod open-cluster-management-observability/observability-thanos-store-memcached-0 evicting pod openshift-oauth-apiserver/apiserver-6796c5d69f-5xmlw evicting pod openshift-operators-redhat/loki-operator-controller-manager-6cd9c69c58-25q78 evicting pod openshift-kube-apiserver/revision-pruner-55-ncpvblvhub-hubmaster-102.ncpvblvhub.t-mobile.lab evicting pod openshift-logging/logging-loki-query-frontend-55bc666b5c-87hmh evicting pod ncd-db/ncd-redis-crdb-redisio-sentinel-1 evicting pod openshift-authentication/oauth-openshift-84fc79f657-c9qhc evicting pod openshift-user-workload-monitoring/thanos-ruler-user-workload-0 evicting pod openshift-machine-api/machine-api-operator-7dc7bbdbb6-d2mmf evicting pod openshift-storage/csi-rbdplugin-provisioner-646d95bdd9-496ng evicting pod openshift-gitops-operator/openshift-gitops-operator-controller-manager-56f89579cc-bbqgp evicting pod openshift-controller-manager/controller-manager-84fdfdf98b-6xbvq evicting pod openshift-monitoring/alertmanager-main-1 evicting pod openshift-etcd/etcd-guard-ncpvblvhub-hubmaster-102.ncpvblvhub.t-mobile.lab evicting pod openshift-kube-apiserver/kube-apiserver-guard-ncpvblvhub-hubmaster-102.ncpvblvhub.t-mobile.lab evicting pod openshift-gitops/gitops-plugin-5dcb489f77-l4jl5 evicting pod openshift-kube-apiserver/installer-57-ncpvblvhub-hubmaster-102.ncpvblvhub.t-mobile.lab evicting pod open-cluster-management-observability/endpoint-observability-operator-64fbfbb8b6-j67mh evicting pod open-cluster-management-observability/observability-thanos-query-frontend-69f4fcb776-qmgw5 evicting pod quay-registry/quay-registry-clair-app-5b9fc47b65-hpctr evicting pod quay-registry/quay-registry-quay-app-7446d956d5-zh4kr evicting pod open-cluster-management/search-api-5b6f649447-s4r79 evicting pod openshift-machine-api/machine-api-controllers-56564f754-9ssz5 evicting pod openshift-machine-api/cluster-autoscaler-operator-66ddd8d9c5-ktjm9 evicting pod multicluster-engine/ocm-proxyserver-554c8fb75-x5vcr evicting pod openshift-kube-apiserver/installer-58-ncpvblvhub-hubmaster-102.ncpvblvhub.t-mobile.lab evicting pod openshift-monitoring/prometheus-k8s-1 evicting pod multicluster-engine/multicluster-engine-operator-6cb65d498f-mxz4t pod/revision-pruner-56-ncpvblvhub-hubmaster-102.ncpvblvhub.t-mobile.lab evicted pod/installer-56-ncpvblvhub-hubmaster-102.ncpvblvhub.t-mobile.lab evicted I0527 17:02:34.668429 3151066 request.go:697] Waited for 1.000860285s due to client-side throttling, not priority and fairness, request: POST:https://api.ncpvblvhub.t-mobile.lab:6443/api/v1/namespaces/open-cluster-management-hub/pods/cluster-manager-registration-webhook-9c6456956-k62pb/eviction pod/multiclusterhub-operator-b8594bb5b-8kdt6 evicted pod/thanos-querier-86bd945c74-5ptmn evicted pod/ncd-git-gitaly-default-0 evicted pod/observability-thanos-receive-controller-56dc887bbf-g2xl4 evicted pod/cluster-baremetal-operator-75fbb58cc5-lpqds evicted pod/ncd-git-gitlab-shell-55ccf645ff-ks77d evicted pod/openshift-kube-scheduler-guard-ncpvblvhub-hubmaster-102.ncpvblvhub.t-mobile.lab evicted pod/observability-grafana-85c6896fd4-7vxzp evicted pod/cluster-manager-registration-webhook-9c6456956-k62pb evicted pod/clusterclaims-controller-7d56ff6fc9-555pz evicted pod/observability-thanos-query-75d9d7758c-b9ndn evicted pod/kube-controller-manager-guard-ncpvblvhub-hubmaster-102.ncpvblvhub.t-mobile.lab evicted pod/ncd-git-praefect-0 evicted pod/klusterlet-addon-controller-v2-c6b9f6f4-zlv9s evicted pod/ncd-postgresql-postgresql-ha-proxy-54c69f6cc-csqbz evicted pod/cluster-manager-registration-controller-7bcbcd64c5-s5p8l evicted pod/logging-loki-gateway-7687c66f64-fxd4d evicted pod/grc-policy-propagator-8798d56f9-hjrpv evicted pod/assisted-image-service-0 evicted pod/klusterlet-566f969d78-gt5ld evicted pod/cluster-backup-chart-clusterbackup-698656f7f-q9q5s evicted pod/revision-pruner-59-ncpvblvhub-hubmaster-102.ncpvblvhub.t-mobile.lab evicted pod/assisted-service-96cf94d9c-wg9zh evicted pod/observability-thanos-rule-1 evicted pod/control-plane-machine-set-operator-65fbf4bd7-j2l2r evicted pod/klusterlet-agent-58cfcbdd89-glgjb evicted pod/observability-rbac-query-proxy-559d84cdb8-r8rbz evicted pod/route-controller-manager-5fcc66b66c-4gf4h evicted pod/revision-pruner-9-ncpvblvhub-hubmaster-102.ncpvblvhub.t-mobile.lab evicted pod/ncd-postgresql-postgresql-ha-sentinel-8556d466c6-kd52n evicted pod/revision-pruner-58-ncpvblvhub-hubmaster-102.ncpvblvhub.t-mobile.lab evicted pod/quay-operator.v3.12.4-7c5644d54b-n9wp9 evicted I0527 17:02:44.676775 3151066 request.go:697] Waited for 4.600338869s due to client-side throttling, not priority and fairness, request: GET:https://api.ncpvblvhub.t-mobile.lab:6443/api/v1/namespaces/open-cluster-management/pods/multicluster-integrations-6fdb6554f4-rcgpf pod/multicluster-integrations-6fdb6554f4-rcgpf evicted pod/observability-thanos-store-memcached-0 evicted pod/loki-operator-controller-manager-6cd9c69c58-25q78 evicted pod/revision-pruner-55-ncpvblvhub-hubmaster-102.ncpvblvhub.t-mobile.lab evicted pod/cluster-logging-operator-5fd7f999cc-hn5tw evicted pod/cluster-manager-addon-manager-controller-7ff747d9c4-gvlvl evicted pod/velero-549fbfb95d-q78p4 evicted pod/installer-55-ncpvblvhub-hubmaster-102.ncpvblvhub.t-mobile.lab evicted pod/ncd-redis-crdb-redisio-sentinel-1 evicted pod/installer-59-ncpvblvhub-hubmaster-102.ncpvblvhub.t-mobile.lab evicted pod/revision-pruner-57-ncpvblvhub-hubmaster-102.ncpvblvhub.t-mobile.lab evicted pod/ncd-redis-crdb-redisio-admin-6c86857d5c-m5x22 evicted pod/machine-api-operator-7dc7bbdbb6-d2mmf evicted pod/thanos-ruler-user-workload-0 evicted pod/csi-rbdplugin-provisioner-646d95bdd9-496ng evicted pod/openshift-gitops-operator-controller-manager-56f89579cc-bbqgp evicted pod/controller-manager-84fdfdf98b-6xbvq evicted pod/alertmanager-main-1 evicted pod/kube-apiserver-guard-ncpvblvhub-hubmaster-102.ncpvblvhub.t-mobile.lab evicted pod/etcd-guard-ncpvblvhub-hubmaster-102.ncpvblvhub.t-mobile.lab evicted pod/gitops-plugin-5dcb489f77-l4jl5 evicted pod/installer-57-ncpvblvhub-hubmaster-102.ncpvblvhub.t-mobile.lab evicted pod/observability-thanos-query-frontend-69f4fcb776-qmgw5 evicted pod/endpoint-observability-operator-64fbfbb8b6-j67mh evicted pod/search-api-5b6f649447-s4r79 evicted pod/ncd-git-webservice-default-69c6749944-fb4vb evicted pod/machine-api-controllers-56564f754-9ssz5 evicted pod/cluster-autoscaler-operator-66ddd8d9c5-ktjm9 evicted pod/ocm-proxyserver-554c8fb75-x5vcr evicted pod/installer-58-ncpvblvhub-hubmaster-102.ncpvblvhub.t-mobile.lab evicted pod/prometheus-k8s-1 evicted pod/multicluster-engine-operator-6cb65d498f-mxz4t evicted pod/quay-registry-clair-app-5b9fc47b65-hpctr evicted I0527 17:02:54.876785 3151066 request.go:697] Waited for 6.390570482s due to client-side throttling, not priority and fairness, request: GET:https://api.ncpvblvhub.t-mobile.lab:6443/api/v1/namespaces/ncd-db/pods/ncd-postgresql-postgresql-ha-keeper-2 pod/console-7c7f65d4d9-dlxdv evicted pod/apiserver-6796c5d69f-5xmlw evicted I0527 17:03:05.476638 3151066 request.go:697] Waited for 1.196994232s due to client-side throttling, not priority and fairness, request: GET:https://api.ncpvblvhub.t-mobile.lab:6443/api/v1/namespaces/quay-registry/pods/quay-registry-quay-app-7446d956d5-zh4kr pod/cluster-permission-66c848c957-f7x8t evicted pod/logging-loki-querier-67f785c56c-hdqpl evicted pod/logging-loki-distributor-647986c9d4-6v25d evicted pod/quay-registry-quay-app-7446d956d5-zh4kr evicted pod/oauth-openshift-84fc79f657-c9qhc evicted pod/console-mce-console-57b6b4968-jfl4x evicted pod/logging-loki-query-frontend-55bc666b5c-87hmh evicted pod/ncd-postgresql-postgresql-ha-keeper-2 evicted pod/apiserver-776b5f87d7-6t6vz evicted node/ncpvblvhub-hubmaster-102.ncpvblvhub.t-mobile.lab drained [root@dom16hub101-infra-manager ~]# <p>[root@dom16hub101-infra-manager ~]# oc get nodes NAME                                               STATUS                     ROLES                                 AGE   VERSION ncpvblvhub-hubmaster-101.ncpvblvhub.t-mobile.lab   Ready                      control-plane,master,monitor,worker   74d   v1.29.10+67d3387 ncpvblvhub-hubmaster-102.ncpvblvhub.t-mobile.lab   Ready,SchedulingDisabled   control-plane,master,monitor,worker   74d   v1.29.10+67d3387 ncpvblvhub-hubmaster-103.ncpvblvhub.t-mobile.lab   Ready                      control-plane,master,monitor,worker   74d   v1.29.10+67d3387 ncpvblvhub-hubworker-101.ncpvblvhub.t-mobile.lab   Ready                      gateway,worker                        74d   v1.29.10+67d3387 ncpvblvhub-hubworker-102.ncpvblvhub.t-mobile.lab   Ready                      gateway,worker                        74d   v1.29.10+67d3387</p> <p>power off the node  [root@dom16hub101-infra-manager ~]# oc get nodes NAME                                               STATUS                        ROLES                                 AGE   VERSION ncpvblvhub-hubmaster-101.ncpvblvhub.t-mobile.lab   Ready                         control-plane,master,monitor,worker   74d   v1.29.10+67d3387 ncpvblvhub-hubmaster-102.ncpvblvhub.t-mobile.lab   NotReady,SchedulingDisabled   control-plane,master,monitor,worker   74d   v1.29.10+67d3387 ncpvblvhub-hubmaster-103.ncpvblvhub.t-mobile.lab   Ready                         control-plane,master,monitor,worker   74d   v1.29.10+67d3387 ncpvblvhub-hubworker-101.ncpvblvhub.t-mobile.lab   Ready                         gateway,worker                        74d   v1.29.10+67d3387 ncpvblvhub-hubworker-102.ncpvblvhub.t-mobile.lab   Ready                         gateway,worker                        74d   v1.29.10+67d3387 [root@dom16hub101-infra-manager ~]# [root@dom16hub101-infra-manager ~]# oc -n openshift-etcd get pods -l k8s-app=etcd -o wide NAME                                                    READY   STATUS    RESTARTS   AGE   IP              NODE                                               NOMINATED NODE   READINESS GATES etcd-ncpvblvhub-hubmaster-101.ncpvblvhub.t-mobile.lab   4/4     Running   12         74d   10.145.151.92   ncpvblvhub-hubmaster-101.ncpvblvhub.t-mobile.lab     etcd-ncpvblvhub-hubmaster-102.ncpvblvhub.t-mobile.lab   4/4     Running   12         74d   10.145.151.93   ncpvblvhub-hubmaster-102.ncpvblvhub.t-mobile.lab     etcd-ncpvblvhub-hubmaster-103.ncpvblvhub.t-mobile.lab   4/4     Running   8          74d   10.145.151.94   ncpvblvhub-hubmaster-103.ncpvblvhub.t-mobile.lab     [root@dom16hub101-infra-manager ~]# oc rsh -n openshift-etcd etcd-ncpvblvhub-hubmaster-101.ncpvblvhub.t-mobile.lab sh-5.1# etcdctl endpoint health {\"level\":\"warn\",\"ts\":\"2025-05-27T17:09:44.374886Z\",\"logger\":\"client\",\"caller\":\"v3@v3.5.14/retry_interceptor.go:63\",\"msg\":\"retrying of unary invoker failed\",\"target\":\"etcd-endpoints://0xc00027a000/10.145.151.93:2379\",\"attempt\":0,\"error\":\"rpc error: code = DeadlineExceeded desc = context deadline exceeded\"} https://10.145.151.94:2379 is healthy: successfully committed proposal: took = 7.162307ms https://10.145.151.92:2379 is healthy: successfully committed proposal: took = 7.213802ms https://10.145.151.93:2379 is unhealthy: failed to commit proposal: context deadline exceeded Error: unhealthy cluster sh-5.1# etcdctl member list -w table +------------------+---------+--------------------------------------------------+----------------------------+----------------------------+------------+ |        ID        | STATUS  |                       NAME                       |         PEER ADDRS         |        CLIENT ADDRS        | IS LEARNER | +------------------+---------+--------------------------------------------------+----------------------------+----------------------------+------------+ | 44ad9888985e068c | started | ncpvblvhub-hubmaster-101.ncpvblvhub.t-mobile.lab | https://10.145.151.92:2380 | https://10.145.151.92:2379 |      false | | f26d12a58d17e571 | started | ncpvblvhub-hubmaster-103.ncpvblvhub.t-mobile.lab | https://10.145.151.94:2380 | https://10.145.151.94:2379 |      false | | fc4de79a3d723a5c | started | ncpvblvhub-hubmaster-102.ncpvblvhub.t-mobile.lab | https://10.145.151.93:2380 | https://10.145.151.93:2379 |      false | +------------------+---------+--------------------------------------------------+----------------------------+----------------------------+------------+ sh-5.1# etcdctl member remove fc4de79a3d723a5c Member fc4de79a3d723a5c removed from cluster 136d42915c2b0516 sh-5.1# etcdctl member list -w table +------------------+---------+--------------------------------------------------+----------------------------+----------------------------+------------+ |        ID        | STATUS  |                       NAME                       |         PEER ADDRS         |        CLIENT ADDRS        | IS LEARNER | +------------------+---------+--------------------------------------------------+----------------------------+----------------------------+------------+ | 44ad9888985e068c | started | ncpvblvhub-hubmaster-101.ncpvblvhub.t-mobile.lab | https://10.145.151.92:2380 | https://10.145.151.92:2379 |      false | | f26d12a58d17e571 | started | ncpvblvhub-hubmaster-103.ncpvblvhub.t-mobile.lab | https://10.145.151.94:2380 | https://10.145.151.94:2379 |      false | +------------------+---------+--------------------------------------------------+----------------------------+----------------------------+------------+ sh-5.1# etcdctl endpoint health {\"level\":\"warn\",\"ts\":\"2025-05-27T17:10:43.025287Z\",\"logger\":\"client\",\"caller\":\"v3@v3.5.14/retry_interceptor.go:63\",\"msg\":\"retrying of unary invoker failed\",\"target\":\"etcd-endpoints://0xc000020000/10.145.151.93:2379\",\"attempt\":0,\"error\":\"rpc error: code = DeadlineExceeded desc = context deadline exceeded\"} https://10.145.151.94:2379 is healthy: successfully committed proposal: took = 6.399106ms https://10.145.151.92:2379 is healthy: successfully committed proposal: took = 6.344247ms https://10.145.151.93:2379 is unhealthy: failed to commit proposal: context deadline exceeded Error: unhealthy cluster sh-5.1# exit command terminated with exit code 1 [root@dom16hub101-infra-manager ~]# #oc patch etcd/cluster --type=merge -p '{\"spec\": {\"unsupportedConfigOverrides\": {\"useUnsupportedUnsafeNonHANonProductionUnstableEtcd\": true}}}' [root@dom16hub101-infra-manager ~]# oc patch etcd/cluster --type=merge -p '{\"spec\":{\"unsupportedConfigOverrides\":{\"useUnsupportedUnsafeNonHANonProductionUnstableEtcd\": true}}}' etcd.operator.openshift.io/cluster patched [root@dom16hub101-infra-manager ~]# oc get secrets -n openshift-etcd | grep master-102 etcd-peer-ncpvblvhub-hubmaster-102.ncpvblvhub.t-mobile.lab              kubernetes.io/tls   2      74d etcd-serving-metrics-ncpvblvhub-hubmaster-102.ncpvblvhub.t-mobile.lab   kubernetes.io/tls   2      74d etcd-serving-ncpvblvhub-hubmaster-102.ncpvblvhub.t-mobile.lab           kubernetes.io/tls   2      74d [root@dom16hub101-infra-manager ~]# #for i in <code>oc get secrets -n openshift-etcd | grep master-2 | awk '{print $1}'</code>; do oc delete secrets -n openshift-etcd $i; done [root@dom16hub101-infra-manager ~]# for i in <code>oc get secrets -n openshift-etcd | grep master-102 | awk '{print $1}'</code>; do oc delete secrets -n openshift-etcd $i; done secret \"etcd-peer-ncpvblvhub-hubmaster-102.ncpvblvhub.t-mobile.lab\" deleted secret \"etcd-serving-metrics-ncpvblvhub-hubmaster-102.ncpvblvhub.t-mobile.lab\" deleted secret \"etcd-serving-ncpvblvhub-hubmaster-102.ncpvblvhub.t-mobile.lab\" deleted [root@dom16hub101-infra-manager ~]# oc get secrets -n openshift-etcd | grep master-102                                                                 etcd-peer-ncpvblvhub-hubmaster-102.ncpvblvhub.t-mobile.lab              kubernetes.io/tls   2      3s etcd-serving-metrics-ncpvblvhub-hubmaster-102.ncpvblvhub.t-mobile.lab   kubernetes.io/tls   2      2s etcd-serving-ncpvblvhub-hubmaster-102.ncpvblvhub.t-mobile.lab           kubernetes.io/tls   2      3s [root@dom16hub101-infra-manager ~]# [root@dom16hub101-infra-manager ~]# [root@dom16hub101-infra-manager ~]# [root@dom16hub101-infra-manager ~]# oc get machines No resources found in default namespace. [root@dom16hub101-infra-manager ~]# oc get machines -A No resources found [root@dom16hub101-infra-manager ~]# oc get nodes NAME                                               STATUS                        ROLES                                 AGE   VERSION ncpvblvhub-hubmaster-101.ncpvblvhub.t-mobile.lab   Ready                         control-plane,master,monitor,worker   74d   v1.29.10+67d3387 ncpvblvhub-hubmaster-102.ncpvblvhub.t-mobile.lab   NotReady,SchedulingDisabled   control-plane,master,monitor,worker   74d   v1.29.10+67d3387 ncpvblvhub-hubmaster-103.ncpvblvhub.t-mobile.lab   Ready                         control-plane,master,monitor,worker   74d   v1.29.10+67d3387 ncpvblvhub-hubworker-101.ncpvblvhub.t-mobile.lab   Ready                         gateway,worker                        74d   v1.29.10+67d3387 ncpvblvhub-hubworker-102.ncpvblvhub.t-mobile.lab   Ready                         gateway,worker                        74d   v1.29.10+67d3387 [root@dom16hub101-infra-manager ~]# oc get machines -A No resources found [root@dom16hub101-infra-manager ~]# oc get machine -A No resources found [root@dom16hub101-infra-manager ~]# oc get machines.machine -A NAMESPACE               NAME                              PHASE     TYPE   REGION   ZONE   AGE openshift-machine-api   ncpvblvhub-b6cjs-master-0         Running                          74d openshift-machine-api   ncpvblvhub-b6cjs-master-1         Running                          74d openshift-machine-api   ncpvblvhub-b6cjs-master-2         Running                          74d openshift-machine-api   ncpvblvhub-b6cjs-worker-0-mlq8w   Running                          74d openshift-machine-api   ncpvblvhub-b6cjs-worker-0-x5dc5   Running                          74d [root@dom16hub101-infra-manager ~]# oc get machines.machine -n openshift-machine-api NAME                              PHASE     TYPE   REGION   ZONE   AGE ncpvblvhub-b6cjs-master-0         Running                          74d ncpvblvhub-b6cjs-master-1         Running                          74d ncpvblvhub-b6cjs-master-2         Running                          74d ncpvblvhub-b6cjs-worker-0-mlq8w   Running                          74d ncpvblvhub-b6cjs-worker-0-x5dc5   Running                          74d [root@dom16hub101-infra-manager ~]# [root@dom16hub101-infra-manager ~]# [root@dom16hub101-infra-manager ~]# # c get machines.machine.openshift.io -n openshift-machine-api -o yaml <code>oc get machines.machine.openshift.io -n openshift-machine-api -o wide |grep ncpvblvhub-hubmaster-102.ncpvblvhub.t-mobile.lab|awk {'print $1'}</code> &gt; backup_machine$master.yaml [root@dom16hub101-infra-manager ~]# oc get machines.machine -n openshift-machine-api [root@dom16hub101-infra-manager ~]# oc get machines.machine.openshift.io -n openshift-machine-api -o yaml <code>oc get machines.machine.openshift.io -n openshift-machine-api -o wide |grep ncpvblvhub-hubmaster-102.ncpvblvhub.t-mobile.lab|awk {'print $1'}</code> &gt; backup_machinemaster-102.yaml [root@dom16hub101-infra-manager ~]# cat backup_machinemaster-102.yaml apiVersion: machine.openshift.io/v1beta1 kind: Machine metadata:   annotations:     machine.openshift.io/instance-state: unmanaged     metal3.io/BareMetalHost: openshift-machine-api/ncpvblvhub-hubmaster-102.ncpvblvhub.t-mobile.lab   creationTimestamp: \"2025-03-13T23:51:45Z\"   finalizers:   - machine.machine.openshift.io   generation: 3   labels:     machine.openshift.io/cluster-api-cluster: ncpvblvhub-b6cjs     machine.openshift.io/cluster-api-machine-role: master     machine.openshift.io/cluster-api-machine-type: master   name: ncpvblvhub-b6cjs-master-1   namespace: openshift-machine-api   resourceVersion: \"140998860\"   uid: a53b879c-a886-4381-a1bb-9322333fa76e spec:   lifecycleHooks:     preDrain:     - name: EtcdQuorumOperator       owner: clusteroperator/etcd   metadata: {}   providerID: baremetalhost:///openshift-machine-api/ncpvblvhub-hubmaster-102.ncpvblvhub.t-mobile.lab/346b01e0-6bb8-40bd-8db3-d3eabf4d44f1   providerSpec:     value:       apiVersion: baremetal.cluster.k8s.io/v1alpha1       customDeploy:         method: install_coreos       hostSelector: {}       image:         checksum: \"\"         url: \"\"       kind: BareMetalMachineProviderSpec       metadata:         creationTimestamp: null       userData:         name: master-user-data-managed status:   addresses:   - address: fde1:53ba:e9a0:de11:912f:2112:633a:4b75     type: InternalIP   - address: \"\"     type: InternalIP   - address: \"\"     type: InternalIP   - address: \"\"     type: InternalIP   - address: \"\"     type: InternalIP   - address: \"\"     type: InternalIP   - address: \"\"     type: InternalIP   - address: \"\"     type: InternalIP   - address: \"\"     type: InternalIP   - address: 10.145.151.93     type: InternalIP   - address: \"\"     type: InternalIP   conditions:   - lastTransitionTime: \"2025-03-14T00:04:57Z\"     message: 'Drain operation currently blocked by: [{Name:EtcdQuorumOperator Owner:clusteroperator/etcd}]'     reason: HookPresent     severity: Warning     status: \"False\"     type: Drainable   - lastTransitionTime: \"2025-03-14T00:03:59Z\"     status: \"True\"     type: InstanceExists   - lastTransitionTime: \"2025-03-13T23:58:56Z\"     status: \"True\"     type: Terminable   lastUpdated: \"2025-05-27T17:08:29Z\"   nodeRef:     kind: Node     name: ncpvblvhub-hubmaster-102.ncpvblvhub.t-mobile.lab     uid: cdc0d098-638f-4c2d-b4ba-4c2c1ebc1c10   phase: Running [root@dom16hub101-infra-manager ~]# cat backup_machinemaster-102_editted.yaml apiVersion: machine.openshift.io/v1beta1 kind: Machine metadata:   annotations:     machine.openshift.io/instance-state: unmanaged     metal3.io/BareMetalHost: openshift-machine-api/ncpvblvhub-hubmaster-102.ncpvblvhub.t-mobile.lab   finalizers:   - machine.machine.openshift.io   generation: 3   labels:     machine.openshift.io/cluster-api-cluster: ncpvblvhub-b6cjs     machine.openshift.io/cluster-api-machine-role: master     machine.openshift.io/cluster-api-machine-type: master   name: ncpvblvhub-b6cjs-master-1   namespace: openshift-machine-api spec:   lifecycleHooks:     preDrain:     - name: EtcdQuorumOperator       owner: clusteroperator/etcd   metadata: {}   providerID: baremetalhost:///openshift-machine-api/ncpvblvhub-hubmaster-102.ncpvblvhub.t-mobile.lab/346b01e0-6bb8-40bd-8db3-d3eabf4d44f1   providerSpec:     value:       apiVersion: baremetal.cluster.k8s.io/v1alpha1       customDeploy:         method: install_coreos       hostSelector: {}       image:         checksum: \"\"         url: \"\"       kind: BareMetalMachineProviderSpec       metadata:         creationTimestamp: null       userData:         name: master-user-data-managed <p>[root@dom16hub101-infra-manager ~]# oc get nodes NAME                                               STATUS                        ROLES                                 AGE   VERSION ncpvblvhub-hubmaster-101.ncpvblvhub.t-mobile.lab   Ready                         control-plane,master,monitor,worker   74d   v1.29.10+67d3387 ncpvblvhub-hubmaster-102.ncpvblvhub.t-mobile.lab   NotReady,SchedulingDisabled   control-plane,master,monitor,worker   74d   v1.29.10+67d3387 ncpvblvhub-hubmaster-103.ncpvblvhub.t-mobile.lab   Ready                         control-plane,master,monitor,worker   74d   v1.29.10+67d3387 ncpvblvhub-hubworker-101.ncpvblvhub.t-mobile.lab   Ready                         gateway,worker                        74d   v1.29.10+67d3387 ncpvblvhub-hubworker-102.ncpvblvhub.t-mobile.lab   Ready                         gateway,worker                        74d   v1.29.10+67d3387</p> <p>[root@dom16hub101-infra-manager ~]# oc label node ncpvblvhub-hubmaster-102.ncpvblvhub.t-mobile.lab cluster.ocs.openshift.io/openshift-storage- node/ncpvblvhub-hubmaster-102.ncpvblvhub.t-mobile.lab unlabeled [root@dom16hub101-infra-manager ~]#</p> <p>[root@dom16hub101-infra-manager ~]# oc process -n openshift-storage ocs-osd-removal -p FAILED_OSD_IDS=0,5 FORCE_OSD_REMOVAL=true | oc create -n openshift-storage -f \u2013 error: the path \"\u2013\" does not exist [root@dom16hub101-infra-manager ~]# oc process -n openshift-storage ocs-osd-removal -p FAILED_OSD_IDS=0,5 FORCE_OSD_REMOVAL=true | oc create -n openshift-storage -f - Error from server (AlreadyExists): error when creating \"STDIN\": jobs.batch \"ocs-osd-removal-job\" already exists [root@dom16hub101-infra-manager ~]# oc get jobs -n openshift-storage | gre removal bash: gre: command not found... [root@dom16hub101-infra-manager ~]# oc get jobs -n openshift-storage | grep removal ocs-osd-removal-job                                      1/1           14m        33d [root@dom16hub101-infra-manager ~]# oc delete jobs -n openshift-storage ocs-osd-removal-job job.batch \"ocs-osd-removal-job\" deleted [root@dom16hub101-infra-manager ~]# oc process -n openshift-storage ocs-osd-removal -p FAILED_OSD_IDS=0,5 FORCE_OSD_REMOVAL=true | oc create -n openshift-storage -f - job.batch/ocs-osd-removal-job created [root@dom16hub101-infra-manager ~]# oc get jobs -n openshift-storage | grep removal                                                                    ocs-osd-removal-job                                      0/1           5s         5s [root@dom16hub101-infra-manager ~]# oc get jobs -n openshift-storage | grep removal ocs-osd-removal-job                                      1/1           13s        14s [root@dom16hub101-infra-manager ~]# oc delete jobs -n openshift-storage ocs-osd-removal-job                                                            job.batch \"ocs-osd-removal-job\" deleted [root@dom16hub101-infra-manager ~]# oc get pv | grep local | grep -i released local-pv-4fcd3797                          3576Gi     RWO            Delete           Released    openshift-storage/ocs-deviceset-localblockstorage-0-data-0ll69z                      localblockstorage                               71d local-pv-cb7421c8                          3576Gi     RWO            Delete           Released    openshift-storage/ocs-deviceset-localblockstorage-2-data-1w8q46                      localblockstorage                               33d [root@dom16hub101-infra-manager ~]# oc delete pv local-pv-4fcd3797 local-pv-cb7421c8 persistentvolume \"local-pv-4fcd3797\" deleted persistentvolume \"local-pv-cb7421c8\" deleted [root@dom16hub101-infra-manager ~]# <p>[root@dom16hub101-infra-manager ~]# oc get clusteroperator baremetal NAME        VERSION   AVAILABLE   PROGRESSING   DEGRADED   SINCE   MESSAGE baremetal   4.16.24   True        False         False      74d [root@dom16hub101-infra-manager ~]# oc get bmh -n openshift-machine-api NAME                                               STATE       CONSUMER                          ONLINE   ERROR   AGE ncpvblvhub-hubmaster-101.ncpvblvhub.t-mobile.lab   unmanaged   ncpvblvhub-b6cjs-master-0         true             74d ncpvblvhub-hubmaster-102.ncpvblvhub.t-mobile.lab   unmanaged   ncpvblvhub-b6cjs-master-1         true             74d ncpvblvhub-hubmaster-103.ncpvblvhub.t-mobile.lab   unmanaged   ncpvblvhub-b6cjs-master-2         true             74d ncpvblvhub-hubworker-101.ncpvblvhub.t-mobile.lab   unmanaged   ncpvblvhub-b6cjs-worker-0-mlq8w   true             74d ncpvblvhub-hubworker-102.ncpvblvhub.t-mobile.lab   unmanaged   ncpvblvhub-b6cjs-worker-0-x5dc5   true             74d [root@dom16hub101-infra-manager ~]# oc delete bmh -n openshift-machine-api ncpvblvhub-hubmaster-102.ncpvblvhub.t-mobile.lab baremetalhost.metal3.io \"ncpvblvhub-hubmaster-102.ncpvblvhub.t-mobile.lab\" deleted [root@dom16hub101-infra-manager ~]# oc get bmh -n openshift-machine-api NAME                                               STATE       CONSUMER                          ONLINE   ERROR   AGE ncpvblvhub-hubmaster-101.ncpvblvhub.t-mobile.lab   unmanaged   ncpvblvhub-b6cjs-master-0         true             74d ncpvblvhub-hubmaster-103.ncpvblvhub.t-mobile.lab   unmanaged   ncpvblvhub-b6cjs-master-2         true             74d ncpvblvhub-hubworker-101.ncpvblvhub.t-mobile.lab   unmanaged   ncpvblvhub-b6cjs-worker-0-mlq8w   true             74d ncpvblvhub-hubworker-102.ncpvblvhub.t-mobile.lab   unmanaged   ncpvblvhub-b6cjs-worker-0-x5dc5   true             74d [root@dom16hub101-infra-manager ~]# oc get machines.machine.openshift.io -n openshift-machine-api NAME                              PHASE     TYPE   REGION   ZONE   AGE ncpvblvhub-b6cjs-master-0         Running                          74d ncpvblvhub-b6cjs-master-1         Failed                           74d ncpvblvhub-b6cjs-master-2         Running                          74d ncpvblvhub-b6cjs-worker-0-mlq8w   Running                          74d ncpvblvhub-b6cjs-worker-0-x5dc5   Running                          74d [root@dom16hub101-infra-manager ~]# #oc delete machines.machine.openshift.io -n openshift-machine-api ncpvblvhub-b6cjs-master-1 [root@dom16hub101-infra-manager ~]# oc describe machines.machine.openshift.io -n openshift-machine-api ncpvblvhub-b6cjs-master-1 | grep -i master-102               metal3.io/BareMetalHost: openshift-machine-api/ncpvblvhub-hubmaster-102.ncpvblvhub.t-mobile.lab   Provider ID:  baremetalhost:///openshift-machine-api/ncpvblvhub-hubmaster-102.ncpvblvhub.t-mobile.lab/346b01e0-6bb8-40bd-8db3-d3eabf4d44f1     Name:  ncpvblvhub-hubmaster-102.ncpvblvhub.t-mobile.lab [root@dom16hub101-infra-manager ~]# [root@dom16hub101-infra-manager ~]# oc delete machines.machine.openshift.io -n openshift-machine-api ncpvblvhub-b6cjs-master-1 machine.machine.openshift.io \"ncpvblvhub-b6cjs-master-1\" deleted [root@dom16hub101-infra-manager ~]# oc get machines.machine.openshift.io -n openshift-machine-api NAME                              PHASE     TYPE   REGION   ZONE   AGE ncpvblvhub-b6cjs-master-0         Running                          74d ncpvblvhub-b6cjs-master-2         Running                          74d ncpvblvhub-b6cjs-worker-0-mlq8w   Running                          74d ncpvblvhub-b6cjs-worker-0-x5dc5   Running                          74d [root@dom16hub101-infra-manager ~]#</p> <p>[root@dom16hub101-infra-manager ~]# oc get machines.machine.openshift.io -n openshift-machine-api NAME                              PHASE     TYPE   REGION   ZONE   AGE ncpvblvhub-b6cjs-master-0         Running                          74d ncpvblvhub-b6cjs-master-2         Running                          74d ncpvblvhub-b6cjs-worker-0-mlq8w   Running                          74d ncpvblvhub-b6cjs-worker-0-x5dc5   Running                          74d [root@dom16hub101-infra-manager ~]# oc get bmh -n openshift-machine-api NAME                                               STATE       CONSUMER                          ONLINE   ERROR   AGE ncpvblvhub-hubmaster-101.ncpvblvhub.t-mobile.lab   unmanaged   ncpvblvhub-b6cjs-master-0         true             74d ncpvblvhub-hubmaster-103.ncpvblvhub.t-mobile.lab   unmanaged   ncpvblvhub-b6cjs-master-2         true             74d ncpvblvhub-hubworker-101.ncpvblvhub.t-mobile.lab   unmanaged   ncpvblvhub-b6cjs-worker-0-mlq8w   true             74d ncpvblvhub-hubworker-102.ncpvblvhub.t-mobile.lab   unmanaged   ncpvblvhub-b6cjs-worker-0-x5dc5   true             74d [root@dom16hub101-infra-manager ~]# oc get machines.machine.openshift.io -n openshift-machine-api NAME                              PHASE     TYPE   REGION   ZONE   AGE ncpvblvhub-b6cjs-master-0         Running                          74d ncpvblvhub-b6cjs-master-2         Running                          74d ncpvblvhub-b6cjs-worker-0-mlq8w   Running                          74d ncpvblvhub-b6cjs-worker-0-x5dc5   Running                          74d [root@dom16hub101-infra-manager ~]# oc get nodes NAME                                               STATUS   ROLES                                 AGE   VERSION ncpvblvhub-hubmaster-101.ncpvblvhub.t-mobile.lab   Ready    control-plane,master,monitor,worker   74d   v1.29.10+67d3387 ncpvblvhub-hubmaster-103.ncpvblvhub.t-mobile.lab   Ready    control-plane,master,monitor,worker   74d   v1.29.10+67d3387 ncpvblvhub-hubworker-101.ncpvblvhub.t-mobile.lab   Ready    gateway,worker                        74d   v1.29.10+67d3387 ncpvblvhub-hubworker-102.ncpvblvhub.t-mobile.lab   Ready    gateway,worker                        74d   v1.29.10+67d3387 [root@dom16hub101-infra-manager ~]#</p> <p>create the BMH </p> <p>[root@dom16hub101-infra-manager ~]# cat master_102_bmh.yaml apiVersion: metal3.io/v1alpha1 kind: BareMetalHost metadata:   finalizers:   - baremetalhost.metal3.io   generation: 3   labels:     installer.openshift.io/role: control-plane   name: ncpvblvhub-hubmaster-102.ncpvblvhub.t-mobile.lab   namespace: openshift-machine-api spec:   automatedCleaningMode: metadata   bmc:     address: \"\"     credentialsName: \"\"   bootMACAddress: c4:70:bd:f9:7f:48   bootMode: UEFISecureBoot   consumerRef:     apiVersion: machine.openshift.io/v1beta1     kind: Machine     name: ncpvblvhub-b6cjs-master-1     namespace: openshift-machine-api   customDeploy:     method: install_coreos   externallyProvisioned: true   hardwareProfile: unknown   online: true   userData:     name: master-user-data-managed     namespace: openshift-machine-api</p> <p>clean up the disks for the ceph use live cd to boot up and clean the disks </p> <p>[root@dom16hub101-infra-manager ~]# oc apply -f master_102_bmh.yaml Warning: metadata.finalizers: \"baremetalhost.metal3.io\": prefer a domain-qualified finalizer name to avoid accidental conflicts with other finalizer writers baremetalhost.metal3.io/ncpvblvhub-hubmaster-102.ncpvblvhub.t-mobile.lab created [root@dom16hub101-infra-manager ~]# oc apply -f backup_machinemaster-102_editted.yaml Warning: metadata.finalizers: \"machine.machine.openshift.io\": prefer a domain-qualified finalizer name to avoid accidental conflicts with other finalizer writers machine.machine.openshift.io/ncpvblvhub-b6cjs-master-1 created</p> <p>create the bmc secret, network secret and bmh file for the node and apply it. </p>"},{"location":"openshift/disaster-management/hub-cluster-redeployment/backmaster-replacment%20copy/#rootdom16hub101-infra-manager-cat-bmc-credential-hubyaml","title":"[root@dom16hub101-infra-manager ~]# cat bmc-credential-hub.yaml","text":"<p>apiVersion: v1 kind: Secret metadata:   name: control-plane-3-bmc-secret   namespace: openshift-machine-api data:   username: \"cm9vdA==\"   password: \"Y2Fsdmlu\" type: opaque [root@dom16hub101-infra-manager ~]# cat master-102_network_config.yaml apiVersion: v1 kind: Secret metadata:   name: openshift-master-102-network-config-secret   namespace: openshift-machine-api type: Opaque stringData:   nmstate: |     interfaces:       - name: infra-1         type: ethernet         state: up         identifier: mac-address         mtu: 9126         mac-address: C4:70:BD:F9:7F:48       - name: infra-2         type: ethernet         state: up         identifier: mac-address         mtu: 9126         mac-address: C4:70:BD:F9:7F:49       - name: tenant-1-1         type: ethernet         state: up         identifier: mac-address         mtu: 9126         mac-address: C4:70:BD:4A:90:8A       - name: tenant-1-2         type: ethernet         state: up         identifier: mac-address         mtu: 9126         mac-address: C4:70:BD:4A:90:8B       - name: tenant-2-1         type: ethernet         state: up         identifier: mac-address         mtu: 9126         mac-address: C4:70:BD:4A:90:8E       - name: tenant-2-2         type: ethernet         state: up         identifier: mac-address         mtu: 9126         mac-address: C4:70:BD:4A:90:8F       - name: infra-bond         type: bond         state: up         link-aggregation:           mode: active-backup           options:             miimon: \"100\"           port:           - infra-1           - infra-2         mtu: 9126       - name: tenant-bond-1         link-aggregation:           mode: active-backup           options:             miimon: \"100\"           port:           - tenant-1-1           - tenant-1-2         mtu: 9126         state: up         type: bond       - name: tenant-bond-2         link-aggregation:           mode: active-backup           options:             miimon: \"100\"           port:           - tenant-2-1           - tenant-2-2         mtu: 9126         state: up         type: bond       - name: infra-bond.200         type: vlan         state: up         mtu: 9126         ipv4:           enabled: true           dhcp: false           address:             - ip: 10.145.151.93               prefix-length: 26         ipv6:           enabled: false           dhcp: false         vlan:           base-iface: infra-bond           id: 200     routes:       config:         - destination: 0.0.0.0/0           next-hop-address: 10.145.151.65           next-hop-interface: infra-bond.200           table-id: 254     dns-resolver:       config:         search:         - t-mobile.lab         server:         - 5.232.32.63         - 10.169.69.10</p> <p>[root@dom16hub101-infra-manager ~]# cat master-102_bmh_wih_secret.yaml apiVersion: metal3.io/v1alpha1 kind: BareMetalHost metadata:   name: ncpvblvhub-hubmaster-102.ncpvblvhub.t-mobile.lab   namespace: openshift-machine-api spec:   automatedCleaningMode: disabled   bmc:     address: idrac-virtualmedia://10.145.151.15/redfish/v1/Systems/System.Embedded.1    #this is for dell server , for HP or other vendor check virtual media path     credentialsName: control-plane-3-bmc-secret     disableCertificateVerification: True   bootMACAddress: c4:70:bd:f9:7f:48   bootMode: UEFISecureBoot   externallyProvisioned: false   hardwareProfile: unknown   online: true   rootDeviceHints:     deviceName: /dev/disk/by-path/pci-0000:4a:00.0-scsi-0:2:0:0   userData:     name: master-user-data-managed     namespace: openshift-machine-api   preprovisioningNetworkDataName: openshift-master-102-network-config-secret [root@dom16hub101-infra-manager ~]#</p> <p>the node status should change from registering -&gt; inspecting -&gt; available. </p> <p>[root@dom16hub101-infra-manager ~]# oc get bmh -n openshift-machine-api ncpvblvhub-hubmaster-102.ncpvblvhub.t-mobile.lab NAME                                               STATE          CONSUMER                    ONLINE   ERROR   AGE ncpvblvhub-hubmaster-102.ncpvblvhub.t-mobile.lab   available   ncpvblvhub-b6cjs-master-1   true             31m [root@dom16hub101-infra-manager ~]#</p> <p>create and apply the machines.yaml  [root@dom16hub101-infra-manager ~]# cat backup_machinemaster-102_editted.yaml apiVersion: machine.openshift.io/v1beta1 kind: Machine metadata:   labels:     machine.openshift.io/cluster-api-cluster: ncpvblvhub-b6cjs     machine.openshift.io/cluster-api-machine-role: master     machine.openshift.io/cluster-api-machine-type: master   name: ncpvblvhub-b6cjs-master-1   namespace: openshift-machine-api spec:   lifecycleHooks:     preDrain:     - name: EtcdQuorumOperator       owner: clusteroperator/etcd   metadata: {}   providerSpec:     value:       apiVersion: baremetal.cluster.k8s.io/v1alpha1       customDeploy:         method: install_coreos       hostSelector: {}       image:         checksum: \"\"         url: \"\"       kind: BareMetalMachineProviderSpec       metadata:         creationTimestamp: null       userData:         name: master-user-data-managed [root@dom16hub101-infra-manager ~]#</p> <p>[root@dom16hub101-infra-manager ~]# oc get machines.machine.openshift.io -n openshift-machine-api NAME                              PHASE          TYPE   REGION   ZONE   AGE ncpvblvhub-b6cjs-master-0         Running                               74d ncpvblvhub-b6cjs-master-1         Provisioning                          4m17s ncpvblvhub-b6cjs-master-2         Running                               74d ncpvblvhub-b6cjs-worker-0-mlq8w   Running                               74d ncpvblvhub-b6cjs-worker-0-x5dc5   Running                               74d [root@dom16hub101-infra-manager ~]#</p> <p>bmh status will change to provisioning [root@dom16hub101-infra-manager ~]# oc get bmh -n openshift-machine-api NAME                                               STATE          CONSUMER                          ONLINE   ERROR   AGE ncpvblvhub-hubmaster-101.ncpvblvhub.t-mobile.lab   unmanaged      ncpvblvhub-b6cjs-master-0         true             74d ncpvblvhub-hubmaster-102.ncpvblvhub.t-mobile.lab   provisioning   ncpvblvhub-b6cjs-master-1         true             31m ncpvblvhub-hubmaster-103.ncpvblvhub.t-mobile.lab   unmanaged      ncpvblvhub-b6cjs-master-2         true             74d ncpvblvhub-hubworker-101.ncpvblvhub.t-mobile.lab   unmanaged      ncpvblvhub-b6cjs-worker-0-mlq8w   true             74d ncpvblvhub-hubworker-102.ncpvblvhub.t-mobile.lab   unmanaged      ncpvblvhub-b6cjs-worker-0-x5dc5   true             74d [root@dom16hub101-infra-manager ~]#</p> <p>nodes will be added to the cluster </p> <p>[root@dom16hub101-infra-manager ~]# oc get nodes NAME                                               STATUS   ROLES                                 AGE   VERSION ncpvblvhub-hubmaster-101.ncpvblvhub.t-mobile.lab   Ready    control-plane,master,monitor,worker   74d   v1.29.10+67d3387 ncpvblvhub-hubmaster-102.ncpvblvhub.t-mobile.lab   Ready    control-plane,master,worker           80s   v1.29.10+67d3387 ncpvblvhub-hubmaster-103.ncpvblvhub.t-mobile.lab   Ready    control-plane,master,monitor,worker   74d   v1.29.10+67d3387 ncpvblvhub-hubworker-101.ncpvblvhub.t-mobile.lab   Ready    gateway,worker                        74d   v1.29.10+67d3387 ncpvblvhub-hubworker-102.ncpvblvhub.t-mobile.lab   Ready    gateway,worker                        74d   v1.29.10+67d3387 [root@dom16hub101-infra-manager ~]#</p> <p>[root@dom16hub101-infra-manager ~]# oc get machines.machine.openshift.io -n openshift-machine-api NAME                              PHASE     TYPE   REGION   ZONE   AGE ncpvblvhub-b6cjs-master-0         Running                          74d ncpvblvhub-b6cjs-master-1         Running                          15m ncpvblvhub-b6cjs-master-2         Running                          74d ncpvblvhub-b6cjs-worker-0-mlq8w   Running                          74d ncpvblvhub-b6cjs-worker-0-x5dc5   Running                          74d [root@dom16hub101-infra-manager ~]# oc get bmh -n openshift-machine-api ncpvblvhub-hubmaster-102.ncpvblvhub.t-mobile.lab NAME                                               STATE         CONSUMER                    ONLINE   ERROR   AGE ncpvblvhub-hubmaster-102.ncpvblvhub.t-mobile.lab   provisioned   ncpvblvhub-b6cjs-master-1   true             44m [root@dom16hub101-infra-manager ~]#</p> <p>[core@ncpvblvhub-hubmaster-102 ~]$ logout Connection to ncpvblvhub-hubmaster-102.ncpvblvhub.t-mobile.lab closed. [root@dom16hub101-infra-manager ~]# oc get bmh No resources found in default namespace. [root@dom16hub101-infra-manager ~]# oc get bmh -A NAMESPACE               NAME                                               STATE         CONSUMER                          ONLINE   ERROR   AGE ncpvblvlab1             ncpvblvlab1-master-101.ncpvblvlab1.t-mobile.lab    provisioned                                     true             54d ncpvblvlab1             ncpvblvlab1-master-102.ncpvblvlab1.t-mobile.lab    provisioned                                     true             53d ncpvblvlab1             ncpvblvlab1-master-201.ncpvblvlab1.t-mobile.lab    provisioned                                     true             54d ncpvblvlab1             ncpvblvlab1-storage-101.ncpvblvlab1.t-mobile.lab   provisioned                                     true             64d ncpvblvlab1             ncpvblvlab1-storage-102.ncpvblvlab1.t-mobile.lab   provisioned                                     true             64d ncpvblvlab1             ncpvblvlab1-storage-103.ncpvblvlab1.t-mobile.lab   provisioned                                     true             64d ncpvblvlab1             ncpvblvlab1-storage-201.ncpvblvlab1.t-mobile.lab   provisioned                                     true             62d ncpvblvlab1             ncpvblvlab1-storage-202.ncpvblvlab1.t-mobile.lab   provisioned                                     true             64d ncpvblvlab1             ncpvblvlab1-storage-203.ncpvblvlab1.t-mobile.lab   provisioned                                     true             64d ncpvblvlab1             ncpvblvlab1-worker-101.ncpvblvlab1.t-mobile.lab    provisioned                                     true             64d ncpvblvlab1             ncpvblvlab1-worker-102.ncpvblvlab1.t-mobile.lab    provisioned                                     true             64d ncpvblvlab1             ncpvblvlab1-worker-103.ncpvblvlab1.t-mobile.lab    provisioned                                     true             64d ncpvblvlab1             ncpvblvlab1-worker-104.ncpvblvlab1.t-mobile.lab    provisioned                                     true             64d ncpvblvlab1             ncpvblvlab1-worker-105.ncpvblvlab1.t-mobile.lab    provisioned                                     true             64d ncpvblvlab1             ncpvblvlab1-worker-106.ncpvblvlab1.t-mobile.lab    provisioned                                     true             64d ncpvblvlab1             ncpvblvlab1-worker-107.ncpvblvlab1.t-mobile.lab    provisioned                                     true             64d ncpvblvlab1             ncpvblvlab1-worker-108.ncpvblvlab1.t-mobile.lab    provisioned                                     true             64d ncpvblvlab1             ncpvblvlab1-worker-109.ncpvblvlab1.t-mobile.lab    provisioned                                     true             64d ncpvblvlab1             ncpvblvlab1-worker-110.ncpvblvlab1.t-mobile.lab    provisioned                                     true             64d ncpvblvlab1             ncpvblvlab1-worker-111.ncpvblvlab1.t-mobile.lab    provisioned                                     true             64d ncpvblvlab1             ncpvblvlab1-worker-112.ncpvblvlab1.t-mobile.lab    provisioned                                     true             64d ncpvblvlab1             ncpvblvlab1-worker-113.ncpvblvlab1.t-mobile.lab    provisioned                                     true             64d ncpvblvlab1             ncpvblvlab1-worker-114.ncpvblvlab1.t-mobile.lab    provisioned                                     true             64d ncpvblvlab1             ncpvblvlab1-worker-115.ncpvblvlab1.t-mobile.lab    provisioned                                     true             64d ncpvblvlab1             ncpvblvlab1-worker-116.ncpvblvlab1.t-mobile.lab    provisioned                                     true             64d ncpvblvlab1             ncpvblvlab1-worker-117.ncpvblvlab1.t-mobile.lab    provisioned                                     true             64d ncpvblvlab1             ncpvblvlab1-worker-118.ncpvblvlab1.t-mobile.lab    provisioned                                     true             64d ncpvblvlab1             ncpvblvlab1-worker-119.ncpvblvlab1.t-mobile.lab    provisioned                                     true             64d ncpvblvlab1             ncpvblvlab1-worker-120.ncpvblvlab1.t-mobile.lab    provisioned                                     true             64d ncpvblvlab1             ncpvblvlab1-worker-121.ncpvblvlab1.t-mobile.lab    provisioned                                     true             64d ncpvblvlab1             ncpvblvlab1-worker-122.ncpvblvlab1.t-mobile.lab    provisioned                                     true             64d ncpvblvlab1             ncpvblvlab1-worker-201.ncpvblvlab1.t-mobile.lab    provisioned                                     true             64d ncpvblvlab1             ncpvblvlab1-worker-202.ncpvblvlab1.t-mobile.lab    provisioned                                     true             64d ncpvblvlab1             ncpvblvlab1-worker-203.ncpvblvlab1.t-mobile.lab    provisioned                                     true             64d ncpvblvlab1             ncpvblvlab1-worker-204.ncpvblvlab1.t-mobile.lab    provisioned                                     true             64d ncpvblvlab1             ncpvblvlab1-worker-205.ncpvblvlab1.t-mobile.lab    provisioned                                     true             64d ncpvblvlab1             ncpvblvlab1-worker-206.ncpvblvlab1.t-mobile.lab    provisioned                                     true             64d ncpvblvlab1             ncpvblvlab1-worker-207.ncpvblvlab1.t-mobile.lab    provisioned                                     true             64d ncpvblvlab1             ncpvblvlab1-worker-208.ncpvblvlab1.t-mobile.lab    provisioned                                     true             64d ncpvblvlab1             ncpvblvlab1-worker-209.ncpvblvlab1.t-mobile.lab    provisioned                                     true             26d ncpvblvlab1             ncpvblvlab1-worker-210.ncpvblvlab1.t-mobile.lab    provisioned                                     true             64d ncpvblvlab1             ncpvblvlab1-worker-211.ncpvblvlab1.t-mobile.lab    provisioned                                     true             64d ncpvblvlab1             ncpvblvlab1-worker-212.ncpvblvlab1.t-mobile.lab    provisioned                                     true             64d ncpvblvlab1             ncpvblvlab1-worker-213.ncpvblvlab1.t-mobile.lab    provisioned                                     true             64d ncpvblvlab1             ncpvblvlab1-worker-214.ncpvblvlab1.t-mobile.lab    provisioned                                     true             64d ncpvblvlab1             ncpvblvlab1-worker-215.ncpvblvlab1.t-mobile.lab    provisioned                                     true             64d ncpvblvlab1             ncpvblvlab1-worker-216.ncpvblvlab1.t-mobile.lab    provisioned                                     true             64d ncpvblvlab1             ncpvblvlab1-worker-217.ncpvblvlab1.t-mobile.lab    provisioned                                     true             53d ncpvblvlab1             ncpvblvlab1-worker-218.ncpvblvlab1.t-mobile.lab    provisioned                                     true             64d ncpvblvlab1             ncpvblvlab1-worker-219.ncpvblvlab1.t-mobile.lab    provisioned                                     true             64d ncpvblvlab1             ncpvblvlab1-worker-220.ncpvblvlab1.t-mobile.lab    provisioned                                     true             62d ncpvblvlab1             ncpvblvlab1-worker-221.ncpvblvlab1.t-mobile.lab    provisioned                                     true             64d ncpvblvlab1             ncpvblvlab1-worker-222.ncpvblvlab1.t-mobile.lab    provisioned                                     true             64d ncpvblvlab2             ncpvblvlab2-master-101.ncpvblvlab2.t-mobile.lab    provisioned                                     true             21d ncpvblvlab2             ncpvblvlab2-master-102.ncpvblvlab2.t-mobile.lab    provisioned                                     true             61d ncpvblvlab2             ncpvblvlab2-master-201.ncpvblvlab2.t-mobile.lab    provisioned                                     true             61d ncpvblvlab2             ncpvblvlab2-storage-101.ncpvblvlab2.t-mobile.lab   provisioned                                     true             61d ncpvblvlab2             ncpvblvlab2-storage-102.ncpvblvlab2.t-mobile.lab   provisioned                                     true             60d ncpvblvlab2             ncpvblvlab2-storage-103.ncpvblvlab2.t-mobile.lab   provisioned                                     true             61d ncpvblvlab2             ncpvblvlab2-storage-201.ncpvblvlab2.t-mobile.lab   provisioned                                     true             61d ncpvblvlab2             ncpvblvlab2-storage-202.ncpvblvlab2.t-mobile.lab   provisioned                                     true             61d ncpvblvlab2             ncpvblvlab2-storage-203.ncpvblvlab2.t-mobile.lab   provisioned                                     true             61d ncpvblvlab2             ncpvblvlab2-worker-101.ncpvblvlab2.t-mobile.lab    provisioned                                     true             61d ncpvblvlab2             ncpvblvlab2-worker-102.ncpvblvlab2.t-mobile.lab    provisioned                                     true             61d ncpvblvlab2             ncpvblvlab2-worker-103.ncpvblvlab2.t-mobile.lab    provisioned                                     true             61d ncpvblvlab2             ncpvblvlab2-worker-104.ncpvblvlab2.t-mobile.lab    provisioned                                     true             61d ncpvblvlab2             ncpvblvlab2-worker-105.ncpvblvlab2.t-mobile.lab    provisioned                                     true             61d ncpvblvlab2             ncpvblvlab2-worker-106.ncpvblvlab2.t-mobile.lab    provisioned                                     true             61d ncpvblvlab2             ncpvblvlab2-worker-107.ncpvblvlab2.t-mobile.lab    provisioned                                     true             61d ncpvblvlab2             ncpvblvlab2-worker-108.ncpvblvlab2.t-mobile.lab    provisioned                                     true             61d ncpvblvlab2             ncpvblvlab2-worker-109.ncpvblvlab2.t-mobile.lab    provisioned                                     true             61d ncpvblvlab2             ncpvblvlab2-worker-110.ncpvblvlab2.t-mobile.lab    provisioned                                     true             61d ncpvblvlab2             ncpvblvlab2-worker-111.ncpvblvlab2.t-mobile.lab    provisioned                                     true             61d ncpvblvlab2             ncpvblvlab2-worker-112.ncpvblvlab2.t-mobile.lab    provisioned                                     true             61d ncpvblvlab2             ncpvblvlab2-worker-113.ncpvblvlab2.t-mobile.lab    provisioned                                     true             61d ncpvblvlab2             ncpvblvlab2-worker-114.ncpvblvlab2.t-mobile.lab    provisioned                                     true             61d ncpvblvlab2             ncpvblvlab2-worker-115.ncpvblvlab2.t-mobile.lab    provisioned                                     true             61d ncpvblvlab2             ncpvblvlab2-worker-116.ncpvblvlab2.t-mobile.lab    provisioned                                     true             61d ncpvblvlab2             ncpvblvlab2-worker-117.ncpvblvlab2.t-mobile.lab    provisioned                                     true             61d ncpvblvlab2             ncpvblvlab2-worker-118.ncpvblvlab2.t-mobile.lab    provisioned                                     true             61d ncpvblvlab2             ncpvblvlab2-worker-119.ncpvblvlab2.t-mobile.lab    provisioned                                     true             61d ncpvblvlab2             ncpvblvlab2-worker-120.ncpvblvlab2.t-mobile.lab    provisioned                                     true             61d ncpvblvlab2             ncpvblvlab2-worker-121.ncpvblvlab2.t-mobile.lab    provisioned                                     true             61d ncpvblvlab2             ncpvblvlab2-worker-122.ncpvblvlab2.t-mobile.lab    provisioned                                     true             57d ncpvblvlab2             ncpvblvlab2-worker-201.ncpvblvlab2.t-mobile.lab    provisioned                                     true             61d ncpvblvlab2             ncpvblvlab2-worker-202.ncpvblvlab2.t-mobile.lab    provisioned                                     true             61d ncpvblvlab2             ncpvblvlab2-worker-203.ncpvblvlab2.t-mobile.lab    provisioned                                     true             61d ncpvblvlab2             ncpvblvlab2-worker-204.ncpvblvlab2.t-mobile.lab    provisioned                                     true             61d ncpvblvlab2             ncpvblvlab2-worker-205.ncpvblvlab2.t-mobile.lab    provisioned                                     true             61d ncpvblvlab2             ncpvblvlab2-worker-206.ncpvblvlab2.t-mobile.lab    provisioned                                     true             61d ncpvblvlab2             ncpvblvlab2-worker-207.ncpvblvlab2.t-mobile.lab    provisioned                                     true             61d ncpvblvlab2             ncpvblvlab2-worker-208.ncpvblvlab2.t-mobile.lab    provisioned                                     true             61d ncpvblvlab2             ncpvblvlab2-worker-209.ncpvblvlab2.t-mobile.lab    provisioned                                     true             61d ncpvblvlab2             ncpvblvlab2-worker-210.ncpvblvlab2.t-mobile.lab    provisioned                                     true             61d ncpvblvlab2             ncpvblvlab2-worker-211.ncpvblvlab2.t-mobile.lab    provisioned                                     true             61d ncpvblvlab2             ncpvblvlab2-worker-212.ncpvblvlab2.t-mobile.lab    provisioned                                     true             61d ncpvblvlab2             ncpvblvlab2-worker-213.ncpvblvlab2.t-mobile.lab    provisioned                                     true             61d ncpvblvlab2             ncpvblvlab2-worker-214.ncpvblvlab2.t-mobile.lab    provisioned                                     true             55d ncpvblvlab2             ncpvblvlab2-worker-215.ncpvblvlab2.t-mobile.lab    provisioned                                     true             61d ncpvblvlab2             ncpvblvlab2-worker-216.ncpvblvlab2.t-mobile.lab    provisioned                                     true             61d ncpvblvlab2             ncpvblvlab2-worker-217.ncpvblvlab2.t-mobile.lab    provisioned                                     true             61d ncpvblvlab2             ncpvblvlab2-worker-218.ncpvblvlab2.t-mobile.lab    provisioned                                     true             61d ncpvblvlab2             ncpvblvlab2-worker-219.ncpvblvlab2.t-mobile.lab    provisioned                                     true             61d ncpvblvlab2             ncpvblvlab2-worker-220.ncpvblvlab2.t-mobile.lab    provisioned                                     true             61d ncpvblvlab2             ncpvblvlab2-worker-221.ncpvblvlab2.t-mobile.lab    provisioned                                     true             61d ncpvblvlab2             ncpvblvlab2-worker-222.ncpvblvlab2.t-mobile.lab    provisioned                                     true             61d ncpvblvmgt              ncpvblvmgt-gateway-101.ncpvblvmgt.t-mobile.lab     provisioned                                     true             25d ncpvblvmgt              ncpvblvmgt-gateway-102.ncpvblvmgt.t-mobile.lab     provisioned                                     true             69d ncpvblvmgt              ncpvblvmgt-master-101.ncpvblvmgt.t-mobile.lab      provisioned                                     true             20d ncpvblvmgt              ncpvblvmgt-master-102.ncpvblvmgt.t-mobile.lab      provisioned                                     true             69d ncpvblvmgt              ncpvblvmgt-master-103.ncpvblvmgt.t-mobile.lab      provisioned                                     true             69d ncpvblvmgt              ncpvblvmgt-storage-101.ncpvblvmgt.t-mobile.lab     provisioned                                     true             69d ncpvblvmgt              ncpvblvmgt-storage-102.ncpvblvmgt.t-mobile.lab     provisioned                                     true             69d ncpvblvmgt              ncpvblvmgt-storage-103.ncpvblvmgt.t-mobile.lab     provisioned                                     true             69d ncpvblvmgt              ncpvblvmgt-storage-104.ncpvblvmgt.t-mobile.lab     provisioned                                     true             25d ncpvblvmgt              ncpvblvmgt-worker-101.ncpvblvmgt.t-mobile.lab      provisioned                                     true             69d ncpvblvmgt              ncpvblvmgt-worker-102.ncpvblvmgt.t-mobile.lab      provisioned                                     true             69d ncpvblvmgt              ncpvblvmgt-worker-103.ncpvblvmgt.t-mobile.lab      provisioned                                     true             69d ncpvblvmgt              ncpvblvmgt-worker-104.ncpvblvmgt.t-mobile.lab      provisioned                                     true             69d ncpvblvmgt              ncpvblvmgt-worker-105.ncpvblvmgt.t-mobile.lab      provisioned                                     true             69d ncpvblvmgt              ncpvblvmgt-worker-106.ncpvblvmgt.t-mobile.lab      provisioned                                     true             69d ncpvblvmgt              ncpvblvmgt-worker-107.ncpvblvmgt.t-mobile.lab      provisioned                                     true             26d ncpvblvmgt              ncpvblvmgt-worker-108.ncpvblvmgt.t-mobile.lab      provisioned                                     true             69d ncpvblvmgt              ncpvblvmgt-worker-109.ncpvblvmgt.t-mobile.lab      provisioned                                     true             69d ncpvblvmgt              ncpvblvmgt-worker-110.ncpvblvmgt.t-mobile.lab      provisioned                                     true             69d openshift-machine-api   ncpvblvhub-hubmaster-101.ncpvblvhub.t-mobile.lab   unmanaged     ncpvblvhub-b6cjs-master-0         true             74d openshift-machine-api   ncpvblvhub-hubmaster-102.ncpvblvhub.t-mobile.lab   available                                       true             24m openshift-machine-api   ncpvblvhub-hubmaster-103.ncpvblvhub.t-mobile.lab   unmanaged     ncpvblvhub-b6cjs-master-2         true             74d openshift-machine-api   ncpvblvhub-hubworker-101.ncpvblvhub.t-mobile.lab   unmanaged     ncpvblvhub-b6cjs-worker-0-mlq8w   true             74d openshift-machine-api   ncpvblvhub-hubworker-102.ncpvblvhub.t-mobile.lab   unmanaged     ncpvblvhub-b6cjs-worker-0-x5dc5   true             74d [root@dom16hub101-infra-manager ~]# oc get bmh -n openshift-machine-api NAME                                               STATE       CONSUMER                          ONLINE   ERROR   AGE ncpvblvhub-hubmaster-101.ncpvblvhub.t-mobile.lab   unmanaged   ncpvblvhub-b6cjs-master-0         true             74d ncpvblvhub-hubmaster-102.ncpvblvhub.t-mobile.lab   available                                     true             24m ncpvblvhub-hubmaster-103.ncpvblvhub.t-mobile.lab   unmanaged   ncpvblvhub-b6cjs-master-2         true             74d ncpvblvhub-hubworker-101.ncpvblvhub.t-mobile.lab   unmanaged   ncpvblvhub-b6cjs-worker-0-mlq8w   true             74d ncpvblvhub-hubworker-102.ncpvblvhub.t-mobile.lab   unmanaged   ncpvblvhub-b6cjs-worker-0-x5dc5   true             74d [root@dom16hub101-infra-manager ~]# ls -ltr total 269344 -rw-------. 1 root root      1741 Feb 18 18:07 anaconda-ks.cfg drwxr-xr-x. 2 root root         6 Feb 19 08:51 Videos drwxr-xr-x. 2 root root         6 Feb 19 08:51 Templates drwxr-xr-x. 2 root root         6 Feb 19 08:51 Public drwxr-xr-x. 2 root root         6 Feb 19 08:51 Pictures drwxr-xr-x. 2 root root         6 Feb 19 08:51 Music drwxr-xr-x. 2 root root         6 Feb 19 08:51 Downloads drwxr-xr-x. 2 root root         6 Feb 19 08:51 Documents drwxr-xr-x. 2 root root         6 Feb 19 08:51 Desktop drwxr-xr-x. 3 root root        21 Feb 20 12:52 om drwxr-xr-x. 6 root root       185 Mar 14 14:19 abi-install drwxr-xr-x. 2 root root       152 Mar 25 21:17 storage-scale_fail drwxr-xr-x. 3 root root       159 Mar 26 16:28 site-config_prepare drwxr-xr-x. 4 root root        34 Mar 27 16:17 ztp -rw-r--r--. 1 root root       902 Mar 27 16:51 oauth.yaml-hub -rw-r--r--. 1 root root      1056 Mar 27 17:04 oauth.yaml -rw-r--r--. 1 root root        67 Mar 27 17:49 users.htpasswd -rw-r--r--. 1 root root        66 Mar 27 18:19 htpasswdlab -rw-r--r--. 1 root root       794 Mar 27 18:32 oauth-nmc.yaml -rw-r--r--. 1 root root         0 Mar 27 19:41 htpasswdnmc -rw-r--r--. 1 root root      1119 Mar 27 21:03 tmolab1-api.pem -rw-r--r--. 1 root root      1119 Mar 27 21:13 tmolab2-api.pem -rw-r--r--. 1 root root       795 Mar 27 22:05 oauth_nwc1.yaml -rw-r--r--. 1 root root       133 Mar 27 22:24 htpasswdcwl -rw-r--r--. 1 root root        66 Mar 27 22:24 htpasswdnwc1 -rw-r--r--. 1 root root 112680960 Mar 28 16:18 minimal.iso -rw-r--r--. 1 root root   4512768 Apr  2 16:02 busybox.tar drwxr-xr-x. 2 root root        49 Apr  7 17:46 healthcheck -rw-------. 1 root root       596 Apr 17 12:21 ncpvblvmgt-kubeconfig -rw-r--r--. 1 root root  40569495 Apr 29 18:12 quay-app1.log -rw-r--r--. 1 root root  20354784 Apr 29 18:13 quay-app2.log drwxr-xr-x. 3 root root       113 Apr 29 20:08 odf_mustgather drwxr-xr-x. 7 root root      4096 Apr 29 20:10 must-gather drwxr-xr-x. 3 root root       113 Apr 29 20:13 must-gather.local.3662385339567218798 drwxr-xr-x. 3 root root      4096 Apr 29 20:17 must-gather.local.7225453970769055250 -rw-r--r--. 1 root root      8458 May  1 20:26 multiclusterhub.yaml_backup drwxr-xr-x. 2 root root        71 May  1 21:25 acm-backup -rw-r--r--. 1 root root     11553 May  2 13:44 oc_bash_completion drwxr-xr-x. 8 root root      4096 May  6 16:32 NCP24_7_MP1 drwxr-xr-x. 3 root root        46 May  7 15:24 netact drwxr-xr-x. 2 root root        87 May  7 15:30 hubnetactCerts -rw-r--r--. 1 root root      1463 May 13 21:04 cluster-monitoring-config.yaml drwxr-xr-x. 2 root root       181 May 21 14:01 kubeconfig -rw-r--r--. 1 root root   1726769 May 26 11:10 ncdvblv-engine-cdn-788d667887-2klww.txt -rw-r--r--. 1 root root   1370683 May 26 11:11 ncdvblv-engine-cdn-788d667887-bzz7b.txt -rw-r--r--. 1 root root  15048264 May 26 11:12 ncdvblv-engine-operations-6c858765d6-s9bn8.txt -rw-r--r--. 1 root root  15059439 May 26 11:12 ncdvblv-engine-operations-6c858765d6-shwmp.txt -rw-r--r--. 1 root root  25747580 May 26 11:13 ncdvblv-engine-repo-7755f9fc45-7j2sf.txt -rw-r--r--. 1 root root  38584969 May 26 11:13 ncdvblv-engine-repo-7755f9fc45-vk8hb.txt -rw-r--r--. 1 root root      3052 May 26 11:15 ncdvblv-engine-repoworker-646c6d958-d2dr6.txt -rw-r--r--. 1 root root      3052 May 26 11:15 ncdvblv-engine-repoworker-646c6d958-x9jlh.txt drwxr-xr-x. 3 root root      4096 May 27 16:10 mini-atp -rw-r--r--. 1 root root      2370 May 27 17:19 backup_machinemaster-102.yaml -rw-r--r--. 1 root root      1156 May 27 17:22 backup_machinemaster-102_editted.yaml -rw-r--r--. 1 root root      6936 May 27 17:50 master_bmh.yaml -rw-r--r--. 1 root root       769 May 27 17:55 master_102_bmh.yaml -rw-r--r--. 1 root root       177 May 27 19:06 bmc-credential-hub.yaml -rw-r--r--. 1 root root       833 May 27 19:29 master-102_bmh_wih_secret.yaml -rw-r--r--. 1 root root      2487 May 27 20:35 master-102_network_config.yaml [root@dom16hub101-infra-manager ~]# cat backup_machinemaster-102_editted.yaml apiVersion: machine.openshift.io/v1beta1 kind: Machine metadata:   annotations:     machine.openshift.io/instance-state: unmanaged     metal3.io/BareMetalHost: openshift-machine-api/ncpvblvhub-hubmaster-102.ncpvblvhub.t-mobile.lab   finalizers:   - machine.machine.openshift.io   generation: 3   labels:     machine.openshift.io/cluster-api-cluster: ncpvblvhub-b6cjs     machine.openshift.io/cluster-api-machine-role: master     machine.openshift.io/cluster-api-machine-type: master   name: ncpvblvhub-b6cjs-master-1   namespace: openshift-machine-api spec:   lifecycleHooks:     preDrain:     - name: EtcdQuorumOperator       owner: clusteroperator/etcd   metadata: {}   providerID: baremetalhost:///openshift-machine-api/ncpvblvhub-hubmaster-102.ncpvblvhub.t-mobile.lab/346b01e0-6bb8-40bd-8db3-d3eabf4d44f1   providerSpec:     value:       apiVersion: baremetal.cluster.k8s.io/v1alpha1       customDeploy:         method: install_coreos       hostSelector: {}       image:         checksum: \"\"         url: \"\"       kind: BareMetalMachineProviderSpec       metadata:         creationTimestamp: null       userData:         name: master-user-data-managed [root@dom16hub101-infra-manager ~]# vim backup_machinemaster-102_editted.yaml [root@dom16hub101-infra-manager ~]# oc apply -f backup_machinemaster-102_editted.yaml machine.machine.openshift.io/ncpvblvhub-b6cjs-master-1 created [root@dom16hub101-infra-manager ~]# oc get machine.machine -n openshift-machine-api NAME                              PHASE          TYPE   REGION   ZONE   AGE ncpvblvhub-b6cjs-master-0         Running                               74d ncpvblvhub-b6cjs-master-1         Provisioning                          25s ncpvblvhub-b6cjs-master-2         Running                               74d ncpvblvhub-b6cjs-worker-0-mlq8w   Running                               74d ncpvblvhub-b6cjs-worker-0-x5dc5   Running                               74d [root@dom16hub101-infra-manager ~]# ls -lrt total 269344 -rw-------. 1 root root      1741 Feb 18 18:07 anaconda-ks.cfg drwxr-xr-x. 2 root root         6 Feb 19 08:51 Videos drwxr-xr-x. 2 root root         6 Feb 19 08:51 Templates drwxr-xr-x. 2 root root         6 Feb 19 08:51 Public drwxr-xr-x. 2 root root         6 Feb 19 08:51 Pictures drwxr-xr-x. 2 root root         6 Feb 19 08:51 Music drwxr-xr-x. 2 root root         6 Feb 19 08:51 Downloads drwxr-xr-x. 2 root root         6 Feb 19 08:51 Documents drwxr-xr-x. 2 root root         6 Feb 19 08:51 Desktop drwxr-xr-x. 3 root root        21 Feb 20 12:52 om drwxr-xr-x. 6 root root       185 Mar 14 14:19 abi-install drwxr-xr-x. 2 root root       152 Mar 25 21:17 storage-scale_fail drwxr-xr-x. 3 root root       159 Mar 26 16:28 site-config_prepare drwxr-xr-x. 4 root root        34 Mar 27 16:17 ztp -rw-r--r--. 1 root root       902 Mar 27 16:51 oauth.yaml-hub -rw-r--r--. 1 root root      1056 Mar 27 17:04 oauth.yaml -rw-r--r--. 1 root root        67 Mar 27 17:49 users.htpasswd -rw-r--r--. 1 root root        66 Mar 27 18:19 htpasswdlab -rw-r--r--. 1 root root       794 Mar 27 18:32 oauth-nmc.yaml -rw-r--r--. 1 root root         0 Mar 27 19:41 htpasswdnmc -rw-r--r--. 1 root root      1119 Mar 27 21:03 tmolab1-api.pem -rw-r--r--. 1 root root      1119 Mar 27 21:13 tmolab2-api.pem -rw-r--r--. 1 root root       795 Mar 27 22:05 oauth_nwc1.yaml -rw-r--r--. 1 root root       133 Mar 27 22:24 htpasswdcwl -rw-r--r--. 1 root root        66 Mar 27 22:24 htpasswdnwc1 -rw-r--r--. 1 root root 112680960 Mar 28 16:18 minimal.iso -rw-r--r--. 1 root root   4512768 Apr  2 16:02 busybox.tar drwxr-xr-x. 2 root root        49 Apr  7 17:46 healthcheck -rw-------. 1 root root       596 Apr 17 12:21 ncpvblvmgt-kubeconfig -rw-r--r--. 1 root root  40569495 Apr 29 18:12 quay-app1.log -rw-r--r--. 1 root root  20354784 Apr 29 18:13 quay-app2.log drwxr-xr-x. 3 root root       113 Apr 29 20:08 odf_mustgather drwxr-xr-x. 7 root root      4096 Apr 29 20:10 must-gather drwxr-xr-x. 3 root root       113 Apr 29 20:13 must-gather.local.3662385339567218798 drwxr-xr-x. 3 root root      4096 Apr 29 20:17 must-gather.local.7225453970769055250 -rw-r--r--. 1 root root      8458 May  1 20:26 multiclusterhub.yaml_backup drwxr-xr-x. 2 root root        71 May  1 21:25 acm-backup -rw-r--r--. 1 root root     11553 May  2 13:44 oc_bash_completion drwxr-xr-x. 8 root root      4096 May  6 16:32 NCP24_7_MP1 drwxr-xr-x. 3 root root        46 May  7 15:24 netact drwxr-xr-x. 2 root root        87 May  7 15:30 hubnetactCerts -rw-r--r--. 1 root root      1463 May 13 21:04 cluster-monitoring-config.yaml drwxr-xr-x. 2 root root       181 May 21 14:01 kubeconfig -rw-r--r--. 1 root root   1726769 May 26 11:10 ncdvblv-engine-cdn-788d667887-2klww.txt -rw-r--r--. 1 root root   1370683 May 26 11:11 ncdvblv-engine-cdn-788d667887-bzz7b.txt -rw-r--r--. 1 root root  15048264 May 26 11:12 ncdvblv-engine-operations-6c858765d6-s9bn8.txt -rw-r--r--. 1 root root  15059439 May 26 11:12 ncdvblv-engine-operations-6c858765d6-shwmp.txt -rw-r--r--. 1 root root  25747580 May 26 11:13 ncdvblv-engine-repo-7755f9fc45-7j2sf.txt -rw-r--r--. 1 root root  38584969 May 26 11:13 ncdvblv-engine-repo-7755f9fc45-vk8hb.txt -rw-r--r--. 1 root root      3052 May 26 11:15 ncdvblv-engine-repoworker-646c6d958-d2dr6.txt -rw-r--r--. 1 root root      3052 May 26 11:15 ncdvblv-engine-repoworker-646c6d958-x9jlh.txt drwxr-xr-x. 3 root root      4096 May 27 16:10 mini-atp -rw-r--r--. 1 root root      2370 May 27 17:19 backup_machinemaster-102.yaml -rw-r--r--. 1 root root      6936 May 27 17:50 master_bmh.yaml -rw-r--r--. 1 root root       769 May 27 17:55 master_102_bmh.yaml -rw-r--r--. 1 root root       177 May 27 19:06 bmc-credential-hub.yaml -rw-r--r--. 1 root root       833 May 27 19:29 master-102_bmh_wih_secret.yaml -rw-r--r--. 1 root root      2487 May 27 20:35 master-102_network_config.yaml -rw-r--r--. 1 root root       788 May 27 21:04 backup_machinemaster-102_editted.yaml [root@dom16hub101-infra-manager ~]# cat master-102_bmh_wih_secret.yaml apiVersion: metal3.io/v1alpha1 kind: BareMetalHost metadata:   name: ncpvblvhub-hubmaster-102.ncpvblvhub.t-mobile.lab   namespace: openshift-machine-api spec:   automatedCleaningMode: disabled   bmc:     address: idrac-virtualmedia://10.145.151.15/redfish/v1/Systems/System.Embedded.1    #this is for dell server , for HP or other vendor check virtual media path     credentialsName: control-plane-3-bmc-secret     disableCertificateVerification: True   bootMACAddress: c4:70:bd:f9:7f:48   bootMode: UEFISecureBoot   externallyProvisioned: false   hardwareProfile: unknown   online: true   rootDeviceHints:     deviceName: /dev/disk/by-path/pci-0000:4a:00.0-scsi-0:2:0:0   userData:     name: master-user-data-managed     namespace: openshift-machine-api   preprovisioningNetworkDataName: openshift-master-102-network-config-secret [root@dom16hub101-infra-manager ~]# oc get machine.machine -n openshift-machine-api NAME                              PHASE          TYPE   REGION   ZONE   AGE ncpvblvhub-b6cjs-master-0         Running                               74d ncpvblvhub-b6cjs-master-1         Provisioning                          115s ncpvblvhub-b6cjs-master-2         Running                               74d ncpvblvhub-b6cjs-worker-0-mlq8w   Running                               74d ncpvblvhub-b6cjs-worker-0-x5dc5   Running                               74d [root@dom16hub101-infra-manager ~]# oc get machine.machine -n openshift-machine-api NAME                              PHASE          TYPE   REGION   ZONE   AGE ncpvblvhub-b6cjs-master-0         Running                               74d ncpvblvhub-b6cjs-master-1         Provisioning                          2m28s ncpvblvhub-b6cjs-master-2         Running                               74d ncpvblvhub-b6cjs-worker-0-mlq8w   Running                               74d ncpvblvhub-b6cjs-worker-0-x5dc5   Running                               74d [root@dom16hub101-infra-manager ~]# watch oc get machine.machine -n openshift-machine-api [root@dom16hub101-infra-manager ~]# oc get no NAME                                               STATUS   ROLES                                 AGE                                                                                                            VERSION ncpvblvhub-hubmaster-101.ncpvblvhub.t-mobile.lab   Ready    control-plane,master,monitor,worker   74d                                                                                                            v1.29.10+67d3387 ncpvblvhub-hubmaster-103.ncpvblvhub.t-mobile.lab   Ready    control-plane,master,monitor,worker   74d                                                                                                            v1.29.10+67d3387 ncpvblvhub-hubworker-101.ncpvblvhub.t-mobile.lab   Ready    gateway,worker                        74d                                                                                                            v1.29.10+67d3387 ncpvblvhub-hubworker-102.ncpvblvhub.t-mobile.lab   Ready    gateway,worker                        74d                                                                                                            v1.29.10+67d3387 [root@dom16hub101-infra-manager ~]# oc get bmh -A NAMESPACE               NAME                                               STATE         CONSUMER                                                                                                                                   ONLINE   ERROR   AGE ncpvblvlab1             ncpvblvlab1-master-101.ncpvblvlab1.t-mobile.lab    provisioned                                                                                                                                              true             54d ncpvblvlab1             ncpvblvlab1-master-102.ncpvblvlab1.t-mobile.lab    provisioned                                                                                                                                              true             53d ncpvblvlab1             ncpvblvlab1-master-201.ncpvblvlab1.t-mobile.lab    provisioned                                                                                                                                              true             54d ncpvblvlab1             ncpvblvlab1-storage-101.ncpvblvlab1.t-mobile.lab   provisioned                                                                                                                                              true             64d ncpvblvlab1             ncpvblvlab1-storage-102.ncpvblvlab1.t-mobile.lab   provisioned                                                                                                                                              true             64d ncpvblvlab1             ncpvblvlab1-storage-103.ncpvblvlab1.t-mobile.lab   provisioned                                                                                                                                              true             64d ncpvblvlab1             ncpvblvlab1-storage-201.ncpvblvlab1.t-mobile.lab   provisioned                                                                                                                                              true             62d ncpvblvlab1             ncpvblvlab1-storage-202.ncpvblvlab1.t-mobile.lab   provisioned                                                                                                                                              true             64d ncpvblvlab1             ncpvblvlab1-storage-203.ncpvblvlab1.t-mobile.lab   provisioned                                                                                                                                              true             64d ncpvblvlab1             ncpvblvlab1-worker-101.ncpvblvlab1.t-mobile.lab    provisioned                                                                                                                                              true             64d ncpvblvlab1             ncpvblvlab1-worker-102.ncpvblvlab1.t-mobile.lab    provisioned                                                                                                                                              true             64d ncpvblvlab1             ncpvblvlab1-worker-103.ncpvblvlab1.t-mobile.lab    provisioned                                                                                                                                              true             64d ncpvblvlab1             ncpvblvlab1-worker-104.ncpvblvlab1.t-mobile.lab    provisioned                                                                                                                                              true             64d ncpvblvlab1             ncpvblvlab1-worker-105.ncpvblvlab1.t-mobile.lab    provisioned                                                                                                                                              true             64d ncpvblvlab1             ncpvblvlab1-worker-106.ncpvblvlab1.t-mobile.lab    provisioned                                                                                                                                              true             64d ncpvblvlab1             ncpvblvlab1-worker-107.ncpvblvlab1.t-mobile.lab    provisioned                                                                                                                                              true             64d ncpvblvlab1             ncpvblvlab1-worker-108.ncpvblvlab1.t-mobile.lab    provisioned                                                                                                                                              true             64d ncpvblvlab1             ncpvblvlab1-worker-109.ncpvblvlab1.t-mobile.lab    provisioned                                                                                                                                              true             64d ncpvblvlab1             ncpvblvlab1-worker-110.ncpvblvlab1.t-mobile.lab    provisioned                                                                                                                                              true             64d ncpvblvlab1             ncpvblvlab1-worker-111.ncpvblvlab1.t-mobile.lab    provisioned                                                                                                                                              true             64d ncpvblvlab1             ncpvblvlab1-worker-112.ncpvblvlab1.t-mobile.lab    provisioned                                                                                                                                              true             64d ncpvblvlab1             ncpvblvlab1-worker-113.ncpvblvlab1.t-mobile.lab    provisioned                                                                                                                                              true             64d ncpvblvlab1             ncpvblvlab1-worker-114.ncpvblvlab1.t-mobile.lab    provisioned                                                                                                                                              true             64d ncpvblvlab1             ncpvblvlab1-worker-115.ncpvblvlab1.t-mobile.lab    provisioned                                                                                                                                              true             64d ncpvblvlab1             ncpvblvlab1-worker-116.ncpvblvlab1.t-mobile.lab    provisioned                                                                                                                                              true             64d ncpvblvlab1             ncpvblvlab1-worker-117.ncpvblvlab1.t-mobile.lab    provisioned                                                                                                                                              true             64d ncpvblvlab1             ncpvblvlab1-worker-118.ncpvblvlab1.t-mobile.lab    provisioned                                                                                                                                              true             64d ncpvblvlab1             ncpvblvlab1-worker-119.ncpvblvlab1.t-mobile.lab    provisioned                                                                                                                                              true             64d ncpvblvlab1             ncpvblvlab1-worker-120.ncpvblvlab1.t-mobile.lab    provisioned                                                                                                                                              true             64d ncpvblvlab1             ncpvblvlab1-worker-121.ncpvblvlab1.t-mobile.lab    provisioned                                                                                                                                              true             64d ncpvblvlab1             ncpvblvlab1-worker-122.ncpvblvlab1.t-mobile.lab    provisioned                                                                                                                                              true             64d ncpvblvlab1             ncpvblvlab1-worker-201.ncpvblvlab1.t-mobile.lab    provisioned                                                                                                                                              true             64d ncpvblvlab1             ncpvblvlab1-worker-202.ncpvblvlab1.t-mobile.lab    provisioned                                                                                                                                              true             64d ncpvblvlab1             ncpvblvlab1-worker-203.ncpvblvlab1.t-mobile.lab    provisioned                                                                                                                                              true             64d ncpvblvlab1             ncpvblvlab1-worker-204.ncpvblvlab1.t-mobile.lab    provisioned                                                                                                                                              true             64d ncpvblvlab1             ncpvblvlab1-worker-205.ncpvblvlab1.t-mobile.lab    provisioned                                                                                                                                              true             64d ncpvblvlab1             ncpvblvlab1-worker-206.ncpvblvlab1.t-mobile.lab    provisioned                                                                                                                                              true             64d ncpvblvlab1             ncpvblvlab1-worker-207.ncpvblvlab1.t-mobile.lab    provisioned                                                                                                                                              true             64d ncpvblvlab1             ncpvblvlab1-worker-208.ncpvblvlab1.t-mobile.lab    provisioned                                                                                                                                              true             64d ncpvblvlab1             ncpvblvlab1-worker-209.ncpvblvlab1.t-mobile.lab    provisioned                                                                                                                                              true             26d ncpvblvlab1             ncpvblvlab1-worker-210.ncpvblvlab1.t-mobile.lab    provisioned                                                                                                                                              true             64d ncpvblvlab1             ncpvblvlab1-worker-211.ncpvblvlab1.t-mobile.lab    provisioned                                                                                                                                              true             64d ncpvblvlab1             ncpvblvlab1-worker-212.ncpvblvlab1.t-mobile.lab    provisioned                                                                                                                                              true             64d ncpvblvlab1             ncpvblvlab1-worker-213.ncpvblvlab1.t-mobile.lab    provisioned                                                                                                                                              true             64d ncpvblvlab1             ncpvblvlab1-worker-214.ncpvblvlab1.t-mobile.lab    provisioned                                                                                                                                              true             64d ncpvblvlab1             ncpvblvlab1-worker-215.ncpvblvlab1.t-mobile.lab    provisioned                                                                                                                                              true             64d ncpvblvlab1             ncpvblvlab1-worker-216.ncpvblvlab1.t-mobile.lab    provisioned                                                                                                                                              true             64d ncpvblvlab1             ncpvblvlab1-worker-217.ncpvblvlab1.t-mobile.lab    provisioned                                                                                                                                              true             53d ncpvblvlab1             ncpvblvlab1-worker-218.ncpvblvlab1.t-mobile.lab    provisioned                                                                                                                                              true             64d ncpvblvlab1             ncpvblvlab1-worker-219.ncpvblvlab1.t-mobile.lab    provisioned                                                                                                                                              true             64d ncpvblvlab1             ncpvblvlab1-worker-220.ncpvblvlab1.t-mobile.lab    provisioned                                                                                                                                              true             62d ncpvblvlab1             ncpvblvlab1-worker-221.ncpvblvlab1.t-mobile.lab    provisioned                                                                                                                                              true             64d ncpvblvlab1             ncpvblvlab1-worker-222.ncpvblvlab1.t-mobile.lab    provisioned                                                                                                                                              true             64d ncpvblvlab2             ncpvblvlab2-master-101.ncpvblvlab2.t-mobile.lab    provisioned                                                                                                                                              true             21d ncpvblvlab2             ncpvblvlab2-master-102.ncpvblvlab2.t-mobile.lab    provisioned                                                                                                                                              true             61d ncpvblvlab2             ncpvblvlab2-master-201.ncpvblvlab2.t-mobile.lab    provisioned                                                                                                                                              true             61d ncpvblvlab2             ncpvblvlab2-storage-101.ncpvblvlab2.t-mobile.lab   provisioned                                                                                                                                              true             61d ncpvblvlab2             ncpvblvlab2-storage-102.ncpvblvlab2.t-mobile.lab   provisioned                                                                                                                                              true             60d ncpvblvlab2             ncpvblvlab2-storage-103.ncpvblvlab2.t-mobile.lab   provisioned                                                                                                                                              true             61d ncpvblvlab2             ncpvblvlab2-storage-201.ncpvblvlab2.t-mobile.lab   provisioned                                                                                                                                              true             61d ncpvblvlab2             ncpvblvlab2-storage-202.ncpvblvlab2.t-mobile.lab   provisioned                                                                                                                                              true             61d ncpvblvlab2             ncpvblvlab2-storage-203.ncpvblvlab2.t-mobile.lab   provisioned                                                                                                                                              true             61d ncpvblvlab2             ncpvblvlab2-worker-101.ncpvblvlab2.t-mobile.lab    provisioned                                                                                                                                              true             61d ncpvblvlab2             ncpvblvlab2-worker-102.ncpvblvlab2.t-mobile.lab    provisioned                                                                                                                                              true             61d ncpvblvlab2             ncpvblvlab2-worker-103.ncpvblvlab2.t-mobile.lab    provisioned                                                                                                                                              true             61d ncpvblvlab2             ncpvblvlab2-worker-104.ncpvblvlab2.t-mobile.lab    provisioned                                                                                                                                              true             61d ncpvblvlab2             ncpvblvlab2-worker-105.ncpvblvlab2.t-mobile.lab    provisioned                                                                                                                                              true             61d ncpvblvlab2             ncpvblvlab2-worker-106.ncpvblvlab2.t-mobile.lab    provisioned                                                                                                                                              true             61d ncpvblvlab2             ncpvblvlab2-worker-107.ncpvblvlab2.t-mobile.lab    provisioned                                                                                                                                              true             61d ncpvblvlab2             ncpvblvlab2-worker-108.ncpvblvlab2.t-mobile.lab    provisioned                                                                                                                                              true             61d ncpvblvlab2             ncpvblvlab2-worker-109.ncpvblvlab2.t-mobile.lab    provisioned                                                                                                                                              true             61d ncpvblvlab2             ncpvblvlab2-worker-110.ncpvblvlab2.t-mobile.lab    provisioned                                                                                                                                              true             61d ncpvblvlab2             ncpvblvlab2-worker-111.ncpvblvlab2.t-mobile.lab    provisioned                                                                                                                                              true             61d ncpvblvlab2             ncpvblvlab2-worker-112.ncpvblvlab2.t-mobile.lab    provisioned                                                                                                                                              true             61d ncpvblvlab2             ncpvblvlab2-worker-113.ncpvblvlab2.t-mobile.lab    provisioned                                                                                                                                              true             61d ncpvblvlab2             ncpvblvlab2-worker-114.ncpvblvlab2.t-mobile.lab    provisioned                                                                                                                                              true             61d ncpvblvlab2             ncpvblvlab2-worker-115.ncpvblvlab2.t-mobile.lab    provisioned                                                                                                                                              true             61d ncpvblvlab2             ncpvblvlab2-worker-116.ncpvblvlab2.t-mobile.lab    provisioned                                                                                                                                              true             61d ncpvblvlab2             ncpvblvlab2-worker-117.ncpvblvlab2.t-mobile.lab    provisioned                                                                                                                                              true             61d ncpvblvlab2             ncpvblvlab2-worker-118.ncpvblvlab2.t-mobile.lab    provisioned                                                                                                                                              true             61d ncpvblvlab2             ncpvblvlab2-worker-119.ncpvblvlab2.t-mobile.lab    provisioned                                                                                                                                              true             61d ncpvblvlab2             ncpvblvlab2-worker-120.ncpvblvlab2.t-mobile.lab    provisioned                                                                                                                                              true             61d ncpvblvlab2             ncpvblvlab2-worker-121.ncpvblvlab2.t-mobile.lab    provisioned                                                                                                                                              true             61d ncpvblvlab2             ncpvblvlab2-worker-122.ncpvblvlab2.t-mobile.lab    provisioned                                                                                                                                              true             57d ncpvblvlab2             ncpvblvlab2-worker-201.ncpvblvlab2.t-mobile.lab    provisioned                                                                                                                                              true             61d ncpvblvlab2             ncpvblvlab2-worker-202.ncpvblvlab2.t-mobile.lab    provisioned                                                                                                                                              true             61d ncpvblvlab2             ncpvblvlab2-worker-203.ncpvblvlab2.t-mobile.lab    provisioned                                                                                                                                              true             61d ncpvblvlab2             ncpvblvlab2-worker-204.ncpvblvlab2.t-mobile.lab    provisioned                                                                                                                                              true             61d ncpvblvlab2             ncpvblvlab2-worker-205.ncpvblvlab2.t-mobile.lab    provisioned                                                                                                                                              true             61d ncpvblvlab2             ncpvblvlab2-worker-206.ncpvblvlab2.t-mobile.lab    provisioned                                                                                                                                              true             61d ncpvblvlab2             ncpvblvlab2-worker-207.ncpvblvlab2.t-mobile.lab    provisioned                                                                                                                                              true             61d ncpvblvlab2             ncpvblvlab2-worker-208.ncpvblvlab2.t-mobile.lab    provisioned                                                                                                                                              true             61d ncpvblvlab2             ncpvblvlab2-worker-209.ncpvblvlab2.t-mobile.lab    provisioned                                                                                                                                              true             61d ncpvblvlab2             ncpvblvlab2-worker-210.ncpvblvlab2.t-mobile.lab    provisioned                                                                                                                                              true             61d ncpvblvlab2             ncpvblvlab2-worker-211.ncpvblvlab2.t-mobile.lab    provisioned                                                                                                                                              true             61d ncpvblvlab2             ncpvblvlab2-worker-212.ncpvblvlab2.t-mobile.lab    provisioned                                                                                                                                              true             61d ncpvblvlab2             ncpvblvlab2-worker-213.ncpvblvlab2.t-mobile.lab    provisioned                                                                                                                                              true             61d ncpvblvlab2             ncpvblvlab2-worker-214.ncpvblvlab2.t-mobile.lab    provisioned                                                                                                                                              true             55d ncpvblvlab2             ncpvblvlab2-worker-215.ncpvblvlab2.t-mobile.lab    provisioned                                                                                                                                              true             61d ncpvblvlab2             ncpvblvlab2-worker-216.ncpvblvlab2.t-mobile.lab    provisioned                                                                                                                                              true             61d ncpvblvlab2             ncpvblvlab2-worker-217.ncpvblvlab2.t-mobile.lab    provisioned                                                                                                                                              true             61d ncpvblvlab2             ncpvblvlab2-worker-218.ncpvblvlab2.t-mobile.lab    provisioned                                                                                                                                              true             61d ncpvblvlab2             ncpvblvlab2-worker-219.ncpvblvlab2.t-mobile.lab    provisioned                                                                                                                                              true             61d ncpvblvlab2             ncpvblvlab2-worker-220.ncpvblvlab2.t-mobile.lab    provisioned                                                                                                                                              true             61d ncpvblvlab2             ncpvblvlab2-worker-221.ncpvblvlab2.t-mobile.lab    provisioned                                                                                                                                              true             61d ncpvblvlab2             ncpvblvlab2-worker-222.ncpvblvlab2.t-mobile.lab    provisioned                                                                                                                                              true             61d ncpvblvmgt              ncpvblvmgt-gateway-101.ncpvblvmgt.t-mobile.lab     provisioned                                                                                                                                              true             25d ncpvblvmgt              ncpvblvmgt-gateway-102.ncpvblvmgt.t-mobile.lab     provisioned                                                                                                                                              true             69d ncpvblvmgt              ncpvblvmgt-master-101.ncpvblvmgt.t-mobile.lab      provisioned                                                                                                                                              true             20d ncpvblvmgt              ncpvblvmgt-master-102.ncpvblvmgt.t-mobile.lab      provisioned                                                                                                                                              true             69d ncpvblvmgt              ncpvblvmgt-master-103.ncpvblvmgt.t-mobile.lab      provisioned                                                                                                                                              true             69d ncpvblvmgt              ncpvblvmgt-storage-101.ncpvblvmgt.t-mobile.lab     provisioned                                                                                                                                              true             69d ncpvblvmgt              ncpvblvmgt-storage-102.ncpvblvmgt.t-mobile.lab     provisioned                                                                                                                                              true             69d ncpvblvmgt              ncpvblvmgt-storage-103.ncpvblvmgt.t-mobile.lab     provisioned                                                                                                                                              true             69d ncpvblvmgt              ncpvblvmgt-storage-104.ncpvblvmgt.t-mobile.lab     provisioned                                                                                                                                              true             25d ncpvblvmgt              ncpvblvmgt-worker-101.ncpvblvmgt.t-mobile.lab      provisioned                                                                                                                                              true             69d ncpvblvmgt              ncpvblvmgt-worker-102.ncpvblvmgt.t-mobile.lab      provisioned                                                                                                                                              true             69d ncpvblvmgt              ncpvblvmgt-worker-103.ncpvblvmgt.t-mobile.lab      provisioned                                                                                                                                              true             69d ncpvblvmgt              ncpvblvmgt-worker-104.ncpvblvmgt.t-mobile.lab      provisioned                                                                                                                                              true             69d ncpvblvmgt              ncpvblvmgt-worker-105.ncpvblvmgt.t-mobile.lab      provisioned                                                                                                                                              true             69d ncpvblvmgt              ncpvblvmgt-worker-106.ncpvblvmgt.t-mobile.lab      provisioned                                                                                                                                              true             69d ncpvblvmgt              ncpvblvmgt-worker-107.ncpvblvmgt.t-mobile.lab      provisioned                                                                                                                                              true             26d ncpvblvmgt              ncpvblvmgt-worker-108.ncpvblvmgt.t-mobile.lab      provisioned                                                                                                                                              true             69d ncpvblvmgt              ncpvblvmgt-worker-109.ncpvblvmgt.t-mobile.lab      provisioned                                                                                                                                              true             69d ncpvblvmgt              ncpvblvmgt-worker-110.ncpvblvmgt.t-mobile.lab      provisioned                                                                                                                                              true             69d openshift-machine-api   ncpvblvhub-hubmaster-101.ncpvblvhub.t-mobile.lab   unmanaged     ncpvblvhub-b                                                                                                         6cjs-master-0         true             74d openshift-machine-api   ncpvblvhub-hubmaster-102.ncpvblvhub.t-mobile.lab   provisioned   ncpvblvhub-b                                                                                                         6cjs-master-1         true             37m openshift-machine-api   ncpvblvhub-hubmaster-103.ncpvblvhub.t-mobile.lab   unmanaged     ncpvblvhub-b                                                                                                         6cjs-master-2         true             74d openshift-machine-api   ncpvblvhub-hubworker-101.ncpvblvhub.t-mobile.lab   unmanaged     ncpvblvhub-b                                                                                                         6cjs-worker-0-mlq8w   true             74d openshift-machine-api   ncpvblvhub-hubworker-102.ncpvblvhub.t-mobile.lab   unmanaged     ncpvblvhub-b                                                                                                         6cjs-worker-0-x5dc5   true             74d [root@dom16hub101-infra-manager ~]# oc get no NAME                                               STATUS   ROLES                                 AGE                                                                                                            VERSION ncpvblvhub-hubmaster-101.ncpvblvhub.t-mobile.lab   Ready    control-plane,master,monitor,worker   74d                                                                                                            v1.29.10+67d3387 ncpvblvhub-hubmaster-103.ncpvblvhub.t-mobile.lab   Ready    control-plane,master,monitor,worker   74d                                                                                                            v1.29.10+67d3387 ncpvblvhub-hubworker-101.ncpvblvhub.t-mobile.lab   Ready    gateway,worker                        74d                                                                                                            v1.29.10+67d3387 ncpvblvhub-hubworker-102.ncpvblvhub.t-mobile.lab   Ready    gateway,worker                        74d                                                                                                            v1.29.10+67d3387 [root@dom16hub101-infra-manager ~]# oc get no NAME                                               STATUS   ROLES                                 AGE                                                                                                            VERSION ncpvblvhub-hubmaster-101.ncpvblvhub.t-mobile.lab   Ready    control-plane,master,monitor,worker   74d                                                                                                            v1.29.10+67d3387 ncpvblvhub-hubmaster-102.ncpvblvhub.t-mobile.lab   Ready    control-plane,master,worker           54s                                                                                                            v1.29.10+67d3387 ncpvblvhub-hubmaster-103.ncpvblvhub.t-mobile.lab   Ready    control-plane,master,monitor,worker   74d                                                                                                            v1.29.10+67d3387 ncpvblvhub-hubworker-101.ncpvblvhub.t-mobile.lab   Ready    gateway,worker                        74d                                                                                                            v1.29.10+67d3387 ncpvblvhub-hubworker-102.ncpvblvhub.t-mobile.lab   Ready    gateway,worker                        74d                                                                                                            v1.29.10+67d3387 [root@dom16hub101-infra-manager ~]# oc get machines.machine.openshift.io -n openshift-machine-apioc get machines.machine.openshift.io -n openshift-machine-api^C [root@dom16hub101-infra-manager ~]# oc get machines.machine.openshift.io -n openshift-machine-api NAME                              PHASE     TYPE   REGION   ZONE   AGE ncpvblvhub-b6cjs-master-0         Running                          74d ncpvblvhub-b6cjs-master-1         Running                          15m ncpvblvhub-b6cjs-master-2         Running                          74d ncpvblvhub-b6cjs-worker-0-mlq8w   Running                          74d ncpvblvhub-b6cjs-worker-0-x5dc5   Running                          74d [root@dom16hub101-infra-manager ~]# oc get no NAME                                               STATUS   ROLES                                 AGE    VERSION ncpvblvhub-hubmaster-101.ncpvblvhub.t-mobile.lab   Ready    control-plane,master,monitor,worker   74d    v1.29.10+67d3387 ncpvblvhub-hubmaster-102.ncpvblvhub.t-mobile.lab   Ready    control-plane,master,worker           110s   v1.29.10+67d3387 ncpvblvhub-hubmaster-103.ncpvblvhub.t-mobile.lab   Ready    control-plane,master,monitor,worker   74d    v1.29.10+67d3387 ncpvblvhub-hubworker-101.ncpvblvhub.t-mobile.lab   Ready    gateway,worker                        74d    v1.29.10+67d3387 ncpvblvhub-hubworker-102.ncpvblvhub.t-mobile.lab   Ready    gateway,worker                        74d    v1.29.10+67d3387 [root@dom16hub101-infra-manager ~]# master=ncpvblvhub-hubmaster-102.ncpvblvhub.t-mobile.lab [root@dom16hub101-infra-manager ~]# oc exec -it $(oc -n openshift-etcd get pods -l k8s-app=etcd -o wide --no-headers |grep -v $master|head -n 1|awk {'print $1'}) -n openshift-etcd -- etcdctl endpoint health https://10.145.151.94:2379 is healthy: successfully committed proposal: took = 7.570208ms https://10.145.151.92:2379 is healthy: successfully committed proposal: took = 7.468077ms [root@dom16hub101-infra-manager ~]# oc exec -it $(oc -n openshift-etcd get pods -l k8s-app=etcd -o wide --no-headers |grep -v $master|head -n 1|awk {'print $1'}) -n openshift-etcd -- etcdctl member list -w table +------------------+---------+--------------------------------------------------+----------------------------+----------------------------+------------+ |        ID        | STATUS  |                       NAME                       |         PEER ADDRS         |        CLIENT ADDRS        | IS LEARNER | +------------------+---------+--------------------------------------------------+----------------------------+----------------------------+------------+ | 44ad9888985e068c | started | ncpvblvhub-hubmaster-101.ncpvblvhub.t-mobile.lab | https://10.145.151.92:2380 | https://10.145.151.92:2379 |      false | | 55fcb74f654c5538 | started | ncpvblvhub-hubmaster-102.ncpvblvhub.t-mobile.lab | https://10.145.151.93:2380 | https://10.145.151.93:2379 |      false | | f26d12a58d17e571 | started | ncpvblvhub-hubmaster-103.ncpvblvhub.t-mobile.lab | https://10.145.151.94:2380 | https://10.145.151.94:2379 |      false | +------------------+---------+--------------------------------------------------+----------------------------+----------------------------+------------+ [root@dom16hub101-infra-manager ~]# oc exec -it $(oc -n openshift-etcd get pods -l k8s-app=etcd -o wide --no-headers |grep -v $master|head -n 1|awk {'print $1'}) -n openshift-etcd -- etcdctl endpoint health https://10.145.151.94:2379 is healthy: successfully committed proposal: took = 6.734404ms https://10.145.151.92:2379 is healthy: successfully committed proposal: took = 7.079121ms [root@dom16hub101-infra-manager ~]# oc get no NAME                                               STATUS   ROLES                                 AGE     VERSION ncpvblvhub-hubmaster-101.ncpvblvhub.t-mobile.lab   Ready    control-plane,master,monitor,worker   74d     v1.29.10+67d3387 ncpvblvhub-hubmaster-102.ncpvblvhub.t-mobile.lab   Ready    control-plane,master,worker           3m55s   v1.29.10+67d3387 ncpvblvhub-hubmaster-103.ncpvblvhub.t-mobile.lab   Ready    control-plane,master,monitor,worker   74d     v1.29.10+67d3387 ncpvblvhub-hubworker-101.ncpvblvhub.t-mobile.lab   Ready    gateway,worker                        74d     v1.29.10+67d3387 ncpvblvhub-hubworker-102.ncpvblvhub.t-mobile.lab   Ready    gateway,worker                        74d     v1.29.10+67d3387 [root@dom16hub101-infra-manager ~]# oc get mcp NAME     CONFIG                                             UPDATED   UPDATING   DEGRADED   MACHINECOUNT   READYMACHINECOUNT   UPDATEDMACHINECOUNT   DEGRADEDMACHINECOUNT   AGE master   rendered-master-d234bb112f0765116acd91ac75545416   True      False      False      3              3                   3                     0                      74d worker   rendered-worker-3e1f74a73d4a683cfbf22ced0aa2792a   False     True       True       2              1                   1                     1                      74d [root@dom16hub101-infra-manager ~]# oc exec -it $(oc -n openshift-etcd get pods -l k8s-app=etcd -o wide --no-headers |grep -v $master|head -n 1|awk {'print $1'}) -n openshift-etcd -- etcdctl endpoint health https://10.145.151.92:2379 is healthy: successfully committed proposal: took = 6.241732ms https://10.145.151.94:2379 is healthy: successfully committed proposal: took = 6.383176ms [root@dom16hub101-infra-manager ~]# oc exec -it $(oc -n openshift-etcd get pods -l k8s-app=etcd -o wide --no-headers |grep -v $master|head -n 1|awk {'print $1'}) -n openshift-etcd -- etcdctl endpoint health error: unable to upgrade connection: container not found (\"etcd\") [root@dom16hub101-infra-manager ~]# oc exec -it $(oc -n openshift-etcd get pods -l k8s-app=etcd -o wide --no-headers |grep -v $master|head -n 1|awk {'print $1'}) -n openshift-etcd -- etcdctl endpoint health error: unable to upgrade connection: container not found (\"etcd\") [root@dom16hub101-infra-manager ~]# oc get no NAME                                               STATUS   ROLES                                 AGE     VERSION ncpvblvhub-hubmaster-101.ncpvblvhub.t-mobile.lab   Ready    control-plane,master,monitor,worker   74d     v1.29.10+67d3387 ncpvblvhub-hubmaster-102.ncpvblvhub.t-mobile.lab   Ready    control-plane,master,worker           5m51s   v1.29.10+67d3387 ncpvblvhub-hubmaster-103.ncpvblvhub.t-mobile.lab   Ready    control-plane,master,monitor,worker   74d     v1.29.10+67d3387 ncpvblvhub-hubworker-101.ncpvblvhub.t-mobile.lab   Ready    gateway,worker                        74d     v1.29.10+67d3387 ncpvblvhub-hubworker-102.ncpvblvhub.t-mobile.lab   Ready    gateway,worker                        74d     v1.29.10+67d3387 [root@dom16hub101-infra-manager ~]# oc exec -it $(oc -n openshift-etcd get pods -l k8s-app=etcd -o wide --no-headers |grep -v $master|head -n 1|awk {'print $1'}) -n openshift-etcd -- etcdctl endpoint health</p> <p>{\"level\":\"warn\",\"ts\":\"2025-05-27T21:24:52.680683Z\",\"logger\":\"client\",\"caller\":\"v3@v3.5.14/retry_interceptor.go:63\",\"msg\":\"retrying of unary invoker failed\",\"target\":\"etcd-endpoints://0xc00022e000/10.145.151.92:2379\",\"attempt\":0,\"error\":\"rpc error: code = DeadlineExceeded desc = latest balancer error: last connection error: connection error: desc = \\\"transport: Error while dialing: dial tcp 10.145.151.92:2379: connect: connection refused\\\"\"} https://10.145.151.93:2379 is healthy: successfully committed proposal: took = 6.311573ms https://10.145.151.94:2379 is healthy: successfully committed proposal: took = 8.305729ms https://10.145.151.92:2379 is unhealthy: failed to commit proposal: context deadline exceeded Error: unhealthy cluster command terminated with exit code 1 [root@dom16hub101-infra-manager ~]# oc get co NAME                                       VERSION   AVAILABLE   PROGRESSING   DEGRADED   SINCE   MESSAGE authentication                             4.16.24   True        False         False      3h48m baremetal                                  4.16.24   True        False         False      74d cloud-controller-manager                   4.16.24   True        False         False      74d cloud-credential                           4.16.24   True        False         False      74d cluster-autoscaler                         4.16.24   True        False         False      74d config-operator                            4.16.24   True        False         False      74d console                                    4.16.24   True        False         False      74d control-plane-machine-set                  4.16.24   True        False         False      74d csi-snapshot-controller                    4.16.24   True        False         False      74d dns                                        4.16.24   True        False         False      74d etcd                                       4.16.24   True        True          False      74d     NodeInstallerProgressing: 1 node is at revision 15; 2 nodes are at revision 17 image-registry                             4.16.24   True        False         False      74d ingress                                    4.16.24   True        False         False      74d insights                                   4.16.24   True        False         False      74d kube-apiserver                             4.16.24   True        True          False      74d     NodeInstallerProgressing: 2 nodes are at revision 60; 1 node is at revision 61 kube-controller-manager                    4.16.24   True        False         False      74d kube-scheduler                             4.16.24   True        False         False      74d kube-storage-version-migrator              4.16.24   True        False         False      13d machine-api                                4.16.24   True        False         False      74d machine-approver                           4.16.24   True        False         False      74d machine-config                             4.16.24   True        False         True       74d     Failed to resync 4.16.24 because: error during syncRequiredMachineConfigPools: [context deadline exceeded, failed to update clusteroperator: [client rate limiter Wait returned an error: context deadline exceeded, error MachineConfigPool worker is not ready, retrying. Status: (pool degraded: true total: 2, ready 1, updated: 1, unavailable: 1)]] marketplace                                4.16.24   True        False         False      74d monitoring                                 4.16.24   Unknown     True          Unknown    35m     Rolling out the stack. network                                    4.16.24   True        False         False      74d node-tuning                                4.16.24   True        False         False      6m20s openshift-apiserver                        4.16.24   True        False         False      4h17m openshift-controller-manager               4.16.24   True        False         False      74d openshift-samples                          4.16.24   True        False         False      74d operator-lifecycle-manager                 4.16.24   True        False         False      74d operator-lifecycle-manager-catalog         4.16.24   True        False         False      74d operator-lifecycle-manager-packageserver   4.16.24   True        False         False      74d service-ca                                 4.16.24   True        False         False      74d storage                                    4.16.24   True        False         False      74d [root@dom16hub101-infra-manager ~]# oc get co NAME                                       VERSION   AVAILABLE   PROGRESSING   DEGRADED   SINCE   MESSAGE authentication                             4.16.24   True        False         False      3h49m baremetal                                  4.16.24   True        False         False      74d cloud-controller-manager                   4.16.24   True        False         False      74d cloud-credential                           4.16.24   True        False         False      74d cluster-autoscaler                         4.16.24   True        False         False      74d config-operator                            4.16.24   True        False         False      74d console                                    4.16.24   True        False         False      74d control-plane-machine-set                  4.16.24   True        False         False      74d csi-snapshot-controller                    4.16.24   True        False         False      74d dns                                        4.16.24   True        False         False      74d etcd                                       4.16.24   True        True          False      74d     NodeInstallerProgressing: 1 node is at revision 15; 2 nodes are at revision 17 image-registry                             4.16.24   True        False         False      74d ingress                                    4.16.24   True        False         False      74d insights                                   4.16.24   True        False         False      74d kube-apiserver                             4.16.24   True        True          False      74d     NodeInstallerProgressing: 2 nodes are at revision 60; 1 node is at revision 61 kube-controller-manager                    4.16.24   True        False         False      74d kube-scheduler                             4.16.24   True        False         False      74d kube-storage-version-migrator              4.16.24   True        False         False      13d machine-api                                4.16.24   True        False         False      74d machine-approver                           4.16.24   True        False         False      74d machine-config                             4.16.24   True        False         True       74d     Failed to resync 4.16.24 because: error during syncRequiredMachineConfigPools: [context deadline exceeded, failed to update clusteroperator: [client rate limiter Wait returned an error: context deadline exceeded, error MachineConfigPool worker is not ready, retrying. Status: (pool degraded: true total: 2, ready 1, updated: 1, unavailable: 1)]] marketplace                                4.16.24   True        False         False      74d monitoring                                 4.16.24   Unknown     True          Unknown    36m     Rolling out the stack. network                                    4.16.24   True        False         False      74d node-tuning                                4.16.24   True        False         False      7m27s openshift-apiserver                        4.16.24   True        False         False      4h18m openshift-controller-manager               4.16.24   True        False         False      74d openshift-samples                          4.16.24   True        False         False      74d operator-lifecycle-manager                 4.16.24   True        False         False      74d operator-lifecycle-manager-catalog         4.16.24   True        False         False      74d operator-lifecycle-manager-packageserver   4.16.24   True        False         False      74d service-ca                                 4.16.24   True        False         False      74d storage                                    4.16.24   True        False         False      74d [root@dom16hub101-infra-manager ~]# oc get co NAME                                       VERSION   AVAILABLE   PROGRESSING   DEGRADED   SINCE   MESSAGE authentication                             4.16.24   True        False         False      3h49m baremetal                                  4.16.24   True        False         False      74d cloud-controller-manager                   4.16.24   True        False         False      74d cloud-credential                           4.16.24   True        False         False      74d cluster-autoscaler                         4.16.24   True        False         False      74d config-operator                            4.16.24   True        False         False      74d console                                    4.16.24   True        False         False      74d control-plane-machine-set                  4.16.24   True        False         False      74d csi-snapshot-controller                    4.16.24   True        False         False      74d dns                                        4.16.24   True        False         False      74d etcd                                       4.16.24   True        True          False      74d     NodeInstallerProgressing: 1 node is at revision 15; 2 nodes are at revision 17 image-registry                             4.16.24   True        False         False      74d ingress                                    4.16.24   True        False         False      74d insights                                   4.16.24   True        False         False      74d kube-apiserver                             4.16.24   True        True          False      74d     NodeInstallerProgressing: 2 nodes are at revision 60; 1 node is at revision 61 kube-controller-manager                    4.16.24   True        False         False      74d kube-scheduler                             4.16.24   True        False         False      74d kube-storage-version-migrator              4.16.24   True        False         False      13d machine-api                                4.16.24   True        False         False      74d machine-approver                           4.16.24   True        False         False      74d machine-config                             4.16.24   True        False         True       74d     Failed to resync 4.16.24 because: error during syncRequiredMachineConfigPools: [context deadline exceeded, failed to update clusteroperator: [client rate limiter Wait returned an error: context deadline exceeded, error MachineConfigPool worker is not ready, retrying. Status: (pool degraded: true total: 2, ready 1, updated: 1, unavailable: 1)]] marketplace                                4.16.24   True        False         False      74d monitoring                                 4.16.24   Unknown     True          Unknown    37m     Rolling out the stack. network                                    4.16.24   True        False         False      74d node-tuning                                4.16.24   True        False         False      7m34s openshift-apiserver                        4.16.24   True        False         False      4h18m openshift-controller-manager               4.16.24   True        False         False      74d openshift-samples                          4.16.24   True        False         False      74d operator-lifecycle-manager                 4.16.24   True        False         False      74d operator-lifecycle-manager-catalog         4.16.24   True        False         False      74d operator-lifecycle-manager-packageserver   4.16.24   True        False         False      74d service-ca                                 4.16.24   True        False         False      74d storage                                    4.16.24   True        False         False      74d [root@dom16hub101-infra-manager ~]# oc exec -it $(oc -n openshift-etcd get pods -l k8s-app=etcd -o wide --no-headers |grep -v $master|head -n 1|awk {'print $1'}) -n openshift-etcd -- etcdctl endpoint health {\"level\":\"warn\",\"ts\":\"2025-05-27T21:26:31.196422Z\",\"logger\":\"client\",\"caller\":\"v3@v3.5.14/retry_interceptor.go:63\",\"msg\":\"retrying of unary invoker failed\",\"target\":\"etcd-endpoints://0xc0001f2000/10.145.151.93:2379\",\"attempt\":0,\"error\":\"rpc error: code = DeadlineExceeded desc = latest balancer error: last connection error: connection error: desc = \\\"transport: Error while dialing: dial tcp 10.145.151.93:2379: connect: connection refused\\\"\"} https://10.145.151.94:2379 is healthy: successfully committed proposal: took = 6.840096ms https://10.145.151.92:2379 is healthy: successfully committed proposal: took = 6.962661ms https://10.145.151.93:2379 is unhealthy: failed to commit proposal: context deadline exceeded Error: unhealthy cluster command terminated with exit code 1 [root@dom16hub101-infra-manager ~]# watch -n 5 oc get co [root@dom16hub101-infra-manager ~]# oc exec -it $(oc -n openshift-etcd get pods -l k8s-app=etcd -o wide --no-headers |grep -v $master|head -n 1|awk {'print $1'}) -n openshift-etcd -- etcdctl endpoint health https://10.145.151.93:2379 is healthy: successfully committed proposal: took = 6.998079ms https://10.145.151.92:2379 is healthy: successfully committed proposal: took = 7.999787ms https://10.145.151.94:2379 is healthy: successfully committed proposal: took = 7.997677ms [root@dom16hub101-infra-manager ~]# oc patch etcd/cluster --type=merge -p '{\"spec\": {\"unsupportedConfigOverrides\": null}}' etcd.operator.openshift.io/cluster patched [root@dom16hub101-infra-manager ~]# oc get pods -n openshift-local-storage -o wide NAME                                      READY   STATUS    RESTARTS        AGE   IP             NODE                                               NOMINATED NODE   READINESS GATES diskmaker-discovery-6mcrl                 2/2     Running   4               71d   172.20.0.163   ncpvblvhub-hubmaster-103.ncpvblvhub.t-mobile.lab     diskmaker-discovery-9htrb                 2/2     Running   6               71d   172.20.2.141   ncpvblvhub-hubmaster-101.ncpvblvhub.t-mobile.lab     diskmaker-manager-fbzkp                   2/2     Running   4               71d   172.20.0.164   ncpvblvhub-hubmaster-103.ncpvblvhub.t-mobile.lab     diskmaker-manager-h6r2t                   2/2     Running   6               71d   172.20.2.142   ncpvblvhub-hubmaster-101.ncpvblvhub.t-mobile.lab     local-storage-operator-6d968c9989-w4vp4   1/1     Running   2 (3h42m ago)   56d   172.23.0.66    ncpvblvhub-hubworker-101.ncpvblvhub.t-mobile.lab     [root@dom16hub101-infra-manager ~]# oc get no -l  cluster.ocs.openshift.io/openshift-storage NAME                                               STATUS   ROLES                                 AGE   VERSION ncpvblvhub-hubmaster-101.ncpvblvhub.t-mobile.lab   Ready    control-plane,master,monitor,worker   74d   v1.29.10+67d3387 ncpvblvhub-hubmaster-103.ncpvblvhub.t-mobile.lab   Ready    control-plane,master,monitor,worker   74d   v1.29.10+67d3387 [root@dom16hub101-infra-manager ~]# oc label node ^C [root@dom16hub101-infra-manager ~]# oc get no NAME                                               STATUS   ROLES                                 AGE   VERSION ncpvblvhub-hubmaster-101.ncpvblvhub.t-mobile.lab   Ready    control-plane,master,monitor,worker   74d   v1.29.10+67d3387 ncpvblvhub-hubmaster-102.ncpvblvhub.t-mobile.lab   Ready    control-plane,master,worker           13m   v1.29.10+67d3387 ncpvblvhub-hubmaster-103.ncpvblvhub.t-mobile.lab   Ready    control-plane,master,monitor,worker   74d   v1.29.10+67d3387 ncpvblvhub-hubworker-101.ncpvblvhub.t-mobile.lab   Ready    gateway,worker                        74d   v1.29.10+67d3387 ncpvblvhub-hubworker-102.ncpvblvhub.t-mobile.lab   Ready    gateway,worker                        74d   v1.29.10+67d3387 [root@dom16hub101-infra-manager ~]# oc label node ncpvblvhub-hubmaster-102.ncpvblvhub.t-mobile.lab cluster.ocs.openshift.io/openshift-storage error: at least one label update is required [root@dom16hub101-infra-manager ~]# oc label node ncpvblvhub-hubmaster-102.ncpvblvhub.t-mobile.lab --help Update the labels on a resource. <ul> <li>A label key and value must begin with a letter or number, and may contain letters, numbers, hyphens, dots, and underscores, up to 63 characters each.</li> <li>Optionally, the key can begin with a DNS subdomain prefix and a single '/', like example.com/my-app.</li> <li>If --overwrite is true, then existing labels can be overwritten, otherwise attempting to overwrite a label will result in an error.</li> <li>If --resource-version is specified, then updates will use this resource version, otherwise the existing resource-version will be used.</li> </ul> <p>Examples:   # Update pod 'foo' with the label 'unhealthy' and the value 'true'   oc label pods foo unhealthy=true</p> <p># Update pod 'foo' with the label 'status' and the value 'unhealthy', overwriting any existing value   oc label --overwrite pods foo status=unhealthy</p> <p># Update all pods in the namespace   oc label pods --all status=unhealthy</p> <p># Update a pod identified by the type and name in \"pod.json\"   oc label -f pod.json status=unhealthy</p> <p># Update pod 'foo' only if the resource is unchanged from version 1   oc label pods foo status=unhealthy --resource-version=1</p> <p># Update pod 'foo' by removing a label named 'bar' if it exists   # Does not require the --overwrite flag   oc label pods foo bar-</p> <p>Options:     --all=false:         Select all resources, in the namespace of the specified resource types</p> <pre><code>-A, --all-namespaces=false:\n    If true, check the specified action in all namespaces.\n\n--allow-missing-template-keys=true:\n    If true, ignore any errors in templates when a field or map key is missing in the template. Only applies to\n    golang and jsonpath output formats.\n\n--dry-run='none':\n    Must be \"none\", \"server\", or \"client\". If client strategy, only print the object that would be sent, without\n    sending it. If server strategy, submit server-side request without persisting the resource.\n\n--field-manager='kubectl-label':\n    Name of the manager used to track field ownership.\n\n--field-selector='':\n    Selector (field query) to filter on, supports '=', '==', and '!='.(e.g. --field-selector\n    key1=value1,key2=value2). The server only supports a limited number of field queries per type.\n\n-f, --filename=[]:\n    Filename, directory, or URL to files identifying the resource to update the labels\n\n-k, --kustomize='':\n    Process the kustomization directory. This flag can't be used together with -f or -R.\n\n--list=false:\n    If true, display the labels for a given resource.\n\n--local=false:\n    If true, label will NOT contact api-server but run locally.\n\n-o, --output='':\n    Output format. One of: (json, yaml, name, go-template, go-template-file, template, templatefile, jsonpath,\n    jsonpath-as-json, jsonpath-file).\n\n--overwrite=false:\n    If true, allow labels to be overwritten, otherwise reject label updates that overwrite existing labels.\n\n-R, --recursive=false:\n    Process the directory used in -f, --filename recursively. Useful when you want to manage related manifests\n    organized within the same directory.\n\n--resource-version='':\n    If non-empty, the labels update will only succeed if this is the current resource-version for the object. Only\n    valid when specifying a single resource.\n\n-l, --selector='':\n    Selector (label query) to filter on, supports '=', '==', and '!='.(e.g. -l key1=value1,key2=value2). Matching\n    objects must satisfy all of the specified label constraints.\n\n--show-managed-fields=false:\n    If true, keep the managedFields when printing objects in JSON or YAML format.\n\n--template='':\n    Template string or path to template file to use when -o=go-template, -o=go-template-file. The template format\n    is golang templates [http://golang.org/pkg/text/template/#pkg-overview].\n</code></pre> <p>Usage:   oc label [--overwrite] (-f FILENAME | TYPE NAME) KEY_1=VAL_1 ... KEY_N=VAL_N [--resource-version=version] [options]</p> <p>Use \"oc options\" for a list of global command-line options (applies to all commands). [root@dom16hub101-infra-manager ~]# oc label node ncpvblvhub-hubmaster-102.ncpvblvhub.t-mobile.lab cluster.ocs.openshift.io/openshift-storage= node/ncpvblvhub-hubmaster-102.ncpvblvhub.t-mobile.lab labeled [root@dom16hub101-infra-manager ~]# oc get no -l  cluster.ocs.openshift.io/openshift-storage NAME                                               STATUS   ROLES                                 AGE   VERSION ncpvblvhub-hubmaster-101.ncpvblvhub.t-mobile.lab   Ready    control-plane,master,monitor,worker   74d   v1.29.10+67d3387 ncpvblvhub-hubmaster-102.ncpvblvhub.t-mobile.lab   Ready    control-plane,master,worker           15m   v1.29.10+67d3387 ncpvblvhub-hubmaster-103.ncpvblvhub.t-mobile.lab   Ready    control-plane,master,monitor,worker   74d   v1.29.10+67d3387 [root@dom16hub101-infra-manager ~]# oc get pods -n openshift-local-storage -o wide NAME                                      READY   STATUS    RESTARTS        AGE   IP             NODE                                               NOMINATED NODE   READINESS GATES diskmaker-discovery-6mcrl                 2/2     Running   4               71d   172.20.0.163   ncpvblvhub-hubmaster-103.ncpvblvhub.t-mobile.lab     diskmaker-discovery-9htrb                 2/2     Running   6               71d   172.20.2.141   ncpvblvhub-hubmaster-101.ncpvblvhub.t-mobile.lab     diskmaker-discovery-b6bxh                 2/2     Running   0               26s   172.21.0.33    ncpvblvhub-hubmaster-102.ncpvblvhub.t-mobile.lab     diskmaker-manager-8kpkt                   2/2     Running   0               26s   172.21.0.31    ncpvblvhub-hubmaster-102.ncpvblvhub.t-mobile.lab     diskmaker-manager-fbzkp                   2/2     Running   4               71d   172.20.0.164   ncpvblvhub-hubmaster-103.ncpvblvhub.t-mobile.lab     diskmaker-manager-h6r2t                   2/2     Running   6               71d   172.20.2.142   ncpvblvhub-hubmaster-101.ncpvblvhub.t-mobile.lab     local-storage-operator-6d968c9989-w4vp4   1/1     Running   2 (3h46m ago)   57d   172.23.0.66    ncpvblvhub-hubworker-101.ncpvblvhub.t-mobile.lab     [root@dom16hub101-infra-manager ~]# oc get co NAME                                       VERSION   AVAILABLE   PROGRESSING   DEGRADED   SINCE   MESSAGE authentication                             4.16.24   True        False         False      3h57m baremetal                                  4.16.24   True        False         False      74d cloud-controller-manager                   4.16.24   True        False         False      74d cloud-credential                           4.16.24   True        False         False      74d cluster-autoscaler                         4.16.24   True        False         False      74d config-operator                            4.16.24   True        False         False      74d console                                    4.16.24   True        False         False      74d control-plane-machine-set                  4.16.24   True        False         False      74d csi-snapshot-controller                    4.16.24   True        False         False      74d dns                                        4.16.24   True        False         False      74d etcd                                       4.16.24   True        False         False      74d image-registry                             4.16.24   True        False         False      74d ingress                                    4.16.24   True        False         False      74d insights                                   4.16.24   True        False         False      74d kube-apiserver                             4.16.24   True        False         False      74d kube-controller-manager                    4.16.24   True        False         False      74d kube-scheduler                             4.16.24   True        False         False      74d kube-storage-version-migrator              4.16.24   True        False         False      13d machine-api                                4.16.24   True        False         False      74d machine-approver                           4.16.24   True        False         False      74d machine-config                             4.16.24   True        False         True       74d     Failed to resync 4.16.24 because: error during syncRequiredMachineConfigPools: [context deadline exceeded, failed to update clusteroperator: [client rate limiter Wait returned an error: context deadline exceeded, error MachineConfigPool worker is not ready, retrying. Status: (pool degraded: true total: 2, ready 1, updated: 1, unavailable: 1)]] marketplace                                4.16.24   True        False         False      74d monitoring                                 4.16.24   Unknown     True          Unknown    45m     Rolling out the stack. network                                    4.16.24   True        False         False      74d node-tuning                                4.16.24   True        False         False      15m openshift-apiserver                        4.16.24   True        False         False      4h26m openshift-controller-manager               4.16.24   True        False         False      74d openshift-samples                          4.16.24   True        False         False      74d operator-lifecycle-manager                 4.16.24   True        False         False      74d operator-lifecycle-manager-catalog         4.16.24   True        False         False      74d operator-lifecycle-manager-packageserver   4.16.24   True        False         False      74d service-ca                                 4.16.24   True        False         False      74d storage                                    4.16.24   True        False         False      74d [root@dom16hub101-infra-manager ~]# oc get pods -n openshift-storage NAME                                                              READY   STATUS     RESTARTS         AGE csi-addons-controller-manager-7fdf8f85d5-tbq47                    2/2     Running    7 (3h45m ago)    57d csi-cephfsplugin-4pnm7                                            2/2     Running    7 (15d ago)      71d csi-cephfsplugin-d6bwn                                            2/2     Running    0                15m csi-cephfsplugin-fl7m9                                            2/2     Running    4                71d csi-cephfsplugin-provisioner-5996485946-58m2k                     6/6     Running    2 (3h47m ago)    54d csi-cephfsplugin-provisioner-5996485946-n66cq                     6/6     Running    10 (3h43m ago)   57d csi-cephfsplugin-vrdx8                                            2/2     Running    4                71d csi-cephfsplugin-wvvg9                                            2/2     Running    6                71d csi-rbdplugin-bxn4s                                               3/3     Running    10 (15d ago)     71d csi-rbdplugin-h8fvq                                               3/3     Running    0                15m csi-rbdplugin-l4xrk                                               3/3     Running    6                71d csi-rbdplugin-provisioner-646d95bdd9-bjp5j                        6/6     Running    1 (3h47m ago)    4h32m csi-rbdplugin-provisioner-646d95bdd9-zdjs8                        6/6     Running    10 (3h47m ago)   57d csi-rbdplugin-rzbl2                                               3/3     Running    6                71d csi-rbdplugin-xlt4f                                               3/3     Running    9                71d noobaa-core-0                                                     2/2     Running    0                27d noobaa-db-pg-0                                                    1/1     Running    0                54d noobaa-endpoint-769ccbd9f4-zs9c5                                  1/1     Running    0                27d noobaa-operator-6b567c8c7f-6vtgr                                  1/1     Running    6 (3h45m ago)    27d noobaa-pv-backing-store-noobaa-pod-1791bf5b                       1/1     Running    0                27d ocs-metrics-exporter-85c8468d88-bxl6x                             1/1     Running    0                57d ocs-operator-69667b5b9f-nb57q                                     1/1     Running    8 (3h43m ago)    35d odf-console-56757bc6ff-g429j                                      1/1     Running    0                56d odf-operator-controller-manager-6b68579866-fsdd7                  2/2     Running    2 (3h49m ago)    35d rook-ceph-crashcollector-11cef195e99cf42211bc5b21918ec486-xwt5h   1/1     Running    0                79s rook-ceph-crashcollector-3b80b15101138b8d06c7b668b8b5c01c-cn7rn   1/1     Running    0                54d rook-ceph-crashcollector-5636ff5c104ab5ecddd6b734c6b8046b-4r9fw   1/1     Running    0                27d rook-ceph-exporter-11cef195e99cf42211bc5b21918ec486-6f8c85tz9t6   1/1     Running    0                79s rook-ceph-exporter-3b80b15101138b8d06c7b668b8b5c01c-7b9774qfg6l   1/1     Running    0                54d rook-ceph-exporter-5636ff5c104ab5ecddd6b734c6b8046b-78f787rwkxn   1/1     Running    0                27d rook-ceph-mds-ocs-storagecluster-cephfilesystem-a-8695d88bhth6r   2/2     Running    8 (27d ago)      27d rook-ceph-mds-ocs-storagecluster-cephfilesystem-b-6656db49lpq92   2/2     Running    9 (35d ago)      54d rook-ceph-mgr-a-6d6d64f779-xgvxm                                  4/4     Running    0                27d rook-ceph-mgr-b-56df549944-j2qs5                                  4/4     Running    0                54d rook-ceph-mon-a-66bcddd94-cxmzf                                   2/2     Running    0                7m16s rook-ceph-mon-b-5cbfb78f84-qqfkc                                  2/2     Running    2                56d rook-ceph-mon-e-758d6b5f75-6d55g                                  2/2     Running    0                56d rook-ceph-operator-7bc4cf5ccd-ld849                               1/1     Running    0                35d rook-ceph-osd-1-f7d7dbd8c-r7wbm                                   2/2     Running    0                56d rook-ceph-osd-2-65899ccf74-tjfdf                                  2/2     Running    0                35d rook-ceph-osd-3-7874d489b6-vtjpn                                  2/2     Running    0                35d rook-ceph-osd-4-57f684b7b8-xhkkd                                  2/2     Running    0                35d rook-ceph-osd-prepare-5478b844168c608648cd4d133a6cc62c-gdk4r      0/1     Init:0/2   0                3h52m rook-ceph-osd-prepare-8f2183643e09a01a95e82d7252249a3d-sbcxj      0/1     Init:0/2   0                3h52m rook-ceph-rgw-ocs-storagecluster-cephobjectstore-a-9969788b9jkl   2/2     Running    0                27d rook-ceph-tools-6f854c4bfc-ncc8b                                  1/1     Running    1                56d ux-backend-server-5c758744c4-cq599                                2/2     Running    0                57d [root@dom16hub101-infra-manager ~]# oc get pods -n openshift-local-storage -o wide NAME                                      READY   STATUS    RESTARTS        AGE    IP             NODE                                               NOMINATED NODE   READINESS GATES diskmaker-discovery-6mcrl                 2/2     Running   4               71d    172.20.0.163   ncpvblvhub-hubmaster-103.ncpvblvhub.t-mobile.lab     diskmaker-discovery-9htrb                 2/2     Running   6               71d    172.20.2.141   ncpvblvhub-hubmaster-101.ncpvblvhub.t-mobile.lab     diskmaker-discovery-b6bxh                 2/2     Running   0               114s   172.21.0.33    ncpvblvhub-hubmaster-102.ncpvblvhub.t-mobile.lab     diskmaker-manager-8kpkt                   2/2     Running   0               114s   172.21.0.31    ncpvblvhub-hubmaster-102.ncpvblvhub.t-mobile.lab     diskmaker-manager-fbzkp                   2/2     Running   4               71d    172.20.0.164   ncpvblvhub-hubmaster-103.ncpvblvhub.t-mobile.lab     diskmaker-manager-h6r2t                   2/2     Running   6               71d    172.20.2.142   ncpvblvhub-hubmaster-101.ncpvblvhub.t-mobile.lab     local-storage-operator-6d968c9989-w4vp4   1/1     Running   2 (3h47m ago)   57d    172.23.0.66    ncpvblvhub-hubworker-101.ncpvblvhub.t-mobile.lab     [root@dom16hub101-infra-manager ~]# oc get pv NAME                                       CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS      CLAIM                                                                                STORAGECLASS          VOLUMEATTRIBUTESCLASS   REASON   AGE local-pv-4b005370                          3576Gi     RWO            Delete           Bound       openshift-storage/ocs-deviceset-localblockstorage-2-data-0bnvhx                      localblockstorage                               71d local-pv-4fcd3797                          3576Gi     RWO            Delete           Bound       openshift-storage/ocs-deviceset-localblockstorage-2-data-1d9cz5                      localblockstorage                               57s local-pv-6283226f                          3576Gi     RWO            Delete           Bound       openshift-storage/ocs-deviceset-localblockstorage-1-data-059b89                      localblockstorage                               71d local-pv-70624cbd                          3576Gi     RWO            Delete           Bound       openshift-storage/ocs-deviceset-localblockstorage-1-data-1ms47g                      localblockstorage                               71d local-pv-85829823                          3576Gi     RWO            Delete           Bound       openshift-storage/ocs-deviceset-localblockstorage-0-data-1gggzx                      localblockstorage                               71d local-pv-cb7421c8                          3576Gi     RWO            Delete           Bound       openshift-storage/ocs-deviceset-localblockstorage-0-data-2cwnr8                      localblockstorage                               57s ls-prometheus-data1                        446Gi      RWO            Delete           Available                                                                                        lsc-prometheus-data                             71d ls-prometheus-data2                        446Gi      RWO            Delete           Bound       openshift-monitoring/prometheus-k8s-db-prometheus-k8s-1                              lsc-prometheus-data                             71d ls-prometheus-data3                        446Gi      RWO            Delete           Bound       openshift-monitoring/prometheus-k8s-db-prometheus-k8s-0                              lsc-prometheus-data                             71d pvc-04609d05-cc59-4d93-98a3-e1b7835e6f68   10Gi       RWO            Delete           Bound       openshift-logging/storage-logging-loki-ingester-0                                    csi-cephrbd                                     71d pvc-053138bc-089c-4053-9690-beb7a742d312   20Gi       RWO            Delete           Bound       ncd-db/datadir-ncd-postgresql-postgresql-ha-keeper-0                                 csi-cephrbd                                     70d pvc-081490ba-b16a-4f10-a990-86fe325be236   2Gi        RWO            Delete           Bound       ncd-db/cburtmp-ncd-postgresql-postgresql-ha-keeper-0                                 csi-cephrbd                                     70d pvc-0841d589-e082-472c-95f5-d424a839a628   1Gi        RWO            Delete           Bound       open-cluster-management-observability/alertmanager-db-observability-alertmanager-0   csi-cephrbd                                     71d pvc-0f0aaf26-2a37-4aac-b9b1-28bed8731e30   10Gi       RWO            Delete           Bound       open-cluster-management-observability/data-observability-thanos-store-shard-0-0      csi-cephrbd                                     71d pvc-1f2fb61d-b136-45ec-86c0-cbc41939a391   20Gi       RWO            Delete           Bound       ncd-db/datadir-ncd-redis-crdb-redisio-server-1                                       csi-cephrbd                                     70d pvc-24c8b083-261e-4f25-be68-f278932dc261   10Gi       RWO            Delete           Bound       open-cluster-management-observability/data-observability-thanos-store-shard-2-0      csi-cephrbd                                     71d pvc-25952614-0e55-414b-bedf-51fa75017e64   100Gi      RWO            Delete           Bound       open-cluster-management-observability/data-observability-thanos-receive-default-2    csi-cephrbd                                     71d pvc-37770663-1123-4cbe-b93b-27e644176750   1Gi        RWO            Delete           Bound       open-cluster-management-observability/alertmanager-db-observability-alertmanager-2   csi-cephrbd                                     71d pvc-388f244b-76ca-4ee8-ad26-b7a0c886f269   20Gi       RWO            Delete           Bound       ncd-db/datadir-ncd-redis-crdb-redisio-server-2                                       csi-cephrbd                                     70d pvc-427f629e-0544-43a9-9755-a78ea9908f99   50Gi       RWO            Delete           Bound       quay-registry/quay-registry-quay-postgres-13                                         csi-cephrbd                                     71d pvc-437ef557-3de5-49b3-bdbf-17d874b457d9   100Gi      RWO            Delete           Bound       open-cluster-management-observability/data-observability-thanos-receive-default-1    csi-cephrbd                                     71d pvc-466c7e2d-6f9a-4fd3-9a72-ff67e652008d   1Gi        RWO            Delete           Bound       open-cluster-management-observability/alertmanager-db-observability-alertmanager-1   csi-cephrbd                                     71d pvc-4b4fb857-a1d4-4564-b098-4391e04889c5   932Gi      RWO            Delete           Bound       multicluster-engine/assisted-service                                                 csi-cephrbd                                     71d pvc-4febe32e-4cf9-4395-ac6b-1b669f4c9eb0   1Gi        RWO            Delete           Bound       open-cluster-management-observability/data-observability-thanos-rule-1               csi-cephrbd                                     71d pvc-5a219089-f77d-497e-a555-edf545993144   100Gi      RWO            Delete           Bound       open-cluster-management-observability/data-observability-thanos-receive-default-0    csi-cephrbd                                     71d pvc-5d4abd4e-ad52-49f3-977e-33c6ff1fa6d4   20Gi       RWO            Delete           Bound       ncd-db/datadir-ncd-postgresql-postgresql-ha-keeper-2                                 csi-cephrbd                                     70d pvc-69c94a85-f007-41ff-b609-0d045147a334   50Gi       RWO            Delete           Bound       quay-registry/quay-registry-clair-postgres-13                                        csi-cephrbd                                     71d pvc-76431f6e-7bf0-4ba0-8b7e-8ae3a704e147   500Gi      RWO            Delete           Bound       ncd-git/repo-data-ncd-git-gitaly-default-0                                           csi-cephrbd                                     70d pvc-7e890510-2d05-4041-aac7-69ece3267aaa   150Gi      RWO            Delete           Bound       openshift-logging/wal-logging-loki-ingester-0                                        csi-cephrbd                                     71d pvc-8c3dea72-cd8b-478e-a5b5-ed728c845026   2Gi        RWO            Delete           Bound       ncd-db/cburtmp-ncd-postgresql-postgresql-ha-keeper-1                                 csi-cephrbd                                     70d pvc-96f1c005-b977-4635-a842-48117c0cc9e4   8Gi        RWO            Delete           Bound       ncd-cbur/ncd-cbur-cbur-repo                                                          csi-cephrbd                                     70d pvc-97a611e9-465f-4f6c-9cee-41d1b054cdd5   150Gi      RWO            Delete           Bound       openshift-logging/wal-logging-loki-ingester-1                                        csi-cephrbd                                     71d pvc-997036ec-ba41-47b1-b319-3dcdc9c9a0ad   1Gi        RWO            Delete           Bound       open-cluster-management-observability/data-observability-thanos-rule-0               csi-cephrbd                                     71d pvc-a1ebf43e-5700-4ec2-be4d-98ca53964047   2Gi        RWO            Delete           Bound       ncd-db/cburtmp-ncd-postgresql-postgresql-ha-keeper-2                                 csi-cephrbd                                     70d pvc-a309bb36-c4e6-4e4f-a44e-c673c732b64a   500Gi      RWO            Delete           Bound       ncd-git/repo-data-ncd-git-gitaly-default-1                                           csi-cephrbd                                     70d pvc-a8d078a4-52bf-47cd-8271-16b4e63a842d   94Gi       RWO            Delete           Bound       multicluster-engine/postgres                                                         csi-cephrbd                                     71d pvc-ab7662bb-943f-4b3b-a642-27f912b0d789   50Gi       RWO            Delete           Bound       openshift-logging/storage-logging-loki-index-gateway-1                               csi-cephrbd                                     71d pvc-abc5bc4f-6015-4753-9b54-6af1b522efbd   20Gi       RWO            Delete           Bound       ncd-db/datadir-ncd-postgresql-postgresql-ha-keeper-1                                 csi-cephrbd                                     70d pvc-bb9c7515-53cc-4f3d-8611-5cf0bd725e9f   100Gi      RWO            Delete           Bound       open-cluster-management-observability/data-observability-thanos-compact-0            csi-cephrbd                                     71d pvc-be0efedb-ed1f-4a15-8436-db39c0fb0443   10Gi       RWO            Delete           Bound       openshift-logging/storage-logging-loki-ingester-1                                    csi-cephrbd                                     71d pvc-c7054ac4-13ce-4de9-92a9-f318bdcf9a95   2800Gi     RWO            Delete           Bound       openshift-storage/noobaa-pv-backing-store-noobaa-pvc-1791bf5b                        csi-cephrbd                                     71d pvc-ca584969-1de4-4d2e-8950-c38b550d74c6   10Gi       RWO            Delete           Bound       open-cluster-management-observability/data-observability-thanos-store-shard-1-0      csi-cephrbd                                     71d pvc-cf944a21-a56a-41b7-81f5-c92e2b1327be   50Gi       RWO            Delete           Bound       openshift-logging/storage-logging-loki-index-gateway-0                               csi-cephrbd                                     71d pvc-d42a0aed-5fe6-4571-bc42-839edd11ed79   500Gi      RWO            Delete           Bound       ncd-git/repo-data-ncd-git-gitaly-default-2                                           csi-cephrbd                                     70d pvc-ddb20a57-e69a-49f5-9b24-fcac4170487f   2Gi        RWO            Delete           Bound       ncd-cbur/ncd-cbur-cbur-backup                                                        csi-cephrbd                                     70d pvc-de330f87-f409-490b-9f3d-a58a77a76c5a   20Gi       RWO            Delete           Bound       ncd-db/datadir-ncd-redis-crdb-redisio-server-0                                       csi-cephrbd                                     70d pvc-e1cbaed9-b92a-4cc9-81e2-9d753b820b8d   10Gi       RWO            Delete           Bound       openshift-logging/storage-logging-loki-compactor-0                                   csi-cephrbd                                     71d pvc-e1f65bd3-2729-4448-9f32-5b2ab93ea348   50Gi       RWO            Delete           Bound       openshift-storage/db-noobaa-db-pg-0                                                  csi-cephrbd                                     71d pvc-eecba7a6-c973-4dca-8e20-9d67c0100dea   466Gi      RWO            Delete           Bound       multicluster-engine/image-service-data-assisted-image-service-0                      csi-cephrbd                                     71d pvc-ef8ad868-7f6a-439e-9815-d7e8c435fd98   1Gi        RWO            Delete           Bound       open-cluster-management-observability/data-observability-thanos-rule-2               csi-cephrbd                                     71d [root@dom16hub101-infra-manager ~]# oc get pods -n openshift-local-storage -o wide |grep local-pv [root@dom16hub101-infra-manager ~]# oc get pv |grep local-pv local-pv-4b005370                          3576Gi     RWO            Delete           Bound       openshift-storage/ocs-deviceset-localblockstorage-2-data-0bnvhx                      localblockstorage                               71d local-pv-4fcd3797                          3576Gi     RWO            Delete           Bound       openshift-storage/ocs-deviceset-localblockstorage-2-data-1d9cz5                      localblockstorage                               88s local-pv-6283226f                          3576Gi     RWO            Delete           Bound       openshift-storage/ocs-deviceset-localblockstorage-1-data-059b89                      localblockstorage                               71d local-pv-70624cbd                          3576Gi     RWO            Delete           Bound       openshift-storage/ocs-deviceset-localblockstorage-1-data-1ms47g                      localblockstorage                               71d local-pv-85829823                          3576Gi     RWO            Delete           Bound       openshift-storage/ocs-deviceset-localblockstorage-0-data-1gggzx                      localblockstorage                               71d local-pv-cb7421c8                          3576Gi     RWO            Delete           Bound       openshift-storage/ocs-deviceset-localblockstorage-0-data-2cwnr8                      localblockstorage                               88s [root@dom16hub101-infra-manager ~]# oc get pods -n openshift-local-storage -o wide NAME                                      READY   STATUS    RESTARTS        AGE     IP             NODE                                               NOMINATED NODE   READINESS GATES diskmaker-discovery-6mcrl                 2/2     Running   4               71d     172.20.0.163   ncpvblvhub-hubmaster-103.ncpvblvhub.t-mobile.lab     diskmaker-discovery-9htrb                 2/2     Running   6               71d     172.20.2.141   ncpvblvhub-hubmaster-101.ncpvblvhub.t-mobile.lab     diskmaker-discovery-b6bxh                 2/2     Running   0               2m48s   172.21.0.33    ncpvblvhub-hubmaster-102.ncpvblvhub.t-mobile.lab     diskmaker-manager-8kpkt                   2/2     Running   0               2m48s   172.21.0.31    ncpvblvhub-hubmaster-102.ncpvblvhub.t-mobile.lab     diskmaker-manager-fbzkp                   2/2     Running   4               71d     172.20.0.164   ncpvblvhub-hubmaster-103.ncpvblvhub.t-mobile.lab     diskmaker-manager-h6r2t                   2/2     Running   6               71d     172.20.2.142   ncpvblvhub-hubmaster-101.ncpvblvhub.t-mobile.lab     local-storage-operator-6d968c9989-w4vp4   1/1     Running   2 (3h48m ago)   57d     172.23.0.66    ncpvblvhub-hubworker-101.ncpvblvhub.t-mobile.lab     [root@dom16hub101-infra-manager ~]# oc get pods -n openshift-storage -o wide NAME                                                              READY   STATUS      RESTARTS         AGE     IP               NODE                                               NOMINATED NODE   READINESS GATES csi-addons-controller-manager-7fdf8f85d5-tbq47                    2/2     Running     7 (3h47m ago)    57d     172.23.0.47      ncpvblvhub-hubworker-101.ncpvblvhub.t-mobile.lab     csi-cephfsplugin-4pnm7                                            2/2     Running     7 (15d ago)      71d     10.145.151.100   ncpvblvhub-hubworker-102.ncpvblvhub.t-mobile.lab     csi-cephfsplugin-d6bwn                                            2/2     Running     0                17m     10.145.151.93    ncpvblvhub-hubmaster-102.ncpvblvhub.t-mobile.lab     csi-cephfsplugin-fl7m9                                            2/2     Running     4                71d     10.145.151.99    ncpvblvhub-hubworker-101.ncpvblvhub.t-mobile.lab     csi-cephfsplugin-provisioner-5996485946-58m2k                     6/6     Running     2 (3h48m ago)    54d     172.20.1.168     ncpvblvhub-hubmaster-103.ncpvblvhub.t-mobile.lab     csi-cephfsplugin-provisioner-5996485946-n66cq                     6/6     Running     10 (3h45m ago)   57d     172.23.0.26      ncpvblvhub-hubworker-101.ncpvblvhub.t-mobile.lab     csi-cephfsplugin-vrdx8                                            2/2     Running     4                71d     10.145.151.94    ncpvblvhub-hubmaster-103.ncpvblvhub.t-mobile.lab     csi-cephfsplugin-wvvg9                                            2/2     Running     6                71d     10.145.151.92    ncpvblvhub-hubmaster-101.ncpvblvhub.t-mobile.lab     csi-rbdplugin-bxn4s                                               3/3     Running     10 (15d ago)     71d     10.145.151.100   ncpvblvhub-hubworker-102.ncpvblvhub.t-mobile.lab     csi-rbdplugin-h8fvq                                               3/3     Running     0                17m     10.145.151.93    ncpvblvhub-hubmaster-102.ncpvblvhub.t-mobile.lab     csi-rbdplugin-l4xrk                                               3/3     Running     6                71d     10.145.151.99    ncpvblvhub-hubworker-101.ncpvblvhub.t-mobile.lab     csi-rbdplugin-provisioner-646d95bdd9-bjp5j                        6/6     Running     1 (3h48m ago)    4h34m   172.22.0.79      ncpvblvhub-hubworker-102.ncpvblvhub.t-mobile.lab     csi-rbdplugin-provisioner-646d95bdd9-zdjs8                        6/6     Running     10 (3h48m ago)   57d     172.23.0.15      ncpvblvhub-hubworker-101.ncpvblvhub.t-mobile.lab     csi-rbdplugin-rzbl2                                               3/3     Running     6                71d     10.145.151.94    ncpvblvhub-hubmaster-103.ncpvblvhub.t-mobile.lab     csi-rbdplugin-xlt4f                                               3/3     Running     9                71d     10.145.151.92    ncpvblvhub-hubmaster-101.ncpvblvhub.t-mobile.lab     noobaa-core-0                                                     2/2     Running     0                27d     172.20.3.62      ncpvblvhub-hubmaster-101.ncpvblvhub.t-mobile.lab     noobaa-db-pg-0                                                    1/1     Running     0                54d     172.20.1.187     ncpvblvhub-hubmaster-103.ncpvblvhub.t-mobile.lab     noobaa-endpoint-769ccbd9f4-zs9c5                                  1/1     Running     0                27d     172.20.0.94      ncpvblvhub-hubmaster-103.ncpvblvhub.t-mobile.lab     noobaa-operator-6b567c8c7f-6vtgr                                  1/1     Running     6 (3h47m ago)    27d     172.20.3.25      ncpvblvhub-hubmaster-101.ncpvblvhub.t-mobile.lab     noobaa-pv-backing-store-noobaa-pod-1791bf5b                       1/1     Running     0                27d     172.20.3.64      ncpvblvhub-hubmaster-101.ncpvblvhub.t-mobile.lab     ocs-metrics-exporter-85c8468d88-bxl6x                             1/1     Running     0                57d     172.23.0.75      ncpvblvhub-hubworker-101.ncpvblvhub.t-mobile.lab     ocs-operator-69667b5b9f-nb57q                                     1/1     Running     8 (3h44m ago)    35d     172.20.2.22      ncpvblvhub-hubmaster-101.ncpvblvhub.t-mobile.lab     odf-console-56757bc6ff-g429j                                      1/1     Running     0                56d     172.23.0.112     ncpvblvhub-hubworker-101.ncpvblvhub.t-mobile.lab     odf-operator-controller-manager-6b68579866-fsdd7                  2/2     Running     2 (3h50m ago)    35d     172.20.0.6       ncpvblvhub-hubmaster-103.ncpvblvhub.t-mobile.lab     rook-ceph-crashcollector-11cef195e99cf42211bc5b21918ec486-xwt5h   1/1     Running     0                2m57s   172.21.0.34      ncpvblvhub-hubmaster-102.ncpvblvhub.t-mobile.lab     rook-ceph-crashcollector-3b80b15101138b8d06c7b668b8b5c01c-cn7rn   1/1     Running     0                54d     172.20.1.127     ncpvblvhub-hubmaster-103.ncpvblvhub.t-mobile.lab     rook-ceph-crashcollector-5636ff5c104ab5ecddd6b734c6b8046b-4r9fw   1/1     Running     0                27d     172.20.3.51      ncpvblvhub-hubmaster-101.ncpvblvhub.t-mobile.lab     rook-ceph-exporter-11cef195e99cf42211bc5b21918ec486-6f8c85tz9t6   1/1     Running     0                2m57s   172.21.0.35      ncpvblvhub-hubmaster-102.ncpvblvhub.t-mobile.lab     rook-ceph-exporter-3b80b15101138b8d06c7b668b8b5c01c-7b9774qfg6l   1/1     Running     0                54d     172.20.1.134     ncpvblvhub-hubmaster-103.ncpvblvhub.t-mobile.lab     rook-ceph-exporter-5636ff5c104ab5ecddd6b734c6b8046b-78f787rwkxn   1/1     Running     0                27d     172.20.3.58      ncpvblvhub-hubmaster-101.ncpvblvhub.t-mobile.lab     rook-ceph-mds-ocs-storagecluster-cephfilesystem-a-8695d88bhth6r   2/2     Running     8 (27d ago)      27d     172.20.3.50      ncpvblvhub-hubmaster-101.ncpvblvhub.t-mobile.lab     rook-ceph-mds-ocs-storagecluster-cephfilesystem-b-6656db49lpq92   2/2     Running     9 (35d ago)      54d     172.20.1.125     ncpvblvhub-hubmaster-103.ncpvblvhub.t-mobile.lab     rook-ceph-mgr-a-6d6d64f779-xgvxm                                  4/4     Running     0                27d     172.20.3.54      ncpvblvhub-hubmaster-101.ncpvblvhub.t-mobile.lab     rook-ceph-mgr-b-56df549944-j2qs5                                  4/4     Running     0                54d     172.20.1.171     ncpvblvhub-hubmaster-103.ncpvblvhub.t-mobile.lab     rook-ceph-mon-a-66bcddd94-cxmzf                                   2/2     Running     0                8m54s   172.21.0.32      ncpvblvhub-hubmaster-102.ncpvblvhub.t-mobile.lab     rook-ceph-mon-b-5cbfb78f84-qqfkc                                  2/2     Running     2                56d     172.20.2.8       ncpvblvhub-hubmaster-101.ncpvblvhub.t-mobile.lab     rook-ceph-mon-e-758d6b5f75-6d55g                                  2/2     Running     0                56d     172.20.0.19      ncpvblvhub-hubmaster-103.ncpvblvhub.t-mobile.lab     rook-ceph-operator-7bc4cf5ccd-ld849                               1/1     Running     0                35d     172.20.2.19      ncpvblvhub-hubmaster-101.ncpvblvhub.t-mobile.lab     rook-ceph-osd-0-5c99849c7c-v57qx                                  2/2     Running     0                83s     172.21.0.38      ncpvblvhub-hubmaster-102.ncpvblvhub.t-mobile.lab     rook-ceph-osd-1-f7d7dbd8c-r7wbm                                   2/2     Running     0                56d     172.20.0.27      ncpvblvhub-hubmaster-103.ncpvblvhub.t-mobile.lab     rook-ceph-osd-2-65899ccf74-tjfdf                                  2/2     Running     0                35d     172.20.2.52      ncpvblvhub-hubmaster-101.ncpvblvhub.t-mobile.lab     rook-ceph-osd-3-7874d489b6-vtjpn                                  2/2     Running     0                35d     172.20.2.53      ncpvblvhub-hubmaster-101.ncpvblvhub.t-mobile.lab     rook-ceph-osd-4-57f684b7b8-xhkkd                                  2/2     Running     0                35d     172.20.0.7       ncpvblvhub-hubmaster-103.ncpvblvhub.t-mobile.lab     rook-ceph-osd-5-785b576665-95h7b                                  2/2     Running     0                83s     172.21.0.39      ncpvblvhub-hubmaster-102.ncpvblvhub.t-mobile.lab     rook-ceph-osd-prepare-5478b844168c608648cd4d133a6cc62c-gdk4r      0/1     Completed   0                3h54m   172.21.0.37      ncpvblvhub-hubmaster-102.ncpvblvhub.t-mobile.lab     rook-ceph-osd-prepare-8f2183643e09a01a95e82d7252249a3d-sbcxj      0/1     Completed   0                3h54m   172.21.0.36      ncpvblvhub-hubmaster-102.ncpvblvhub.t-mobile.lab     rook-ceph-rgw-ocs-storagecluster-cephobjectstore-a-9969788b9jkl   2/2     Running     0                27d     172.20.0.93      ncpvblvhub-hubmaster-103.ncpvblvhub.t-mobile.lab     rook-ceph-tools-6f854c4bfc-ncc8b                                  1/1     Running     1                56d     172.20.2.60      ncpvblvhub-hubmaster-101.ncpvblvhub.t-mobile.lab     ux-backend-server-5c758744c4-cq599                                2/2     Running     0                57d     172.23.0.77      ncpvblvhub-hubworker-101.ncpvblvhub.t-mobile.lab     [root@dom16hub101-infra-manager ~]# oc get pods -n openshift-storage -o wide |grep ncpvblvhub-hubmaster-102.ncpvblvhub.t-mobile.lab csi-cephfsplugin-d6bwn                                            2/2     Running     0                17m     10.145.151.93    ncpvblvhub-hubmaster-102.ncpvblvhub.t-mobile.lab     csi-rbdplugin-h8fvq                                               3/3     Running     0                17m     10.145.151.93    ncpvblvhub-hubmaster-102.ncpvblvhub.t-mobile.lab     rook-ceph-crashcollector-11cef195e99cf42211bc5b21918ec486-xwt5h   1/1     Running     0                3m21s   172.21.0.34      ncpvblvhub-hubmaster-102.ncpvblvhub.t-mobile.lab     rook-ceph-exporter-11cef195e99cf42211bc5b21918ec486-6f8c85tz9t6   1/1     Running     0                3m21s   172.21.0.35      ncpvblvhub-hubmaster-102.ncpvblvhub.t-mobile.lab     rook-ceph-mon-a-66bcddd94-cxmzf                                   2/2     Running     0                9m18s   172.21.0.32      ncpvblvhub-hubmaster-102.ncpvblvhub.t-mobile.lab     rook-ceph-osd-0-5c99849c7c-v57qx                                  2/2     Running     0                107s    172.21.0.38      ncpvblvhub-hubmaster-102.ncpvblvhub.t-mobile.lab     rook-ceph-osd-5-785b576665-95h7b                                  2/2     Running     0                107s    172.21.0.39      ncpvblvhub-hubmaster-102.ncpvblvhub.t-mobile.lab     rook-ceph-osd-prepare-5478b844168c608648cd4d133a6cc62c-gdk4r      0/1     Completed   0                3h54m   172.21.0.37      ncpvblvhub-hubmaster-102.ncpvblvhub.t-mobile.lab     rook-ceph-osd-prepare-8f2183643e09a01a95e82d7252249a3d-sbcxj      0/1     Completed   0                3h54m   172.21.0.36      ncpvblvhub-hubmaster-102.ncpvblvhub.t-mobile.lab     [root@dom16hub101-infra-manager ~]# oc exec -it $(oc get pod -n openshift-storage -l app=rook-ceph-operator -o name) -n openshift-storage -- ceph status -c /var/lib/rook/openshift-storage/openshift-storage.config   cluster:     id:     27b507a3-b496-4549-9271-a3bc0705dd13     health: HEALTH_WARN             Degraded data redundancy: 87816/433023 objects degraded (20.280%), 68 pgs degraded, 74 pgs undersized <p>services:     mon: 3 daemons, quorum a,b,e (age 3m)     mgr: b(active, since 3h), standbys: a     mds: 1/1 daemons up, 1 hot standby     osd: 6 osds: 6 up (since 112s), 6 in (since 2m); 109 remapped pgs     rgw: 1 daemon active (1 hosts, 1 zones)</p> <p>data:     volumes: 1/1 healthy     pools:   12 pools, 201 pgs     objects: 144.34k objects, 541 GiB     usage:   1.2 TiB used, 20 TiB / 21 TiB avail     pgs:     0.498% pgs not active              87816/433023 objects degraded (20.280%)              55928/433023 objects misplaced (12.916%)              91 active+clean              68 active+undersized+degraded+remapped+backfill_wait              35 active+remapped+backfill_wait              6  active+undersized+remapped+backfill_wait              1  peering</p> <p>io:     client:   6.1 MiB/s rd, 475 KiB/s wr, 7 op/s rd, 39 op/s wr     recovery: 386 MiB/s, 0 keys/s, 101 objects/s</p> <p>[root@dom16hub101-infra-manager ~]# oc exec -it $(oc get pod -n openshift-storage -l app=rook-ceph-operator -o name) -n openshift-storage -- ceph status -c /var/lib/rook/openshift-storage/openshift-storage.config   cluster:     id:     27b507a3-b496-4549-9271-a3bc0705dd13     health: HEALTH_WARN             Degraded data redundancy: 86292/433023 objects degraded (19.928%), 68 pgs degraded, 74 pgs undersized</p> <p>services:     mon: 3 daemons, quorum a,b,e (age 3m)     mgr: b(active, since 3h), standbys: a     mds: 1/1 daemons up, 1 hot standby     osd: 6 osds: 6 up (since 2m), 6 in (since 2m); 109 remapped pgs     rgw: 1 daemon active (1 hosts, 1 zones)</p> <p>data:     volumes: 1/1 healthy     pools:   12 pools, 201 pgs     objects: 144.34k objects, 541 GiB     usage:   1.3 TiB used, 20 TiB / 21 TiB avail     pgs:     86292/433023 objects degraded (19.928%)              55928/433023 objects misplaced (12.916%)              92 active+clean              67 active+undersized+degraded+remapped+backfill_wait              35 active+remapped+backfill_wait              6  active+undersized+remapped+backfill_wait              1  active+undersized+degraded+remapped+backfilling</p> <p>io:     client:   1.4 KiB/s rd, 614 KiB/s wr, 2 op/s rd, 47 op/s wr     recovery: 410 MiB/s, 0 keys/s, 107 objects/s</p> <p>[root@dom16hub101-infra-manager ~]# oc get no NAME                                               STATUS   ROLES                                 AGE   VERSION ncpvblvhub-hubmaster-101.ncpvblvhub.t-mobile.lab   Ready    control-plane,master,monitor,worker   74d   v1.29.10+67d3387 ncpvblvhub-hubmaster-102.ncpvblvhub.t-mobile.lab   Ready    control-plane,master,worker           19m   v1.29.10+67d3387 ncpvblvhub-hubmaster-103.ncpvblvhub.t-mobile.lab   Ready    control-plane,master,monitor,worker   74d   v1.29.10+67d3387 ncpvblvhub-hubworker-101.ncpvblvhub.t-mobile.lab   Ready    gateway,worker                        74d   v1.29.10+67d3387 ncpvblvhub-hubworker-102.ncpvblvhub.t-mobile.lab   Ready    gateway,worker                        74d   v1.29.10+67d3387 [root@dom16hub101-infra-manager ~]# oc edit no ncpvblvhub-hubmaster-102.ncpvblvhub.t-mobile.lab Edit cancelled, no changes made. [root@dom16hub101-infra-manager ~]# oc get no --show-labels ncpvblvhub-hubmaster-101.ncpvblvhub.t-mobile.lab NAME                                               STATUS   ROLES                                 AGE   VERSION            LABELS ncpvblvhub-hubmaster-101.ncpvblvhub.t-mobile.lab   Ready    control-plane,master,monitor,worker   74d   v1.29.10+67d3387   beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,cluster.ocs.openshift.io/openshift-storage=,kubernetes.io/arch=amd64,kubernetes.io/hostname=ncpvblvhub-hubmaster-101.ncpvblvhub.t-mobile.lab,kubernetes.io/os=linux,node-role.kubernetes.io/control-plane=,node-role.kubernetes.io/master=,node-role.kubernetes.io/monitor=,node-role.kubernetes.io/worker=,node.openshift.io/os_id=rhcos [root@dom16hub101-infra-manager ~]# oc label node ncpvblvhub-hubmaster-102.ncpvblvhub.t-mobile.lab node-role.kubernetes.io/monitor= node/ncpvblvhub-hubmaster-102.ncpvblvhub.t-mobile.lab labeled [root@dom16hub101-infra-manager ~]# oc get no NAME                                               STATUS   ROLES                                 AGE   VERSION ncpvblvhub-hubmaster-101.ncpvblvhub.t-mobile.lab   Ready    control-plane,master,monitor,worker   74d   v1.29.10+67d3387 ncpvblvhub-hubmaster-102.ncpvblvhub.t-mobile.lab   Ready    control-plane,master,monitor,worker   20m   v1.29.10+67d3387 ncpvblvhub-hubmaster-103.ncpvblvhub.t-mobile.lab   Ready    control-plane,master,monitor,worker   74d   v1.29.10+67d3387 ncpvblvhub-hubworker-101.ncpvblvhub.t-mobile.lab   Ready    gateway,worker                        74d   v1.29.10+67d3387 ncpvblvhub-hubworker-102.ncpvblvhub.t-mobile.lab   Ready    gateway,worker                        74d   v1.29.10+67d3387 [root@dom16hub101-infra-manager ~]# oc get co NAME                                       VERSION   AVAILABLE   PROGRESSING   DEGRADED   SINCE   MESSAGE authentication                             4.16.24   True        False         False      4h2m baremetal                                  4.16.24   True        False         False      74d cloud-controller-manager                   4.16.24   True        False         False      74d cloud-credential                           4.16.24   True        False         False      74d cluster-autoscaler                         4.16.24   True        False         False      74d config-operator                            4.16.24   True        False         False      74d console                                    4.16.24   True        False         False      74d control-plane-machine-set                  4.16.24   True        False         False      74d csi-snapshot-controller                    4.16.24   True        False         False      74d dns                                        4.16.24   True        False         False      74d etcd                                       4.16.24   True        False         False      74d image-registry                             4.16.24   True        False         False      74d ingress                                    4.16.24   True        False         False      74d insights                                   4.16.24   True        False         False      74d kube-apiserver                             4.16.24   True        False         False      74d kube-controller-manager                    4.16.24   True        False         False      74d kube-scheduler                             4.16.24   True        False         False      74d kube-storage-version-migrator              4.16.24   True        False         False      13d machine-api                                4.16.24   True        False         False      74d machine-approver                           4.16.24   True        False         False      74d machine-config                             4.16.24   True        False         True       74d     Failed to resync 4.16.24 because: error during syncRequiredMachineConfigPools: [context deadline exceeded, failed to update clusteroperator: [client rate limiter Wait returned an error: context deadline exceeded, error MachineConfigPool worker is not ready, retrying. Status: (pool degraded: true total: 2, ready 1, updated: 1, unavailable: 1)]] marketplace                                4.16.24   True        False         False      74d monitoring                                 4.16.24   Unknown     True          Unknown    50m     Rolling out the stack. network                                    4.16.24   True        False         False      74d node-tuning                                4.16.24   True        False         False      20m openshift-apiserver                        4.16.24   True        False         False      4h31m openshift-controller-manager               4.16.24   True        False         False      74d openshift-samples                          4.16.24   True        False         False      74d operator-lifecycle-manager                 4.16.24   True        False         False      74d operator-lifecycle-manager-catalog         4.16.24   True        False         False      74d operator-lifecycle-manager-packageserver   4.16.24   True        False         False      74d service-ca                                 4.16.24   True        False         False      74d storage                                    4.16.24   True        False         False      74d [root@dom16hub101-infra-manager ~]# oc get co NAME                                       VERSION   AVAILABLE   PROGRESSING   DEGRADED   SINCE   MESSAGE authentication                             4.16.24   True        False         False      4h3m baremetal                                  4.16.24   True        False         False      74d cloud-controller-manager                   4.16.24   True        False         False      74d cloud-credential                           4.16.24   True        False         False      74d cluster-autoscaler                         4.16.24   True        False         False      74d config-operator                            4.16.24   True        False         False      74d console                                    4.16.24   True        False         False      74d control-plane-machine-set                  4.16.24   True        False         False      74d csi-snapshot-controller                    4.16.24   True        False         False      74d dns                                        4.16.24   True        False         False      74d etcd                                       4.16.24   True        False         False      74d image-registry                             4.16.24   True        False         False      74d ingress                                    4.16.24   True        False         False      74d insights                                   4.16.24   True        False         False      74d kube-apiserver                             4.16.24   True        False         False      74d kube-controller-manager                    4.16.24   True        False         False      74d kube-scheduler                             4.16.24   True        False         False      74d kube-storage-version-migrator              4.16.24   True        False         False      13d machine-api                                4.16.24   True        False         False      74d machine-approver                           4.16.24   True        False         False      74d machine-config                             4.16.24   True        False         True       74d     Failed to resync 4.16.24 because: error during syncRequiredMachineConfigPools: [context deadline exceeded, failed to update clusteroperator: [client rate limiter Wait returned an error: context deadline exceeded, error MachineConfigPool worker is not ready, retrying. Status: (pool degraded: true total: 2, ready 1, updated: 1, unavailable: 1)]] marketplace                                4.16.24   True        False         False      74d monitoring                                 4.16.24   Unknown     True          Unknown    51m     Rolling out the stack. network                                    4.16.24   True        False         False      74d node-tuning                                4.16.24   True        False         False      21m openshift-apiserver                        4.16.24   True        False         False      4h32m openshift-controller-manager               4.16.24   True        False         False      74d openshift-samples                          4.16.24   True        False         False      74d operator-lifecycle-manager                 4.16.24   True        False         False      74d operator-lifecycle-manager-catalog         4.16.24   True        False         False      74d operator-lifecycle-manager-packageserver   4.16.24   True        False         False      74d service-ca                                 4.16.24   True        False         False      74d storage                                    4.16.24   True        False         False      74d [root@dom16hub101-infra-manager ~]# oc get po No resources found in default namespace. [root@dom16hub101-infra-manager ~]# oc get co NAME                                       VERSION   AVAILABLE   PROGRESSING   DEGRADED   SINCE   MESSAGE authentication                             4.16.24   True        False         False      4h5m baremetal                                  4.16.24   True        False         False      74d cloud-controller-manager                   4.16.24   True        False         False      74d cloud-credential                           4.16.24   True        False         False      74d cluster-autoscaler                         4.16.24   True        False         False      74d config-operator                            4.16.24   True        False         False      74d console                                    4.16.24   True        False         False      74d control-plane-machine-set                  4.16.24   True        False         False      74d csi-snapshot-controller                    4.16.24   True        False         False      74d dns                                        4.16.24   True        False         False      74d etcd                                       4.16.24   True        False         False      74d image-registry                             4.16.24   True        False         False      74d ingress                                    4.16.24   True        False         False      74d insights                                   4.16.24   True        False         False      74d kube-apiserver                             4.16.24   True        False         False      74d kube-controller-manager                    4.16.24   True        False         False      74d kube-scheduler                             4.16.24   True        False         False      74d kube-storage-version-migrator              4.16.24   True        False         False      13d machine-api                                4.16.24   True        False         False      74d machine-approver                           4.16.24   True        False         False      74d machine-config                             4.16.24   True        False         True       74d     Failed to resync 4.16.24 because: error during syncRequiredMachineConfigPools: [context deadline exceeded, failed to update clusteroperator: [client rate limiter Wait returned an error: context deadline exceeded, error MachineConfigPool worker is not ready, retrying. Status: (pool degraded: true total: 2, ready 1, updated: 1, unavailable: 1)]] marketplace                                4.16.24   True        False         False      74d monitoring                                 4.16.24   True        False         False      2m15s network                                    4.16.24   True        False         False      74d node-tuning                                4.16.24   True        False         False      24m openshift-apiserver                        4.16.24   True        False         False      4h34m openshift-controller-manager               4.16.24   True        False         False      74d openshift-samples                          4.16.24   True        False         False      74d operator-lifecycle-manager                 4.16.24   True        False         False      74d operator-lifecycle-manager-catalog         4.16.24   True        False         False      74d operator-lifecycle-manager-packageserver   4.16.24   True        False         False      74d service-ca                                 4.16.24   True        False         False      74d storage                                    4.16.24   True        False         False      74d [root@dom16hub101-infra-manager ~]# oc get co NAME                                       VERSION   AVAILABLE   PROGRESSING   DEGRADED   SINCE   MESSAGE authentication                             4.16.24   True        False         False      4h6m baremetal                                  4.16.24   True        False         False      74d cloud-controller-manager                   4.16.24   True        False         False      74d cloud-credential                           4.16.24   True        False         False      74d cluster-autoscaler                         4.16.24   True        False         False      74d config-operator                            4.16.24   True        False         False      74d console                                    4.16.24   True        False         False      74d control-plane-machine-set                  4.16.24   True        False         False      74d csi-snapshot-controller                    4.16.24   True        False         False      74d dns                                        4.16.24   True        False         False      74d etcd                                       4.16.24   True        False         False      74d image-registry                             4.16.24   True        False         False      74d ingress                                    4.16.24   True        False         False      74d insights                                   4.16.24   True        False         False      74d kube-apiserver                             4.16.24   True        False         False      74d kube-controller-manager                    4.16.24   True        False         False      74d kube-scheduler                             4.16.24   True        False         False      74d kube-storage-version-migrator              4.16.24   True        False         False      13d machine-api                                4.16.24   True        False         False      74d machine-approver                           4.16.24   True        False         False      74d machine-config                             4.16.24   True        False         True       74d     Failed to resync 4.16.24 because: error during syncRequiredMachineConfigPools: [context deadline exceeded, failed to update clusteroperator: [client rate limiter Wait returned an error: context deadline exceeded, error MachineConfigPool worker is not ready, retrying. Status: (pool degraded: true total: 2, ready 1, updated: 1, unavailable: 1)]] marketplace                                4.16.24   True        False         False      74d monitoring                                 4.16.24   True        False         False      2m47s network                                    4.16.24   True        False         False      74d node-tuning                                4.16.24   True        False         False      24m openshift-apiserver                        4.16.24   True        False         False      4h35m openshift-controller-manager               4.16.24   True        False         False      74d openshift-samples                          4.16.24   True        False         False      74d operator-lifecycle-manager                 4.16.24   True        False         False      74d operator-lifecycle-manager-catalog         4.16.24   True        False         False      74d operator-lifecycle-manager-packageserver   4.16.24   True        False         False      74d service-ca                                 4.16.24   True        False         False      74d storage                                    4.16.24   True        False         False      74d [root@dom16hub101-infra-manager ~]# oc get co NAME                                       VERSION   AVAILABLE   PROGRESSING   DEGRADED   SINCE   MESSAGE authentication                             4.16.24   True        False         False      4h6m baremetal                                  4.16.24   True        False         False      74d cloud-controller-manager                   4.16.24   True        False         False      74d cloud-credential                           4.16.24   True        False         False      74d cluster-autoscaler                         4.16.24   True        False         False      74d config-operator                            4.16.24   True        False         False      74d console                                    4.16.24   True        False         False      74d control-plane-machine-set                  4.16.24   True        False         False      74d csi-snapshot-controller                    4.16.24   True        False         False      74d dns                                        4.16.24   True        False         False      74d etcd                                       4.16.24   True        False         False      74d image-registry                             4.16.24   True        False         False      74d ingress                                    4.16.24   True        False         False      74d insights                                   4.16.24   True        False         False      74d kube-apiserver                             4.16.24   True        False         False      74d kube-controller-manager                    4.16.24   True        False         False      74d kube-scheduler                             4.16.24   True        False         False      74d kube-storage-version-migrator              4.16.24   True        False         False      13d machine-api                                4.16.24   True        False         False      74d machine-approver                           4.16.24   True        False         False      74d machine-config                             4.16.24   True        False         True       74d     Failed to resync 4.16.24 because: error during syncRequiredMachineConfigPools: [context deadline exceeded, failed to update clusteroperator: [client rate limiter Wait returned an error: context deadline exceeded, error MachineConfigPool worker is not ready, retrying. Status: (pool degraded: true total: 2, ready 1, updated: 1, unavailable: 1)]] marketplace                                4.16.24   True        False         False      74d monitoring                                 4.16.24   True        False         False      2m49s network                                    4.16.24   True        False         False      74d node-tuning                                4.16.24   True        False         False      24m openshift-apiserver                        4.16.24   True        False         False      4h35m openshift-controller-manager               4.16.24   True        False         False      74d openshift-samples                          4.16.24   True        False         False      74d operator-lifecycle-manager                 4.16.24   True        False         False      74d operator-lifecycle-manager-catalog         4.16.24   True        False         False      74d operator-lifecycle-manager-packageserver   4.16.24   True        False         False      74d service-ca                                 4.16.24   True        False         False      74d storage                                    4.16.24   True        False         False      74d [root@dom16hub101-infra-manager ~]# oc exec -it $(oc get pod -n openshift-storage -l app=rook-ceph-operator -o name) -n openshift-storage -- ceph status -c /var/lib/rook/openshift-storage/openshift-storage.config   cluster:     id:     27b507a3-b496-4549-9271-a3bc0705dd13     health: HEALTH_WARN             Degraded data redundancy: 48355/433023 objects degraded (11.167%), 41 pgs degraded, 45 pgs undersized</p> <p>services:     mon: 3 daemons, quorum a,b,e (age 9m)     mgr: b(active, since 3h), standbys: a     mds: 1/1 daemons up, 1 hot standby     osd: 6 osds: 6 up (since 7m), 6 in (since 8m); 80 remapped pgs     rgw: 1 daemon active (1 hosts, 1 zones)</p> <p>data:     volumes: 1/1 healthy     pools:   12 pools, 201 pgs     objects: 144.34k objects, 541 GiB     usage:   1.4 TiB used, 20 TiB / 21 TiB avail     pgs:     48355/433023 objects degraded (11.167%)              55928/433023 objects misplaced (12.916%)              121 active+clean              39  active+undersized+degraded+remapped+backfill_wait              35  active+remapped+backfill_wait              4   active+undersized+remapped+backfill_wait              2   active+undersized+degraded+remapped+backfilling</p> <p>io:     client:   4.6 MiB/s rd, 394 KiB/s wr, 6 op/s rd, 22 op/s wr     recovery: 410 MiB/s, 0 keys/s, 107 objects/s</p> <p>[root@dom16hub101-infra-manager ~]# oc exec -it $(oc get pod -n openshift-storage -l app=rook-ceph-operator -o name) -n openshift-storage -- ceph status -c /var/lib/rook/openshift-storage/openshift-storage.config   cluster:     id:     27b507a3-b496-4549-9271-a3bc0705dd13     health: HEALTH_WARN             Degraded data redundancy: 44107/433023 objects degraded (10.186%), 39 pgs degraded, 43 pgs undersized</p> <p>services:     mon: 3 daemons, quorum a,b,e (age 9m)     mgr: b(active, since 3h), standbys: a     mds: 1/1 daemons up, 1 hot standby     osd: 6 osds: 6 up (since 8m), 6 in (since 8m); 77 remapped pgs     rgw: 1 daemon active (1 hosts, 1 zones)</p> <p>data:     volumes: 1/1 healthy     pools:   12 pools, 201 pgs     objects: 144.34k objects, 541 GiB     usage:   1.4 TiB used, 20 TiB / 21 TiB avail     pgs:     44107/433023 objects degraded (10.186%)              55928/433023 objects misplaced (12.916%)              123 active+clean              37  active+undersized+degraded+remapped+backfill_wait              35  active+remapped+backfill_wait              4   active+undersized+remapped+backfill_wait              2   active+undersized+degraded+remapped+backfilling</p> <p>io:     client:   7.9 MiB/s rd, 355 KiB/s wr, 8 op/s rd, 27 op/s wr     recovery: 550 MiB/s, 1 keys/s, 145 objects/s</p> <p>[root@dom16hub101-infra-manager ~]# oc get co NAME                                       VERSION   AVAILABLE   PROGRESSING   DEGRADED   SINCE   MESSAGE authentication                             4.16.24   True        False         False      4h7m baremetal                                  4.16.24   True        False         False      74d cloud-controller-manager                   4.16.24   True        False         False      74d cloud-credential                           4.16.24   True        False         False      74d cluster-autoscaler                         4.16.24   True        False         False      74d config-operator                            4.16.24   True        False         False      74d console                                    4.16.24   True        False         False      74d control-plane-machine-set                  4.16.24   True        False         False      74d csi-snapshot-controller                    4.16.24   True        False         False      74d dns                                        4.16.24   True        False         False      74d etcd                                       4.16.24   True        False         False      74d image-registry                             4.16.24   True        False         False      74d ingress                                    4.16.24   True        False         False      74d insights                                   4.16.24   True        False         False      74d kube-apiserver                             4.16.24   True        False         False      74d kube-controller-manager                    4.16.24   True        False         False      74d kube-scheduler                             4.16.24   True        False         False      74d kube-storage-version-migrator              4.16.24   True        False         False      13d machine-api                                4.16.24   True        False         False      74d machine-approver                           4.16.24   True        False         False      74d machine-config                             4.16.24   True        False         True       74d     Failed to resync 4.16.24 because: error during syncRequiredMachineConfigPools: [context deadline exceeded, failed to update clusteroperator: [client rate limiter Wait returned an error: context deadline exceeded, error MachineConfigPool worker is not ready, retrying. Status: (pool degraded: true total: 2, ready 1, updated: 1, unavailable: 1)]] marketplace                                4.16.24   True        False         False      74d monitoring                                 4.16.24   True        False         False      3m47s network                                    4.16.24   True        False         False      74d node-tuning                                4.16.24   True        False         False      25m openshift-apiserver                        4.16.24   True        False         False      4h36m openshift-controller-manager               4.16.24   True        False         False      74d openshift-samples                          4.16.24   True        False         False      74d operator-lifecycle-manager                 4.16.24   True        False         False      74d operator-lifecycle-manager-catalog         4.16.24   True        False         False      74d operator-lifecycle-manager-packageserver   4.16.24   True        False         False      74d service-ca                                 4.16.24   True        False         False      74d storage                                    4.16.24   True        False         False      74d [root@dom16hub101-infra-manager ~]# oc exec -it $(oc get pod -n openshift-storage -l app=rook-ceph-operator -o name) -n openshift-storage -- ceph status -c /var/lib/rook/openshift-storage/openshift-storage.config   cluster:     id:     27b507a3-b496-4549-9271-a3bc0705dd13     health: HEALTH_OK</p> <p>services:     mon: 3 daemons, quorum a,b,e (age 18m)     mgr: b(active, since 3h), standbys: a     mds: 1/1 daemons up, 1 hot standby     osd: 6 osds: 6 up (since 16m), 6 in (since 17m); 30 remapped pgs     rgw: 1 daemon active (1 hosts, 1 zones)</p> <p>data:     volumes: 1/1 healthy     pools:   12 pools, 201 pgs     objects: 144.51k objects, 542 GiB     usage:   1.6 TiB used, 19 TiB / 21 TiB avail     pgs:     46224/433527 objects misplaced (10.662%)              171 active+clean              28  active+remapped+backfill_wait              2   active+remapped+backfilling</p> <p>io:     client:   5.7 MiB/s rd, 396 KiB/s wr, 8 op/s rd, 37 op/s wr     recovery: 155 MiB/s, 41 objects/s</p> <p>[root@dom16hub101-infra-manager ~]#</p>"},{"location":"openshift/disaster-management/hub-cluster-redeployment/master-replacment/","title":"Hub Cluster - Master Replacement","text":"<p>Here is the steps to do the master replacement on the hub cluster.  Dont use this steps for NMC/NWC.  </p>"},{"location":"openshift/disaster-management/hub-cluster-redeployment/master-replacment/#customer-details","title":"Customer Details","text":"<p>1) This mop is created for oneweb cluster and since nokia automation tool populated incorrect <code>/prefix</code> for master node. </p> <p>2) for Oneweb london site, hub cluster having two nodes with incorrect <code>/prefix</code> problem.</p>"},{"location":"openshift/disaster-management/hub-cluster-redeployment/master-replacment/#duration","title":"Duration","text":"<ul> <li>It will take 4 hours per node + 1 hour buffer. totall 10 hours for two servers.</li> </ul>"},{"location":"openshift/disaster-management/hub-cluster-redeployment/master-replacment/#impact","title":"Impact","text":"<ul> <li>CWL cluster LCM operation should not be performed</li> <li>CNF installed on the CWL cluster remains undistrubed, because hub quay will be time to time unaccess. So application onboarding process may make hub quay to be stressed.</li> </ul>"},{"location":"openshift/disaster-management/hub-cluster-redeployment/master-replacment/#highlevel-steps","title":"Highlevel Steps","text":"<ul> <li>Removing the failed node from the cluster</li> <li>Adding back the node control plane node</li> </ul>"},{"location":"openshift/disaster-management/hub-cluster-redeployment/master-replacment/#removing-the-failed-node-from-the-cluster","title":"Removing the failed node from the cluster","text":""},{"location":"openshift/disaster-management/hub-cluster-redeployment/master-replacment/#etcd-backup-for-master-nodes","title":"ETCD backup for master nodes.","text":"<p>1) Start a debug session for a control plane node:</p> <pre><code>oc debug node/&lt;node_name&gt;\n</code></pre> <p>2) Change your root directory to /host:</p> <pre><code>chroot /host\n</code></pre> <p>3) If the cluster-wide proxy is enabled, be sure that you have exported the NO_PROXY, HTTP_PROXY, and HTTPS_PROXY environment variables. (optional)</p> <p>4) Run the cluster-backup.sh script and pass in the location to save the backup to.</p> <pre><code>sh-4.4# /usr/local/bin/cluster-backup.sh /home/core/assets/backup\n</code></pre> <p><code>Example script output</code></p> <pre><code>found latest kube-apiserver: /etc/kubernetes/static-pod-resources/kube-apiserver-pod-6\nfound latest kube-controller-manager: /etc/kubernetes/static-pod-resources/kube-controller-manager-pod-7\nfound latest kube-scheduler: /etc/kubernetes/static-pod-resources/kube-scheduler-pod-6\nfound latest etcd: /etc/kubernetes/static-pod-resources/etcd-pod-3\nede95fe6b88b87ba86a03c15e669fb4aa5bf0991c180d3c6895ce72eaade54a1\netcdctl version: 3.4.14\nAPI version: 3.4\n{\"level\":\"info\",\"ts\":1624647639.0188997,\"caller\":\"snapshot/v3_snapshot.go:119\",\"msg\":\"created temporary db file\",\"path\":\"/home/core/assets/backup/snapshot_2021-06-25_190035.db.part\"}\n{\"level\":\"info\",\"ts\":\"2021-06-25T19:00:39.030Z\",\"caller\":\"clientv3/maintenance.go:200\",\"msg\":\"opened snapshot stream; downloading\"}\n{\"level\":\"info\",\"ts\":1624647639.0301006,\"caller\":\"snapshot/v3_snapshot.go:127\",\"msg\":\"fetching snapshot\",\"endpoint\":\"https://10.0.0.5:2379\"}\n{\"level\":\"info\",\"ts\":\"2021-06-25T19:00:40.215Z\",\"caller\":\"clientv3/maintenance.go:208\",\"msg\":\"completed snapshot read; closing\"}\n{\"level\":\"info\",\"ts\":1624647640.6032252,\"caller\":\"snapshot/v3_snapshot.go:142\",\"msg\":\"fetched snapshot\",\"endpoint\":\"https://10.0.0.5:2379\",\"size\":\"114 MB\",\"took\":1.584090459}\n{\"level\":\"info\",\"ts\":1624647640.6047094,\"caller\":\"snapshot/v3_snapshot.go:152\",\"msg\":\"saved\",\"path\":\"/home/core/assets/backup/snapshot_2021-06-25_190035.db\"}\nSnapshot saved at /home/core/assets/backup/snapshot_2021-06-25_190035.db\n{\"hash\":3866667823,\"revision\":31407,\"totalKey\":12828,\"totalSize\":114446336}\nsnapshot db and kube resources are successfully saved to /home/core/assets/backup\n</code></pre> <p>Transfer the backup files locally on the <code>infra-manager</code> node.</p>"},{"location":"openshift/disaster-management/hub-cluster-redeployment/master-replacment/#identifying-the-failed-control-plane-node","title":"Identifying the failed control plane node","text":"<p>1) First identify which node is the failed one, e.g. which is in NotReady state, using the <code>oc get nodes</code> command.</p> <pre><code>[root@dom16hub101-infra-manager ~]# oc get nodes\nNAME                                               STATUS   ROLES                                 AGE   VERSION\nncpvblvhub-hubmaster-101.ncpvblvhub.t-mobile.lab   Ready    control-plane,master,monitor,worker   74d   v1.29.10+67d3387\nncpvblvhub-hubmaster-102.ncpvblvhub.t-mobile.lab   Ready    control-plane,master,monitor,worker   74d   v1.29.10+67d3387\nncpvblvhub-hubmaster-103.ncpvblvhub.t-mobile.lab   Ready    control-plane,master,monitor,worker   74d   v1.29.10+67d3387\nncpvblvhub-hubworker-101.ncpvblvhub.t-mobile.lab   Ready    gateway,worker                        74d   v1.29.10+67d3387\nncpvblvhub-hubworker-102.ncpvblvhub.t-mobile.lab   Ready    gateway,worker                        74d   v1.29.10+67d3387\n[root@dom16hub101-infra-manager ~]#\n</code></pre> <p>Example <code>ncpvblvhub-hubmaster-102.ncpvblvhub.t-mobile.lab</code> will use this node for replacement.</p> <pre><code>[root@dom16hub101-infra-manager ~]# oc get nodes\nNAME                                               STATUS                     ROLES                                 AGE   VERSION\nncpvblvhub-hubmaster-101.ncpvblvhub.t-mobile.lab   Ready                      control-plane,master,monitor,worker   74d   v1.29.10+67d3387\nncpvblvhub-hubmaster-102.ncpvblvhub.t-mobile.lab   Ready,SchedulingDisabled   control-plane,master,monitor,worker   74d   v1.29.10+67d3387\nncpvblvhub-hubmaster-103.ncpvblvhub.t-mobile.lab   Ready                      control-plane,master,monitor,worker   74d   v1.29.10+67d3387\nncpvblvhub-hubworker-101.ncpvblvhub.t-mobile.lab   Ready                      gateway,worker                        74d   v1.29.10+67d3387\nncpvblvhub-hubworker-102.ncpvblvhub.t-mobile.lab   Ready                      gateway,worker                        74d   v1.29.10+67d3387\n[root@dom16hub101-infra-manager ~]# \n</code></pre> <p>2) Drain the node, using oc adm drain command. </p> <pre><code>[root@dom16hub101-infra-manager ~]# oc adm drain ncpvblvhub-hubmaster-102.ncpvblvhub.t-mobile.lab --ignore-daemonsets --delete-emptydir-data --force\nnode/ncpvblvhub-hubmaster-102.ncpvblvhub.t-mobile.lab already cordoned\nWarning: ignoring DaemonSet-managed Pods: open-cluster-management-backup/node-agent-lvlpm, openshift-cluster-node-tuning-operator/tuned-2xqzr, openshift-dns/dns-default-x7x7p, openshift-dns/node-resolver-sd5qh, openshift-image-registry/node-ca-5nkn2, openshift-ingress-canary/ingress-canary-rngvh, openshift-local-storage/diskmaker-discovery-4snb5, openshift-local-storage/diskmaker-manager-vkkxk, openshift-machine-api/ironic-proxy-24kth, openshift-machine-config-operator/machine-config-daemon-9j45d, openshift-machine-config-operator/machine-config-server-ww4tz, openshift-monitoring/node-exporter-9xshs, openshift-multus/multus-additional-cni-plugins-hkcdv, openshift-multus/multus-dt7qt, openshift-multus/network-metrics-daemon-4j5fx, openshift-multus/whereabouts-reconciler-bv24v, openshift-network-diagnostics/network-check-target-btgf8, openshift-network-node-identity/network-node-identity-td874, openshift-network-operator/iptables-alerter-75rfw, openshift-nmstate/nmstate-handler-tngrh, openshift-ovn-kubernetes/ovnkube-node-svz9x, openshift-storage/csi-cephfsplugin-rnkrk, openshift-storage/csi-rbdplugin-zg88h; deleting Pods that declare no controller: openshift-etcd/etcd-guard-ncpvblvhub-hubmaster-102.ncpvblvhub.t-mobile.lab, openshift-kube-apiserver/kube-apiserver-guard-ncpvblvhub-hubmaster-102.ncpvblvhub.t-mobile.lab, openshift-kube-controller-manager/kube-controller-manager-guard-ncpvblvhub-hubmaster-102.ncpvblvhub.t-mobile.lab, openshift-kube-scheduler/openshift-kube-scheduler-guard-ncpvblvhub-hubmaster-102.ncpvblvhub.t-mobile.lab\nevicting pod multicluster-engine/clusterclaims-controller-7d56ff6fc9-555pz\nevicting pod ncd-git/ncd-git-gitlab-shell-55ccf645ff-ks77d\nevicting pod multicluster-engine/assisted-image-service-0\nevicting pod multicluster-engine/assisted-service-96cf94d9c-wg9zh\nevicting pod ncd-db/ncd-postgresql-postgresql-ha-sentinel-8556d466c6-kd52n\nevicting pod openshift-logging/logging-loki-gateway-7687c66f64-fxd4d\nevicting pod ncd-db/ncd-postgresql-postgresql-ha-proxy-54c69f6cc-csqbz\nevicting pod open-cluster-management-observability/observability-thanos-rule-1\nevicting pod openshift-machine-api/cluster-baremetal-operator-75fbb58cc5-lpqds\nevicting pod open-cluster-management-hub/cluster-manager-registration-controller-7bcbcd64c5-s5p8l\nevicting pod open-cluster-management-observability/observability-grafana-85c6896fd4-7vxzp\nevicting pod openshift-machine-api/control-plane-machine-set-operator-65fbf4bd7-j2l2r\n** output Omitted **\npod/console-mce-console-57b6b4968-jfl4x evicted\npod/logging-loki-query-frontend-55bc666b5c-87hmh evicted\npod/ncd-postgresql-postgresql-ha-keeper-2 evicted\npod/apiserver-776b5f87d7-6t6vz evicted\nnode/ncpvblvhub-hubmaster-102.ncpvblvhub.t-mobile.lab drained\n[root@dom16hub101-infra-manager ~]#\n</code></pre> <p>3) Once drain complete, shutdown the node. (power off)</p> <pre><code>[root@dom16hub101-infra-manager ~]# oc get nodes\nNAME                                               STATUS                     ROLES                                 AGE   VERSION\nncpvblvhub-hubmaster-101.ncpvblvhub.t-mobile.lab   Ready                      control-plane,master,monitor,worker   74d   v1.29.10+67d3387\nncpvblvhub-hubmaster-102.ncpvblvhub.t-mobile.lab   Ready,SchedulingDisabled   control-plane,master,monitor,worker   74d   v1.29.10+67d3387\nncpvblvhub-hubmaster-103.ncpvblvhub.t-mobile.lab   Ready                      control-plane,master,monitor,worker   74d   v1.29.10+67d3387\nncpvblvhub-hubworker-101.ncpvblvhub.t-mobile.lab   Ready                      gateway,worker                        74d   v1.29.10+67d3387\nncpvblvhub-hubworker-102.ncpvblvhub.t-mobile.lab   Ready                      gateway,worker                        74d   v1.29.10+67d3387\n</code></pre> <p>4) Post power off, node will become not ready. </p> <pre><code>[root@dom16hub101-infra-manager ~]# oc get nodes\nNAME                                               STATUS                        ROLES                                 AGE   VERSION\nncpvblvhub-hubmaster-101.ncpvblvhub.t-mobile.lab   Ready                         control-plane,master,monitor,worker   74d   v1.29.10+67d3387\nncpvblvhub-hubmaster-102.ncpvblvhub.t-mobile.lab   NotReady,SchedulingDisabled   control-plane,master,monitor,worker   74d   v1.29.10+67d3387\nncpvblvhub-hubmaster-103.ncpvblvhub.t-mobile.lab   Ready                         control-plane,master,monitor,worker   74d   v1.29.10+67d3387\nncpvblvhub-hubworker-101.ncpvblvhub.t-mobile.lab   Ready                         gateway,worker                        74d   v1.29.10+67d3387\nncpvblvhub-hubworker-102.ncpvblvhub.t-mobile.lab   Ready                         gateway,worker                        74d   v1.29.10+67d3387\n[root@dom16hub101-infra-manager ~]#\n</code></pre>"},{"location":"openshift/disaster-management/hub-cluster-redeployment/master-replacment/#removing-the-node-from-the-etcd-cluster","title":"Removing the node from the etcd cluster","text":"<p>1) First fetch the pods from the openshift-etcd namespace which have the label k8s-app=etcd.</p> <pre><code>[root@dom16hub101-infra-manager ~]# oc -n openshift-etcd get pods -l k8s-app=etcd -o wide\nNAME                                                    READY   STATUS    RESTARTS   AGE   IP              NODE                                               NOMINATED NODE   READINESS GATES\netcd-ncpvblvhub-hubmaster-101.ncpvblvhub.t-mobile.lab   4/4     Running   12         74d   10.145.151.92   ncpvblvhub-hubmaster-101.ncpvblvhub.t-mobile.lab   &lt;none&gt;           &lt;none&gt;\netcd-ncpvblvhub-hubmaster-102.ncpvblvhub.t-mobile.lab   0/0     not running   12         74d   10.145.151.93   ncpvblvhub-hubmaster-102.ncpvblvhub.t-mobile.lab   &lt;none&gt;           &lt;none&gt;\netcd-ncpvblvhub-hubmaster-103.ncpvblvhub.t-mobile.lab   4/4     Running   8          74d   10.145.151.94   ncpvblvhub-hubmaster-103.ncpvblvhub.t-mobile.lab   &lt;none&gt;           &lt;none&gt;\n[root@dom16hub101-infra-manager ~]# \n</code></pre> <p>2) Start a remote shall to one of the pods which shall be running, and not scheduled on the failed node.</p> <pre><code>[root@dom16hub101-infra-manager ~]# oc rsh -n openshift-etcd etcd-ncpvblvhub-hubmaster-102.ncpvblvhub.t-mobile.lab\nsh-5.1# etcdctl endpoint health\n{\"level\":\"warn\",\"ts\":\"2025-05-27T17:09:44.374886Z\",\"logger\":\"client\",\"caller\":\"v3@v3.5.14/retry_interceptor.go:63\",\"msg\":\"retrying of unary invoker failed\",\"target\":\"etcd-endpoints://0xc00027a000/10.145.151.93:2379\",\"attempt\":0,\"error\":\"rpc error: code = DeadlineExceeded desc = context deadline exceeded\"}\nhttps://10.145.151.94:2379 is healthy: successfully committed proposal: took = 7.162307ms\nhttps://10.145.151.92:2379 is healthy: successfully committed proposal: took = 7.213802ms\nhttps://10.145.151.93:2379 is unhealthy: failed to commit proposal: context deadline exceeded\nError: unhealthy cluster\nsh-5.1#\n</code></pre> <p>3) check the status of the etdctl memebers. and removed the scale-in commpute. </p> <pre><code>sh-5.1# etcdctl member list -w table\n+------------------+---------+--------------------------------------------------+----------------------------+----------------------------+------------+\n|        ID        | STATUS  |                       NAME                       |         PEER ADDRS         |        CLIENT ADDRS        | IS LEARNER |\n+------------------+---------+--------------------------------------------------+----------------------------+----------------------------+------------+\n| 44ad9888985e068c | started | ncpvblvhub-hubmaster-101.ncpvblvhub.t-mobile.lab | https://10.145.151.92:2380 | https://10.145.151.92:2379 |      false |\n| f26d12a58d17e571 | started | ncpvblvhub-hubmaster-103.ncpvblvhub.t-mobile.lab | https://10.145.151.94:2380 | https://10.145.151.94:2379 |      false |\n| fc4de79a3d723a5c | Not running | ncpvblvhub-hubmaster-102.ncpvblvhub.t-mobile.lab | https://10.145.151.93:2380 | https://10.145.151.93:2379 |      false |\n+------------------+---------+--------------------------------------------------+----------------------------+----------------------------+------------+\nsh-5.1# etcdctl member remove fc4de79a3d723a5c\nMember fc4de79a3d723a5c removed from cluster 136d42915c2b0516\nsh-5.1# etcdctl member list -w table\n+------------------+---------+--------------------------------------------------+----------------------------+----------------------------+------------+\n|        ID        | STATUS  |                       NAME                       |         PEER ADDRS         |        CLIENT ADDRS        | IS LEARNER |\n+------------------+---------+--------------------------------------------------+----------------------------+----------------------------+------------+\n| 44ad9888985e068c | started | ncpvblvhub-hubmaster-101.ncpvblvhub.t-mobile.lab | https://10.145.151.92:2380 | https://10.145.151.92:2379 |      false |\n| f26d12a58d17e571 | started | ncpvblvhub-hubmaster-103.ncpvblvhub.t-mobile.lab | https://10.145.151.94:2380 | https://10.145.151.94:2379 |      false |\n+------------------+---------+--------------------------------------------------+----------------------------+----------------------------+------------+\nsh-5.1#\n</code></pre> <p>4) check the etcd health now. </p> <pre><code>sh-5.1#  etcdctl endpoint health\n{\"level\":\"warn\",\"ts\":\"2025-05-27T17:10:43.025287Z\",\"logger\":\"client\",\"caller\":\"v3@v3.5.14/retry_interceptor.go:63\",\"msg\":\"retrying of unary invoker failed\",\"target\":\"etcd-endpoints://0xc000020000/10.145.151.93:2379\",\"attempt\":0,\"error\":\"rpc error: code = DeadlineExceeded desc = context deadline exceeded\"}\nhttps://10.145.151.94:2379 is healthy: successfully committed proposal: took = 6.399106ms\nhttps://10.145.151.92:2379 is healthy: successfully committed proposal: took = 6.344247ms\nhttps://10.145.151.93:2379 is unhealthy: failed to commit proposal: context deadline exceeded\nError: unhealthy cluster\nsh-5.1#\nexit\ncommand terminated with exit code 1\n[root@dom16hub101-infra-manager ~]#\n</code></pre> <p>5) run the following command to Turn off the quorum guard:</p> <pre><code>[root@dom16hub101-infra-manager ~]# oc patch etcd/cluster --type=merge -p '{\"spec\":{\"unsupportedConfigOverrides\":{\"useUnsupportedUnsafeNonHANonProductionUnstableEtcd\": true}}}'\netcd.operator.openshift.io/cluster patched\n[root@dom16hub101-infra-manager ~]# \n</code></pre> <p>6) Remove the old secrets for the unhealthy etcd member that was removed by running the following commands.</p> <pre><code>[root@dom16hub101-infra-manager ~]# oc get secrets -n openshift-etcd | grep master-102\netcd-peer-ncpvblvhub-hubmaster-102.ncpvblvhub.t-mobile.lab              kubernetes.io/tls   2      74d\netcd-serving-metrics-ncpvblvhub-hubmaster-102.ncpvblvhub.t-mobile.lab   kubernetes.io/tls   2      74d\netcd-serving-ncpvblvhub-hubmaster-102.ncpvblvhub.t-mobile.lab           kubernetes.io/tls   2      74d\n[root@dom16hub101-infra-manager ~]# #for i in `oc get secrets -n openshift-etcd | grep master-2 | awk\n'{print $1}'`; do oc delete secrets -n openshift-etcd $i; done\n[root@dom16hub101-infra-manager ~]# for i in `oc get secrets -n openshift-etcd | grep master-102 | awk '{print $1}'`; do oc delete secrets -n openshift-etcd $i; done\nsecret \"etcd-peer-ncpvblvhub-hubmaster-102.ncpvblvhub.t-mobile.lab\" deleted\nsecret \"etcd-serving-metrics-ncpvblvhub-hubmaster-102.ncpvblvhub.t-mobile.lab\" deleted\nsecret \"etcd-serving-ncpvblvhub-hubmaster-102.ncpvblvhub.t-mobile.lab\" deleted\n[root@dom16hub101-infra-manager ~]# oc get secrets -n openshift-etcd | grep master-102                                                                 etcd-peer-ncpvblvhub-hubmaster-102.ncpvblvhub.t-mobile.lab              kubernetes.io/tls   2      3s\netcd-serving-metrics-ncpvblvhub-hubmaster-102.ncpvblvhub.t-mobile.lab   kubernetes.io/tls   2      2s\netcd-serving-ncpvblvhub-hubmaster-102.ncpvblvhub.t-mobile.lab           kubernetes.io/tls   2      3s\n[root@dom16hub101-infra-manager ~]#\n[root@dom16hub101-infra-manager ~]#\n</code></pre>"},{"location":"openshift/disaster-management/hub-cluster-redeployment/master-replacment/#fetch-the-output-of-the-failed-nodes-machine-cr","title":"Fetch the output of the failed node\u2019s machine CR","text":"<p>It is needed to fetch the output of the failed node\u2019s machine CR as it contains labels related to the cluster. Then it shall be m odified, e.g. status part shall be removed, annotation of last applied configuration, <code>machine.openshift.op/instance-state: provisioned</code> shall be removed, creation timestamp, finalizer, generation, resource version, uid, from spec, the providerID shall be also removed.</p> <p>1) Get the list of node and find the node you want to remove from machines. </p> <pre><code>[root@dom16hub101-infra-manager ~]# oc get nodes\nNAME                                               STATUS                        ROLES                                 AGE   VERSION\nncpvblvhub-hubmaster-101.ncpvblvhub.t-mobile.lab   Ready                         control-plane,master,monitor,worker   74d   v1.29.10+67d3387\nncpvblvhub-hubmaster-102.ncpvblvhub.t-mobile.lab   NotReady,SchedulingDisabled   control-plane,master,monitor,worker   74d   v1.29.10+67d3387\nncpvblvhub-hubmaster-103.ncpvblvhub.t-mobile.lab   Ready                         control-plane,master,monitor,worker   74d   v1.29.10+67d3387\nncpvblvhub-hubworker-101.ncpvblvhub.t-mobile.lab   Ready                         gateway,worker                        74d   v1.29.10+67d3387\nncpvblvhub-hubworker-102.ncpvblvhub.t-mobile.lab   Ready                         gateway,worker                        74d   v1.29.10+67d3387\n[root@dom16hub101-infra-manager ~]# \n</code></pre> <p>2) using the follow command to ge the node need to removed.  using <code>-o wide</code></p> <pre><code>[root@dom16hub101-infra-manager ~]# oc get machines.machine -A\nNAMESPACE               NAME                              PHASE     TYPE   REGION   ZONE   AGE\nopenshift-machine-api   ncpvblvhub-b6cjs-master-0         Running                          74d\nopenshift-machine-api   ncpvblvhub-b6cjs-master-1         Running                          74d\nopenshift-machine-api   ncpvblvhub-b6cjs-master-2         Running                          74d\nopenshift-machine-api   ncpvblvhub-b6cjs-worker-0-mlq8w   Running                          74d\nopenshift-machine-api   ncpvblvhub-b6cjs-worker-0-x5dc5   Running                          74d\n[root@dom16hub101-infra-manager ~]# oc get machines.machine -n openshift-machine-api\nNAME                              PHASE     TYPE   REGION   ZONE   AGE\nncpvblvhub-b6cjs-master-0         Running                          74d\nncpvblvhub-b6cjs-master-1         Running                          74d\nncpvblvhub-b6cjs-master-2         Running                          74d\nncpvblvhub-b6cjs-worker-0-mlq8w   Running                          74d\nncpvblvhub-b6cjs-worker-0-x5dc5   Running                          74d\n[root@dom16hub101-infra-manager ~]#\n</code></pre> <p>3) take a backup of these nodes from file. </p> <pre><code>[root@dom16hub101-infra-manager ~]# oc get machines.machine.openshift.io -n openshift-machine-api -o yaml `oc get machines.machine.openshift.io -n openshift-machine-api -o wide |grep ncpvblvhub-hubmaster-102.ncpvblvhub.t-mobile.lab|awk {'print $1'}` &gt; backup_machinemaster-102.yaml\n</code></pre> <p>4) just collecting all those outputs here. </p> <p>just refer to productline guide for reference sample tempalte</p> <p></p> <pre><code>[root@dom16hub101-infra-manager ~]# cat backup_machinemaster-102.yaml\napiVersion: machine.openshift.io/v1beta1\nkind: Machine\nmetadata:\n  annotations:\n    machine.openshift.io/instance-state: unmanaged\n    metal3.io/BareMetalHost: openshift-machine-api/ncpvblvhub-hubmaster-102.ncpvblvhub.t-mobile.lab\n  creationTimestamp: \"2025-03-13T23:51:45Z\"\n  finalizers:\n  - machine.machine.openshift.io\n  generation: 3\n  labels:\n    machine.openshift.io/cluster-api-cluster: ncpvblvhub-b6cjs\n    machine.openshift.io/cluster-api-machine-role: master\n    machine.openshift.io/cluster-api-machine-type: master\n  name: ncpvblvhub-b6cjs-master-1\n  namespace: openshift-machine-api\n  resourceVersion: \"140998860\"\n  uid: a53b879c-a886-4381-a1bb-9322333fa76e\nspec:\n  lifecycleHooks:\n    preDrain:\n    - name: EtcdQuorumOperator\n      owner: clusteroperator/etcd\n  metadata: {}\n  providerID: baremetalhost:///openshift-machine-api/ncpvblvhub-hubmaster-102.ncpvblvhub.t-mobile.lab/346b01e0-6bb8-40bd-8db3-d3eabf4d44f1\n  providerSpec:\n    value:\n      apiVersion: baremetal.cluster.k8s.io/v1alpha1\n      customDeploy:\n        method: install_coreos\n      hostSelector: {}\n      image:\n        checksum: \"\"\n        url: \"\"\n      kind: BareMetalMachineProviderSpec\n      metadata:\n        creationTimestamp: null\n      userData:\n        name: master-user-data-managed\nstatus:\n  addresses:\n  - address: fde1:53ba:e9a0:de11:912f:2112:633a:4b75\n    type: InternalIP\n  - address: \"\"\n    type: InternalIP\n  - address: \"\"\n    type: InternalIP\n  - address: \"\"\n    type: InternalIP\n  - address: \"\"\n    type: InternalIP\n  - address: \"\"\n    type: InternalIP\n  - address: \"\"\n    type: InternalIP\n  - address: \"\"\n    type: InternalIP\n  - address: \"\"\n    type: InternalIP\n  - address: 10.145.151.93\n    type: InternalIP\n  - address: \"\"\n    type: InternalIP\n  conditions:\n  - lastTransitionTime: \"2025-03-14T00:04:57Z\"\n    message: 'Drain operation currently blocked by: [{Name:EtcdQuorumOperator Owner:clusteroperator/etcd}]'\n    reason: HookPresent\n    severity: Warning\n    status: \"False\"\n    type: Drainable\n  - lastTransitionTime: \"2025-03-14T00:03:59Z\"\n    status: \"True\"\n    type: InstanceExists\n  - lastTransitionTime: \"2025-03-13T23:58:56Z\"\n    status: \"True\"\n    type: Terminable\n  lastUpdated: \"2025-05-27T17:08:29Z\"\n  nodeRef:\n    kind: Node\n    name: ncpvblvhub-hubmaster-102.ncpvblvhub.t-mobile.lab\n    uid: cdc0d098-638f-4c2d-b4ba-4c2c1ebc1c10\n  phase: Running\n[root@dom16hub101-infra-manager ~]# cat backup_machinemaster-102_editted.yaml\napiVersion: machine.openshift.io/v1beta1\nkind: Machine\nmetadata:\n  annotations:\n    machine.openshift.io/instance-state: unmanaged\n    metal3.io/BareMetalHost: openshift-machine-api/ncpvblvhub-hubmaster-102.ncpvblvhub.t-mobile.lab\n  finalizers:\n  - machine.machine.openshift.io\n  generation: 3\n  labels:\n    machine.openshift.io/cluster-api-cluster: ncpvblvhub-b6cjs\n    machine.openshift.io/cluster-api-machine-role: master\n    machine.openshift.io/cluster-api-machine-type: master\n  name: ncpvblvhub-b6cjs-master-1\n  namespace: openshift-machine-api\nspec:\n  lifecycleHooks:\n    preDrain:\n    - name: EtcdQuorumOperator\n      owner: clusteroperator/etcd\n  metadata: {}\n  providerID: baremetalhost:///openshift-machine-api/ncpvblvhub-hubmaster-102.ncpvblvhub.t-mobile.lab/346b01e0-6bb8-40bd-8db3-d3eabf4d44f1\n  providerSpec:\n    value:\n      apiVersion: baremetal.cluster.k8s.io/v1alpha1\n      customDeploy:\n        method: install_coreos\n      hostSelector: {}\n      image:\n        checksum: \"\"\n        url: \"\"\n      kind: BareMetalMachineProviderSpec\n      metadata:\n        creationTimestamp: null\n      userData:\n        name: master-user-data-managed\n\n[root@dom16hub101-infra-manager ~]# oc get nodes\nNAME                                               STATUS                        ROLES                                 AGE   VERSION\nncpvblvhub-hubmaster-101.ncpvblvhub.t-mobile.lab   Ready                         control-plane,master,monitor,worker   74d   v1.29.10+67d3387\nncpvblvhub-hubmaster-102.ncpvblvhub.t-mobile.lab   NotReady,SchedulingDisabled   control-plane,master,monitor,worker   74d   v1.29.10+67d3387\nncpvblvhub-hubmaster-103.ncpvblvhub.t-mobile.lab   Ready                         control-plane,master,monitor,worker   74d   v1.29.10+67d3387\nncpvblvhub-hubworker-101.ncpvblvhub.t-mobile.lab   Ready                         gateway,worker                        74d   v1.29.10+67d3387\nncpvblvhub-hubworker-102.ncpvblvhub.t-mobile.lab   Ready                         gateway,worker                        74d   v1.29.10+67d3387\n</code></pre>"},{"location":"openshift/disaster-management/hub-cluster-redeployment/master-replacment/#removing-failed-nodes-osds-from-odf","title":"Removing failed node\u2019s OSDs from ODF","text":"<p>1) Get the list of pods from <code>openshift-storage</code> namespace here. </p> <pre><code>[root@dom16hub101-infra-manager ~]# oc get pods -n openshift-storage -o wide | grep -i ncpvblvhub-hubmaster-102.ncpvblvhub.t-mobile.lab\ncsi-cephfsplugin-rnkrk                                            2/2     Running   6              71d   10.145.151.93    ncpvblvhub-hubmaster-102.ncpvblvhub.t-mobile.lab   &lt;none&gt;           &lt;none&gt;\ncsi-rbdplugin-provisioner-646d95bdd9-496ng                        6/6     Running   0              15d   172.21.1.43      ncpvblvhub-hubmaster-102.ncpvblvhub.t-mobile.lab   &lt;none&gt;           &lt;none&gt;\ncsi-rbdplugin-zg88h                                               3/3     Running   9              71d   10.145.151.93    ncpvblvhub-hubmaster-102.ncpvblvhub.t-mobile.lab   &lt;none&gt;           &lt;none&gt;\nrook-ceph-crashcollector-11cef195e99cf42211bc5b21918ec486-b8jpz   1/1     Running   0              26d   172.21.0.19      ncpvblvhub-hubmaster-102.ncpvblvhub.t-mobile.lab   &lt;none&gt;           &lt;none&gt;\nrook-ceph-exporter-11cef195e99cf42211bc5b21918ec486-6f8c85r84bf   1/1     Running   0              26d   172.21.0.20      ncpvblvhub-hubmaster-102.ncpvblvhub.t-mobile.lab   &lt;none&gt;           &lt;none&gt;\nrook-ceph-mon-a-66bcddd94-wbfs4                                   2/2     Running   0              26d   172.21.0.9       ncpvblvhub-hubmaster-102.ncpvblvhub.t-mobile.lab   &lt;none&gt;           &lt;none&gt;\nrook-ceph-osd-0-54d5b7dd6b-vjf4f                                  2/2     Running   0              27d   172.21.0.11      ncpvblvhub-hubmaster-102.ncpvblvhub.t-mobile.lab   &lt;none&gt;           &lt;none&gt;\nrook-ceph-osd-5-79fb5f7965-wpgpq                                  2/2     Running   0              27d   172.21.0.17      ncpvblvhub-hubmaster-102.ncpvblvhub.t-mobile.lab   &lt;none&gt;           &lt;none&gt;\n[root@dom16hub101-infra-manager ~]# \n</code></pre> <p>2) remove the mon and osd pods running on the master2 nodes. then check the status of the pod's to make sure, it's terminated. </p> <pre><code>[root@dom16hub101-infra-manager ~]# oc scale deployment rook-ceph-mon-a --replicas=0 -n openshift-storage\ndeployment.apps/rook-ceph-mon-a scaled\n[root@dom16hub101-infra-manager ~]# oc scale deployment rook-ceph-osd-0 --replicas=0 -n openshift-storage\ndeployment.apps/rook-ceph-osd-0 scaled\n[root@dom16hub101-infra-manager ~]#\n[root@dom16hub101-infra-manager ~]# oc scale deployment rook-ceph-osd-5 --replicas=0 -n openshift-storage\ndeployment.apps/rook-ceph-osd-5 scaled\n[root@dom16hub101-infra-manager ~]# oc get pods -n openshift-storage -o wide | grep -i ncpvblvhub-hubmaster-102.ncpvblvhub.t-mobile.lab\ncsi-cephfsplugin-rnkrk                                            2/2     Running       6              71d   10.145.151.93    ncpvblvhub-hubmaster-102.ncpvblvhub.t-mobile.lab   &lt;none&gt;           &lt;none&gt;\ncsi-rbdplugin-provisioner-646d95bdd9-496ng                        6/6     Running       0              15d   172.21.1.43      ncpvblvhub-hubmaster-102.ncpvblvhub.t-mobile.lab   &lt;none&gt;           &lt;none&gt;\ncsi-rbdplugin-zg88h                                               3/3     Running       9              71d   10.145.151.93    ncpvblvhub-hubmaster-102.ncpvblvhub.t-mobile.lab   &lt;none&gt;           &lt;none&gt;\nrook-ceph-exporter-11cef195e99cf42211bc5b21918ec486-6f8c85r84bf   0/1     Terminating   0              26d   172.21.0.20      ncpvblvhub-hubmaster-102.ncpvblvhub.t-mobile.lab   &lt;none&gt;           &lt;none&gt;\n[root@dom16hub101-infra-manager ~]# oc scale deployment --selector=app=rook-ceph-crashcollector,node_name=ncpvblvhub-hubmaster-102.ncpvblvhub.t-mobile.lab  --replicas=0 -n openshift-storage\nerror: no objects passed to scale\n[root@dom16hub101-infra-manager ~]# oc get pods -n openshift-storage -o wide | grep -i ncpvblvhub-hubmaster-102.ncpvblvhub.t-mobile.lab                csi-cephfsplugin-rnkrk                                            2/2     Running   6              71d   10.145.151.93    ncpvblvhub-hubmaster-102.ncpvblvhub.t-mobile.lab   &lt;none&gt;           &lt;none&gt;\ncsi-rbdplugin-provisioner-646d95bdd9-496ng                        6/6     Running   0              15d   172.21.1.43      ncpvblvhub-hubmaster-102.ncpvblvhub.t-mobile.lab   &lt;none&gt;           &lt;none&gt;\ncsi-rbdplugin-zg88h                                               3/3     Running   9              71d   10.145.151.93    ncpvblvhub-hubmaster-102.ncpvblvhub.t-mobile.lab   &lt;none&gt;           &lt;none&gt;\n[root@dom16hub101-infra-manager ~]#\n</code></pre> <p>2.1) remove the label from the node. </p> <pre><code>[root@dom16hub101-infra-manager ~]# oc label node ncpvblvhub-hubmaster-102.ncpvblvhub.t-mobile.lab cluster.ocs.openshift.io/openshift-storage-\nnode/ncpvblvhub-hubmaster-102.ncpvblvhub.t-mobile.lab unlabeled\n[root@dom16hub101-infra-manager ~]#\n</code></pre> <p>3) Now run the OSD removal script to delete the OSD completely from the cluster.</p> <pre><code>[root@dom16hub101-infra-manager ~]# oc process -n openshift-storage ocs-osd-removal -p FAILED_OSD_IDS=0,5 FORCE_OSD_REMOVAL=true | oc create -n openshift-storage -f -\nError from server (AlreadyExists): error when creating \"STDIN\": jobs.batch \"ocs-osd-removal-job\" already exists\n[root@dom16hub101-infra-manager ~]# oc get jobs -n openshift-storage | gre removal\nbash: gre: command not found...\n[root@dom16hub101-infra-manager ~]# oc get jobs -n openshift-storage | grep removal\nocs-osd-removal-job                                      1/1           14m        33d\n[root@dom16hub101-infra-manager ~]# oc delete jobs -n openshift-storage ocs-osd-removal-job\njob.batch \"ocs-osd-removal-job\" deleted\n[root@dom16hub101-infra-manager ~]# oc process -n openshift-storage ocs-osd-removal -p FAILED_OSD_IDS=0,5 FORCE_OSD_REMOVAL=true | oc create -n openshift-storage -f -\njob.batch/ocs-osd-removal-job created\n[root@dom16hub101-infra-manager ~]# oc get jobs -n openshift-storage | grep removal                                                                    ocs-osd-removal-job                                      0/1           5s         5s\n[root@dom16hub101-infra-manager ~]# oc get jobs -n openshift-storage | grep removal\nocs-osd-removal-job                                      1/1           13s        14s\n[root@dom16hub101-infra-manager ~]# oc delete jobs -n openshift-storage ocs-osd-removal-job                                                            job.batch \"ocs-osd-removal-job\" deleted\n[root@dom16hub101-infra-manager ~]#\n</code></pre> <p>4) At last remove the pv as well. post check that ceph cluster.</p> <pre><code>[root@dom16hub101-infra-manager ~]#oc get pv | grep local | grep -i released\nlocal-pv-4fcd3797                          3576Gi     RWO            Delete           Released    openshift-storage/ocs-deviceset-localblockstorage-0-data-0ll69z                      localblockstorage     &lt;unset&gt;                          71d\nlocal-pv-cb7421c8                          3576Gi     RWO            Delete           Released    openshift-storage/ocs-deviceset-localblockstorage-2-data-1w8q46                      localblockstorage     &lt;unset&gt;                          33d\n[root@dom16hub101-infra-manager ~]# oc delete pv local-pv-4fcd3797 local-pv-cb7421c8\npersistentvolume \"local-pv-4fcd3797\" deleted\npersistentvolume \"local-pv-cb7421c8\" deleted\n[root@dom16hub101-infra-manager ~]#\n\n[root@dom16hub101-infra-manager ~]# oc get clusteroperator baremetal\nNAME        VERSION   AVAILABLE   PROGRESSING   DEGRADED   SINCE   MESSAGE\nbaremetal   4.16.24   True        False         False      74d\n</code></pre>"},{"location":"openshift/disaster-management/hub-cluster-redeployment/master-replacment/#removing-the-node-from-the-cluster","title":"Removing the node from the cluster","text":"<p>1) Get the node status of <code>master-102</code>. from <code>bmh</code>, <code>machine</code> etc. </p> <pre><code>[root@dom16hub101-infra-manager ~]# oc get bmh -n openshift-machine-api\nNAME                                               STATE       CONSUMER                          ONLINE   ERROR   AGE\nncpvblvhub-hubmaster-101.ncpvblvhub.t-mobile.lab   unmanaged   ncpvblvhub-b6cjs-master-0         true             74d\nncpvblvhub-hubmaster-102.ncpvblvhub.t-mobile.lab   unmanaged   ncpvblvhub-b6cjs-master-1         true             74d\nncpvblvhub-hubmaster-103.ncpvblvhub.t-mobile.lab   unmanaged   ncpvblvhub-b6cjs-master-2         true             74d\nncpvblvhub-hubworker-101.ncpvblvhub.t-mobile.lab   unmanaged   ncpvblvhub-b6cjs-worker-0-mlq8w   true             74d\nncpvblvhub-hubworker-102.ncpvblvhub.t-mobile.lab   unmanaged   ncpvblvhub-b6cjs-worker-0-x5dc5   true             74d\n[root@dom16hub101-infra-manager ~]# oc delete bmh -n openshift-machine-api ncpvblvhub-hubmaster-102.ncpvblvhub.t-mobile.lab\nbaremetalhost.metal3.io \"ncpvblvhub-hubmaster-102.ncpvblvhub.t-mobile.lab\" deleted\n\n[root@dom16hub101-infra-manager ~]# oc get bmh -n openshift-machine-api\nNAME                                               STATE       CONSUMER                          ONLINE   ERROR   AGE\nncpvblvhub-hubmaster-101.ncpvblvhub.t-mobile.lab   unmanaged   ncpvblvhub-b6cjs-master-0         true             74d\nncpvblvhub-hubmaster-103.ncpvblvhub.t-mobile.lab   unmanaged   ncpvblvhub-b6cjs-master-2         true             74d\nncpvblvhub-hubworker-101.ncpvblvhub.t-mobile.lab   unmanaged   ncpvblvhub-b6cjs-worker-0-mlq8w   true             74d\nncpvblvhub-hubworker-102.ncpvblvhub.t-mobile.lab   unmanaged   ncpvblvhub-b6cjs-worker-0-x5dc5   true             74d\n[root@dom16hub101-infra-manager ~]# \n</code></pre> <p>2) Check the <code>machines.machine</code> api and delete it. </p> <pre><code>[root@dom16hub101-infra-manager ~]# oc get machines.machine.openshift.io -n openshift-machine-api\nNAME                              PHASE     TYPE   REGION   ZONE   AGE\nncpvblvhub-b6cjs-master-0         Running                          74d\nncpvblvhub-b6cjs-master-1         Failed                           74d\nncpvblvhub-b6cjs-master-2         Running                          74d\nncpvblvhub-b6cjs-worker-0-mlq8w   Running                          74d\nncpvblvhub-b6cjs-worker-0-x5dc5   Running                          74d\n[root@dom16hub101-infra-manager ~]# #oc delete machines.machine.openshift.io -n openshift-machine-api ncpvblvhub-b6cjs-master-1\n[root@dom16hub101-infra-manager ~]# oc describe machines.machine.openshift.io -n openshift-machine-api ncpvblvhub-b6cjs-master-1 | grep -i master-102\n              metal3.io/BareMetalHost: openshift-machine-api/ncpvblvhub-hubmaster-102.ncpvblvhub.t-mobile.lab\n  Provider ID:  baremetalhost:///openshift-machine-api/ncpvblvhub-hubmaster-102.ncpvblvhub.t-mobile.lab/346b01e0-6bb8-40bd-8db3-d3eabf4d44f1\n    Name:  ncpvblvhub-hubmaster-102.ncpvblvhub.t-mobile.lab\n[root@dom16hub101-infra-manager ~]#\n[root@dom16hub101-infra-manager ~]# oc delete machines.machine.openshift.io -n openshift-machine-api ncpvblvhub-b6cjs-master-1\nmachine.machine.openshift.io \"ncpvblvhub-b6cjs-master-1\" deleted\n[root@dom16hub101-infra-manager ~]# oc get machines.machine.openshift.io -n openshift-machine-api\nNAME                              PHASE     TYPE   REGION   ZONE   AGE\nncpvblvhub-b6cjs-master-0         Running                          74d\nncpvblvhub-b6cjs-master-2         Running                          74d\nncpvblvhub-b6cjs-worker-0-mlq8w   Running                          74d\nncpvblvhub-b6cjs-worker-0-x5dc5   Running                          74d\n[root@dom16hub101-infra-manager ~]#\n\n[root@dom16hub101-infra-manager ~]# oc get machines.machine.openshift.io -n openshift-machine-api\nNAME                              PHASE     TYPE   REGION   ZONE   AGE\nncpvblvhub-b6cjs-master-0         Running                          74d\nncpvblvhub-b6cjs-master-2         Running                          74d\nncpvblvhub-b6cjs-worker-0-mlq8w   Running                          74d\nncpvblvhub-b6cjs-worker-0-x5dc5   Running                          74d\n[root@dom16hub101-infra-manager ~]# \n</code></pre> <p>3) Now check the status of <code>bmh</code>, <code>machines</code> and <code>nodes</code>, to make sure <code>master-2</code> is completly removed. </p> <pre><code>[root@dom16hub101-infra-manager ~]#oc get bmh -n openshift-machine-api\nNAME                                               STATE       CONSUMER                          ONLINE   ERROR   AGE\nncpvblvhub-hubmaster-101.ncpvblvhub.t-mobile.lab   unmanaged   ncpvblvhub-b6cjs-master-0         true             74d\nncpvblvhub-hubmaster-103.ncpvblvhub.t-mobile.lab   unmanaged   ncpvblvhub-b6cjs-master-2         true             74d\nncpvblvhub-hubworker-101.ncpvblvhub.t-mobile.lab   unmanaged   ncpvblvhub-b6cjs-worker-0-mlq8w   true             74d\nncpvblvhub-hubworker-102.ncpvblvhub.t-mobile.lab   unmanaged   ncpvblvhub-b6cjs-worker-0-x5dc5   true             74d\n[root@dom16hub101-infra-manager ~]# oc get machines.machine.openshift.io -n openshift-machine-api\nNAME                              PHASE     TYPE   REGION   ZONE   AGE\nncpvblvhub-b6cjs-master-0         Running                          74d\nncpvblvhub-b6cjs-master-2         Running                          74d\nncpvblvhub-b6cjs-worker-0-mlq8w   Running                          74d\nncpvblvhub-b6cjs-worker-0-x5dc5   Running                          74d\n[root@dom16hub101-infra-manager ~]# oc get nodes\nNAME                                               STATUS   ROLES                                 AGE   VERSION\nncpvblvhub-hubmaster-101.ncpvblvhub.t-mobile.lab   Ready    control-plane,master,monitor,worker   74d   v1.29.10+67d3387\nncpvblvhub-hubmaster-103.ncpvblvhub.t-mobile.lab   Ready    control-plane,master,monitor,worker   74d   v1.29.10+67d3387\nncpvblvhub-hubworker-101.ncpvblvhub.t-mobile.lab   Ready    gateway,worker                        74d   v1.29.10+67d3387\nncpvblvhub-hubworker-102.ncpvblvhub.t-mobile.lab   Ready    gateway,worker                        74d   v1.29.10+67d3387\n[root@dom16hub101-infra-manager ~]#\n</code></pre> <p>After this step, the node or failed parts of it can be safely replaced.</p>"},{"location":"openshift/disaster-management/hub-cluster-redeployment/master-replacment/#adding-back-the-node-control-plane-node","title":"Adding back the node control plane node","text":""},{"location":"openshift/disaster-management/hub-cluster-redeployment/master-replacment/#create-bmh-and-machine-crs","title":"Create BMH and Machine CRs","text":"<p>1) Check the list of nodes in cluster now. </p> <pre><code>[root@dom16hub101-infra-manager ~]# oc get nodes\nNAME                                               STATUS   ROLES                                 AGE   VERSION\nncpvblvhub-hubmaster-101.ncpvblvhub.t-mobile.lab   Ready    control-plane,master,monitor,worker   74d   v1.29.10+67d3387\nncpvblvhub-hubmaster-103.ncpvblvhub.t-mobile.lab   Ready    control-plane,master,monitor,worker   74d   v1.29.10+67d3387\nncpvblvhub-hubworker-101.ncpvblvhub.t-mobile.lab   Ready    gateway,worker                        74d   v1.29.10+67d3387\nncpvblvhub-hubworker-102.ncpvblvhub.t-mobile.lab   Ready    gateway,worker                        74d   v1.29.10+67d3387\n[root@dom16hub101-infra-manager ~]#\n</code></pre> <p>2) In the openshift-machine-api namespace two secrets shall be created. One is for the BMC access and the other one stores the networking configuration.</p> <p>2.0) The data of secret for BMC access is simply base64 encoded.</p> <pre><code>[root@dom16hub101-infra-manager ~]# cat bmc-credential-hub.yaml\n---\napiVersion: v1\nkind: Secret\nmetadata:\n  name: control-plane-3-bmc-secret\n  namespace: openshift-machine-api\ndata:\n  username: \"cm9vdA==\"\n  password: \"Y2Fsdmlu\"\ntype: opaque\n[root@dom16hub101-infra-manager ~]#\n</code></pre> <p>2.1) Then the networking configuration shall be created as a secret. It is needed as the default setting is DHCP for the nodes. These information can be simply copied from the agent-config.yaml which was used for the deployment of the HUB cluster. If node replacement was done (including NIC) make sure to update the MAC addresses!</p> <pre><code>[root@dom16hub101-infra-manager ~]# cat master-102_network_config.yaml\napiVersion: v1\nkind: Secret\nmetadata:\n  name: openshift-master-102-network-config-secret\n  namespace: openshift-machine-api\ntype: Opaque\nstringData:\n  nmstate: |\n    interfaces:\n      - name: infra-1\n        type: ethernet\n        state: up\n        identifier: mac-address\n        mtu: 9126\n        mac-address: C4:70:BD:F9:7F:48\n      - name: infra-2\n        type: ethernet\n        state: up\n        identifier: mac-address\n        mtu: 9126\n        mac-address: C4:70:BD:F9:7F:49\n      - name: tenant-1-1\n        type: ethernet\n        state: up\n        identifier: mac-address\n        mtu: 9126\n        mac-address: C4:70:BD:4A:90:8A\n      - name: tenant-1-2\n        type: ethernet\n        state: up\n        identifier: mac-address\n        mtu: 9126\n        mac-address: C4:70:BD:4A:90:8B\n      - name: tenant-2-1\n        type: ethernet\n        state: up\n        identifier: mac-address\n        mtu: 9126\n        mac-address: C4:70:BD:4A:90:8E\n      - name: tenant-2-2\n        type: ethernet\n        state: up\n        identifier: mac-address\n        mtu: 9126\n        mac-address: C4:70:BD:4A:90:8F\n      - name: infra-bond\n        type: bond\n        state: up\n        link-aggregation:\n          mode: active-backup\n          options:\n            miimon: \"100\"\n          port:\n          - infra-1\n          - infra-2\n        mtu: 9126\n      - name: tenant-bond-1\n        link-aggregation:\n          mode: active-backup\n          options:\n            miimon: \"100\"\n          port:\n          - tenant-1-1\n          - tenant-1-2\n        mtu: 9126\n        state: up\n        type: bond\n      - name: tenant-bond-2\n        link-aggregation:\n          mode: active-backup\n          options:\n            miimon: \"100\"\n          port:\n          - tenant-2-1\n          - tenant-2-2\n        mtu: 9126\n        state: up\n        type: bond\n      - name: infra-bond.200\n        type: vlan\n        state: up\n        mtu: 9126\n        ipv4:\n          enabled: true\n          dhcp: false\n          address:\n            - ip: 10.145.151.93\n              prefix-length: 26\n        ipv6:\n          enabled: false\n          dhcp: false\n        vlan:\n          base-iface: infra-bond\n          id: 200\n    routes:\n      config:\n        - destination: 0.0.0.0/0\n          next-hop-address: 10.145.151.65\n          next-hop-interface: infra-bond.200\n          table-id: 254\n    dns-resolver:\n      config:\n        search:\n        - t-mobile.lab\n        server:\n        - 5.232.32.63\n        - 10.169.69.10\n\n[root@dom16hub101-infra-manager ~]#\n</code></pre> <p>3) create the BMH resource using <code>baremetalhost</code> file.</p> <pre><code>[root@dom16hub101-infra-manager ~]# cat master-102_bmh_wih_secret.yaml\napiVersion: metal3.io/v1alpha1\nkind: BareMetalHost\nmetadata:\n  name: ncpvblvhub-hubmaster-102.ncpvblvhub.t-mobile.lab\n  namespace: openshift-machine-api\nspec:\n  automatedCleaningMode: disabled\n  bmc:\n    address: idrac-virtualmedia://10.145.151.15/redfish/v1/Systems/System.Embedded.1    #this is for dell server , for HP or other vendor check virtual media path\n    credentialsName: control-plane-3-bmc-secret\n    disableCertificateVerification: True\n  bootMACAddress: c4:70:bd:f9:7f:48\n  bootMode: UEFISecureBoot\n  externallyProvisioned: false\n  hardwareProfile: unknown\n  online: true\n  rootDeviceHints:\n    deviceName: /dev/disk/by-path/pci-0000:4a:00.0-scsi-0:2:0:0\n  userData:\n    name: master-user-data-managed\n    namespace: openshift-machine-api\n  preprovisioningNetworkDataName: openshift-master-102-network-config-secret\n[root@dom16hub101-infra-manager ~]#\n</code></pre> <p>3.1) After creating this resource, the node will be inspected, and after a few minutes it shall be in available state.</p> <pre><code>[root@dom16hub101-infra-manager ~]# oc apply -f master_102_bmh.yaml\nWarning: metadata.finalizers: \"baremetalhost.metal3.io\": prefer a domain-qualified finalizer name to avoid accidental conflicts with other finalizer writers\nbaremetalhost.metal3.io/ncpvblvhub-hubmaster-102.ncpvblvhub.t-mobile.lab created\n</code></pre> <p>3.2) the node status should change from registering -&gt; inspecting -&gt; available. </p> <pre><code>[root@dom16hub101-infra-manager ~]# oc get bmh -n openshift-machine-api ncpvblvhub-hubmaster-102.ncpvblvhub.t-mobile.lab\nNAME                                               STATE          CONSUMER                    ONLINE   ERROR   AGE\nncpvblvhub-hubmaster-102.ncpvblvhub.t-mobile.lab   available   ncpvblvhub-b6cjs-master-1   true             31m\n[root@dom16hub101-infra-manager ~]#\n</code></pre> <p>Once the BMH is in available state, the machine CR can be created. The same shall be created which was done in chapter 1.3.</p> <p>6) create and apply the machines.yaml </p> <pre><code>[root@dom16hub101-infra-manager ~]# cat backup_machinemaster-102_editted.yaml\napiVersion: machine.openshift.io/v1beta1\nkind: Machine\nmetadata:\n  labels:\n    machine.openshift.io/cluster-api-cluster: ncpvblvhub-b6cjs\n    machine.openshift.io/cluster-api-machine-role: master\n    machine.openshift.io/cluster-api-machine-type: master\n  name: ncpvblvhub-b6cjs-master-1\n  namespace: openshift-machine-api\nspec:\n  lifecycleHooks:\n    preDrain:\n    - name: EtcdQuorumOperator\n      owner: clusteroperator/etcd\n  metadata: {}\n  providerSpec:\n    value:\n      apiVersion: baremetal.cluster.k8s.io/v1alpha1\n      customDeploy:\n        method: install_coreos\n      hostSelector: {}\n      image:\n        checksum: \"\"\n        url: \"\"\n      kind: BareMetalMachineProviderSpec\n      metadata:\n        creationTimestamp: null\n      userData:\n        name: master-user-data-managed\n[root@dom16hub101-infra-manager ~]#\n\n[root@dom16hub101-infra-manager ~]# oc apply -f backup_machinemaster-102_editted.yaml\nWarning: metadata.finalizers: \"machine.machine.openshift.io\": prefer a domain-qualified finalizer name to avoid accidental conflicts with other finalizer writers\nmachine.machine.openshift.io/ncpvblvhub-b6cjs-master-1 created\n[root@dom16hub101-infra-manager ~]#\n\n[root@dom16hub101-infra-manager ~]# oc get machines.machine.openshift.io -n openshift-machine-api\nNAME                              PHASE          TYPE   REGION   ZONE   AGE\nncpvblvhub-b6cjs-master-0         Running                               74d\nncpvblvhub-b6cjs-master-1         Provisioning                          4m17s\nncpvblvhub-b6cjs-master-2         Running                               74d\nncpvblvhub-b6cjs-worker-0-mlq8w   Running                               74d\nncpvblvhub-b6cjs-worker-0-x5dc5   Running                               74d\n[root@dom16hub101-infra-manager ~]#\n</code></pre> <p>7) Monitor the status of bmh, will change to provisioning</p> <pre><code>[root@dom16hub101-infra-manager ~]# oc get bmh -n openshift-machine-api\nNAME                                               STATE          CONSUMER                          ONLINE   ERROR   AGE\nncpvblvhub-hubmaster-101.ncpvblvhub.t-mobile.lab   unmanaged      ncpvblvhub-b6cjs-master-0         true             74d\nncpvblvhub-hubmaster-102.ncpvblvhub.t-mobile.lab   provisioning   ncpvblvhub-b6cjs-master-1         true             31m\nncpvblvhub-hubmaster-103.ncpvblvhub.t-mobile.lab   unmanaged      ncpvblvhub-b6cjs-master-2         true             74d\nncpvblvhub-hubworker-101.ncpvblvhub.t-mobile.lab   unmanaged      ncpvblvhub-b6cjs-worker-0-mlq8w   true             74d\nncpvblvhub-hubworker-102.ncpvblvhub.t-mobile.lab   unmanaged      ncpvblvhub-b6cjs-worker-0-x5dc5   true             74d\n[root@dom16hub101-infra-manager ~]#\n\n#nodes will be added to the cluster \n\n[root@dom16hub101-infra-manager ~]# oc get nodes\nNAME                                               STATUS   ROLES                                 AGE   VERSION\nncpvblvhub-hubmaster-101.ncpvblvhub.t-mobile.lab   Ready    control-plane,master,monitor,worker   74d   v1.29.10+67d3387\nncpvblvhub-hubmaster-102.ncpvblvhub.t-mobile.lab   Ready    control-plane,master,worker           80s   v1.29.10+67d3387\nncpvblvhub-hubmaster-103.ncpvblvhub.t-mobile.lab   Ready    control-plane,master,monitor,worker   74d   v1.29.10+67d3387\nncpvblvhub-hubworker-101.ncpvblvhub.t-mobile.lab   Ready    gateway,worker                        74d   v1.29.10+67d3387\nncpvblvhub-hubworker-102.ncpvblvhub.t-mobile.lab   Ready    gateway,worker                        74d   v1.29.10+67d3387\n[root@dom16hub101-infra-manager ~]#\n\n[root@dom16hub101-infra-manager ~]# oc get machines.machine.openshift.io -n openshift-machine-api\nNAME                              PHASE     TYPE   REGION   ZONE   AGE\nncpvblvhub-b6cjs-master-0         Running                          74d\nncpvblvhub-b6cjs-master-1         Running                          15m\nncpvblvhub-b6cjs-master-2         Running                          74d\nncpvblvhub-b6cjs-worker-0-mlq8w   Running                          74d\nncpvblvhub-b6cjs-worker-0-x5dc5   Running                          74d\n[root@dom16hub101-infra-manager ~]# oc get bmh -n openshift-machine-api ncpvblvhub-hubmaster-102.ncpvblvhub.t-mobile.lab\nNAME                                               STATE         CONSUMER                    ONLINE   ERROR   AGE\nncpvblvhub-hubmaster-102.ncpvblvhub.t-mobile.lab   provisioned   ncpvblvhub-b6cjs-master-1   true             44m\n[root@dom16hub101-infra-manager ~]#\n</code></pre> <p>8) At last node successfully added back to the cluster here. </p> <pre><code>[root@dom16hub101-infra-manager ~]# oc get no\nNAME                                               STATUS   ROLES                                 AGE                                                                                                            VERSION\nncpvblvhub-hubmaster-101.ncpvblvhub.t-mobile.lab   Ready    control-plane,master,monitor,worker   74d                                                                                                            v1.29.10+67d3387\nncpvblvhub-hubmaster-103.ncpvblvhub.t-mobile.lab   Ready    control-plane,master,monitor,worker   74d                                                                                                            v1.29.10+67d3387\nncpvblvhub-hubworker-101.ncpvblvhub.t-mobile.lab   Ready    gateway,worker                        74d                                                                                                            v1.29.10+67d3387\nncpvblvhub-hubworker-102.ncpvblvhub.t-mobile.lab   Ready    gateway,worker                        74d                                                                                                            v1.29.10+67d3387\n[root@dom16hub101-infra-manager ~]# oc get no\nNAME                                               STATUS   ROLES                                 AGE                                                                                                            VERSION\nncpvblvhub-hubmaster-101.ncpvblvhub.t-mobile.lab   Ready    control-plane,master,monitor,worker   74d                                                                                                            v1.29.10+67d3387\nncpvblvhub-hubmaster-102.ncpvblvhub.t-mobile.lab   Ready    control-plane,master,worker           54s                                                                                                            v1.29.10+67d3387\nncpvblvhub-hubmaster-103.ncpvblvhub.t-mobile.lab   Ready    control-plane,master,monitor,worker   74d                                                                                                            v1.29.10+67d3387\nncpvblvhub-hubworker-101.ncpvblvhub.t-mobile.lab   Ready    gateway,worker                        74d                                                                                                            v1.29.10+67d3387\nncpvblvhub-hubworker-102.ncpvblvhub.t-mobile.lab   Ready    gateway,worker                        74d                                                                                                            v1.29.10+67d3387\n[root@dom16hub101-infra-manager ~]# oc get machines.machine.openshift.io -n openshift-machine-apioc get machines.machine.openshift.io -n openshift-machine-api^C\n[root@dom16hub101-infra-manager ~]# oc get machines.machine.openshift.io -n openshift-machine-api\nNAME                              PHASE     TYPE   REGION   ZONE   AGE\nncpvblvhub-b6cjs-master-0         Running                          74d\nncpvblvhub-b6cjs-master-1         Running                          15m\nncpvblvhub-b6cjs-master-2         Running                          74d\nncpvblvhub-b6cjs-worker-0-mlq8w   Running                          74d\nncpvblvhub-b6cjs-worker-0-x5dc5   Running                          74d\n[root@dom16hub101-infra-manager ~]# oc get no\nNAME                                               STATUS   ROLES                                 AGE    VERSION\nncpvblvhub-hubmaster-101.ncpvblvhub.t-mobile.lab   Ready    control-plane,master,monitor,worker   74d    v1.29.10+67d3387\nncpvblvhub-hubmaster-102.ncpvblvhub.t-mobile.lab   Ready    control-plane,master,worker           110s   v1.29.10+67d3387\nncpvblvhub-hubmaster-103.ncpvblvhub.t-mobile.lab   Ready    control-plane,master,monitor,worker   74d    v1.29.10+67d3387\nncpvblvhub-hubworker-101.ncpvblvhub.t-mobile.lab   Ready    gateway,worker                        74d    v1.29.10+67d3387\nncpvblvhub-hubworker-102.ncpvblvhub.t-mobile.lab   Ready    gateway,worker                        74d    v1.29.10+67d3387\n[root@dom16hub101-infra-manager ~]#\n</code></pre>"},{"location":"openshift/disaster-management/hub-cluster-redeployment/master-replacment/#verifying-etcd","title":"Verifying etcd","text":"<p>1) Verify the etcd-guard- and etcd- pods are started and all containers of it are in running state in the openshift-etcd namespace. <pre><code>[root@dom16hub101-infra-manager ~]# oc get no\nNAME                                               STATUS   ROLES                                 AGE    VERSION\nncpvblvhub-hubmaster-101.ncpvblvhub.t-mobile.lab   Ready    control-plane,master,monitor,worker   74d    v1.29.10+67d3387\nncpvblvhub-hubmaster-102.ncpvblvhub.t-mobile.lab   Ready    control-plane,master,worker           110s   v1.29.10+67d3387\nncpvblvhub-hubmaster-103.ncpvblvhub.t-mobile.lab   Ready    control-plane,master,monitor,worker   74d    v1.29.10+67d3387\nncpvblvhub-hubworker-101.ncpvblvhub.t-mobile.lab   Ready    gateway,worker                        74d    v1.29.10+67d3387\nncpvblvhub-hubworker-102.ncpvblvhub.t-mobile.lab   Ready    gateway,worker                        74d    v1.29.10+67d3387\n[root@dom16hub101-infra-manager ~]# master=ncpvblvhub-hubmaster-102.ncpvblvhub.t-mobile.lab\n[root@dom16hub101-infra-manager ~]# oc exec -it $(oc -n openshift-etcd get pods -l k8s-app=etcd -o wide --no-headers |grep -v $master|head -n 1|awk {'print $1'}) -n openshift-etcd -- etcdctl endpoint health\nhttps://10.145.151.94:2379 is healthy: successfully committed proposal: took = 7.570208ms\nhttps://10.145.151.92:2379 is healthy: successfully committed proposal: took = 7.468077ms\n[root@dom16hub101-infra-manager ~]# oc exec -it $(oc -n openshift-etcd get pods -l k8s-app=etcd -o wide --no-headers |grep -v $master|head -n 1|awk {'print $1'}) -n openshift-etcd -- etcdctl member list -w table\n+------------------+---------+--------------------------------------------------+----------------------------+----------------------------+------------+\n|        ID        | STATUS  |                       NAME                       |         PEER ADDRS         |        CLIENT ADDRS        | IS LEARNER |\n+------------------+---------+--------------------------------------------------+----------------------------+----------------------------+------------+\n| 44ad9888985e068c | started | ncpvblvhub-hubmaster-101.ncpvblvhub.t-mobile.lab | https://10.145.151.92:2380 | https://10.145.151.92:2379 |      false |\n| 55fcb74f654c5538 | started | ncpvblvhub-hubmaster-102.ncpvblvhub.t-mobile.lab | https://10.145.151.93:2380 | https://10.145.151.93:2379 |      false |\n| f26d12a58d17e571 | started | ncpvblvhub-hubmaster-103.ncpvblvhub.t-mobile.lab | https://10.145.151.94:2380 | https://10.145.151.94:2379 |      false |\n+------------------+---------+--------------------------------------------------+----------------------------+----------------------------+------------+\n[root@dom16hub101-infra-manager ~]# oc exec -it $(oc -n openshift-etcd get pods -l k8s-app=etcd -o wide --no-headers |grep -v $master|head -n 1|awk {'print $1'}) -n openshift-etcd -- etcdctl endpoint health\nhttps://10.145.151.94:2379 is healthy: successfully committed proposal: took = 6.734404ms\nhttps://10.145.151.92:2379 is healthy: successfully committed proposal: took = 7.079121ms\n[root@dom16hub101-infra-manager ~]# oc get no\nNAME                                               STATUS   ROLES                                 AGE     VERSION\nncpvblvhub-hubmaster-101.ncpvblvhub.t-mobile.lab   Ready    control-plane,master,monitor,worker   74d     v1.29.10+67d3387\nncpvblvhub-hubmaster-102.ncpvblvhub.t-mobile.lab   Ready    control-plane,master,worker           3m55s   v1.29.10+67d3387\nncpvblvhub-hubmaster-103.ncpvblvhub.t-mobile.lab   Ready    control-plane,master,monitor,worker   74d     v1.29.10+67d3387\nncpvblvhub-hubworker-101.ncpvblvhub.t-mobile.lab   Ready    gateway,worker                        74d     v1.29.10+67d3387\nncpvblvhub-hubworker-102.ncpvblvhub.t-mobile.lab   Ready    gateway,worker                        74d     v1.29.10+67d3387\n[root@dom16hub101-infra-manager ~]# oc get mcp\nNAME     CONFIG                                             UPDATED   UPDATING   DEGRADED   MACHINECOUNT   READYMACHINECOUNT   UPDATEDMACHINECOUNT   DEGRADEDMACHINECOUNT   AGE\nmaster   rendered-master-d234bb112f0765116acd91ac75545416   True      False      False      3              3                   3                     0                      74d\nworker   rendered-worker-3e1f74a73d4a683cfbf22ced0aa2792a   False     True       True       2              1                   1                     1                      74d\n[root@dom16hub101-infra-manager ~]# oc exec -it $(oc -n openshift-etcd get pods -l k8s-app=etcd -o wide --no-headers |grep -v $master|head -n 1|awk {'print $1'}) -n openshift-etcd -- etcdctl endpoint health\nhttps://10.145.151.92:2379 is healthy: successfully committed proposal: took = 6.241732ms\nhttps://10.145.151.94:2379 is healthy: successfully committed proposal: took = 6.383176ms\n[root@dom16hub101-infra-manager ~]# oc exec -it $(oc -n openshift-etcd get pods -l k8s-app=etcd -o wide --no-headers |grep -v $master|head -n 1|awk {'print $1'}) -n openshift-etcd -- etcdctl endpoint health\nerror: unable to upgrade connection: container not found (\"etcd\")\n[root@dom16hub101-infra-manager ~]# oc exec -it $(oc -n openshift-etcd get pods -l k8s-app=etcd -o wide --no-headers |grep -v $master|head -n 1|awk {'print $1'}) -n openshift-etcd -- etcdctl endpoint health\nerror: unable to upgrade connection: container not found (\"etcd\")\n[root@dom16hub101-infra-manager ~]# oc get no\nNAME                                               STATUS   ROLES                                 AGE     VERSION\nncpvblvhub-hubmaster-101.ncpvblvhub.t-mobile.lab   Ready    control-plane,master,monitor,worker   74d     v1.29.10+67d3387\nncpvblvhub-hubmaster-102.ncpvblvhub.t-mobile.lab   Ready    control-plane,master,worker           5m51s   v1.29.10+67d3387\nncpvblvhub-hubmaster-103.ncpvblvhub.t-mobile.lab   Ready    control-plane,master,monitor,worker   74d     v1.29.10+67d3387\nncpvblvhub-hubworker-101.ncpvblvhub.t-mobile.lab   Ready    gateway,worker                        74d     v1.29.10+67d3387\nncpvblvhub-hubworker-102.ncpvblvhub.t-mobile.lab   Ready    gateway,worker                        74d     v1.29.10+67d3387\n[root@dom16hub101-infra-manager ~]# oc exec -it $(oc -n openshift-etcd get pods -l k8s-app=etcd -o wide --no-headers |grep -v $master|head -n 1|awk {'print $1'}) -n openshift-etcd -- etcdctl endpoint health\n\n{\"level\":\"warn\",\"ts\":\"2025-05-27T21:24:52.680683Z\",\"logger\":\"client\",\"caller\":\"v3@v3.5.14/retry_interceptor.go:63\",\"msg\":\"retrying of unary invoker failed\",\"target\":\"etcd-endpoints://0xc00022e000/10.145.151.92:2379\",\"attempt\":0,\"error\":\"rpc error: code = DeadlineExceeded desc = latest balancer error: last connection error: connection error: desc = \\\"transport: Error while dialing: dial tcp 10.145.151.92:2379: connect: connection refused\\\"\"}\nhttps://10.145.151.93:2379 is healthy: successfully committed proposal: took = 6.311573ms\nhttps://10.145.151.94:2379 is healthy: successfully committed proposal: took = 8.305729ms\nhttps://10.145.151.92:2379 is unhealthy: failed to commit proposal: context deadline exceeded\nError: unhealthy cluster\ncommand terminated with exit code 1\n[root@dom16hub101-infra-manager ~]# oc get co\nNAME                                       VERSION   AVAILABLE   PROGRESSING   DEGRADED   SINCE   MESSAGE\nauthentication                             4.16.24   True        False         False      3h48m\nbaremetal                                  4.16.24   True        False         False      74d\ncloud-controller-manager                   4.16.24   True        False         False      74d\ncloud-credential                           4.16.24   True        False         False      74d\ncluster-autoscaler                         4.16.24   True        False         False      74d\nconfig-operator                            4.16.24   True        False         False      74d\nconsole                                    4.16.24   True        False         False      74d\ncontrol-plane-machine-set                  4.16.24   True        False         False      74d\ncsi-snapshot-controller                    4.16.24   True        False         False      74d\ndns                                        4.16.24   True        False         False      74d\netcd                                       4.16.24   True        True          False      74d     NodeInstallerProgressing: 1 node is at revision 15; 2 nodes are at revision 17\nimage-registry                             4.16.24   True        False         False      74d\ningress                                    4.16.24   True        False         False      74d\ninsights                                   4.16.24   True        False         False      74d\nkube-apiserver                             4.16.24   True        True          False      74d     NodeInstallerProgressing: 2 nodes are at revision 60; 1 node is at revision 61\nkube-controller-manager                    4.16.24   True        False         False      74d\nkube-scheduler                             4.16.24   True        False         False      74d\nkube-storage-version-migrator              4.16.24   True        False         False      13d\nmachine-api                                4.16.24   True        False         False      74d\nmachine-approver                           4.16.24   True        False         False      74d\nmachine-config                             4.16.24   True        False         True       74d     Failed to resync 4.16.24 because: error during syncRequiredMachineConfigPools: [context deadline exceeded, failed to update clusteroperator: [client rate limiter Wait returned an error: context deadline exceeded, error MachineConfigPool worker is not ready, retrying. Status: (pool degraded: true total: 2, ready 1, updated: 1, unavailable: 1)]]\nmarketplace                                4.16.24   True        False         False      74d\nmonitoring                                 4.16.24   Unknown     True          Unknown    35m     Rolling out the stack.\nnetwork                                    4.16.24   True        False         False      74d\nnode-tuning                                4.16.24   True        False         False      6m20s\nopenshift-apiserver                        4.16.24   True        False         False      4h17m\nopenshift-controller-manager               4.16.24   True        False         False      74d\nopenshift-samples                          4.16.24   True        False         False      74d\noperator-lifecycle-manager                 4.16.24   True        False         False      74d\noperator-lifecycle-manager-catalog         4.16.24   True        False         False      74d\noperator-lifecycle-manager-packageserver   4.16.24   True        False         False      74d\nservice-ca                                 4.16.24   True        False         False      74d\nstorage                                    4.16.24   True        False         False      74d\n\n[root@dom16hub101-infra-manager ~]# watch -n 5 oc get co\n[root@dom16hub101-infra-manager ~]# oc exec -it $(oc -n openshift-etcd get pods -l k8s-app=etcd -o wide --no-headers |grep -v $master|head -n 1|awk {'print $1'}) -n openshift-etcd -- etcdctl endpoint health\nhttps://10.145.151.93:2379 is healthy: successfully committed proposal: took = 6.998079ms\nhttps://10.145.151.92:2379 is healthy: successfully committed proposal: took = 7.999787ms\nhttps://10.145.151.94:2379 is healthy: successfully committed proposal: took = 7.997677ms\n[root@dom16hub101-infra-manager ~]# oc patch etcd/cluster --type=merge -p '{\"spec\": {\"unsupportedConfigOverrides\": null}}'\netcd.operator.openshift.io/cluster patched\n[root@dom16hub101-infra-manager ~]# \n</code></pre>"},{"location":"openshift/disaster-management/hub-cluster-redeployment/master-replacment/#adding-back-the-osds","title":"Adding back the OSDs","text":"<p>1) The OSDs are automatically added, after the PVs are created by the Local Storage Operator.</p> <p>2) After adding the labels back to the node (which was applied initially during the deployment) including the <code>cluster.ocs.openshift.io/openshift-storage</code>, the Local Storage Operator\u2019s two daemonsets pods will be scheduled on this node as well, namely the diskmaker discovery and the diskmaker manager. </p> <p>3) The discovery will inspect the node for available disks while the manager will create the PVs which will be used by ODF.</p> <p>oc get pods -n openshift-local-storage -o wide</p> <p>4) After the new PVs are created the new OSDs deployments will be recreated and OSD pods and mon pod will start automatically.</p> <p>5) If those would not start automatically for some reason, the rook-ceph-operator pod shall be restarted.</p> <pre><code>[root@dom16hub101-infra-manager ~]#oc get pods -n openshift-local-storage -o wide\nNAME                                      READY   STATUS    RESTARTS        AGE   IP             NODE                                               NOMINATED NODE   READINESS GATES\ndiskmaker-discovery-6mcrl                 2/2     Running   4               71d   172.20.0.163   ncpvblvhub-hubmaster-103.ncpvblvhub.t-mobile.lab   &lt;none&gt;           &lt;none&gt;\ndiskmaker-discovery-9htrb                 2/2     Running   6               71d   172.20.2.141   ncpvblvhub-hubmaster-101.ncpvblvhub.t-mobile.lab   &lt;none&gt;           &lt;none&gt;\ndiskmaker-manager-fbzkp                   2/2     Running   4               71d   172.20.0.164   ncpvblvhub-hubmaster-103.ncpvblvhub.t-mobile.lab   &lt;none&gt;           &lt;none&gt;\ndiskmaker-manager-h6r2t                   2/2     Running   6               71d   172.20.2.142   ncpvblvhub-hubmaster-101.ncpvblvhub.t-mobile.lab   &lt;none&gt;           &lt;none&gt;\nlocal-storage-operator-6d968c9989-w4vp4   1/1     Running   2 (3h42m ago)   56d   172.23.0.66    ncpvblvhub-hubworker-101.ncpvblvhub.t-mobile.lab   &lt;none&gt;           &lt;none&gt;\n[root@dom16hub101-infra-manager ~]# oc get no -l  cluster.ocs.openshift.io/openshift-storage\nNAME                                               STATUS   ROLES                                 AGE   VERSION\nncpvblvhub-hubmaster-101.ncpvblvhub.t-mobile.lab   Ready    control-plane,master,monitor,worker   74d   v1.29.10+67d3387\nncpvblvhub-hubmaster-103.ncpvblvhub.t-mobile.lab   Ready    control-plane,master,monitor,worker   74d   v1.29.10+67d3387\n[root@dom16hub101-infra-manager ~]# oc get no\nNAME                                               STATUS   ROLES                                 AGE   VERSION\nncpvblvhub-hubmaster-101.ncpvblvhub.t-mobile.lab   Ready    control-plane,master,monitor,worker   74d   v1.29.10+67d3387\nncpvblvhub-hubmaster-102.ncpvblvhub.t-mobile.lab   Ready    control-plane,master,worker           13m   v1.29.10+67d3387\nncpvblvhub-hubmaster-103.ncpvblvhub.t-mobile.lab   Ready    control-plane,master,monitor,worker   74d   v1.29.10+67d3387\nncpvblvhub-hubworker-101.ncpvblvhub.t-mobile.lab   Ready    gateway,worker                        74d   v1.29.10+67d3387\nncpvblvhub-hubworker-102.ncpvblvhub.t-mobile.lab   Ready    gateway,worker                        74d   v1.29.10+67d3387\n[root@dom16hub101-infra-manager ~]# oc label node ncpvblvhub-hubmaster-102.ncpvblvhub.t-mobile.lab cluster.ocs.openshift.io/openshift-storage=\nnode/ncpvblvhub-hubmaster-102.ncpvblvhub.t-mobile.lab labeled\n[root@dom16hub101-infra-manager ~]# oc get no -l  cluster.ocs.openshift.io/openshift-storage\nNAME                                               STATUS   ROLES                                 AGE   VERSION\nncpvblvhub-hubmaster-101.ncpvblvhub.t-mobile.lab   Ready    control-plane,master,monitor,worker   74d   v1.29.10+67d3387\nncpvblvhub-hubmaster-102.ncpvblvhub.t-mobile.lab   Ready    control-plane,master,worker           15m   v1.29.10+67d3387\nncpvblvhub-hubmaster-103.ncpvblvhub.t-mobile.lab   Ready    control-plane,master,monitor,worker   74d   v1.29.10+67d3387\n</code></pre> <p>After the new PVs are created the new OSDs deployments will be recreated and OSD pods and mon pod will start automatically. If those would not start automatically for some reason, the rook-ceph-operator pod shall be restarted.</p> <pre><code>[root@dom16hub101-infra-manager ~]# oc get pods -n openshift-local-storage -o wide\nNAME                                      READY   STATUS    RESTARTS        AGE   IP             NODE                                               NOMINATED NODE   READINESS GATES\ndiskmaker-discovery-6mcrl                 2/2     Running   4               71d   172.20.0.163   ncpvblvhub-hubmaster-103.ncpvblvhub.t-mobile.lab   &lt;none&gt;           &lt;none&gt;\ndiskmaker-discovery-9htrb                 2/2     Running   6               71d   172.20.2.141   ncpvblvhub-hubmaster-101.ncpvblvhub.t-mobile.lab   &lt;none&gt;           &lt;none&gt;\ndiskmaker-discovery-b6bxh                 2/2     Running   0               26s   172.21.0.33    ncpvblvhub-hubmaster-102.ncpvblvhub.t-mobile.lab   &lt;none&gt;           &lt;none&gt;\ndiskmaker-manager-8kpkt                   2/2     Running   0               26s   172.21.0.31    ncpvblvhub-hubmaster-102.ncpvblvhub.t-mobile.lab   &lt;none&gt;           &lt;none&gt;\ndiskmaker-manager-fbzkp                   2/2     Running   4               71d   172.20.0.164   ncpvblvhub-hubmaster-103.ncpvblvhub.t-mobile.lab   &lt;none&gt;           &lt;none&gt;\ndiskmaker-manager-h6r2t                   2/2     Running   6               71d   172.20.2.142   ncpvblvhub-hubmaster-101.ncpvblvhub.t-mobile.lab   &lt;none&gt;           &lt;none&gt;\nlocal-storage-operator-6d968c9989-w4vp4   1/1     Running   2 (3h46m ago)   57d   172.23.0.66    ncpvblvhub-hubworker-101.ncpvblvhub.t-mobile.lab   &lt;none&gt;           &lt;none&gt;\n[root@dom16hub101-infra-manager ~]# oc get co\nNAME                                       VERSION   AVAILABLE   PROGRESSING   DEGRADED   SINCE   MESSAGE\nauthentication                             4.16.24   True        False         False      3h57m\nbaremetal                                  4.16.24   True        False         False      74d\ncloud-controller-manager                   4.16.24   True        False         False      74d\ncloud-credential                           4.16.24   True        False         False      74d\ncluster-autoscaler                         4.16.24   True        False         False      74d\nconfig-operator                            4.16.24   True        False         False      74d\nconsole                                    4.16.24   True        False         False      74d\ncontrol-plane-machine-set                  4.16.24   True        False         False      74d\ncsi-snapshot-controller                    4.16.24   True        False         False      74d\ndns                                        4.16.24   True        False         False      74d\netcd                                       4.16.24   True        False         False      74d\nimage-registry                             4.16.24   True        False         False      74d\ningress                                    4.16.24   True        False         False      74d\ninsights                                   4.16.24   True        False         False      74d\nkube-apiserver                             4.16.24   True        False         False      74d\nkube-controller-manager                    4.16.24   True        False         False      74d\nkube-scheduler                             4.16.24   True        False         False      74d\nkube-storage-version-migrator              4.16.24   True        False         False      13d\nmachine-api                                4.16.24   True        False         False      74d\nmachine-approver                           4.16.24   True        False         False      74d\nmachine-config                             4.16.24   True        False         True       74d     Failed to resync 4.16.24 because: error during syncRequiredMachineConfigPools: [context deadline exceeded, failed to update clusteroperator: [client rate limiter Wait returned an error: context deadline exceeded, error MachineConfigPool worker is not ready, retrying. Status: (pool degraded: true total: 2, ready 1, updated: 1, unavailable: 1)]]\nmarketplace                                4.16.24   True        False         False      74d\nmonitoring                                 4.16.24   Unknown     True          Unknown    45m     Rolling out the stack.\nnetwork                                    4.16.24   True        False         False      74d\nnode-tuning                                4.16.24   True        False         False      15m\nopenshift-apiserver                        4.16.24   True        False         False      4h26m\nopenshift-controller-manager               4.16.24   True        False         False      74d\nopenshift-samples                          4.16.24   True        False         False      74d\noperator-lifecycle-manager                 4.16.24   True        False         False      74d\noperator-lifecycle-manager-catalog         4.16.24   True        False         False      74d\noperator-lifecycle-manager-packageserver   4.16.24   True        False         False      74d\nservice-ca                                 4.16.24   True        False         False      74d\nstorage                                    4.16.24   True        False         False      74d\n\n[root@dom16hub101-infra-manager ~]# oc exec -it $(oc get pod -n openshift-storage -l app=rook-ceph-operator -o name) -n openshift-storage -- ceph status -c /var/lib/rook/openshift-storage/openshift-storage.config\n  cluster:\n    id:     27b507a3-b496-4549-9271-a3bc0705dd13\n    health: HEALTH_WARN\n            Degraded data redundancy: 87816/433023 objects degraded (20.280%), 68 pgs degraded, 74 pgs undersized\n\n  services:\n    mon: 3 daemons, quorum a,b,e (age 3m)\n    mgr: b(active, since 3h), standbys: a\n    mds: 1/1 daemons up, 1 hot standby\n    osd: 6 osds: 6 up (since 112s), 6 in (since 2m); 109 remapped pgs\n    rgw: 1 daemon active (1 hosts, 1 zones)\n\n  data:\n    volumes: 1/1 healthy\n    pools:   12 pools, 201 pgs\n    objects: 144.34k objects, 541 GiB\n    usage:   1.2 TiB used, 20 TiB / 21 TiB avail\n    pgs:     0.498% pgs not active\n             87816/433023 objects degraded (20.280%)\n             55928/433023 objects misplaced (12.916%)\n             91 active+clean\n             68 active+undersized+degraded+remapped+backfill_wait\n             35 active+remapped+backfill_wait\n             6  active+undersized+remapped+backfill_wait\n             1  peering\n\n  io:\n    client:   6.1 MiB/s rd, 475 KiB/s wr, 7 op/s rd, 39 op/s wr\n    recovery: 386 MiB/s, 0 keys/s, 101 objects/s\n\n\n[root@dom16hub101-infra-manager ~]# oc get no\nNAME                                               STATUS   ROLES                                 AGE   VERSION\nncpvblvhub-hubmaster-101.ncpvblvhub.t-mobile.lab   Ready    control-plane,master,monitor,worker   74d   v1.29.10+67d3387\nncpvblvhub-hubmaster-102.ncpvblvhub.t-mobile.lab   Ready    control-plane,master,worker           19m   v1.29.10+67d3387\nncpvblvhub-hubmaster-103.ncpvblvhub.t-mobile.lab   Ready    control-plane,master,monitor,worker   74d   v1.29.10+67d3387\nncpvblvhub-hubworker-101.ncpvblvhub.t-mobile.lab   Ready    gateway,worker                        74d   v1.29.10+67d3387\nncpvblvhub-hubworker-102.ncpvblvhub.t-mobile.lab   Ready    gateway,worker                        74d   v1.29.10+67d3387\n[root@dom16hub101-infra-manager ~]# oc edit no ncpvblvhub-hubmaster-102.ncpvblvhub.t-mobile.lab\nEdit cancelled, no changes made.\n[root@dom16hub101-infra-manager ~]# oc get no --show-labels ncpvblvhub-hubmaster-101.ncpvblvhub.t-mobile.lab\nNAME                                               STATUS   ROLES                                 AGE   VERSION            LABELS\nncpvblvhub-hubmaster-101.ncpvblvhub.t-mobile.lab   Ready    control-plane,master,monitor,worker   74d   v1.29.10+67d3387   beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,cluster.ocs.openshift.io/openshift-storage=,kubernetes.io/arch=amd64,kubernetes.io/hostname=ncpvblvhub-hubmaster-101.ncpvblvhub.t-mobile.lab,kubernetes.io/os=linux,node-role.kubernetes.io/control-plane=,node-role.kubernetes.io/master=,node-role.kubernetes.io/monitor=,node-role.kubernetes.io/worker=,node.openshift.io/os_id=rhcos\n[root@dom16hub101-infra-manager ~]# oc label node ncpvblvhub-hubmaster-102.ncpvblvhub.t-mobile.lab node-role.kubernetes.io/monitor=\nnode/ncpvblvhub-hubmaster-102.ncpvblvhub.t-mobile.lab labeled\n[root@dom16hub101-infra-manager ~]# oc get no\nNAME                                               STATUS   ROLES                                 AGE   VERSION\nncpvblvhub-hubmaster-101.ncpvblvhub.t-mobile.lab   Ready    control-plane,master,monitor,worker   74d   v1.29.10+67d3387\nncpvblvhub-hubmaster-102.ncpvblvhub.t-mobile.lab   Ready    control-plane,master,monitor,worker   20m   v1.29.10+67d3387\nncpvblvhub-hubmaster-103.ncpvblvhub.t-mobile.lab   Ready    control-plane,master,monitor,worker   74d   v1.29.10+67d3387\nncpvblvhub-hubworker-101.ncpvblvhub.t-mobile.lab   Ready    gateway,worker                        74d   v1.29.10+67d3387\nncpvblvhub-hubworker-102.ncpvblvhub.t-mobile.lab   Ready    gateway,worker                        74d   v1.29.10+67d3387\n\n\n[root@dom16hub101-infra-manager ~]# oc exec -it $(oc get pod -n openshift-storage -l app=rook-ceph-operator -o name) -n openshift-storage -- ceph status -c /var/lib/rook/openshift-storage/openshift-storage.config\n  cluster:\n    id:     27b507a3-b496-4549-9271-a3bc0705dd13\n    health: HEALTH_WARN\n            Degraded data redundancy: 44107/433023 objects degraded (10.186%), 39 pgs degraded, 43 pgs undersized\n\n  services:\n    mon: 3 daemons, quorum a,b,e (age 9m)\n    mgr: b(active, since 3h), standbys: a\n    mds: 1/1 daemons up, 1 hot standby\n    osd: 6 osds: 6 up (since 8m), 6 in (since 8m); 77 remapped pgs\n    rgw: 1 daemon active (1 hosts, 1 zones)\n\n  data:\n    volumes: 1/1 healthy\n    pools:   12 pools, 201 pgs\n    objects: 144.34k objects, 541 GiB\n    usage:   1.4 TiB used, 20 TiB / 21 TiB avail\n    pgs:     44107/433023 objects degraded (10.186%)\n             55928/433023 objects misplaced (12.916%)\n             123 active+clean\n             37  active+undersized+degraded+remapped+backfill_wait\n             35  active+remapped+backfill_wait\n             4   active+undersized+remapped+backfill_wait\n             2   active+undersized+degraded+remapped+backfilling\n\n  io:\n    client:   7.9 MiB/s rd, 355 KiB/s wr, 8 op/s rd, 27 op/s wr\n    recovery: 550 MiB/s, 1 keys/s, 145 objects/s\n\n\n[root@dom16hub101-infra-manager ~]# oc exec -it $(oc get pod -n openshift-storage -l app=rook-ceph-operator -o name) -n openshift-storage -- ceph status -c /var/lib/rook/openshift-storage/openshift-storage.config\n  cluster:\n    id:     27b507a3-b496-4549-9271-a3bc0705dd13\n    health: HEALTH_OK\n\n  services:\n    mon: 3 daemons, quorum a,b,e (age 18m)\n    mgr: b(active, since 3h), standbys: a\n    mds: 1/1 daemons up, 1 hot standby\n    osd: 6 osds: 6 up (since 16m), 6 in (since 17m); 30 remapped pgs\n    rgw: 1 daemon active (1 hosts, 1 zones)\n\n  data:\n    volumes: 1/1 healthy\n    pools:   12 pools, 201 pgs\n    objects: 144.51k objects, 542 GiB\n    usage:   1.6 TiB used, 19 TiB / 21 TiB avail\n    pgs:     46224/433527 objects misplaced (10.662%)\n             171 active+clean\n             28  active+remapped+backfill_wait\n             2   active+remapped+backfilling\n\n  io:\n    client:   5.7 MiB/s rd, 396 KiB/s wr, 8 op/s rd, 37 op/s wr\n    recovery: 155 MiB/s, 41 objects/s\n\n[root@dom16hub101-infra-manager ~]#\n</code></pre> <p>Continue the same steps for <code>master1</code> and <code>master3</code> as well, if you want to replace all three master nodes. </p>"},{"location":"openshift/disaster-management/infra-manager-redeployment/","title":"Infra-manager node redeployment after hub and cwl clusters are deployed.","text":""},{"location":"openshift/disaster-management/infra-manager-redeployment/#issue-description","title":"Issue description:","text":"<ul> <li>someone accidentally formatted and reinstalled the OS on the infra-manager host at the Eastlink PP site. This impacted the hub cluster\u2019s lifecycle management capabilities. The infra-Quay registry was lost, along with all hub cluster installation files and manifests previously stored on the infra-manager. Most critically, we also lost the SSH private key that was stored on this host</li> </ul>"},{"location":"openshift/disaster-management/infra-manager-redeployment/#impact-analysis","title":"Impact Analysis:","text":""},{"location":"openshift/disaster-management/infra-manager-redeployment/#what-are-all-the-impact-from-operational-point-of-view","title":"what are all the impact from operational point of view?","text":"<ul> <li>Hub Cluster LCM is non-functional due to infra-quay is lost.</li> <li>Infra Quay registry is completely lost and must be redeployed. </li> <li>SSH private key used for access to hub and CWL cluster nodes is lost. So, we can\u2019t troubleshoot anything at the host level on both clusters.</li> <li>No SSH access to hub or CWL nodes, blocking all remote administration and recovery tasks.</li> <li>Hub installation files and other critical configurations are permanently lost.</li> </ul>"},{"location":"openshift/disaster-management/infra-manager-redeployment/#restore-plan-highlevel","title":"Restore plan (Highlevel)","text":"<ul> <li>Access Reinstalled Infra-Manager Node</li> <li>Installing the necessary packages for the Hub Cluster</li> <li>Installing the mirror-registry(infra-quay) on the infrastructure manager node</li> <li>Add the infra-quay registry to the trusted registries</li> <li>Configuring an organization in Quay</li> <li>Copy OpenShift related binaries</li> <li>Mirroring images to infra-quay mirror-registry</li> <li>Generate the ssh key for authenticating with core OS.</li> <li>Configure Quay SSL to Match Hub Cluster TLS</li> <li>Validate Operator Catalog, ITMS, and DTMS</li> <li>Restore SSH Key for Hub Cluster</li> <li>Restore SSH Key for CWL Cluster</li> <li>Verification Testing</li> </ul>"},{"location":"openshift/disaster-management/infra-manager-redeployment/#implementation","title":"Implementation","text":""},{"location":"openshift/disaster-management/infra-manager-redeployment/#access-reinstalled-infra-manager-node","title":"Access Reinstalled Infra-Manager Node","text":"<p>1) Login to Customer VPN, followed by ssh to infra-manager node using username <code>ncpuser</code> with given password. </p> <pre><code>    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n    \u2502                 \u2022 MobaXterm Personal Edition v25.1 \u2022                 \u2502\n    \u2502               (SSH client, X server and network tools)               \u2502\n    \u2502                                                                      \u2502\n    \u2502 \u2b9e SSH session to ncpuser@10.236.6.230                                \u2502\n    \u2502   \u2022 Direct SSH      :  \u2713                                             \u2502\n    \u2502   \u2022 SSH compression :  \u2713                                             \u2502\n    \u2502   \u2022 SSH-browser     :  \u2713                                             \u2502\n    \u2502   \u2022 X11-forwarding  :  \u2713  (remote display is forwarded through SSH)  \u2502\n    \u2502                                                                      \u2502\n    \u2502 \u2b9e For more info, ctrl+click on help or visit our website.            \u2502\n    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\nActivate the web console with: systemctl enable --now cockpit.socket\n\nRegister this system with Red Hat Insights: insights-client --register\nCreate an account or view all your systems at https://red.ht/insights-dashboard\nLast login: Fri Jun 27 15:36:03 2025 from 10.192.52.140\n[ncpuser@ncputility ~]$\n[ncpuser@ncputility ~]$ sudo su -\n[sudo] password for ncpuser:\n[root@ncputility ~]# cat /etc/redhat-release\nRed Hat Enterprise Linux release 9.4 (Plow)\n[root@ncputility ~]#\n</code></pre>"},{"location":"openshift/disaster-management/infra-manager-redeployment/#installing-the-necessary-packages-for-the-hub-cluster","title":"Installing the necessary packages for the Hub Cluster","text":"<p>1) Login to the node via SSH, using the user created with Administrator privileges. Root user is not needed.     For higher security, key based authentication can be used for SSH and the password authentication may     be disabled.</p> <p>2) Verify the hostname of the node using <code>hostname -f</code> command</p> <p>3) Copy the <code>rhel_rhcos.tar.gz</code> to the node. Uncompress it using the command tar <code>rhel_rhcos.tar.gz.-zxvf</code></p> <p>4) Create a directory to mount the ISO file</p> <pre><code>sudo cp rhel-9.4-x86_64-dvd.iso /mnt\nsudo mkdir -p /mnt/rhel9.4\necho '/mnt/rhel-9.4-x86_64-dvd.iso /mnt/rhel9.4/ iso9660 loop 0 0' | sudo\ntee -a /etc/fstab\nsudo mount -a\n</code></pre> <p>5) Create a local repo from the ISO which is mounted:</p> <pre><code>cat &lt;&lt; 'EOF' | sudo tee -a /etc/yum.repos.d/rhel9.4dvd.repo\n[BaseOS]\nname=BaseOS Packages Red Hat Enterprise Linux 9\nmetadata_expire=-1\ngpgcheck=1\nenabled=1\nbaseurl=file:///mnt/rhel9.4/BaseOS/\ngpgkey=file:///etc/pki/rpm-gpg/RPM-GPG-KEY-redhat-release\n[AppStream]\nname=AppStream Packages Red Hat Enterprise Linux 9\nmetadata_expire=-1\ngpgcheck=1\nenabled=1\nbaseurl=file:///mnt/rhel9.4/AppStream/\ngpgkey=file:///etc/pki/rpm-gpg/RPM-GPG-KEY-redhat-release\nEOF\n</code></pre> <p>6) Install the necessary packages:</p> <pre><code>[root@ncputility ~]# sudo dnf clean all\nUpdating Subscription Management repositories.\nUnable to read consumer identity\n\nThis system is not registered with an entitlement server. You can use \"rhc\" or \"subscription-manager\" to register.\n\n13 files removed\n[root@ncputility ~]# sudo dnf repolist enabled\nUpdating Subscription Management repositories.\nUnable to read consumer identity\n\nThis system is not registered with an entitlement server. You can use \"rhc\" or \"subscription-manager\" to register.\n\nrepo id                                                                     repo name\nAppStream                                                                   AppStream Packages Red Hat Enterprise Linux 9\nBaseOS                                                                      BaseOS Packages Red Hat Enterprise Linux 9\n[root@ncputility ~]# sudo dnf install wget httpd mkpasswd nmstate podman tcpdump rsync tmux bind-utils chrony net-tools unzip bash-completion\nUpdating Subscription Management repositories.\nUnable to read consumer identity\n\nThis system is not registered with an entitlement server. You can use \"rhc\" or \"subscription-manager\" to register.\n\nBaseOS Packages Red Hat Enterprise Linux 9                                                                                                                  136 MB/s | 2.1 MB     00:00    \nAppStream Packages Red Hat Enterprise Linux 9                                                                                                               208 MB/s | 7.0 MB     00:00    \nPackage wget-1.21.1-7.el9.x86_64 is already installed.\nPackage podman-2:4.9.4-0.1.el9.x86_64 is already installed.\nPackage tcpdump-14:4.99.0-9.el9.x86_64 is already installed.\nPackage rsync-3.2.3-19.el9.x86_64 is already installed.\nPackage bind-utils-32:9.16.23-15.el9.x86_64 is already installed.\nPackage chrony-4.5-1.el9.x86_64 is already installed.\nPackage net-tools-2.0-0.62.20160912git.el9.x86_64 is already installed.\nPackage unzip-6.0-56.el9.x86_64 is already installed.\nPackage bash-completion-1:2.11-5.el9.noarch is already installed.\nDependencies resolved.\n============================================================================================================================================================================================\n Package                                            Architecture                           Version                                          Repository                                 Size\n============================================================================================================================================================================================\nInstalling:\n httpd                                              x86_64                                 2.4.57-8.el9                                     \n ** output **\n\nInstalled:\n  apr-1.7.0-12.el9_3.x86_64         apr-util-1.6.1-23.el9.x86_64            apr-util-bdb-1.6.1-23.el9.x86_64        apr-util-openssl-1.6.1-23.el9.x86_64    httpd-2.4.57-8.el9.x86_64       \n  httpd-core-2.4.57-8.el9.x86_64    httpd-filesystem-2.4.57-8.el9.noarch    httpd-tools-2.4.57-8.el9.x86_64         mkpasswd-5.5.9-4.el9.x86_64             mod_http2-2.0.26-1.el9.x86_64   \n  mod_lua-2.4.57-8.el9.x86_64       nmstate-2.2.25-1.el9.x86_64             redhat-logos-httpd-90.4-2.el9.noarch    tmux-3.2a-5.el9.x86_64                  whois-nls-5.5.9-4.el9.noarch    \n\nComplete!\n[root@ncputility ~]# \n</code></pre> <p>7) Start and enable HTTP server:</p> <pre><code>[root@ncputility ~]# sudo systemctl start httpd\n[root@ncputility ~]# sudo systemctl enable httpd\nCreated symlink /etc/systemd/system/multi-user.target.wants/httpd.service \u2192 /usr/lib/systemd/system/httpd.service.\n[root@ncputility ~]# \n</code></pre> <p>8) Allow ports 80 for the webserver (later it will be used by the Hub Cluster for fetching CoreOS) and 8443 which will be the mirror-registry\u2019s port.</p> <pre><code>[root@ncputility ~]# sudo firewall-cmd --zone=public --add-port=80/tcp --permanent\nsuccess\n[root@ncputility ~]# sudo firewall-cmd --zone=public --add-port=8443/tcp --permanent\nsuccess\n[root@ncputility ~]# sudo firewall-cmd \u2013reload\nusage: 'firewall-cmd --help' for usage information or see firewall-cmd(1) man page\nfirewall-cmd: error: unrecognized arguments: \u2013reload\n[root@ncputility ~]# sudo firewall-cmd --reload\nsuccess\n[root@ncputility ~]# sudo firewall-cmd --list-all\npublic (active)\n  target: default\n  icmp-block-inversion: no\n  interfaces: 140\n  sources: \n  services: cockpit dhcpv6-client ssh vnc-server\n  ports: 5902/tcp 53/tcp 53/udp 80/tcp 8443/tcp\n  protocols: \n  forward: yes\n  masquerade: no\n  forward-ports: \n  source-ports: \n  icmp-blocks: \n  rich rules: \n[root@ncputility ~]# \n</code></pre> <p>9) Set NTP servers, Udate the correct ntp ip on the configuration file <code>/etc/chrony.conf</code></p> <pre><code>[root@ncputility ~]# sudo vi /etc/chrony.conf^C\n[root@ncputility ~]# chronyc sources\nMS Name/IP address         Stratum Poll Reach LastRx Last sample               \n===============================================================================\n^* 135.112.203.53                2   7   377    43   -221ns[  +11us] +/-   22ms\n[root@ncputility ~]# timedatectl \n               Local time: Wed 2025-03-26 17:19:06 EDT\n           Universal time: Wed 2025-03-26 21:19:06 UTC\n                 RTC time: Wed 2025-03-26 21:19:06\n                Time zone: America/New_York (EDT, -0400)\nSystem clock synchronized: yes\n              NTP service: active\n          RTC in local TZ: no\n[root@ncputility ~]# \n</code></pre> <p>10) Modify the chrony configuration according to the available NTP servers:</p> <pre><code>cat /etc/chrony.conf | grep -v '#'\nserver 10.171.8.4 iburst\nserver 10.171.8.5 iburst\nsourcedir /run/chrony-dhcp\ndriftfile /var/lib/chrony/drift\nmakestep 1.0 3\nrtcsync\nkeyfile /etc/chrony.keys\nntsdumpdir /var/lib/chrony\nleapsectz right/UTC\nlogdir /var/log/chrony\n</code></pre> <p>11) Enable and restart chronyd</p> <pre><code>sudo systemctl enable chronyd\nsudo systemctl restart chronyd\n</code></pre> <p>12) Verify NTP sources:</p> <pre><code>[root@ncputility ~]# sudo vi /etc/chrony.conf^C\n[root@ncputility ~]# chronyc sources\nMS Name/IP address         Stratum Poll Reach LastRx Last sample               \n===============================================================================\n^* 135.112.203.53                2   7   377    43   -221ns[  +11us] +/-   22ms\n[root@ncputility ~]#\n</code></pre>"},{"location":"openshift/disaster-management/infra-manager-redeployment/#installing-the-mirror-registry-on-the-infrastructure-manager-node","title":"Installing the mirror-registry on the infrastructure manager node","text":"<p>1) Uncompress the mirror-registry, which can be found in the previously uncompressed ncp_tools.tar.gz in compressed format. This will be a local registry from where the Hub Cluster can fetch its images.</p> <pre><code>[root@ncputility ncp]# tar -xvf ncp_tools.tar.gz\nbutane-amd64\nbutane-amd64.sha256\nbutane-windows-amd64.exe\nbutane-windows-amd64.exe.sha256\nhelm-linux-amd64\nhelm-linux-amd64.sha256\nhelm-windows-amd64.exe\nhelm-windows-amd64.exe.sha256\nkustomize_v5.4.3_linux_amd64.tar.gz\nkustomize_v5.4.3_linux_amd64.tar.gz.sha256\nmirror-registry.tar.gz\nmirror-registry.tar.gz.sha256\noc-mirror.tar.gz\noc-mirror.tar.gz.sha256\nopenshift-client-linux.tar.gz\nopenshift-client-linux.tar.gz.sha256\nopenshift-client-windows.zip\nopenshift-client-windows.zip.sha256\nopenshift-install-linux.tar.gz\nopenshift-install-linux.tar.gz.sha256\nopm-linux.tar.gz\nopm-linux.tar.gz.sha256\nopm-windows.tar.gz\nopm-windows.tar.gz.sha256\ns3cmd-2.4.0.tar.gz\ns3cmd-2.4.0.tar.gz.sha256\n[root@ncputility ncp]#\n</code></pre> <p>2) Create a folder for the quay root:</p> <pre><code>[root@ncputility ~]#  mkdir -p /opt/quayroot\n[root@ncputility ~]# \n</code></pre> <p>The owner shall be the Linux user who is installing it. For example: sudo chown  /opt/quayroot/ <pre><code>a. extract the mirror-registry file.\n\n```\n[root@ncputility ncp]# tar -xvf mirror-registry.tar.gz\nimage-archive.tar\nexecution-environment.tar\nmirror-registry\n[root@ncputility ncp]#\n\n```\n</code></pre> <p>2.1) Quay installation</p> <pre><code>[root@ncputility ncp]# ./mirror-registry install --initUser infraquay --initPassword Nokia12234 --quayRoot /opt/quayroot\n\n   __   __\n  /  \\ /  \\     ______   _    _     __   __   __\n / /\\ / /\\ \\   /  __  \\ | |  | |   /  \\  \\ \\ / /\n/ /  / /  \\ \\  | |  | | | |  | |  / /\\ \\  \\   /\n\\ \\  \\ \\  / /  | |__| | | |__| | / ____ \\  | |\n \\ \\/ \\ \\/ /   \\_  ___/  \\____/ /_/    \\_\\ |_|\n  \\__/ \\__/      \\ \\__\n                  \\___\\ by Red Hat\n Build, Store, and Distribute your Containers\n\nINFO[2025-03-26 17:25:38] Install has begun                            \nINFO[2025-03-26 17:25:38] Found execution environment at /root/ncp/execution-environment.tar \nINFO[2025-03-26 17:25:38] Loading execution environment from execution-environment.tar \nINFO[2025-03-26 17:25:49] Detected an installation to localhost        \nINFO[2025-03-26 17:25:49] Did not find SSH key in default location. Attempting to set up SSH keys. \nINFO[2025-03-26 17:25:49] Generating SSH Key                           \nINFO[2025-03-26 17:25:50] Generated SSH Key at /root/.ssh/quay_installer \nINFO[2025-03-26 17:25:50] Adding key to ~/.ssh/authorized_keys         \nINFO[2025-03-26 17:25:50] Successfully set up SSH keys                 \nINFO[2025-03-26 17:25:50] Attempting to set SELinux rules on /root/.ssh/quay_installer \nINFO[2025-03-26 17:25:50] Found image archive at /root/ncp/image-archive.tar \nINFO[2025-03-26 17:25:50] Detected an installation to localhost        \nINFO[2025-03-26 17:25:50] Unpacking image archive from /root/ncp/image-archive.tar \nINFO[2025-03-26 17:25:52] Loading pause image archive from pause.tar   \nINFO[2025-03-26 17:25:58] Loading redis image archive from redis.tar   \nINFO[2025-03-26 17:26:05] Loading postgres image archive from postgres.tar \nINFO[2025-03-26 17:26:17] Loading Quay image archive from quay.tar     \nINFO[2025-03-26 17:26:45] Attempting to set SELinux rules on image archive \nINFO[2025-03-26 17:26:45] Running install playbook. This may take some time. To see playbook output run the installer with -v (verbose) flag. \nINFO[2025-03-26 17:26:45] Detected an installation to localhost        \n\nPLAY [Install Mirror Appliance] ************************************************************************************************************************************************************\n\nTASK [Gathering Facts] *********************************************************************************************************************************************************************\nok: [root@ncputility.nokiptchub01.nokia.usa]\n\nTASK [mirror_appliance : Expand variables] *************************************************************************************************************************************************\nincluded: /runner/project/roles/mirror_appliance/tasks/expand-vars.yaml for root@ncputility.nokiptchub01.nokia.usa\n\nTASK [mirror_appliance : Expand pg_storage] ************************************************************************************************************************************************\nchanged: [root@ncputility.nokiptchub01.nokia.usa]\n\nTASK [mirror_appliance : Expand quay_root] *************************************************************************************************************************************************\nchanged: [root@ncputility.nokiptchub01.nokia.usa]\n\nTASK [mirror_appliance : Expand quay_storage] **********************************************************************************************************************************************\nchanged: [root@ncputility.nokiptchub01.nokia.usa]\n\nTASK [mirror_appliance : Set expanded variables] *******************************************************************************************************************************************\nok: [root@ncputility.nokiptchub01.nokia.usa]\n\nTASK [mirror_appliance : Install Dependencies] *********************************************************************************************************************************************\nincluded: /runner/project/roles/mirror_appliance/tasks/install-deps.yaml for root@ncputility.nokiptchub01.nokia.usa\n\nTASK [mirror_appliance : Create user service directory] ************************************************************************************************************************************\nok: [root@ncputility.nokiptchub01.nokia.usa]\n\nTASK [mirror_appliance : Set SELinux Rules] ************************************************************************************************************************************************\nincluded: /runner/project/roles/mirror_appliance/tasks/set-selinux-rules.yaml for root@ncputility.nokiptchub01.nokia.usa\n\nTASK [mirror_appliance : Set container_manage_cgroup flag on and keep it persistent across reboots] ****************************************************************************************\nskipping: [root@ncputility.nokiptchub01.nokia.usa]\n\nTASK [mirror_appliance : Install Quay Pod Service] *****************************************************************************************************************************************\nincluded: /runner/project/roles/mirror_appliance/tasks/install-pod-service.yaml for root@ncputility.nokiptchub01.nokia.usa\n\nTASK [mirror_appliance : Copy Quay Pod systemd service file] *******************************************************************************************************************************\nchanged: [root@ncputility.nokiptchub01.nokia.usa]\n\nTASK [mirror_appliance : Check if pod pause image is loaded] *******************************************************************************************************************************\nchanged: [root@ncputility.nokiptchub01.nokia.usa]\n\nTASK [mirror_appliance : Pull Infra image] *************************************************************************************************************************************************\nskipping: [root@ncputility.nokiptchub01.nokia.usa]\n\nTASK [mirror_appliance : Start Quay Pod service] *******************************************************************************************************************************************\nchanged: [root@ncputility.nokiptchub01.nokia.usa]\n\nTASK [mirror_appliance : Autodetect Image Archive] *****************************************************************************************************************************************\nincluded: /runner/project/roles/mirror_appliance/tasks/autodetect-image-archive.yaml for root@ncputility.nokiptchub01.nokia.usa\n\nTASK [mirror_appliance : Checking for Image Archive] ***************************************************************************************************************************************\nok: [root@ncputility.nokiptchub01.nokia.usa -&gt; localhost]\n\nTASK [mirror_appliance : Create install directory for image-archive.tar dest] **************************************************************************************************************\nok: [root@ncputility.nokiptchub01.nokia.usa]\n\nTASK [mirror_appliance : Copy Images if /runner/image-archive.tar exists] ******************************************************************************************************************\nskipping: [root@ncputility.nokiptchub01.nokia.usa]\n\nTASK [mirror_appliance : Unpack Images if /runner/image-archive.tar exists] ****************************************************************************************************************\nskipping: [root@ncputility.nokiptchub01.nokia.usa]\n\nTASK [mirror_appliance : Loading Redis if redis.tar exists] ********************************************************************************************************************************\nskipping: [root@ncputility.nokiptchub01.nokia.usa]\n\nTASK [mirror_appliance : Loading Quay if quay.tar exists] **********************************************************************************************************************************\nskipping: [root@ncputility.nokiptchub01.nokia.usa]\n\nTASK [mirror_appliance : Loading Postgres if postgres.tar exists] **************************************************************************************************************************\nskipping: [root@ncputility.nokiptchub01.nokia.usa]\n\nTASK [mirror_appliance : Install Postgres Service] *****************************************************************************************************************************************\nincluded: /runner/project/roles/mirror_appliance/tasks/install-postgres-service.yaml for root@ncputility.nokiptchub01.nokia.usa\n\nTASK [mirror_appliance : Create necessary directory for Postgres persistent data] **********************************************************************************************************\nskipping: [root@ncputility.nokiptchub01.nokia.usa]\n\nTASK [mirror_appliance : Set permissions on local storage directory] ***********************************************************************************************************************\nskipping: [root@ncputility.nokiptchub01.nokia.usa]\n\nTASK [mirror_appliance : Copy Postgres systemd service file] *******************************************************************************************************************************\nchanged: [root@ncputility.nokiptchub01.nokia.usa]\n\nTASK [mirror_appliance : Check if Postgres image is loaded] ********************************************************************************************************************************\nchanged: [root@ncputility.nokiptchub01.nokia.usa]\n\nTASK [mirror_appliance : Pull Postgres image] **********************************************************************************************************************************************\nskipping: [root@ncputility.nokiptchub01.nokia.usa]\n\nTASK [mirror_appliance : Create Postgres Storage named volume] *****************************************************************************************************************************\nchanged: [root@ncputility.nokiptchub01.nokia.usa]\n\nTASK [mirror_appliance : Start Postgres service] *******************************************************************************************************************************************\nchanged: [root@ncputility.nokiptchub01.nokia.usa]\n\nTASK [mirror_appliance : Wait for pg_trgm to be installed] *********************************************************************************************************************************\nFAILED - RETRYING: [root@ncputility.nokiptchub01.nokia.usa]: Wait for pg_trgm to be installed (20 retries left).\nchanged: [root@ncputility.nokiptchub01.nokia.usa]\n\nTASK [mirror_appliance : Install Redis Service] ********************************************************************************************************************************************\nincluded: /runner/project/roles/mirror_appliance/tasks/install-redis-service.yaml for root@ncputility.nokiptchub01.nokia.usa\n\nTASK [mirror_appliance : Copy Redis systemd service file] **********************************************************************************************************************************\nchanged: [root@ncputility.nokiptchub01.nokia.usa]\n\nTASK [mirror_appliance : Check if Redis image is loaded] ***********************************************************************************************************************************\nchanged: [root@ncputility.nokiptchub01.nokia.usa]\n\nTASK [mirror_appliance : Pull Redis image] *************************************************************************************************************************************************\nskipping: [root@ncputility.nokiptchub01.nokia.usa]\n\nTASK [mirror_appliance : Start Redis service] **********************************************************************************************************************************************\nchanged: [root@ncputility.nokiptchub01.nokia.usa]\n\nTASK [mirror_appliance : Install Quay Service] *********************************************************************************************************************************************\nincluded: /runner/project/roles/mirror_appliance/tasks/install-quay-service.yaml for root@ncputility.nokiptchub01.nokia.usa\n\nTASK [mirror_appliance : Create necessary directory for Quay local storage] ****************************************************************************************************************\nskipping: [root@ncputility.nokiptchub01.nokia.usa]\n\nTASK [mirror_appliance : Set permissions on local storage directory] ***********************************************************************************************************************\nskipping: [root@ncputility.nokiptchub01.nokia.usa]\n\nTASK [mirror_appliance : Create necessary directory for Quay config bundle] ****************************************************************************************************************\nchanged: [root@ncputility.nokiptchub01.nokia.usa]\n\nTASK [mirror_appliance : Copy Quay config.yaml file] ***************************************************************************************************************************************\nchanged: [root@ncputility.nokiptchub01.nokia.usa]\n\nTASK [mirror_appliance : Check if SSL Cert exists] *****************************************************************************************************************************************\nok: [root@ncputility.nokiptchub01.nokia.usa -&gt; localhost]\n\nTASK [mirror_appliance : Check if SSL Key exists] ******************************************************************************************************************************************\nok: [root@ncputility.nokiptchub01.nokia.usa -&gt; localhost]\n\nTASK [mirror_appliance : Create necessary directory for Quay rootCA files] *****************************************************************************************************************\nchanged: [root@ncputility.nokiptchub01.nokia.usa]\n\nTASK [mirror_appliance : Create OpenSSL Config] ********************************************************************************************************************************************\nchanged: [root@ncputility.nokiptchub01.nokia.usa]\n\nTASK [mirror_appliance : Create root CA key] ***********************************************************************************************************************************************\nchanged: [root@ncputility.nokiptchub01.nokia.usa]\n\nTASK [mirror_appliance : Create root CA pem] ***********************************************************************************************************************************************\nchanged: [root@ncputility.nokiptchub01.nokia.usa]\n\nTASK [mirror_appliance : Create ssl key] ***************************************************************************************************************************************************\nchanged: [root@ncputility.nokiptchub01.nokia.usa]\n\nTASK [mirror_appliance : Create CSR] *******************************************************************************************************************************************************\nchanged: [root@ncputility.nokiptchub01.nokia.usa]\n\nTASK [mirror_appliance : Create self-signed cert] ******************************************************************************************************************************************\nchanged: [root@ncputility.nokiptchub01.nokia.usa]\n\nTASK [mirror_appliance : Create chain cert] ************************************************************************************************************************************************\nchanged: [root@ncputility.nokiptchub01.nokia.usa]\n\nTASK [mirror_appliance : Replace ssl cert with chain cert] *********************************************************************************************************************************\nchanged: [root@ncputility.nokiptchub01.nokia.usa]\n\nTASK [mirror_appliance : Copy SSL certificate] *********************************************************************************************************************************************\nskipping: [root@ncputility.nokiptchub01.nokia.usa]\n\nTASK [mirror_appliance : Copy SSL key] *****************************************************************************************************************************************************\nskipping: [root@ncputility.nokiptchub01.nokia.usa]\n\nTASK [mirror_appliance : Set permissions for key] ******************************************************************************************************************************************\nchanged: [root@ncputility.nokiptchub01.nokia.usa]\n\nTASK [mirror_appliance : Set permissions for cert] *****************************************************************************************************************************************\nok: [root@ncputility.nokiptchub01.nokia.usa]\n\nTASK [mirror_appliance : Copy Quay systemd service file] ***********************************************************************************************************************************\nchanged: [root@ncputility.nokiptchub01.nokia.usa]\n\nTASK [mirror_appliance : Check if Quay image is loaded] ************************************************************************************************************************************\nchanged: [root@ncputility.nokiptchub01.nokia.usa]\n\nTASK [mirror_appliance : Pull Quay image] **************************************************************************************************************************************************\nskipping: [root@ncputility.nokiptchub01.nokia.usa]\n\nTASK [mirror_appliance : Create Quay Storage named volume] *********************************************************************************************************************************\nchanged: [root@ncputility.nokiptchub01.nokia.usa]\n\nTASK [mirror_appliance : Start Quay service] ***********************************************************************************************************************************************\nchanged: [root@ncputility.nokiptchub01.nokia.usa]\n\nTASK [mirror_appliance : Wait for Quay] ****************************************************************************************************************************************************\nincluded: /runner/project/roles/mirror_appliance/tasks/wait-for-quay.yaml for root@ncputility.nokiptchub01.nokia.usa\n\nTASK [mirror_appliance : Waiting up to 3 minutes for Quay to become alive at https://ncputility.nokiptchub01.nokia.usa:8443/health/instance] ***********************************************\nFAILED - RETRYING: [root@ncputility.nokiptchub01.nokia.usa]: Waiting up to 3 minutes for Quay to become alive at https://ncputility.nokiptchub01.nokia.usa:8443/health/instance (10 retries left).\nok: [root@ncputility.nokiptchub01.nokia.usa]\n\nTASK [mirror_appliance : Create init user] *************************************************************************************************************************************************\nincluded: /runner/project/roles/mirror_appliance/tasks/create-init-user.yaml for root@ncputility.nokiptchub01.nokia.usa\n\nTASK [mirror_appliance : Creating init user at endpoint https://ncputility.nokiptchub01.nokia.usa:8443/api/v1/user/initialize] *************************************************************\nok: [root@ncputility.nokiptchub01.nokia.usa]\n\nTASK [mirror_appliance : Enable lingering for systemd user processes] **********************************************************************************************************************\nskipping: [root@ncputility.nokiptchub01.nokia.usa]\n\nPLAY RECAP *********************************************************************************************************************************************************************************\nroot@ncputility.nokiptchub01.nokia.usa : ok=50   changed=30   unreachable=0    failed=0    skipped=17   rescued=0    ignored=0   \n\nINFO[2025-03-26 17:28:27] Quay installed successfully, config data is stored in /opt/quayroot \nINFO[2025-03-26 17:28:27] Quay is available at https://ncputility.nokiptchub01.nokia.usa:8443 with credentials (infraquay, Nokia12234) \n[root@ncputility ncp]# \n</code></pre>"},{"location":"openshift/disaster-management/infra-manager-redeployment/#add-the-infra-quay-registry-to-the-trusted-registries","title":"Add the infra-quay registry to the trusted registries","text":"<p>Prerequisites Perform the following actions:</p> <p>1)  Make a dir first to copy the cert.</p> <pre><code>[root@ncputility ncp]# sudo mkdir -p /etc/containers/certs.d/ncputility.nokiptchub01.nokia.usa:8443/\n[root@ncputility ncp]# \n</code></pre> <p>2)  Copy the certificate here Add to the systems trusted list </p> <pre><code>[root@ncputility ncp]# sudo cp /opt/quayroot/quay-rootCA/rootCA.pem  /etc/containers/certs.d/ncputility.nokiptchub01.nokia.usa:8443/\n[root@ncputility ncp]# sudo cp /opt/quayroot/quay-rootCA/rootCA.pem /etc/pki/ca-trust/source/anchors/\n[root@ncputility ncp]# sudo update-ca-trust extract\n[root@ncputility ncp]# \n</code></pre> <p>3)  Create the pullsecret to ~/.docker/config.json</p> <pre><code>[root@ncputility ncp]# mkdir -p ~/.docker\n[root@ncputility ncp]# \n</code></pre> <p>4) Create the base64 encoded username:password pair of the mirror-registry:</p> <pre><code>[root@ncputility ncp]# authpull=`echo -n 'infraquay:Nokia12234' | base64 -w0`\n[root@ncputility ncp]# echo '{\"auths\": {\"ncputility.nokiptchub01.nokia.usa:8443\": {\"auth\": \"'\"$authpull\"'\", \"email\": \"raj@ncputility.ncputility.nokiptchub01.nokia.usa\"}}}' | tee ~/.docker/config.json\n{\"auths\": {\"ncputility.nokiptchub01.nokia.usa:8443\": {\"auth\": \"aW5mcmFxdWF5Ok5va2lhMTIyMzQ=\", \"email\": \"raj@ncputility.ncputility.nokiptchub01.nokia.usa\"}}}\n[root@ncputility ncp]# cat ~/.docker/config.json\n{\"auths\": {\"ncputility.nokiptchub01.nokia.usa:8443\": {\"auth\": \"aW5mcmFxdWF5Ok5va2lhMTIyMzQ=\", \"email\": \"raj@ncputility.ncputility.nokiptchub01.nokia.usa\"}}}\n[root@ncputility ncp]# cat ~/.docker/config.json |jq .\n{\n  \"auths\": {\n    \"ncputility.nokiptchub01.nokia.usa:8443\": {\n      \"auth\": \"aW5mcmFxdWF5Ok5va2lhMTIyMzQ=\",\n      \"email\": \"raj@ncputility.ncputility.nokiptchub01.nokia.usa\"\n    }\n  }\n}\n[root@ncputility ncp]# \n</code></pre> <p>5)  Validate the pull secret file here</p> <pre><code>[root@ncputility ncp]# podman login https://ncputility.nokiptchub01.nokia.usa:8443  --authfile .docker/config.json -u infraquay -p Nokia12234\nLogin Succeeded!\n[root@ncputility ncp]# \n</code></pre>"},{"location":"openshift/disaster-management/infra-manager-redeployment/#configuring-an-organization-in-quay","title":"Configuring an organization in Quay","text":"<p>1)  Create an organization in Quay:</p> <p></p> <p>2)  Click create New organization</p> <p></p> <p>3)  In the example above the ocmirror organization was created. </p>"},{"location":"openshift/disaster-management/infra-manager-redeployment/#copy-openshift-related-binaries","title":"Copy OpenShift related binaries","text":"<p>1)  From the uncompressed ncp_tools.tar.gz copy (or move) the necessary binaries:</p> <pre><code>[root@ncputility ~]# cd ncp/\n[root@ncputility ncp]# sudo cp butane-amd64 /usr/local/bin/butane\n[root@ncputility ncp]# sudo chmod +x /usr/local/bin/butane\n[root@ncputility ncp]# sudo cp helm-linux-amd64 /usr/local/bin/helm\n[root@ncputility ncp]# sudo chmod +x /usr/local/bin/helm\n[root@ncputility ncp]# tar -zxvf openshift-client-linux.tar.gz\nREADME.md\noc\nkubectl\n[root@ncputility ncp]# sudo cp kubectl /usr/local/bin/\n[root@ncputility ncp]# sudo cp oc /usr/local/bin/\n[root@ncputility ncp]# tar -zxvf openshift-install-linux.tar.gz\nREADME.md\nopenshift-install\n[root@ncputility ncp]# sudo cp openshift-install /usr/local/bin/\n[root@ncputility ncp]# tar -xvf oc-mirror.tar.gz\noc-mirror\n[root@ncputility ncp]# sudo cp oc-mirror /usr/local/bin/\n[root@ncputility ncp]# sudo chmod +x /usr/local/bin/oc-mirror\n[root@ncputility ncp]# cd /root/ncp\n[root@ncputility ncp]# \n</code></pre>"},{"location":"openshift/disaster-management/infra-manager-redeployment/#mirroring-images-to-infra-quay-mirror-registry","title":"Mirroring images to infra-quay mirror-registry","text":"<p>1) Copy the <code>ncp-platform.partaa.tar ncp-platform.partab.tar and ncp-platform.partac.tar</code> files to the infrastructure node.</p> <p>2) Combine them to one: cat ncp-platform.parta* &gt; ncp-platform-parta.tar To avoid any connectivity issues to the infrastructure node and to avoid the mirroring process disturbance, tmux can be used and suggested.</p> <pre><code>oc-mirror --from=./ncp-platform-parta.tar docker://ncputility.nokiptchub01.nokia.usa:8443/ocmirror\n</code></pre> <p>3) Copy <code>ncp_operators.partaa.tar, ncp_operators.partab.tar, ncp_operators.partac.tar, ncp_operators.partad.tar, ncp_operators.partae.tar, ncp_operators.partaf.tar, ncp_operators.partag.tar, ncp_operators.partah.tar, ncp_operators.partai.tar and ncp_operators.partaj.tar</code> files to the Infrastructure node.</p> <p>4) Combine them together: cat ncp_operators.part* &gt; ncp_operators.tar</p> <pre><code>[root@ncputility ncp]# cat ncp-operators.part* &gt; ncp_operators.tar\n</code></pre> <pre><code>oc-mirror --from=./ncp_operators.tar docker://ncputility.nokiptchub01.nokia.usa:8443/ocmirror\nuploading: ncputility.nokiptchub01.nokia.usa:8443/ocmirror/openshift4/ose-csi-external-resizer sha256:ba4c748f34a20478f7334a3e29888150985cd77bf5667cb28df709f8349e14fb 55.28MiB\nuploading: ncputility.nokiptchub01.nokia.usa:8443/ocmirror/openshift4/ose-csi-external-resizer sha256:8bad510b7a13af38e38c6a4b66c8beaac49b44c2f1bac22068b1001bae266f3e 81.92MiB\nuploading: ncputility.nokiptchub01.nokia.usa:8443/ocmirror/openshift4/ose-csi-external-resizer sha256:d074e0420aa5a047ac468c239178ef5f660b03920ad2f7f7e6a1d71530069db5 10.1MiB\nuploading: ncputility.nokiptchub01.nokia.usa:8443/ocmirror/openshift4/ose-csi-external-resizer sha256:f26b1ec0865cd5bcc30994a1d00d16f6f1bad44dd538ee1df0e8840753595738 21.78KiB\nuploading: ncputility.nokiptchub01.nokia.usa:8443/ocmirror/openshift4/ose-csi-external-resizer sha256:70a5bdb4264951ee2dfdc73b51c3c8ab99a611d9ac3acaf4a81988a075b5b845 21.69MiB\nuploading: ncputility.nokiptchub01.nokia.usa:8443/ocmirror/openshift4/ose-csi-external-resizer sha256:abb828c6dd68e53a22f7a11904b72f9d5a7802fb5e7c380b3a22b18a02e9a463 73.31MiB\nuploading: ncputility.nokiptchub01.nokia.usa:8443/ocmirror/openshift4/ose-csi-external-resizer sha256:f44954f3b43a0b5ee76801b112b069aecdb6ce7b387ed3fb72bec893f848c89a 54.58MiB\nuploading: ncputility.nokiptchub01.nokia.usa:8443/ocmirror/openshift4/ose-csi-external-resizer sha256:7be6430836eaa16b36440931da447277c710585b321a59f555d455e6b9f9689e 10.28MiB\nuploading: ncputility.nokiptchub01.nokia.usa:8443/ocmirror/openshift4/ose-csi-external-resizer sha256:e892ff5d6fa3cef8357fcaa97f0f534e4f454439edadf9cbb820482420817585 21.91MiB\nuploading: ncputility.nokiptchub01.nokia.usa:8443/ocmirror/openshift4/ose-csi-external-resizer sha256:4caf23c20c871ed5c847c163905c27ee724b9e10975f1711715589a52942b3d5 53.47MiB\nuploading: ncputility.nokiptchub01.nokia.usa:8443/ocmirror/openshift4/ose-csi-external-resizer sha256:ca4e3da08236e2c5f895043898246830e8a394a483c5ba66aa25635ea19d4c69 21.78KiB\nuploading: ncputility.nokiptchub01.nokia.usa:8443/ocmirror/openshift4/ose-csi-external-resizer sha256:357d8d6388d50c5e60475784a9956fcff8a1e66f48ae978c0e691fd587b76248 10.25MiB\nuploading: ncputility.nokiptchub01.nokia.usa:8443/ocmirror/openshift4/ose-csi-external-resizer sha256:bfdc7c9d33c28bd179f2b99aea50e8e86a372a209dfe272757d32965644f8c59 21.76KiB\nuploading: ncputility.nokiptchub01.nokia.usa:8443/ocmirror/openshift4/ose-csi-external-resizer sha256:89176521898ace7eab9ccf870f6e4e5e3a21c3688df09c9346a3cf6f909f1188 23.84MiB\nuploading: ncputility.nokiptchub01.nokia.usa:8443/ocmirror/openshift4/ose-csi-external-resizer sha256:00f24979220b92c40dbe75e7f85475c5d14efe31c4b0f2c8c2ea50bf359986ec 10.97MiB\nuploading: ncputility.nokiptchub01.nokia.usa:8443/ocmirror/openshift4/ose-csi-external-resizer sha256:96e84521ae3a855d5ef74239d41bdc2cd5c2d0db0fd71006823f9cab7b5ae5d6 21.76KiB\nuploading: ncputility.nokiptchub01.nokia.usa:8443/ocmirror/openshift4/ose-csi-external-resizer sha256:c7965aa7086045a59bdb113a1fb8a19d7ccf7af4133e59af8ecefd39cda8e0b1 75.31MiB\nuploading: ncputility.nokiptchub01.nokia.usa:8443/ocmirror/openshift4/ose-csi-external-resizer sha256:5b92631cae6df8d9aef617e5a07532336e5c2a09d7c5418592ea6188a7e10a0c 72.73MiB\nuploading: ncputility.nokiptchub01.nokia.usa:8443/ocmirror/openshift4/ose-csi-external-resizer sha256:5cac740fb8a68c72180f72cbe2227abcdb92f2acc1ee68141b720e2e16e3daed 23.64MiB\nsha256:a1395b66e122f878c6abdb229134d5150832e9f95b6278e823095bb66da7203d ncputility.nokiptchub01.nokia.usa:8443/ocmirror/openshift4/ose-csi-external-resizer\nsha256:a007417b5129999c3aa2391802b9d1131d67bd735acb0e4e83f2232da56b51a2 ncputility.nokiptchub01.nokia.usa:8443/ocmirror/openshift4/ose-csi-external-resizer\nsha256:69640e0eade25e28d6f08c98bed2a0a9e7120b4b6e5a3510ca3741681c79c096 ncputility.nokiptchub01.nokia.usa:8443/ocmirror/openshift4/ose-csi-external-resizer\nsha256:91b6872c930e6d846fb94eb314df9fa4e0f35a7a234462329ea1e0911ed98e00 ncputility.nokiptchub01.nokia.usa:8443/ocmirror/openshift4/ose-csi-external-resizer\nsha256:152e719fa5c0a8326fb845884a761c7976521201ac912932992df020bda77fae ncputility.nokiptchub01.nokia.usa:8443/ocmirror/openshift4/ose-csi-external-resizer:b068f7cc\ninfo: Mirroring completed in 5.52s (125MB/s)\n\nuploading: ncputility.nokiptchub01.nokia.usa:8443/ocmirror/openshift4/ose-csi-external-resizer sha256:ba4c748f34a20478f7334a3e29888150985cd77bf5667cb28df709f8349e14fb 55.28MiB\nuploading: ncputility.nokiptchub01.nokia.usa:8443/ocmirror/openshift4/ose-csi-external-resizer sha256:8bad510b7a13af38e38c6a4b66c8beaac49b44c2f1bac22068b1001bae266f3e 81.92MiB\nuploading: ncputility.nokiptchub01.nokia.usa:8443/ocmirror/openshift4/ose-csi-external-resizer sha256:d074e0420aa5a047ac468c239178ef5f660b03920ad2f7f7e6a1d71530069db5 10.1MiB\nuploading: ncputility.nokiptchub01.nokia.usa:8443/ocmirror/openshift4/ose-csi-external-resizer sha256:f26b1ec0865cd5bcc30994a1d00d16f6f1bad44dd538ee1df0e8840753595738 21.78KiB\nuploading: ncputility.nokiptchub01.nokia.usa:8443/ocmirror/openshift4/ose-csi-external-resizer sha256:70a5bdb4264951ee2dfdc73b51c3c8ab99a611d9ac3acaf4a81988a075b5b845 21.69MiB\nuploading: ncputility.nokiptchub01.nokia.usa:8443/ocmirror/openshift4/ose-csi-external-resizer sha256:abb828c6dd68e53a22f7a11904b72f9d5a7802fb5e7c380b3a22b18a02e9a463 73.31MiB\nuploading: ncputility.nokiptchub01.nokia.usa:8443/ocmirror/openshift4/ose-csi-external-resizer sha256:f44954f3b43a0b5ee76801b112b069aecdb6ce7b387ed3fb72bec893f848c89a 54.58MiB\nuploading: ncputility.nokiptchub01.nokia.usa:8443/ocmirror/openshift4/ose-csi-external-resizer sha256:7be6430836eaa16b36440931da447277c710585b321a59f555d455e6b9f9689e 10.28MiB\nuploading: ncputility.nokiptchub01.nokia.usa:8443/ocmirror/openshift4/ose-csi-external-resizer sha256:e892ff5d6fa3cef8357fcaa97f0f534e4f454439edadf9cbb820482420817585 21.91MiB\nuploading: ncputility.nokiptchub01.nokia.usa:8443/ocmirror/openshift4/ose-csi-external-resizer sha256:4caf23c20c871ed5c847c163905c27ee724b9e10975f1711715589a52942b3d5 53.47MiB\nuploading: ncputility.nokiptchub01.nokia.usa:8443/ocmirror/openshift4/ose-csi-external-resizer sha256:ca4e3da08236e2c5f895043898246830e8a394a483c5ba66aa25635ea19d4c69 21.78KiB\nuploading: ncputility.nokiptchub01.nokia.usa:8443/ocmirror/openshift4/ose-csi-external-resizer sha256:357d8d6388d50c5e60475784a9956fcff8a1e66f48ae978c0e691fd587b76248 10.25MiB\nuploading: ncputility.nokiptchub01.nokia.usa:8443/ocmirror/openshift4/ose-csi-external-resizer sha256:bfdc7c9d33c28bd179f2b99aea50e8e86a372a209dfe272757d32965644f8c59 21.76KiB\nuploading: ncputility.nokiptchub01.nokia.usa:8443/ocmirror/openshift4/ose-csi-external-resizer sha256:89176521898ace7eab9ccf870f6e4e5e3a21c3688df09c9346a3cf6f909f1188 23.84MiB\nuploading: ncputility.nokiptchub01.nokia.usa:8443/ocmirror/openshift4/ose-csi-external-resizer sha256:00f24979220b92c40dbe75e7f85475c5d14efe31c4b0f2c8c2ea50bf359986ec 10.97MiB\nuploading: ncputility.nokiptchub01.nokia.usa:8443/ocmirror/openshift4/ose-csi-external-resizer sha256:96e84521ae3a855d5ef74239d41bdc2cd5c2d0db0fd71006823f9cab7b5ae5d6 21.76KiB\nuploading: ncputility.nokiptchub01.nokia.usa:8443/ocmirror/openshift4/ose-csi-external-resizer sha256:c7965aa7086045a59bdb113a1fb8a19d7ccf7af4133e59af8ecefd39cda8e0b1 75.31MiB\nuploading: ncputility.nokiptchub01.nokia.usa:8443/ocmirror/openshift4/ose-csi-external-resizer sha256:5b92631cae6df8d9aef617e5a07532336e5c2a09d7c5418592ea6188a7e10a0c 72.73MiB\nuploading: ncputility.nokiptchub01.nokia.usa:8443/ocmirror/openshift4/ose-csi-external-resizer sha256:5cac740fb8a68c72180f72cbe2227abcdb92f2acc1ee68141b720e2e16e3daed 23.64MiB\nsha256:a1395b66e122f878c6abdb229134d5150832e9f95b6278e823095bb66da7203d ncputility.nokiptchub01.nokia.usa:8443/ocmirror/openshift4/ose-csi-external-resizer\nsha256:a007417b5129999c3aa2391802b9d1131d67bd735acb0e4e83f2232da56b51a2 ncputility.nokiptchub01.nokia.usa:8443/ocmirror/openshift4/ose-csi-external-resizer\nsha256:69640e0eade25e28d6f08c98bed2a0a9e7120b4b6e5a3510ca3741681c79c096 ncputility.nokiptchub01.nokia.usa:8443/ocmirror/openshift4/ose-csi-external-resizer\nsha256:91b6872c930e6d846fb94eb314df9fa4e0f35a7a234462329ea1e0911ed98e00 ncputility.nokiptchub01.nokia.usa:8443/ocmirror/openshift4/ose-csi-external-resizer\nsha256:152e719fa5c0a8326fb845884a761c7976521201ac912932992df020bda77fae ncputility.nokiptchub01.nokia.usa:8443/ocmirror/openshift4/ose-csi-external-resizer:b068f7cc\ninfo: Mirroring completed in 5.52s (125MB/s)\n\ncatalogs/registry.redhat.io/redhat/redhat-operator-index/v4.16/layout/blobs/sha256/d74e8b702f03952802c351c423f5a15bd670d0558f3f25e04d27bfc1964d68f6\ncatalogs/registry.redhat.io/redhat/redhat-operator-index/v4.16/layout/blobs/sha256/d97215b78c445a91c5e9676d137c6c9fa1cbef72ce82425316432b221360f9a1\ncatalogs/registry.redhat.io/redhat/redhat-operator-index/v4.16/layout/blobs/sha256/ed1748ba861420cf9f23989a033c63968dcb227442f5d70ff5088fc2f163b44b\ncatalogs/registry.redhat.io/redhat/redhat-operator-index/v4.16/layout/blobs/sha256/f20db0aa394f8fd833f0f3d35bd505ecbddcc38d501afaea85ef28ac5c446ee9\ncatalogs/registry.redhat.io/redhat/redhat-operator-index/v4.16/layout/blobs/sha256/f371e370ccf3b0b9ed6e19bf73c29f180e3e288bf25cea4df13d52c27f24ebff\ncatalogs/registry.redhat.io/redhat/redhat-operator-index/v4.16/layout/blobs/sha256/f6eee3a6ec21314c3c091044142276b65fb7ae526e21cb2288af902735496b96\ncatalogs/registry.redhat.io/redhat/redhat-operator-index/v4.16/layout/blobs/sha256/f85f9539a5bfe9b773c454c22e77875de533ce99aacf5f24efd6d159c06b0405\ncatalogs/registry.redhat.io/redhat/redhat-operator-index/v4.16/layout/index.json\ncatalogs/registry.redhat.io/redhat/redhat-operator-index/v4.16/layout/oci-layout\nRendering catalog image \"ncputility.nokiptchub01.nokia.usa:8443/ocmirror/redhat/certified-operator-index:v4.16\" with file-based catalog \nRendering catalog image \"ncputility.nokiptchub01.nokia.usa:8443/ocmirror/redhat/redhat-operator-index:v4.16\" with file-based catalog \nWriting image mapping to oc-mirror-workspace/results-1743029387/mapping.txt\nWriting CatalogSource manifests to oc-mirror-workspace/results-1743029387\nWriting ICSP manifests to oc-mirror-workspace/results-1743029387\n[root@ncputility ncp]# \n</code></pre> <p>5) When you reload your quay registry, you will be able to see the images uploaded. </p> <p></p>"},{"location":"openshift/disaster-management/infra-manager-redeployment/#generate-the-ssh-key-for-authenticating-with-core-os","title":"Generate the ssh key for authenticating with core OS.","text":"<p>1) Generate a keypair for ssh access of the HUB cluster nodes, using ssh-keygen. Preferred algorithm is <code>ed25519</code>. Use the command, It will keep pub in .ssh folder.</p> <pre><code>[root@ncputility ncp]# ssh-keygen -t ed25519\nGenerating public/private ed25519 key pair.\nEnter file in which to save the key (/root/.ssh/id_ed25519): \nEnter passphrase (empty for no passphrase): \nEnter same passphrase again: \nYour identification has been saved in /root/.ssh/id_ed25519\nYour public key has been saved in /root/.ssh/id_ed25519.pub\nThe key fingerprint is:\nSHA256:cPryPsjzimE7ks5xceRKXYFr5QlpGk00t5q01qrYxJU root@ncputility.nokiptchub01.nokia.usa\nThe key's randomart image is:\n+--[ED25519 256]--+\n|     ++oo        |\n|    . *o.o       |\n|     ++=+.       |\n|    .=o@o        |\n|    o.E S        |\n|   o * o         |\n|  ..B.o..        |\n| .oB.=+o.        |\n| .+.=..=+.       |\n+----[SHA256]-----+\n[root@ncputility ncp]# \n</code></pre>"},{"location":"openshift/disaster-management/infra-manager-redeployment/#configure-quay-ssl-to-match-hub-cluster","title":"Configure Quay SSL to Match Hub Cluster.","text":"<p>1) login to in the hub cluster using respective RC file. </p> <pre><code>[root@ncputility ~ cwl_rc]$source hubrc\nWARNING: Using insecure TLS client config. Setting this option is not supported!\n\nLogin successful.\n\nYou have access to 105 projects, the list has been suppressed. You can list all projects with 'oc projects'\n\nUsing project \"default\".\n[root@ncputility ~ hub_rc]$ oc get nodes\nNAME                                       STATUS   ROLES                                 AGE   VERSION\nmaster-0.pphncp01.infra.mobi.eastlink.ca   Ready    control-plane,master,monitor,worker   30d   v1.29.10+67d3387\nmaster-1.pphncp01.infra.mobi.eastlink.ca   Ready    control-plane,master,monitor,worker   30d   v1.29.10+67d3387\nmaster-2.pphncp01.infra.mobi.eastlink.ca   Ready    control-plane,master,monitor,worker   30d   v1.29.10+67d3387\n[root@ncputility ~ hub_rc]$ oc get pods\nNo resources found in default namespace.\n</code></pre> <p>2) now try to prepare file for new certificate with correct file content using --dry-run</p> <pre><code>oc create configmap user-ca-bundle \\\n     --from-file=ncputility.pphncp01.infra.mobi.eastlink.ca:8443=/opt/root/ca.crt \\\n     -n openshift-config --dry-run=client -o yaml \n</code></pre> <p>3) Now try to copy the ncputility and it's certificate line's and then add it as last list of the certificate on the edit then apply it.</p> <pre><code>oc edit configmap user-ca-bundle -n openshift-config\n</code></pre>"},{"location":"openshift/disaster-management/infra-manager-redeployment/#validate-operator-catalog-itms-and-dtms","title":"Validate Operator Catalog, ITMS, and DTMS","text":"<p>1) login to in the hub cluster using respective RC file. </p> <pre><code>[root@ncputility ~ cwl_rc]$source hubrc\nWARNING: Using insecure TLS client config. Setting this option is not supported!\n\nLogin successful.\n\nYou have access to 105 projects, the list has been suppressed. You can list all projects with 'oc projects'\n\nUsing project \"default\".\n[root@ncputility ~ hub_rc]$ oc get nodes\nNAME                                       STATUS   ROLES                                 AGE   VERSION\nmaster-0.pphncp01.infra.mobi.eastlink.ca   Ready    control-plane,master,monitor,worker   30d   v1.29.10+67d3387\nmaster-1.pphncp01.infra.mobi.eastlink.ca   Ready    control-plane,master,monitor,worker   30d   v1.29.10+67d3387\nmaster-2.pphncp01.infra.mobi.eastlink.ca   Ready    control-plane,master,monitor,worker   30d   v1.29.10+67d3387\n[root@ncputility ~ hub_rc]$ oc get pods\nNo resources found in default namespace.\n</code></pre> <p>2)  get the oc get idms, itms and catalogsource.</p> <pre><code>[root@ncputility ~ hub_rc]$ oc get idms\nNAME                  AGE\ngeneric-0             29d\ngeneric-1             29d\nimage-digest-mirror   30d\noperator-0            29d\nrelease-0             29d\n[root@ncputility ~ hub_rc]$ oc get itms\nNAME       AGE\nplatform   29d\n[root@ncputility ~ hub_rc]$ oc get catalogsource -A\nNAMESPACE               NAME                       DISPLAY                    TYPE   PUBLISHER   AGE\nopenshift-marketplace   cs-redhat-operator-index   Disconnected OperatorHub   grpc   Nokia       29d\n[root@ncputility ~ hub_rc]$\n</code></pre> <p>3) check the status of catalogsource on marketplace. </p> <pre><code>[root@ncputility ~ hub_rc]$ oc get catalogsource cs-redhat-operator-index -n openshift-marketplace  -o yaml\napiVersion: operators.coreos.com/v1alpha1\nkind: CatalogSource\nmetadata:\n  annotations:\n    kubectl.kubernetes.io/last-applied-configuration: |\n      {\"apiVersion\":\"operators.coreos.com/v1alpha1\",\"kind\":\"CatalogSource\",\"metadata\":{\"annotations\":{},\"name\":\"cs-redhat-operator-index\",\"namespace\":\"openshift-marketplace\"},\"spec\":{\"displayName\":\"Disconnected OperatorHub\",\"image\":\"ncputility.pphncp01.infra.mobi.eastlink.ca:8443/ocmirror-ncp-24-7-mp1/redhat/redhat-operator-index:v4.16\",\"publisher\":\"Nokia\",\"sourceType\":\"grpc\"}}\n  creationTimestamp: \"2025-05-29T11:56:32Z\"\n  generation: 1\n  name: cs-redhat-operator-index\n  namespace: openshift-marketplace\n  resourceVersion: \"42579673\"\n  uid: 2415d59c-172d-42e9-af1c-067c966b5222\nspec:\n  displayName: Disconnected OperatorHub\n  image: ncputility.pphncp01.infra.mobi.eastlink.ca:8443/ocmirror-ncp-24-7-mp1/redhat/redhat-operator-index:v4.16\n  publisher: Nokia\n  sourceType: grpc\nstatus:\n  connectionState:\n    address: cs-redhat-operator-index.openshift-marketplace.svc:50051\n    lastConnect: \"2025-06-22T21:19:42Z\"\n    lastObservedState: READY\n  registryService:\n    createdAt: \"2025-06-22T21:15:42Z\"\n    port: \"50051\"\n    protocol: grpc\n    serviceName: cs-redhat-operator-index\n    serviceNamespace: openshift-marketplace\n[root@ncputility ~ hub_rc]$\n</code></pre>"},{"location":"openshift/disaster-management/infra-manager-redeployment/#restore-ssh-key-for-hub-cluster","title":"Restore SSH Key for Hub Cluster","text":"<p>1) Login to hub cluster with kubeadmin or cluster-admin role.</p> <pre><code>[root@ncputility ~ hub_rc]$ source  hubrc\nWARNING: Using insecure TLS client config. Setting this option is not supported!\n\nLogin successful.\n\nYou have access to 105 projects, the list has been suppressed. You can list all projects with 'oc projects'\n\nUsing project \"default\".\n[root@ncputility ~ hub_rc]$\n</code></pre> <p>2) list the mc for ssh on this hold and take a backup of it. </p> <pre><code>[root@ncputility ~ hub_rc]$ oc get mc 99-master-ssh  99-worker-ssh\nNAME            GENERATEDBYCONTROLLER   IGNITIONVERSION   AGE\n99-master-ssh                           3.2.0             29d\n99-worker-ssh                           3.2.0             29d\n[root@ncputility ~ hub_rc]$ oc get mc 99-master-ssh  99-worker-ssh -o yaml &gt; 99-master-worker-ssh.yaml\n[root@ncputility ~ hub_rc]$ \n[root@ncputility ~ hub_rc]$ oc get mc 99-master-ssh  99-worker-ssh -o yaml &gt; 99-master-worker-ssh-back.yaml\n[root@ncputility ~ hub_rc]$\n</code></pre> <p>3) update the <code>sshAuthorizedKeys</code> key with your latest key and then apply it. </p> <pre><code>[root@ncputility ~ hub_rc]$ vi 99-master-worker-ssh-back.yaml\n</code></pre> <pre><code>    - name: core\n      sshAuthorizedKeys:\n      - ssh-ed25519 AAAAC3NzaC1lZDI1NTE5AAAAIGW2BXbCPiDgL3m4y8y3UYdehEmdUsrQBNLOR27waVBr\n        root@ncputility.pphncp01.infra.mobi.eastlink.ca\n</code></pre> <p>this part need to updated. </p> <p>4) apply the change </p> <pre><code>[root@ncputility ~ hub_rc]$ oc apply -f 99-master-worker-ssh-back.yaml\nWarning: resource machineconfigs/99-master-ssh is missing the kubectl.kubernetes.io/last-applied-configuration annotation which is required by oc apply. oc apply should only be used on resources created declaratively by either oc create --save-config or oc apply. The missing annotation will be patched automatically.\nmachineconfig.machineconfiguration.openshift.io/99-master-ssh configured\nWarning: resource machineconfigs/99-worker-ssh is missing the kubectl.kubernetes.io/last-applied-configuration annotation which is required by oc apply. oc apply should only be used on resources created declaratively by either oc create --save-config or oc apply. The missing annotation will be patched automatically.\nmachineconfig.machineconfiguration.openshift.io/99-worker-ssh configured\n[root@ncputility ~ hub_rc]$\n</code></pre> <p>5) Machine Config Operator should start applying the changes shortly. Observe which MachineConfigPools are being updated with oc get mcp and check more details on which node is being updated by following this solution.</p> <pre><code>[root@ncputility ~ hub_rc]$ oc get mcp\nNAME     CONFIG                                             UPDATED   UPDATING   DEGRADED   MACHINECOUNT   READYMACHINECOUNT   UPDATEDMACHINECOUNT   DEGRADEDMACHINECOUNT   AGE\nmaster   rendered-master-adf9d3edadac5fe32aa27a42deeb773e   True      False      False      3              3                   3                     0                      29d\nworker   rendered-worker-9ad974bc81cec3f37ff34bd866f0e906   True      False      False      0              0                   0                     0                      29d\n[root@ncputility ~ hub_rc]$\n</code></pre> <p>6) For more insights on how the change is being applied in a single node, then check the machine config daemon logs:</p> <pre><code>NODE='master-0.pphncp01.infra.mobi.eastlink.ca'\noc -n openshift-machine-config-operator logs -c machine-config-daemon $(oc -n openshift-machine-config-operator get pod -l k8s-app=machine-config-daemon --field-selector spec.nodeName=${NODE} -o name) -f \n</code></pre> <p>Where ${NODE} must be replaced with the name of the node. Omit -f if logs are not required to be followed. If the update was successfully applied, then logs similar to below lines are expected:</p> <pre><code>I0111 19:59:07.360110    7993 update.go:258] SSH Keys reconcilable\n...\nI0111 19:59:07.371253    7993 update.go:569] Writing SSHKeys at \"/home/core/.ssh\"\n...\nI0111 19:59:07.372208    7993 update.go:613] machine-config-daemon initiating reboot: Node will reboot into config worker-96b48815fa067f651fa50541ea6a9b5d\n</code></pre>"},{"location":"openshift/disaster-management/infra-manager-redeployment/#restore-ssh-key-for-cwl-cluster","title":"Restore SSH Key for CWL Cluster","text":"<p>1) login to infra-manager node and followed by navigate to git files location.</p> <pre><code>[root@ncputility ~ hub_rc]$ pwd\n/root/ncp-pp/ppwncp01/site-config/ppwncp01\n[root@ncputility ~ hub_rc]$ vi ppwncp01.yaml\n</code></pre> <p>2) update the site-config.yaml with recent ssh pub key directly. </p> <p>vi to this file <code>ppwncp01.yaml</code> and update <code>sshPublicKey</code></p> <pre><code>---\napiVersion: ran.openshift.io/v1\nkind: SiteConfig\nmetadata:\n  name: \"ppwncp01\"\n  namespace: \"ppwncp01\"\nspec:\n  # The base domain used by our SNOs\n  baseDomain: \"infra.mobi.eastlink.ca\"\n  # The secret name of the secret containing the pull secret for our disconnected registry\n  pullSecretRef:\n    name: \"ppwncp01-disconnected-registry-pull-secret\"\n  # The OCP release we will be deploying otherwise specified (this can be configured per cluster as well)\n  clusterImageSetNameRef: \"ncp-24-7-mp1-release\"\n  # The ssh public key that will be injected into our SNOs authorized_keys\n  sshPublicKey: \"ssh-ed25519 AAAAC3NzaC1lZDI1NTE5AAAAIGW2BXbCPiDgL3m4y8y3UYdehEmdUsrQBNLOR27waVBr\"\n  clusters:\n    - clusterName: \"ppwncp01\"\n      # The sdn plugin that will be used\n</code></pre> <p>3) Commit this change on the git server too </p> <pre><code>[root@ncputility ~ hub_rc]$ git commit -m \"update\"\nAuthor identity unknown\n\n*** Please tell me who you are.\n\nRun\n\n  git config --global user.email \"you@example.com\"\n  git config --global user.name \"Your Name\"\n\nto set your account's default identity.\nOmit --global to set the identity only in this repository.\n\nfatal: unable to auto-detect email address (got 'root@ncputility.(none)')\n[root@ncputility ~ hub_rc]$ git push\nUsername for 'https://gitlab.apps.pphncp01.infra.mobi.eastlink.ca': ncpadmin\nPassword for 'https://ncpadmin@gitlab.apps.pphncp01.infra.mobi.eastlink.ca':\nEverything up-to-date\n[root@ncputility ~ hub_rc]$\n</code></pre> <p>4)  Open up the Argocd web console and select sync for site-config application tiles. once sync completed, monitor the progress via mcp. </p> <p>ARGO-CD URL</p> <p></p> <p>4.1) Continue with step 5 to 7, if step 4 does not trigger the mcp update automatically. </p> <p>5) list the mc for ssh on this hold and take a backup of it. </p> <pre><code>[root@ncputility ~ hub_rc]$ oc get mc 99-master-ssh  99-worker-ssh\nNAME            GENERATEDBYCONTROLLER   IGNITIONVERSION   AGE\n99-master-ssh                           3.2.0             29d\n99-worker-ssh                           3.2.0             29d\n[root@ncputility ~ hub_rc]$ oc get mc 99-master-ssh  99-worker-ssh -o yaml &gt; 99-master-worker-ssh.yaml\n[root@ncputility ~ hub_rc]$ \n[root@ncputility ~ hub_rc]$ oc get mc 99-master-ssh  99-worker-ssh -o yaml &gt; 99-master-worker-ssh-back.yaml\n[root@ncputility ~ hub_rc]$\n</code></pre> <p>6) update the <code>sshAuthorizedKeys</code> key with your latest key and then apply it. </p> <pre><code>[root@ncputility ~ hub_rc]$ vi 99-master-worker-ssh-back.yaml\n</code></pre> <pre><code>    - name: core\n      sshAuthorizedKeys:\n      - ssh-ed25519 AAAAC3NzaC1lZDI1NTE5AAAAIGW2BXbCPiDgL3m4y8y3UYdehEmdUsrQBNLOR27waVBr\n        root@ncputility.pphncp01.infra.mobi.eastlink.ca\n</code></pre> <p>this part need to updated. </p> <p>7) apply the change </p> <pre><code>[root@ncputility ~ hub_rc]$ oc apply -f 99-master-worker-ssh-back.yaml\nWarning: resource machineconfigs/99-master-ssh is missing the kubectl.kubernetes.io/last-applied-configuration annotation which is required by oc apply. oc apply should only be used on resources created declaratively by either oc create --save-config or oc apply. The missing annotation will be patched automatically.\nmachineconfig.machineconfiguration.openshift.io/99-master-ssh configured\nWarning: resource machineconfigs/99-worker-ssh is missing the kubectl.kubernetes.io/last-applied-configuration annotation which is required by oc apply. oc apply should only be used on resources created declaratively by either oc create --save-config or oc apply. The missing annotation will be patched automatically.\nmachineconfig.machineconfiguration.openshift.io/99-worker-ssh configured\n[root@ncputility ~ hub_rc]$\n</code></pre> <p>8)  Machine Config Operator should start applying the changes shortly. Observe which MachineConfigPools are being updated with oc get mcp and check more details on which node is being updated by following this solution.</p> <pre><code>[root@ncputility ~ hub_rc]$ oc get mcp\nNAME     CONFIG                                             UPDATED   UPDATING   DEGRADED   MACHINECOUNT   READYMACHINECOUNT   UPDATEDMACHINECOUNT   DEGRADEDMACHINECOUNT   AGE\nmaster   rendered-master-adf9d3edadac5fe32aa27a42deeb773e   True      False      False      3              3                   3                     0                      29d\nworker   rendered-worker-9ad974bc81cec3f37ff34bd866f0e906   True      False      False      0              0                   0                     0                      29d\n[root@ncputility ~ hub_rc]$\n</code></pre> <p>9) For more insights on how the change is being applied in a single node, then check the machine config daemon logs:</p> <pre><code>NODE='master-0.pphncp01.infra.mobi.eastlink.ca'\noc -n openshift-machine-config-operator logs -c machine-config-daemon $(oc -n openshift-machine-config-operator get pod -l k8s-app=machine-config-daemon --field-selector spec.nodeName=${NODE} -o name) -f \n</code></pre> <p>Where ${NODE} must be replaced with the name of the node. Omit -f if logs are not required to be followed. If the update was successfully applied, then logs similar to below lines are expected:</p> <pre><code>I0111 19:59:07.360110    7993 update.go:258] SSH Keys reconcilable\n...\nI0111 19:59:07.371253    7993 update.go:569] Writing SSHKeys at \"/home/core/.ssh\"\n...\nI0111 19:59:07.372208    7993 update.go:613] machine-config-daemon initiating reboot: Node will reboot into config worker-96b48815fa067f651fa50541ea6a9b5d\n</code></pre>"},{"location":"openshift/disaster-management/infra-manager-redeployment/#verification","title":"Verification","text":""},{"location":"openshift/disaster-management/infra-manager-redeployment/#verification-for-new-infra-quay-tls-on-the-hub-cluster-registry","title":"Verification for new infra quay TLS on the hub cluster registry.","text":"<p>1) Login to Customer VPN, followed by ssh to infra-manager node using username <code>ncpuser</code> with given password. </p> <pre><code>    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n    \u2502                 \u2022 MobaXterm Personal Edition v25.1 \u2022                 \u2502\n    \u2502               (SSH client, X server and network tools)               \u2502\n    \u2502                                                                      \u2502\n    \u2502 \u2b9e SSH session to ncpuser@10.236.6.230                                \u2502\n    \u2502   \u2022 Direct SSH      :  \u2713                                             \u2502\n    \u2502   \u2022 SSH compression :  \u2713                                             \u2502\n    \u2502   \u2022 SSH-browser     :  \u2713                                             \u2502\n    \u2502   \u2022 X11-forwarding  :  \u2713  (remote display is forwarded through SSH)  \u2502\n    \u2502                                                                      \u2502\n    \u2502 \u2b9e For more info, ctrl+click on help or visit our website.            \u2502\n    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\nActivate the web console with: systemctl enable --now cockpit.socket\n\nRegister this system with Red Hat Insights: insights-client --register\nCreate an account or view all your systems at https://red.ht/insights-dashboard\nLast login: Fri Jun 27 15:36:03 2025 from 10.192.52.140\n[ncpuser@ncputility ~]$\n[ncpuser@ncputility ~]$ sudo su -\n[sudo] password for ncpuser:\n[root@ncputility ~]# \n</code></pre> <p>2) login to hub cluster using oc login or rc file. </p> <pre><code>[root@ncputility ~ cwl_rc]$source hubrc\nWARNING: Using insecure TLS client config. Setting this option is not supported!\n\nLogin successful.\n\nYou have access to 105 projects, the list has been suppressed. You can list all projects with 'oc projects'\n\nUsing project \"default\".\n[root@ncputility ~ hub_rc]$ oc get pods -n openshift-nmstate\nNAME                                       STATUS   ROLES                                 AGE   VERSION\n\n[root@ncputility ~ hub_rc]$ \n</code></pre> <p>3) now try deleting some low profile pods.  l</p> <pre><code>oc get pods -A -o wide |grep -i controller \n\n# when you delete it you should be seeing pod recreated without any issue. \n</code></pre> <p>or</p> <pre><code>oc debug -t node/node.name\n\nchroot /root\n\npodman pull any/image/from/infra/quay.\n\nit should not through any TLS error. \n</code></pre>"},{"location":"openshift/disaster-management/infra-manager-redeployment/#verification-for-new-ssh-key-testing-on-hub-clusters","title":"Verification for new ssh key testing on hub clusters.","text":"<p>1) Login to Customer VPN, followed by ssh to infra-manager node using username <code>ncpuser</code> with given password. </p> <pre><code>    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n    \u2502                 \u2022 MobaXterm Personal Edition v25.1 \u2022                 \u2502\n    \u2502               (SSH client, X server and network tools)               \u2502\n    \u2502                                                                      \u2502\n    \u2502 \u2b9e SSH session to ncpuser@10.236.6.230                                \u2502\n    \u2502   \u2022 Direct SSH      :  \u2713                                             \u2502\n    \u2502   \u2022 SSH compression :  \u2713                                             \u2502\n    \u2502   \u2022 SSH-browser     :  \u2713                                             \u2502\n    \u2502   \u2022 X11-forwarding  :  \u2713  (remote display is forwarded through SSH)  \u2502\n    \u2502                                                                      \u2502\n    \u2502 \u2b9e For more info, ctrl+click on help or visit our website.            \u2502\n    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\nActivate the web console with: systemctl enable --now cockpit.socket\n\nRegister this system with Red Hat Insights: insights-client --register\nCreate an account or view all your systems at https://red.ht/insights-dashboard\nLast login: Fri Jun 27 15:36:03 2025 from 10.192.52.140\n[ncpuser@ncputility ~]$\n[ncpuser@ncputility ~]$ sudo su -\n[sudo] password for ncpuser:\n[root@ncputility ~]# \n</code></pre> <p>2) login to hub cluster using oc login or rc file. </p> <p>login to in the hub cluster using respective RC file. </p> <pre><code>[root@ncputility ~ cwl_rc]$source hubrc\nWARNING: Using insecure TLS client config. Setting this option is not supported!\n\nLogin successful.\n\nYou have access to 105 projects, the list has been suppressed. You can list all projects with 'oc projects'\n\nUsing project \"default\".\n[root@ncputility ~ hub_rc]$ oc get nodes\nNAME                                       STATUS   ROLES                                 AGE   VERSION\nmaster-0.pphncp01.infra.mobi.eastlink.ca   Ready    control-plane,master,monitor,worker   30d   v1.29.10+67d3387\nmaster-1.pphncp01.infra.mobi.eastlink.ca   Ready    control-plane,master,monitor,worker   30d   v1.29.10+67d3387\nmaster-2.pphncp01.infra.mobi.eastlink.ca   Ready    control-plane,master,monitor,worker   30d   v1.29.10+67d3387\n[root@ncputility ~ hub_rc]$ \n</code></pre> <p>3) now try ssh for each and every node in the list. </p> <p>do proper ssh test for all the nodes. </p> <pre><code>ssh core@master-0.pphncp01.infra.mobi.eastlink.ca\n</code></pre>"},{"location":"openshift/disaster-management/infra-manager-redeployment/#verification-for-new-ssh-key-testing-on-cwl-clusters","title":"Verification for new ssh key testing on cwl clusters.","text":"<p>1) Login to Customer VPN, followed by ssh to infra-manager node using username <code>ncpuser</code> with given password. </p> <pre><code>    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n    \u2502                 \u2022 MobaXterm Personal Edition v25.1 \u2022                 \u2502\n    \u2502               (SSH client, X server and network tools)               \u2502\n    \u2502                                                                      \u2502\n    \u2502 \u2b9e SSH session to ncpuser@10.236.6.230                                \u2502\n    \u2502   \u2022 Direct SSH      :  \u2713                                             \u2502\n    \u2502   \u2022 SSH compression :  \u2713                                             \u2502\n    \u2502   \u2022 SSH-browser     :  \u2713                                             \u2502\n    \u2502   \u2022 X11-forwarding  :  \u2713  (remote display is forwarded through SSH)  \u2502\n    \u2502                                                                      \u2502\n    \u2502 \u2b9e For more info, ctrl+click on help or visit our website.            \u2502\n    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\nActivate the web console with: systemctl enable --now cockpit.socket\n\nRegister this system with Red Hat Insights: insights-client --register\nCreate an account or view all your systems at https://red.ht/insights-dashboard\nLast login: Fri Jun 27 15:36:03 2025 from 10.192.52.140\n[ncpuser@ncputility ~]$\n[ncpuser@ncputility ~]$ sudo su -\n[sudo] password for ncpuser:\n[root@ncputility ~]# \n</code></pre> <p>2) login to hub cluster using oc login or rc file. </p> <p>login to in the hub cluster using respective RC file. </p> <pre><code>[root@ncputility ~ cwl_rc]$source cwlrc\nWARNING: Using insecure TLS client config. Setting this option is not supported!\n\nLogin successful.\n\nYou have access to 105 projects, the list has been suppressed. You can list all projects with 'oc projects'\n\nUsing project \"default\".\n[root@ncputility ~ cwl_rc]$ oc get nodes # this list sample, you will get big list of nodes.\nNAME                                       STATUS   ROLES                                 AGE   VERSION\nmaster-0.pphncp01.infra.mobi.eastlink.ca   Ready    control-plane,master,monitor,worker   30d   v1.29.10+67d3387\nmaster-1.pphncp01.infra.mobi.eastlink.ca   Ready    control-plane,master,monitor,worker   30d   v1.29.10+67d3387\nmaster-2.pphncp01.infra.mobi.eastlink.ca   Ready    control-plane,master,monitor,worker   30d   v1.29.10+67d3387\n\n**output omitted**\n[root@ncputility ~ cwl_rc]$ \n</code></pre> <p>3) now try ssh for each and every node in the list. </p> <p>do proper ssh test for all the nodes. </p> <pre><code>ssh core@master-0.pphncp01.infra.mobi.eastlink.ca\n</code></pre>"},{"location":"openshift/maintenace/openshift_scalein_guide/","title":"OpenShift Scale-In Procedure for Compute Node","text":"<p>This document outlines the steps required to scale in a compute node (e.g., <code>appworker2.panclypcwl01.mnc020.mcc714</code>) from an OpenShift cluster managed via ZTP/Agent-based deployment. This procedure assumes you are working within the OpenShift GitOps (ArgoCD) deployment flow.</p>"},{"location":"openshift/maintenace/openshift_scalein_guide/#edit-and-push-siteconfig","title":"Edit and Push SiteConfig","text":"<ol> <li>Open the Git repository used for ZTP automation.</li> <li>Navigate to the appropriate `` YAML file.</li> <li>Remove the node definition (e.g., <code>appworker2</code>) from the SiteConfig.</li> <li>Commit and push the changes to the Git repository.</li> </ol> <p>ArgoCD will automatically sync (if infra does not have auto sync enabeld, manually sync it) and delete the corresponding resources from the hub cluster. However, this will not remove the <code>agents.agent-install.openshift.io</code> resource, which must be deleted manually.</p>"},{"location":"openshift/maintenace/openshift_scalein_guide/#delete-agent-resource-from-hub-cluster","title":"Delete Agent Resource from Hub Cluster","text":"<pre><code>oc get agents.agent-install.openshift.io -n &lt;namespace&gt; -o wide | grep -i appworker2\n\n# Example:\noc get agents.agent-install.openshift.io -n ncpvnpvlab1 -o wide | grep -i appworker2\n\n# Then delete the matching agents\noc delete agents.agent-install.openshift.io -n ncpvnpvlab1 \\\n  0d4c5f4b-4751-96e1-4a02-a0dd5f8deb8e \n</code></pre>"},{"location":"openshift/maintenace/openshift_scalein_guide/#login-to-cwlspoke-cluster-and-verify-resources","title":"Login to CWL/Spoke Cluster and Verify Resources","text":""},{"location":"openshift/maintenace/openshift_scalein_guide/#list-machine-and-bmh-resources","title":"List Machine and BMH Resources","text":"<pre><code>oc -n openshift-machine-api get machine | grep -i appworker2\noc -n openshift-machine-api get bmh | grep -i appworker2\n</code></pre>"},{"location":"openshift/maintenace/openshift_scalein_guide/#delete-bare-metal-host-bmh","title":"Delete Bare Metal Host (BMH)","text":"<pre><code>oc -n openshift-machine-api delete bmh appworker2.panclypcwl01.scdsgplab.com\n</code></pre>"},{"location":"openshift/maintenace/openshift_scalein_guide/#if-bmh-is-not-deleted-automatically","title":"If BMH is Not Deleted Automatically:","text":"<p>Patch the BMH resource to remove finalizers:</p> <pre><code>oc -n openshift-machine-api patch bmh appworker2.panclypcwl01.scdsgplab.com \\\n  --type=merge -p '{\"metadata\": {\"finalizers\":null}}'\n</code></pre>"},{"location":"openshift/maintenace/openshift_scalein_guide/#delete-machine-resource","title":"Delete Machine Resource","text":"<pre><code>oc -n openshift-machine-api delete machine appworker2.panclypcwl01.scdsgplab.com \\\n  --kubeconfig /root/hubfeb19/auth/pan-cwl.yaml\n</code></pre>"},{"location":"openshift/maintenace/openshift_scalein_guide/#scale-in-nodes-from-initial-deployment-optional","title":"Scale-In Nodes from Initial Deployment (Optional)","text":"<p>If the node you wish to scale in was part of the initial deployment, you must also update the MachineSet count:</p> <pre><code>oc get machinesets.machine.openshift.io -n openshift-machine-api\n</code></pre> <p>Identify the relevant MachineSet and decrease the <code>replicas</code> count accordingly.</p> <p>Update with:</p> <pre><code>oc scale machineset &lt;machineset-name&gt; --replicas=&lt;desired-count&gt; -n openshift-machine-api\n</code></pre>"},{"location":"openshift/maintenace/openshift_scalein_guide/#notes","title":"Notes","text":"<ul> <li>Be careful when deleting or modifying cluster resources; take necessary etcd backups and must-gather output from hub and cwl.</li> <li>Always you informed the DTM before doing any changes to the cluster.</li> <li>Ensure that Git and ArgoCD are synced post any changes.</li> </ul>"},{"location":"openshift/maintenace/reboot-nodes/","title":"Rebooting nodes on the openshift cluster","text":"<p>steps to show, boot on master, worker/gateway and storage. </p>"},{"location":"openshift/maintenace/reboot-nodes/#master-node-reboot","title":"Master node reboot","text":"<p>1) login to right cluster using cluster-admin based role. </p> <pre><code>[root@dom14npv101-infra-manager ~ management]# source  /root/raj/managementrc\nWARNING: Using insecure TLS client config. Setting this option is not supported!\n\nLogin successful.\n\nYou have access to 99 projects, the list has been suppressed. You can list all projects with 'oc projects'\n\nUsing project \"default\".\n[root@dom14npv101-infra-manager ~ management]# \n</code></pre> <p>2) get the list of master nodes and also check the node status. </p> <pre><code>[root@dom14npv101-infra-manager ~ management]# oc get nodes |grep -i master\nncpvnpvmgt-master-101.ncpvnpvmgt.pnwlab.nsn-rdnet.net    Ready    control-plane,master,monitor       32d   v1.29.10+67d3387\nncpvnpvmgt-master-102.ncpvnpvmgt.pnwlab.nsn-rdnet.net    Ready    control-plane,master,monitor       32d   v1.29.10+67d3387\nncpvnpvmgt-master-103.ncpvnpvmgt.pnwlab.nsn-rdnet.net    Ready    control-plane,master,monitor       32d   v1.29.10+67d3387\n[root@dom14npv101-infra-manager ~ management]#\n</code></pre> <p>3) drain the node completely </p> <pre><code>node=ncpvnpvmgt-master-101.ncpvnpvmgt.pnwlab.nsn-rdnet.net\noc adm drain $node --ignore-daemonsets --delete-emptydir-data --force \noc adm drain $node --ignore-daemonsets --delete-emptydir-data --force --disable-eviction \n</code></pre> <p>3.1) Check for any pod got stuck at termination phase or failed to evicate due to pod distrubation policy etc. </p> <p>3.2) those stuck pods should be fully removed/deleted to move to next step here. </p> <p>4) trigger the shutdown now. </p> <pre><code>oc debug node/$node -- chroot /host shutdown -r +1 \n</code></pre> <p>5) once this server came up, waiting for server to be in ready state </p> <pre><code>oc wait --for=condition=Ready node/$node --timeout=800s \n</code></pre> <p>6) uncordon the node and make it schedulable now. </p> <pre><code>oc adm uncordon $node \n</code></pre>"},{"location":"openshift/maintenace/reboot-nodes/#workergateway-node-reboot","title":"Worker/Gateway node reboot","text":"<p>1) login to right cluster using cluster-admin based role. </p> <pre><code>[root@dom14npv101-infra-manager ~ management]# source  /root/raj/managementrc\nWARNING: Using insecure TLS client config. Setting this option is not supported!\n\nLogin successful.\n\nYou have access to 99 projects, the list has been suppressed. You can list all projects with 'oc projects'\n\nUsing project \"default\".\n[root@dom14npv101-infra-manager ~ management]# \n</code></pre> <p>2) get the list of worker/gateway nodes and also check the node status. </p> <pre><code>[root@dom14npv101-infra-manager ~ management]# oc get nodes |grep -i master\nncpvnpvmgt-master-101.ncpvnpvmgt.pnwlab.nsn-rdnet.net    Ready    control-plane,master,monitor       32d   v1.29.10+67d3387\nncpvnpvmgt-master-102.ncpvnpvmgt.pnwlab.nsn-rdnet.net    Ready    control-plane,master,monitor       32d   v1.29.10+67d3387\nncpvnpvmgt-master-103.ncpvnpvmgt.pnwlab.nsn-rdnet.net    Ready    control-plane,master,monitor       32d   v1.29.10+67d3387\n[root@dom14npv101-infra-manager ~ management]#\n</code></pre> <p>3) drain the node completely </p> <pre><code>node=ncpvnpvmgt-master-101.ncpvnpvmgt.pnwlab.nsn-rdnet.net\noc adm drain $node --ignore-daemonsets --delete-emptydir-data --force \noc adm drain $node --ignore-daemonsets --delete-emptydir-data --force --disable-eviction \n</code></pre> <p>3.1) Check for any pod got stuck at termination phase or failed to evicate due to pod distrubation policy etc. </p> <p>3.2) those stuck pods should be fully removed/deleted to move to next step here. </p> <p>4) trigger the shutdown now. </p> <pre><code>oc debug node/$node -- chroot /host shutdown -r +1 \n</code></pre> <p>5) once this server came up, waiting for server to be in ready state </p> <pre><code>oc wait --for=condition=Ready node/$node --timeout=800s \n</code></pre> <p>6) uncordon the node and make it schedulable now. </p> <pre><code>oc adm uncordon $node \n</code></pre>"},{"location":"openshift/maintenace/reboot-nodes/#storage-node-reboot","title":"Storage node reboot","text":"<p>1) login to right cluster using cluster-admin based role. </p> <pre><code>[root@dom14npv101-infra-manager ~ management]# source  /root/raj/managementrc\nWARNING: Using insecure TLS client config. Setting this option is not supported!\n\nLogin successful.\n\nYou have access to 99 projects, the list has been suppressed. You can list all projects with 'oc projects'\n\nUsing project \"default\".\n[root@dom14npv101-infra-manager ~ management]# \n</code></pre> <p>2) get the list of storage nodes and also check the ceph status. </p> <pre><code>[root@dom14npv101-infra-manager ~ vlabrc]# oc get no |grep -i storage\nncpvnpvlab1-storage-101.ncpvnpvlab1.pnwlab.nsn-rdnet.net   Ready                      storage,worker                     24d   v1.29.10+67d3387\nncpvnpvlab1-storage-102.ncpvnpvlab1.pnwlab.nsn-rdnet.net   Ready                      storage,worker                     26d   v1.29.10+67d3387\nncpvnpvlab1-storage-103.ncpvnpvlab1.pnwlab.nsn-rdnet.net   Ready                      storage,worker                     26d   v1.29.10+67d3387\nncpvnpvlab1-storage-201.ncpvnpvlab1.pnwlab.nsn-rdnet.net   Ready                      storage,worker                     24d   v1.29.10+67d3387\nncpvnpvlab1-storage-202.ncpvnpvlab1.pnwlab.nsn-rdnet.net   Ready                      storage,worker                     26d   v1.29.10+67d3387\nncpvnpvlab1-storage-203.ncpvnpvlab1.pnwlab.nsn-rdnet.net   Ready                      storage,worker                     26d   v1.29.10+67d3387\n[root@dom14npv101-infra-manager ~ vlabrc]# \n\n[root@dom14npv101-infra-manager ~ vlabrc]# oc exec -it $(oc get pod -n openshift-storage -l app=rook-ceph-operator -o name) -n openshift-storage -- ceph -s -c /var/lib/rook/openshift-storage/openshift-storage.config\n</code></pre> <p>3) define a variable called and value as respective storage node, easy run of reboot</p> <pre><code>[root@dom14npv101-infra-manager ~ vlabrc]# node=ncpvnpvlab1-storage-203.ncpvnpvlab1.pnwlab.nsn-rdnet.net\n</code></pre> <p>4) get the list ceph storage related pods </p> <pre><code>[root@dom14npv101-infra-manager ~ vlabrc]# oc -n openshift-storage get po -o wide | grep -e mon -e osd | grep  ${node}|grep -iv complete|awk '{print $1}'\nrook-ceph-osd-12-589c6485c9-ptqs7\nrook-ceph-osd-16-74cbc54b4f-tg8lw\nrook-ceph-osd-2-7fcc4ff664-hjc2w\nrook-ceph-osd-26-67d86fbf5d-lqdv8\nrook-ceph-osd-30-78fb4f588b-474jn\nrook-ceph-osd-37-bd549c676-jn5ld\nrook-ceph-osd-40-5b4ddb6d7d-7qd8z\nrook-ceph-osd-6-5b64cf8d49-n55zf\n[root@dom14npv101-infra-manager ~ vlabrc]# \n</code></pre> <p>5) use notepad++ to create these following commands to remove those pods on that host. </p> <pre><code>[root@dom14npv101-infra-manager ~ vlabrc]# oc -n openshift-storage scale deployment rook-ceph-osd-12 --replicas=0\noc -n openshift-storage scale deployment rook-ceph-osd-16 --replicas=0\noc -n openshift-storage scale deployment rook-ceph-osd-2  --replicas=0\noc -n openshift-storage scale deployment rook-ceph-osd-26 --replicas=0\noc -n openshift-storage scale deployment rook-ceph-osd-30 --replicas=0\noc -n openshift-storage scale deployment rook-ceph-osd-37 --replicas=0\noc -n openshift-storage scale deployment rook-ceph-osd-40 --replicas=0\noc -n openshift-storage scale deployment rook-ceph-osd-6  --replicas=0\ndeployment.apps/rook-ceph-osd-12 scaled\ndeployment.apps/rook-ceph-osd-16 scaled\ndeployment.apps/rook-ceph-osd-2 scaled\ndeployment.apps/rook-ceph-osd-26 scaled\ndeployment.apps/rook-ceph-osd-30 scaled\ndeployment.apps/rook-ceph-osd-37 scaled\ndeployment.apps/rook-ceph-osd-40 scaled\ndeployment.apps/rook-ceph-osd-6 scaled\n[root@dom14npv101-infra-manager ~ vlabrc]#\n</code></pre> <p>6) Completely drain that storage node, using oc adm command </p> <pre><code>[root@dom14npv101-infra-manager ~ vlabrc]#oc adm drain ${node} --delete-emptydir-data --ignore-daemonsets=true --timeout=500s --force\nnode/ncpvnpvlab1-storage-203.ncpvnpvlab1.pnwlab.nsn-rdnet.net cordoned\nWarning: ignoring DaemonSet-managed Pods: openshift-cluster-node-tuning-operator/tuned-kqqfn, openshift-dns/dns-default-27nmr, openshift-dns/node-resolver-4dcng, openshift-image-registry/node-ca-2xtbk, openshift-ingress-canary/ingress-canary-285m2, openshift-local-storage/diskmaker-discovery-zmmwj, openshift-local-storage/diskmaker-manager-dvpjq, openshift-logging/collector-7g5zv, openshift-machine-config-operator/machine-config-daemon-c7c4v, openshift-monitoring/node-exporter-gsh5l, openshift-multus/multus-6q7c8, openshift-multus/multus-additional-cni-plugins-pwq85, openshift-multus/network-metrics-daemon-f6flp, openshift-multus/whereabouts-reconciler-fjpmv, openshift-network-diagnostics/network-check-target-9h7cj, openshift-network-operator/iptables-alerter-km8jq, openshift-nmstate/nmstate-handler-6hmz9, openshift-operators/istio-cni-node-v2-5-4qpnk, openshift-ovn-kubernetes/ovnkube-node-w672f, openshift-storage/csi-cephfsplugin-ck4mf, openshift-storage/csi-rbdplugin-gkmjm\nevicting pod openshift-storage/rook-ceph-osd-6-5b64cf8d49-n55zf\nevicting pod openshift-compliance/openscap-pod-24dd5998a72563f75be98757ffbe02e424df617e\nevicting pod openshift-storage/rook-ceph-crashcollector-9c7a6e51d7dc201a808c754612468a82-j84n5\nevicting pod openshift-storage/rook-ceph-mds-ocs-storagecluster-cephfilesystem-b-78f7bbf8bn4rl\nevicting pod openshift-storage/rook-ceph-mgr-b-5468b7cf-nx4d4\nevicting pod openshift-storage/rook-ceph-osd-40-5b4ddb6d7d-7qd8z\nevicting pod openshift-storage/rook-ceph-exporter-9c7a6e51d7dc201a808c754612468a82-c854b4r2tn4\nevicting pod openshift-compliance/openscap-pod-5f54688367da49304dfd83a2e0d564582b073f2b\npod/openscap-pod-5f54688367da49304dfd83a2e0d564582b073f2b evicted\npod/openscap-pod-24dd5998a72563f75be98757ffbe02e424df617e evicted\npod/rook-ceph-crashcollector-9c7a6e51d7dc201a808c754612468a82-j84n5 evicted\npod/rook-ceph-mgr-b-5468b7cf-nx4d4 evicted\npod/rook-ceph-exporter-9c7a6e51d7dc201a808c754612468a82-c854b4r2tn4 evicted\npod/rook-ceph-mds-ocs-storagecluster-cephfilesystem-b-78f7bbf8bn4rl evicted\nnode/ncpvnpvlab1-storage-203.ncpvnpvlab1.pnwlab.nsn-rdnet.net drained\n[root@dom14npv101-infra-manager ~ vlabrc]#\n</code></pre> <p>7) reboot the respective storage node using following command. </p> <pre><code>[root@dom14npv101-infra-manager ~ vlabrc]#oc debug node/${node} -- chroot /host systemctl reboot \n</code></pre> <p>8) waiting for node to be fully up. then check the kubelet status </p> <pre><code>[root@dom14npv101-infra-manager ~ vlabrc]# oc get no |grep -i storage\nncpvnpvlab1-storage-101.ncpvnpvlab1.pnwlab.nsn-rdnet.net   Ready                      storage,worker                     24d   v1.29.10+67d3387\nncpvnpvlab1-storage-102.ncpvnpvlab1.pnwlab.nsn-rdnet.net   Ready                      storage,worker                     26d   v1.29.10+67d3387\nncpvnpvlab1-storage-103.ncpvnpvlab1.pnwlab.nsn-rdnet.net   Ready                      storage,worker                     26d   v1.29.10+67d3387\nncpvnpvlab1-storage-201.ncpvnpvlab1.pnwlab.nsn-rdnet.net   Ready                      storage,worker                     24d   v1.29.10+67d3387\nncpvnpvlab1-storage-202.ncpvnpvlab1.pnwlab.nsn-rdnet.net   Ready                      storage,worker                     26d   v1.29.10+67d3387\nncpvnpvlab1-storage-203.ncpvnpvlab1.pnwlab.nsn-rdnet.net   Ready,sechd-disable        storage,worker                     26d   v1.29.10+67d3387\n[root@dom14npv101-infra-manager ~ vlabrc]# \n</code></pre> <p>9) uncordon the storage node.</p> <pre><code>[root@dom14npv101-infra-manager ~ vlabrc]# oc adm uncordon ncpvnpvlab1-storage-203.ncpvnpvlab1.pnwlab.nsn-rdnet.net \n</code></pre> <p>10) check the ceph health and wait for all osd to be fully up.  give 10mins</p> <pre><code>[root@dom14npv101-infra-manager ~ vlabrc]# oc exec -it $(oc get pod -n openshift-storage -l app=rook-ceph-operator -o name) -n openshift-storage -- ceph -s -c /var/lib/rook/openshift-storage/openshift-storage.config\n</code></pre>"},{"location":"openshift/networking/metalb-troubleshooting/","title":"metalb configuration troubleshooting","text":"<p>1)  login to cluster </p> <pre><code>[root@ncputility ~ pancwl_rc]$ source /root/pancwlrc\nWARNING: Using insecure TLS client config. Setting this option is not supported!\n\nLogin successful.\n\nYou have access to 113 projects, the list has been suppressed. You can list all projects with 'oc projects'\n\nUsing project \"ncd01pan\".\n</code></pre> <p>2) check for <code>metallb-system</code> namespace exist</p> <pre><code>[root@ncputility ~ pancwl_rc]$ oc get ns | grep -i metallb\nmetallb-system                                     Active   26d\n[root@ncputility ~ pancwl_rc]$\n</code></pre> <p>3) check the status of the pods on the <code>metallb-system</code> namespace here. </p> <pre><code>[root@ncputility ~ pancwl_rc]$ oc get pods -n metallb-system\nNAME                                                   READY   STATUS    RESTARTS   AGE\ncontroller-5785bc85cb-qpk8h                            2/2     Running   0          24d\nmetallb-operator-controller-manager-7bf5d8978d-clpd7   1/1     Running   0          26d\nmetallb-operator-webhook-server-86784c6c8c-49ncp       1/1     Running   0          26d\nspeaker-dlrn5                                          6/6     Running   0          24d\nspeaker-g2g77                                          6/6     Running   0          24d\nspeaker-jzbw7                                          6/6     Running   0          24d\nspeaker-pjstl                                          6/6     Running   0          24d\n</code></pre> <p>4) check for <code>bfdprofile</code> on this cluster </p> <p>here transmit interval and receiver interval should be equal from local and remote end. </p> <pre><code> oc -n metallb-system get BFDProfile -o wide\nNAME                                PASSIVE MODE   TRANSMIT INTERVAL   RECEIVE INTERVAL   MULTIPLIER\nncp-metallb-oam-pa-hn-bfd-profile   true           300                 300                3\nncp-metallb-oam-pa-ni-bfd-profile   true           300                 300                3\nncp-metallb-oam-pa-pa-bfd-profile   true           300                 300                3\nncp-metallb-oam-pa-sv-bfd-profile   true           300                 300                3\n</code></pre> <p>5) make sure, desination having the backward route configured here .</p> <pre><code>[root@ncputility ~ pancwl_rc]$ oc -n metallb-system get nncp -o wide\nNAME                                           STATUS      REASON\nbackward-route-for-oam-pa-pa-metallb-vlan104   Available   SuccessfullyConfigured\nncp-metallb-oam-pa-hn-route-for-switches-105   Available   SuccessfullyConfigured\nncp-metallb-oam-pa-ni-route-for-switches-107   Available   SuccessfullyConfigured\nncp-metallb-oam-pa-pa-route-for-switches-104   Available   SuccessfullyConfigured\nncp-metallb-oam-pa-sv-route-for-switches-106   Available   SuccessfullyConfigured\ntenant-bond-bgp-oam-vlan104-gateway-0          Available   SuccessfullyConfigured\ntenant-bond-bgp-oam-vlan104-gateway-1          Available   SuccessfullyConfigured\ntenant-bond-bgp-oam-vlan104-gateway-2          Available   SuccessfullyConfigured\n** output omitted **\ntenantvlan-373                                 Available   SuccessfullyConfigured\ntenantvlan-374                                 Available   SuccessfullyConfigured\n</code></pre> <p>6) check for <code>ipaddresspool</code> exist here, so that application can create their <code>service</code> as <code>loadbalancer</code> here . </p> <pre><code>[root@ncputility ~ pancwl_rc]$ oc -n metallb-system get IPAddressPool -o wide\nNAME                                AUTO ASSIGN   AVOID BUGGY IPS   ADDRESSES\nncp-metallb-oam-pa-hn-addresspool   false         false             [\"10.89.147.128/28\"]\nncp-metallb-oam-pa-ni-addresspool   false         false             [\"10.86.10.16/28\"]\nncp-metallb-oam-pa-pa-addresspool   false         false             [\"10.89.101.128/27\",\"10.89.97.208/28\"]\nncp-metallb-oam-pa-sv-addresspool   false         false             [\"10.85.186.240/28\"]\n</code></pre> <p>7) check for <code>bgppeer</code> are up on the metallb speakers thats important for this communication</p> <pre><code>[root@ncputility ~ pancwl_rc]$ oc -n metallb-system get BGPPeer -o wide\nNAME                               ADDRESS         ASN          BFD PROFILE                         MULTI HOPS\nncp-metallb-oam-pa-hn-bgp-peer-1   10.89.147.194   4200000320   ncp-metallb-oam-pa-hn-bfd-profile\nncp-metallb-oam-pa-hn-bgp-peer-2   10.89.147.195   4200000320   ncp-metallb-oam-pa-hn-bfd-profile\nncp-metallb-oam-pa-ni-bgp-peer-1   10.86.10.98     4200000320   ncp-metallb-oam-pa-ni-bfd-profile\nncp-metallb-oam-pa-ni-bgp-peer-2   10.86.10.99     4200000320   ncp-metallb-oam-pa-ni-bfd-profile\nncp-metallb-oam-pa-pa-bgp-peer-1   10.89.97.162    4200000320   ncp-metallb-oam-pa-pa-bfd-profile\nncp-metallb-oam-pa-pa-bgp-peer-2   10.89.97.163    4200000320   ncp-metallb-oam-pa-pa-bfd-profile\nncp-metallb-oam-pa-sv-bgp-peer-1   10.85.187.34    4200000320   ncp-metallb-oam-pa-sv-bfd-profile\nncp-metallb-oam-pa-sv-bgp-peer-2   10.85.187.35    4200000320   ncp-metallb-oam-pa-sv-bfd-profile\n</code></pre> <p>8) check for <code>BGPAdvertisement</code> are created on this cluster and it should be in the <code>metallb-system</code> namespace. </p> <pre><code>[root@ncputility ~ pancwl_rc]$ oc -n metallb-system get BGPAdvertisement -o wide\nNAME                                      IPADDRESSPOOLS                          IPADDRESSPOOL SELECTORS   PEERS                                                                     NODE SELECTORS\nncp-metallb-oam-pa-hn-bgp-advertisement   [\"ncp-metallb-oam-pa-hn-addresspool\"]                             [\"ncp-metallb-oam-pa-hn-bgp-peer-1\",\"ncp-metallb-oam-pa-hn-bgp-peer-2\"]\nncp-metallb-oam-pa-ni-bgp-advertisement   [\"ncp-metallb-oam-pa-ni-addresspool\"]                             [\"ncp-metallb-oam-pa-ni-bgp-peer-1\",\"ncp-metallb-oam-pa-ni-bgp-peer-2\"]\nncp-metallb-oam-pa-pa-bgp-advertisement   [\"ncp-metallb-oam-pa-pa-addresspool\"]                             [\"ncp-metallb-oam-pa-pa-bgp-peer-1\",\"ncp-metallb-oam-pa-pa-bgp-peer-2\"]\nncp-metallb-oam-pa-sv-bgp-advertisement   [\"ncp-metallb-oam-pa-sv-addresspool\"]                             [\"ncp-metallb-oam-pa-sv-bgp-peer-1\",\"ncp-metallb-oam-pa-sv-bgp-peer-2\"]\n</code></pre> <p>10) No, error from the container logs on this namespace .</p> <pre><code>[root@ncputility ~ pancwl_rc]$ oc -n metallb-system logs -l component=speaker\nDefaulted container \"speaker\" out of: speaker, frr, reloader, frr-metrics, kube-rbac-proxy, kube-rbac-proxy-frr, cp-frr-files (init), cp-reloader (init), cp-metrics (init)\nDefaulted container \"speaker\" out of: speaker, frr, reloader, frr-metrics, kube-rbac-proxy, kube-rbac-proxy-frr, cp-frr-files (init), cp-reloader (init), cp-metrics (init)\nDefaulted container \"speaker\" out of: speaker, frr, reloader, frr-metrics, kube-rbac-proxy, kube-rbac-proxy-frr, cp-frr-files (init), cp-reloader (init), cp-metrics (init)\nDefaulted container \"speaker\" out of: speaker, frr, reloader, frr-metrics, kube-rbac-proxy, kube-rbac-proxy-frr, cp-frr-files (init), cp-reloader (init), cp-metrics (init)\n{\"caller\":\"node_controller.go:46\",\"controller\":\"NodeReconciler\",\"level\":\"info\",\"start reconcile\":\"/gateway4.panclypcwl01.mnc020.mcc714\",\"ts\":\"2025-03-31T04:44:07Z\"}\n{\"caller\":\"node_controller.go:69\",\"controller\":\"NodeReconciler\",\"end reconcile\":\"/gateway4.panclypcwl01.mnc020.mcc714\",\"level\":\"info\",\"ts\":\"2025-03-31T04:44:07Z\"}\n{\"caller\":\"node_controller.go:46\",\"controller\":\"NodeReconciler\",\"level\":\"info\",\"start reconcile\":\"/gateway3.panclypcwl01.mnc020.mcc714\",\"ts\":\"2025-03-31T04:44:12Z\"}\n{\"caller\":\"node_controller.go:69\",\"controller\":\"NodeReconciler\",\"end reconcile\":\"/gateway3.panclypcwl01.mnc020.mcc714\",\"level\":\"info\",\"ts\":\"2025-03-31T04:44:12Z\"}\n{\"caller\":\"node_controller.go:46\",\"controller\":\"NodeReconciler\",\"level\":\"info\",\"start reconcile\":\"/appworker16.panclypcwl01.mnc020.mcc714\",\"ts\":\"2025-03-31T04:44:21Z\"}\n{\"caller\":\"node_controller.go:69\",\"controller\":\"NodeReconciler\",\"end reconcile\":\"/appworker16.panclypcwl01.mnc020.mcc714\",\"level\":\"info\",\"ts\":\"2025-03-31T04:44:21Z\"}\n{\"caller\":\"node_controller.go:46\",\"controller\":\"NodeReconciler\",\"level\":\"info\",\"start reconcile\":\"/appworker20.panclypcwl01.mnc020.mcc714\",\"ts\":\"2025-03-31T04:44:34Z\"}\n{\"caller\":\"node_controller.go:69\",\"controller\":\"NodeReconciler\",\"end reconcile\":\"/appworker20.panclypcwl01.mnc020.mcc714\",\"level\":\"info\",\"ts\":\"2025-03-31T04:44:34Z\"}\n{\"caller\":\"node_controller.go:46\",\"controller\":\"NodeReconciler\",\"level\":\"info\",\"start reconcile\":\"/appworker28.panclypcwl01.mnc020.mcc714\",\"ts\":\"2025-03-31T04:44:44Z\"}\n{\"caller\":\"node_controller.go:69\",\"controller\":\"NodeReconciler\",\"end reconcile\":\"/appworker28.panclypcwl01.mnc020.mcc714\",\"level\":\"info\",\"ts\":\"2025-03-31T04:44:44Z\"}\n{\"caller\":\"node_controller.go:69\",\"controller\":\"NodeReconciler\",\"end reconcile\":\"/gateway4.panclypcwl01.mnc020.mcc714\",\"level\":\"info\",\"ts\":\"2025-03-31T04:44:07Z\"}\n{\"caller\":\"node_controller.go:46\",\"controller\":\"NodeReconciler\",\"level\":\"info\",\"start reconcile\":\"/gateway3.panclypcwl01.mnc020.mcc714\",\"ts\":\"2025-03-31T04:44:12Z\"}\n{\"caller\":\"speakerlist.go:274\",\"level\":\"info\",\"msg\":\"triggering discovery\",\"op\":\"memberDiscovery\",\"ts\":\"2025-03-31T04:44:12Z\"}\n{\"caller\":\"node_controller.go:69\",\"controller\":\"NodeReconciler\",\"end reconcile\":\"/gateway3.panclypcwl01.mnc020.mcc714\",\"level\":\"info\",\"ts\":\"2025-03-31T04:44:12Z\"}\n{\"caller\":\"node_controller.go:46\",\"controller\":\"NodeReconciler\",\"level\":\"info\",\"start reconcile\":\"/appworker16.panclypcwl01.mnc020.mcc714\",\"ts\":\"2025-03-31T04:44:21Z\"}\n{\"caller\":\"node_controller.go:69\",\"controller\":\"NodeReconciler\",\"end reconcile\":\"/appworker16.panclypcwl01.mnc020.mcc714\",\"level\":\"info\",\"ts\":\"2025-03-31T04:44:21Z\"}\n{\"caller\":\"node_controller.go:46\",\"controller\":\"NodeReconciler\",\"level\":\"info\",\"start reconcile\":\"/appworker20.panclypcwl01.mnc020.mcc714\",\"ts\":\"2025-03-31T04:44:34Z\"}\n{\"caller\":\"node_controller.go:69\",\"controller\":\"NodeReconciler\",\"end reconcile\":\"/appworker20.panclypcwl01.mnc020.mcc714\",\"level\":\"info\",\"ts\":\"2025-03-31T04:44:34Z\"}\n{\"caller\":\"node_controller.go:46\",\"controller\":\"NodeReconciler\",\"level\":\"info\",\"start reconcile\":\"/appworker28.panclypcwl01.mnc020.mcc714\",\"ts\":\"2025-03-31T04:44:44Z\"}\n{\"caller\":\"node_controller.go:69\",\"controller\":\"NodeReconciler\",\"end reconcile\":\"/appworker28.panclypcwl01.mnc020.mcc714\",\"level\":\"info\",\"ts\":\"2025-03-31T04:44:44Z\"}\n{\"caller\":\"node_controller.go:46\",\"controller\":\"NodeReconciler\",\"level\":\"info\",\"start reconcile\":\"/gateway4.panclypcwl01.mnc020.mcc714\",\"ts\":\"2025-03-31T04:44:07Z\"}\n{\"caller\":\"node_controller.go:69\",\"controller\":\"NodeReconciler\",\"end reconcile\":\"/gateway4.panclypcwl01.mnc020.mcc714\",\"level\":\"info\",\"ts\":\"2025-03-31T04:44:07Z\"}\n{\"caller\":\"node_controller.go:46\",\"controller\":\"NodeReconciler\",\"level\":\"info\",\"start reconcile\":\"/gateway3.panclypcwl01.mnc020.mcc714\",\"ts\":\"2025-03-31T04:44:12Z\"}\n{\"caller\":\"node_controller.go:69\",\"controller\":\"NodeReconciler\",\"end reconcile\":\"/gateway3.panclypcwl01.mnc020.mcc714\",\"level\":\"info\",\"ts\":\"2025-03-31T04:44:12Z\"}\n{\"caller\":\"node_controller.go:46\",\"controller\":\"NodeReconciler\",\"level\":\"info\",\"start reconcile\":\"/appworker16.panclypcwl01.mnc020.mcc714\",\"ts\":\"2025-03-31T04:44:21Z\"}\n{\"caller\":\"node_controller.go:69\",\"controller\":\"NodeReconciler\",\"end reconcile\":\"/appworker16.panclypcwl01.mnc020.mcc714\",\"level\":\"info\",\"ts\":\"2025-03-31T04:44:21Z\"}\n{\"caller\":\"node_controller.go:46\",\"controller\":\"NodeReconciler\",\"level\":\"info\",\"start reconcile\":\"/appworker20.panclypcwl01.mnc020.mcc714\",\"ts\":\"2025-03-31T04:44:34Z\"}\n{\"caller\":\"node_controller.go:69\",\"controller\":\"NodeReconciler\",\"end reconcile\":\"/appworker20.panclypcwl01.mnc020.mcc714\",\"level\":\"info\",\"ts\":\"2025-03-31T04:44:34Z\"}\n{\"caller\":\"node_controller.go:46\",\"controller\":\"NodeReconciler\",\"level\":\"info\",\"start reconcile\":\"/appworker28.panclypcwl01.mnc020.mcc714\",\"ts\":\"2025-03-31T04:44:44Z\"}\n{\"caller\":\"node_controller.go:69\",\"controller\":\"NodeReconciler\",\"end reconcile\":\"/appworker28.panclypcwl01.mnc020.mcc714\",\"level\":\"info\",\"ts\":\"2025-03-31T04:44:44Z\"}\n{\"caller\":\"speakerlist.go:274\",\"level\":\"info\",\"msg\":\"triggering discovery\",\"op\":\"memberDiscovery\",\"ts\":\"2025-03-31T04:44:07Z\"}\n{\"caller\":\"node_controller.go:69\",\"controller\":\"NodeReconciler\",\"end reconcile\":\"/gateway4.panclypcwl01.mnc020.mcc714\",\"level\":\"info\",\"ts\":\"2025-03-31T04:44:07Z\"}\n{\"caller\":\"node_controller.go:46\",\"controller\":\"NodeReconciler\",\"level\":\"info\",\"start reconcile\":\"/gateway3.panclypcwl01.mnc020.mcc714\",\"ts\":\"2025-03-31T04:44:12Z\"}\n{\"caller\":\"node_controller.go:69\",\"controller\":\"NodeReconciler\",\"end reconcile\":\"/gateway3.panclypcwl01.mnc020.mcc714\",\"level\":\"info\",\"ts\":\"2025-03-31T04:44:12Z\"}\n{\"caller\":\"node_controller.go:46\",\"controller\":\"NodeReconciler\",\"level\":\"info\",\"start reconcile\":\"/appworker16.panclypcwl01.mnc020.mcc714\",\"ts\":\"2025-03-31T04:44:21Z\"}\n{\"caller\":\"node_controller.go:69\",\"controller\":\"NodeReconciler\",\"end reconcile\":\"/appworker16.panclypcwl01.mnc020.mcc714\",\"level\":\"info\",\"ts\":\"2025-03-31T04:44:21Z\"}\n{\"caller\":\"node_controller.go:46\",\"controller\":\"NodeReconciler\",\"level\":\"info\",\"start reconcile\":\"/appworker20.panclypcwl01.mnc020.mcc714\",\"ts\":\"2025-03-31T04:44:34Z\"}\n{\"caller\":\"node_controller.go:69\",\"controller\":\"NodeReconciler\",\"end reconcile\":\"/appworker20.panclypcwl01.mnc020.mcc714\",\"level\":\"info\",\"ts\":\"2025-03-31T04:44:34Z\"}\n{\"caller\":\"node_controller.go:46\",\"controller\":\"NodeReconciler\",\"level\":\"info\",\"start reconcile\":\"/appworker28.panclypcwl01.mnc020.mcc714\",\"ts\":\"2025-03-31T04:44:44Z\"}\n{\"caller\":\"node_controller.go:69\",\"controller\":\"NodeReconciler\",\"end reconcile\":\"/appworker28.panclypcwl01.mnc020.mcc714\",\"level\":\"info\",\"ts\":\"2025-03-31T04:44:44Z\"}\n</code></pre> <p>11)  just showing an backwards route here for comparison  and your destination should be exist here . </p> <pre><code>[root@ncputility ~ pancwl_rc]$ oc get nncp -A -o yaml\napiVersion: v1\nitems:\n- apiVersion: nmstate.io/v1\n  kind: NodeNetworkConfigurationPolicy\n  metadata:\n    annotations:\n      kubectl.kubernetes.io/last-applied-configuration: |\n        {\"apiVersion\":\"nmstate.io/v1\",\"kind\":\"NodeNetworkConfigurationPolicy\",\"metadata\":{\"annotations\":{},\"name\":\"backward-route-for-oam-pa-pa-metallb-vlan104\"},\"spec\":{\"desiredState\":{\"routes\":{\"config\":[{\"destination\":\"10.89.100.66/32\",\"metric\":150,\"next-hop-address\":\"10.89.97.161\",\"next-hop-interface\":\"vlan104\",\"table-id\":254},{\"destination\":\"10.89.27.4/32\",\"metric\":150,\"next-hop-address\":\"10.89.97.161\",\"next-hop-interface\":\"vlan104\",\"table-id\":254}]}},\"nodeSelector\":{\"node-role.kubernetes.io/gateway\":\"\"}}}\n      nmstate.io/webhook-mutating-timestamp: \"1743107464714866687\"\n    creationTimestamp: \"2025-03-27T20:31:04Z\"\n    generation: 1\n    name: backward-route-for-oam-pa-pa-metallb-vlan104\n    resourceVersion: \"36077334\"\n    uid: d72a67aa-44d0-403f-8ed5-29a49994eaf6\n  spec:\n    desiredState:\n      routes:\n        config:\n        - destination: 10.89.100.66/32\n          metric: 150\n          next-hop-address: 10.89.97.161\n          next-hop-interface: vlan104\n          table-id: 254\n        - destination: 10.89.27.4/32\n          metric: 150\n          next-hop-address: 10.89.97.161\n          next-hop-interface: vlan104\n          table-id: 254\n    nodeSelector:\n      node-role.kubernetes.io/gateway: \"\"\n  status:\n    conditions:\n    - lastHeartbeatTime: \"2025-03-27T20:32:10Z\"\n      lastTransitionTime: \"2025-03-27T20:32:10Z\"\n      message: 4/4 nodes successfully configured\n      reason: SuccessfullyConfigured\n      status: \"True\"\n      type: Available\n    - lastHeartbeatTime: \"2025-03-27T20:32:10Z\"\n      lastTransitionTime: \"2025-03-27T20:32:10Z\"\n      reason: SuccessfullyConfigured\n      status: \"False\"\n      type: Degraded\n    - lastHeartbeatTime: \"2025-03-27T20:32:10Z\"\n      lastTransitionTime: \"2025-03-27T20:32:10Z\"\n      reason: ConfigurationProgressing\n      status: \"False\"\n      type: Progressing\n    lastUnavailableNodeCountUpdate: \"2025-03-27T20:32:09Z\"\n</code></pre> <ol> <li>Check the status of local and remote configuration by login into the metallb-system namespace. </li> </ol> <p><code>bdf and bgp should be fully up</code></p> <pre><code>[root@ncputility ~ pancwl_rc]$ oc get -n metallb-system pods -l component=speaker -o wide\nNAME            READY   STATUS    RESTARTS   AGE   IP            NODE                                  NOMINATED NODE   READINESS GATES\nspeaker-dlrn5   6/6     Running   0          24d   10.89.96.18   gateway2.panclypcwl01.mnc020.mcc714   &lt;none&gt;           &lt;none&gt;\nspeaker-g2g77   6/6     Running   0          24d   10.89.96.19   gateway3.panclypcwl01.mnc020.mcc714   &lt;none&gt;           &lt;none&gt;\nspeaker-jzbw7   6/6     Running   0          24d   10.89.96.17   gateway1.panclypcwl01.mnc020.mcc714   &lt;none&gt;           &lt;none&gt;\nspeaker-pjstl   6/6     Running   0          24d   10.89.96.20   gateway4.panclypcwl01.mnc020.mcc714   &lt;none&gt;           &lt;none&gt;\n</code></pre> <pre><code>[root@ncputility ~ pancwl_rc]$ oc exec -n metallb-system  speaker-dlrn5 -c frr -- vtysh -c \"show running-config\"\nBuilding configuration...\n\nCurrent configuration:\n!\nfrr version 8.3.1\nfrr defaults traditional\nhostname gateway2.panclypcwl01.mnc020.mcc714\nlog file /etc/frr/frr.log informational\nlog timestamp precision 3\nservice integrated-vtysh-config\n!\nrouter bgp 4200000320\n no bgp ebgp-requires-policy\n no bgp default ipv4-unicast\n no bgp network import-check\n neighbor 10.85.187.34 remote-as 4200000320\n neighbor 10.85.187.34 bfd\n neighbor 10.85.187.34 bfd profile ncp-metallb-oam-pa-sv-bfd-profile\n neighbor 10.85.187.34 timers 30 90\n neighbor 10.85.187.35 remote-as 4200000320\n neighbor 10.85.187.35 bfd\n neighbor 10.85.187.35 bfd profile ncp-metallb-oam-pa-sv-bfd-profile\n neighbor 10.85.187.35 timers 30 90\n neighbor 10.86.10.98 remote-as 4200000320\n neighbor 10.86.10.98 bfd\n neighbor 10.86.10.98 bfd profile ncp-metallb-oam-pa-ni-bfd-profile\n neighbor 10.86.10.98 timers 30 90\n neighbor 10.86.10.99 remote-as 4200000320\n neighbor 10.86.10.99 bfd\n neighbor 10.86.10.99 bfd profile ncp-metallb-oam-pa-ni-bfd-profile\n neighbor 10.86.10.99 timers 30 90\n neighbor 10.89.97.162 remote-as 4200000320\n neighbor 10.89.97.162 bfd\n neighbor 10.89.97.162 bfd profile ncp-metallb-oam-pa-pa-bfd-profile\n neighbor 10.89.97.162 timers 30 90\n neighbor 10.89.97.163 remote-as 4200000320\n neighbor 10.89.97.163 bfd\n neighbor 10.89.97.163 bfd profile ncp-metallb-oam-pa-pa-bfd-profile\n neighbor 10.89.97.163 timers 30 90\n neighbor 10.89.147.194 remote-as 4200000320\n neighbor 10.89.147.194 bfd\n neighbor 10.89.147.194 bfd profile ncp-metallb-oam-pa-hn-bfd-profile\n neighbor 10.89.147.194 timers 30 90\n neighbor 10.89.147.195 remote-as 4200000320\n neighbor 10.89.147.195 bfd\n neighbor 10.89.147.195 bfd profile ncp-metallb-oam-pa-hn-bfd-profile\n neighbor 10.89.147.195 timers 30 90\n !\n address-family ipv4 unicast\n  network 10.89.97.210/32\n  neighbor 10.85.187.34 activate\n  neighbor 10.85.187.34 route-map 10.85.187.34-in in\n  neighbor 10.85.187.34 route-map 10.85.187.34-out out\n  neighbor 10.85.187.35 activate\n  neighbor 10.85.187.35 route-map 10.85.187.35-in in\n  neighbor 10.85.187.35 route-map 10.85.187.35-out out\n  neighbor 10.86.10.98 activate\n  neighbor 10.86.10.98 route-map 10.86.10.98-in in\n  neighbor 10.86.10.98 route-map 10.86.10.98-out out\n  neighbor 10.86.10.99 activate\n  neighbor 10.86.10.99 route-map 10.86.10.99-in in\n  neighbor 10.86.10.99 route-map 10.86.10.99-out out\n  neighbor 10.89.97.162 activate\n  neighbor 10.89.97.162 route-map 10.89.97.162-in in\n  neighbor 10.89.97.162 route-map 10.89.97.162-out out\n  neighbor 10.89.97.163 activate\n  neighbor 10.89.97.163 route-map 10.89.97.163-in in\n  neighbor 10.89.97.163 route-map 10.89.97.163-out out\n  neighbor 10.89.147.194 activate\n  neighbor 10.89.147.194 route-map 10.89.147.194-in in\n  neighbor 10.89.147.194 route-map 10.89.147.194-out out\n  neighbor 10.89.147.195 activate\n  neighbor 10.89.147.195 route-map 10.89.147.195-in in\n  neighbor 10.89.147.195 route-map 10.89.147.195-out out\n exit-address-family\n !\n address-family ipv6 unicast\n  neighbor 10.85.187.34 activate\n  neighbor 10.85.187.34 route-map 10.85.187.34-in in\n  neighbor 10.85.187.34 route-map 10.85.187.34-out out\n  neighbor 10.85.187.35 activate\n  neighbor 10.85.187.35 route-map 10.85.187.35-in in\n  neighbor 10.85.187.35 route-map 10.85.187.35-out out\n  neighbor 10.86.10.98 activate\n  neighbor 10.86.10.98 route-map 10.86.10.98-in in\n  neighbor 10.86.10.98 route-map 10.86.10.98-out out\n  neighbor 10.86.10.99 activate\n  neighbor 10.86.10.99 route-map 10.86.10.99-in in\n  neighbor 10.86.10.99 route-map 10.86.10.99-out out\n  neighbor 10.89.97.162 activate\n  neighbor 10.89.97.162 route-map 10.89.97.162-in in\n  neighbor 10.89.97.162 route-map 10.89.97.162-out out\n  neighbor 10.89.97.163 activate\n  neighbor 10.89.97.163 route-map 10.89.97.163-in in\n  neighbor 10.89.97.163 route-map 10.89.97.163-out out\n  neighbor 10.89.147.194 activate\n  neighbor 10.89.147.194 route-map 10.89.147.194-in in\n  neighbor 10.89.147.194 route-map 10.89.147.194-out out\n  neighbor 10.89.147.195 activate\n  neighbor 10.89.147.195 route-map 10.89.147.195-in in\n  neighbor 10.89.147.195 route-map 10.89.147.195-out out\n exit-address-family\nexit\n!\nip prefix-list 10.89.97.162-pl-ipv4 seq 1 permit 10.89.97.210/32\nip prefix-list 10.89.97.163-pl-ipv4 seq 1 permit 10.89.97.210/32\nip prefix-list 10.85.187.34-pl-ipv4 seq 1 deny any\nip prefix-list 10.85.187.35-pl-ipv4 seq 1 deny any\nip prefix-list 10.86.10.98-pl-ipv4 seq 1 deny any\nip prefix-list 10.86.10.99-pl-ipv4 seq 1 deny any\nip prefix-list 10.89.147.194-pl-ipv4 seq 1 deny any\nip prefix-list 10.89.147.195-pl-ipv4 seq 1 deny any\n!\nipv6 prefix-list 10.89.97.162-pl-ipv4 seq 2 deny any\nipv6 prefix-list 10.89.97.163-pl-ipv4 seq 2 deny any\nipv6 prefix-list 10.85.187.34-pl-ipv4 seq 2 deny any\nipv6 prefix-list 10.85.187.35-pl-ipv4 seq 2 deny any\nipv6 prefix-list 10.86.10.98-pl-ipv4 seq 2 deny any\nipv6 prefix-list 10.86.10.99-pl-ipv4 seq 2 deny any\nipv6 prefix-list 10.89.147.194-pl-ipv4 seq 2 deny any\nipv6 prefix-list 10.89.147.195-pl-ipv4 seq 2 deny any\n!\nroute-map 10.85.187.34-in deny 20\nexit\n!\nroute-map 10.85.187.34-out permit 1\n match ip address prefix-list 10.85.187.34-pl-ipv4\nexit\n!\nroute-map 10.85.187.34-out permit 2\n match ipv6 address prefix-list 10.85.187.34-pl-ipv4\nexit\n!\nroute-map 10.85.187.35-in deny 20\nexit\n!\nroute-map 10.85.187.35-out permit 1\n match ip address prefix-list 10.85.187.35-pl-ipv4\nexit\n!\nroute-map 10.85.187.35-out permit 2\n match ipv6 address prefix-list 10.85.187.35-pl-ipv4\nexit\n!\nroute-map 10.86.10.98-in deny 20\nexit\n!\nroute-map 10.86.10.98-out permit 1\n match ip address prefix-list 10.86.10.98-pl-ipv4\nexit\n!\nroute-map 10.86.10.98-out permit 2\n match ipv6 address prefix-list 10.86.10.98-pl-ipv4\nexit\n!\nroute-map 10.86.10.99-in deny 20\nexit\n!\nroute-map 10.86.10.99-out permit 1\n match ip address prefix-list 10.86.10.99-pl-ipv4\nexit\n!\nroute-map 10.86.10.99-out permit 2\n match ipv6 address prefix-list 10.86.10.99-pl-ipv4\nexit\n!\nroute-map 10.89.147.194-in deny 20\nexit\n!\nroute-map 10.89.147.194-out permit 1\n match ip address prefix-list 10.89.147.194-pl-ipv4\nexit\n!\nroute-map 10.89.147.194-out permit 2\n match ipv6 address prefix-list 10.89.147.194-pl-ipv4\nexit\n!\nroute-map 10.89.147.195-in deny 20\nexit\n!\nroute-map 10.89.147.195-out permit 1\n match ip address prefix-list 10.89.147.195-pl-ipv4\nexit\n!\nroute-map 10.89.147.195-out permit 2\n match ipv6 address prefix-list 10.89.147.195-pl-ipv4\nexit\n!\nroute-map 10.89.97.162-in deny 20\nexit\n!\nroute-map 10.89.97.162-out permit 1\n match ip address prefix-list 10.89.97.162-pl-ipv4\nexit\n!\nroute-map 10.89.97.162-out permit 2\n match ipv6 address prefix-list 10.89.97.162-pl-ipv4\nexit\n!\nroute-map 10.89.97.163-in deny 20\nexit\n!\nroute-map 10.89.97.163-out permit 1\n match ip address prefix-list 10.89.97.163-pl-ipv4\nexit\n!\nroute-map 10.89.97.163-out permit 2\n match ipv6 address prefix-list 10.89.97.163-pl-ipv4\nexit\n!\nip nht resolve-via-default\n!\nipv6 nht resolve-via-default\n!\nbfd\n profile ncp-metallb-oam-pa-hn-bfd-profile\n  passive-mode\n exit\n !\n profile ncp-metallb-oam-pa-ni-bfd-profile\n  passive-mode\n exit\n !\n profile ncp-metallb-oam-pa-pa-bfd-profile\n  passive-mode\n exit\n !\n profile ncp-metallb-oam-pa-sv-bfd-profile\n  passive-mode\n exit\n !\nexit\n!\nend\n</code></pre> <pre><code>[root@ncputility ~ pancwl_rc]$ oc exec -n metallb-system  speaker-dlrn5 -c frr -- vtysh -c \"show bgp summary\"\n\nIPv4 Unicast Summary (VRF default):\nBGP router identifier 172.16.2.2, local AS number 4200000320 vrf-id 0\nBGP table version 1\nRIB entries 1, using 192 bytes of memory\nPeers 8, using 5788 KiB of memory\n\nNeighbor        V         AS   MsgRcvd   MsgSent   TblVer  InQ OutQ  Up/Down State/PfxRcd   PfxSnt Desc\n10.85.187.34    4 4200000320         0         0        0    0    0    never       Active        0 N/A\n10.85.187.35    4 4200000320         0         0        0    0    0    never       Active        0 N/A\n10.86.10.98     4 4200000320         0         0        0    0    0    never       Active        0 N/A\n10.86.10.99     4 4200000320         0         0        0    0    0    never       Active        0 N/A\n10.89.97.162    4 4200000320      7306      7307        0    0    0 2d12h50m            0        1 N/A\n10.89.97.163    4 4200000320      7305      7307        0    0    0 2d12h50m            0        1 N/A\n10.89.147.194   4 4200000320         0         0        0    0    0    never       Active        0 N/A\n10.89.147.195   4 4200000320         0         0        0    0    0    never       Active        0 N/A\n\nTotal number of neighbors 8\n\nIPv6 Unicast Summary (VRF default):\nBGP router identifier 172.16.2.2, local AS number 4200000320 vrf-id 0\nBGP table version 0\nRIB entries 0, using 0 bytes of memory\nPeers 8, using 5788 KiB of memory\n\nNeighbor        V         AS   MsgRcvd   MsgSent   TblVer  InQ OutQ  Up/Down State/PfxRcd   PfxSnt Desc\n10.85.187.34    4 4200000320         0         0        0    0    0    never       Active        0 N/A\n10.85.187.35    4 4200000320         0         0        0    0    0    never       Active        0 N/A\n10.86.10.98     4 4200000320         0         0        0    0    0    never       Active        0 N/A\n10.86.10.99     4 4200000320         0         0        0    0    0    never       Active        0 N/A\n10.89.97.162    4 4200000320      7306      7307        0    0    0 2d12h50m        NoNeg    NoNeg N/A\n10.89.97.163    4 4200000320      7305      7307        0    0    0 2d12h50m        NoNeg    NoNeg N/A\n10.89.147.194   4 4200000320         0         0        0    0    0    never       Active        0 N/A\n10.89.147.195   4 4200000320         0         0        0    0    0    never       Active        0 N/A\n\nTotal number of neighbors 8\n</code></pre> <pre><code>[root@ncputility ~ pancwl_rc]$ oc exec -n metallb-system  speaker-dlrn5 -c frr -- vtysh -c \"show bfd peers brief\"\nSession count: 2\nSessionId  LocalAddress                             PeerAddress                             Status\n=========  ============                             ===========                             ======\n62069252   10.89.97.165                             10.89.97.162                            down\n3159552171 10.89.97.165                             10.89.97.163                            down\n</code></pre> <pre><code>[root@ncputility ~ pancwl_rc]$ oc exec -n metallb-system  speaker-dlrn5 -c frr -- vtysh -c \"show bfd peer\"\nBFD Peers:\n        peer 10.89.97.162 local-address 10.89.97.165 vrf default interface vlan104\n                ID: 62069252\n                Remote ID: 0\n                Passive mode\n                Status: down\n                Downtime: 2 day(s), 10 hour(s), 3 minute(s), 16 second(s)\n                Diagnostics: ok\n                Remote diagnostics: ok\n                Peer Type: dynamic\n                Local timers:\n                        Detect-multiplier: 3\n                        Receive interval: 300ms\n                        Transmission interval: 300ms\n                        Echo receive interval: 50ms\n                        Echo transmission interval: disabled\n                Remote timers:\n                        Detect-multiplier: 3\n                        Receive interval: 1000ms\n                        Transmission interval: 1000ms\n                        Echo receive interval: disabled\n\n        peer 10.89.97.163 local-address 10.89.97.165 vrf default interface vlan104\n                ID: 3159552171\n                Remote ID: 0\n                Passive mode\n                Status: down\n                Downtime: 2 day(s), 10 hour(s), 3 minute(s), 16 second(s)\n                Diagnostics: ok\n                Remote diagnostics: ok\n                Peer Type: dynamic\n                Local timers:\n                        Detect-multiplier: 3\n                        Receive interval: 300ms\n                        Transmission interval: 300ms\n                        Echo receive interval: 50ms\n                        Echo transmission interval: disabled\n                Remote timers:\n                        Detect-multiplier: 3\n                        Receive interval: 1000ms\n                        Transmission interval: 1000ms\n                        Echo receive interval: disabled\n</code></pre> <pre><code>[root@ncputility ~ pancwl_rc]$ oc exec -n metallb-system  speaker-dlrn5 -c frr -- vtysh -c \"show ip bgp neighbors\"\nBGP neighbor is 10.85.187.34, remote AS 4200000320, local AS 4200000320, internal link\n  BGP version 4, remote router ID 0.0.0.0, local router ID 172.16.2.2\n  BGP state = Active\n  Last read 2d10h03m, Last write never\n  Hold time is 90, keepalive interval is 30 seconds\n  Configured hold time is 90, keepalive interval is 30 seconds\n  Configured conditional advertisements interval is 60 seconds\n  Graceful restart information:\n    Local GR Mode: Helper*\n    Remote GR Mode: NotApplicable\n    R bit: False\n    N bit: False\n    Timers:\n      Configured Restart Time(sec): 120\n      Received Restart Time(sec): 0\n  Message statistics:\n    Inq depth is 0\n    Outq depth is 0\n                         Sent       Rcvd\n    Opens:                  0          0\n    Notifications:          0          0\n    Updates:                0          0\n    Keepalives:             0          0\n    Route Refresh:          0          0\n    Capability:             0          0\n    Total:                  0          0\n  Minimum time between advertisement runs is 0 seconds\n\n For address family: IPv4 Unicast\n  Not part of any update group\n  Community attribute sent to this neighbor(all)\n  Inbound path policy configured\n  Outbound path policy configured\n  Route map for incoming advertisements is *10.85.187.34-in\n  Route map for outgoing advertisements is *10.85.187.34-out\n  0 accepted prefixes\n\n For address family: IPv6 Unicast\n  Not part of any update group\n  Community attribute sent to this neighbor(all)\n  Inbound path policy configured\n  Outbound path policy configured\n  Route map for incoming advertisements is *10.85.187.34-in\n  Route map for outgoing advertisements is *10.85.187.34-out\n  0 accepted prefixes\n\n  Connections established 0; dropped 0\n  Last reset 2d10h03m,  Waiting for peer OPEN\nBGP Connect Retry Timer in Seconds: 120\nNext connect timer due in 80 seconds\nRead thread: off  Write thread: off  FD used: -1\n\n  BFD: Type: multi hop\n  Detect Multiplier: 3, Min Rx interval: 300, Min Tx interval: 300\n  Status: Unknown, Last update: never\n\nBGP neighbor is 10.85.187.35, remote AS 4200000320, local AS 4200000320, internal link\n  BGP version 4, remote router ID 0.0.0.0, local router ID 172.16.2.2\n  BGP state = Active\n  Last read 2d10h03m, Last write never\n  Hold time is 90, keepalive interval is 30 seconds\n  Configured hold time is 90, keepalive interval is 30 seconds\n  Configured conditional advertisements interval is 60 seconds\n  Graceful restart information:\n    Local GR Mode: Helper*\n    Remote GR Mode: NotApplicable\n    R bit: False\n    N bit: False\n    Timers:\n      Configured Restart Time(sec): 120\n      Received Restart Time(sec): 0\n  Message statistics:\n    Inq depth is 0\n    Outq depth is 0\n                         Sent       Rcvd\n    Opens:                  0          0\n    Notifications:          0          0\n    Updates:                0          0\n    Keepalives:             0          0\n    Route Refresh:          0          0\n    Capability:             0          0\n    Total:                  0          0\n  Minimum time between advertisement runs is 0 seconds\n\n For address family: IPv4 Unicast\n  Not part of any update group\n  Community attribute sent to this neighbor(all)\n  Inbound path policy configured\n  Outbound path policy configured\n  Route map for incoming advertisements is *10.85.187.35-in\n  Route map for outgoing advertisements is *10.85.187.35-out\n  0 accepted prefixes\n\n For address family: IPv6 Unicast\n  Not part of any update group\n  Community attribute sent to this neighbor(all)\n  Inbound path policy configured\n  Outbound path policy configured\n  Route map for incoming advertisements is *10.85.187.35-in\n  Route map for outgoing advertisements is *10.85.187.35-out\n  0 accepted prefixes\n\n  Connections established 0; dropped 0\n  Last reset 2d10h03m,  Waiting for peer OPEN\nBGP Connect Retry Timer in Seconds: 120\nNext connect timer due in 80 seconds\nRead thread: off  Write thread: off  FD used: -1\n\n  BFD: Type: multi hop\n  Detect Multiplier: 3, Min Rx interval: 300, Min Tx interval: 300\n  Status: Unknown, Last update: never\n\nBGP neighbor is 10.86.10.98, remote AS 4200000320, local AS 4200000320, internal link\n  BGP version 4, remote router ID 0.0.0.0, local router ID 172.16.2.2\n  BGP state = Active\n  Last read 2d10h03m, Last write never\n  Hold time is 90, keepalive interval is 30 seconds\n  Configured hold time is 90, keepalive interval is 30 seconds\n  Configured conditional advertisements interval is 60 seconds\n  Graceful restart information:\n    Local GR Mode: Helper*\n    Remote GR Mode: NotApplicable\n    R bit: False\n    N bit: False\n    Timers:\n      Configured Restart Time(sec): 120\n      Received Restart Time(sec): 0\n  Message statistics:\n    Inq depth is 0\n    Outq depth is 0\n                         Sent       Rcvd\n    Opens:                  0          0\n    Notifications:          0          0\n    Updates:                0          0\n    Keepalives:             0          0\n    Route Refresh:          0          0\n    Capability:             0          0\n    Total:                  0          0\n  Minimum time between advertisement runs is 0 seconds\n\n For address family: IPv4 Unicast\n  Not part of any update group\n  Community attribute sent to this neighbor(all)\n  Inbound path policy configured\n  Outbound path policy configured\n  Route map for incoming advertisements is *10.86.10.98-in\n  Route map for outgoing advertisements is *10.86.10.98-out\n  0 accepted prefixes\n\n For address family: IPv6 Unicast\n  Not part of any update group\n  Community attribute sent to this neighbor(all)\n  Inbound path policy configured\n  Outbound path policy configured\n  Route map for incoming advertisements is *10.86.10.98-in\n  Route map for outgoing advertisements is *10.86.10.98-out\n  0 accepted prefixes\n\n  Connections established 0; dropped 0\n  Last reset 2d10h03m,  Waiting for peer OPEN\nBGP Connect Retry Timer in Seconds: 120\nNext connect timer due in 80 seconds\nRead thread: off  Write thread: off  FD used: -1\n\n  BFD: Type: multi hop\n  Detect Multiplier: 3, Min Rx interval: 300, Min Tx interval: 300\n  Status: Unknown, Last update: never\n\nBGP neighbor is 10.86.10.99, remote AS 4200000320, local AS 4200000320, internal link\n  BGP version 4, remote router ID 0.0.0.0, local router ID 172.16.2.2\n  BGP state = Active\n  Last read 2d10h03m, Last write never\n  Hold time is 90, keepalive interval is 30 seconds\n  Configured hold time is 90, keepalive interval is 30 seconds\n  Configured conditional advertisements interval is 60 seconds\n  Graceful restart information:\n    Local GR Mode: Helper*\n    Remote GR Mode: NotApplicable\n    R bit: False\n    N bit: False\n    Timers:\n      Configured Restart Time(sec): 120\n      Received Restart Time(sec): 0\n  Message statistics:\n    Inq depth is 0\n    Outq depth is 0\n                         Sent       Rcvd\n    Opens:                  0          0\n    Notifications:          0          0\n    Updates:                0          0\n    Keepalives:             0          0\n    Route Refresh:          0          0\n    Capability:             0          0\n    Total:                  0          0\n  Minimum time between advertisement runs is 0 seconds\n\n For address family: IPv4 Unicast\n  Not part of any update group\n  Community attribute sent to this neighbor(all)\n  Inbound path policy configured\n  Outbound path policy configured\n  Route map for incoming advertisements is *10.86.10.99-in\n  Route map for outgoing advertisements is *10.86.10.99-out\n  0 accepted prefixes\n\n For address family: IPv6 Unicast\n  Not part of any update group\n  Community attribute sent to this neighbor(all)\n  Inbound path policy configured\n  Outbound path policy configured\n  Route map for incoming advertisements is *10.86.10.99-in\n  Route map for outgoing advertisements is *10.86.10.99-out\n  0 accepted prefixes\n\n  Connections established 0; dropped 0\n  Last reset 2d10h03m,  Waiting for peer OPEN\nBGP Connect Retry Timer in Seconds: 120\nNext connect timer due in 80 seconds\nRead thread: off  Write thread: off  FD used: -1\n\n  BFD: Type: multi hop\n  Detect Multiplier: 3, Min Rx interval: 300, Min Tx interval: 300\n  Status: Unknown, Last update: never\n\nBGP neighbor is 10.89.97.162, remote AS 4200000320, local AS 4200000320, internal link\n  BGP version 4, remote router ID 10.29.90.34, local router ID 172.16.2.2\n  BGP state = Established, up for 2d12h53m\n  Last read 00:00:00, Last write 00:00:03\n  Hold time is 90, keepalive interval is 30 seconds\n  Configured hold time is 90, keepalive interval is 30 seconds\n  Configured conditional advertisements interval is 60 seconds\n  Neighbor capabilities:\n    4 Byte AS: advertised and received\n    Extended Message: advertised\n    AddPath:\n      IPv4 Unicast: RX advertised\n      IPv6 Unicast: RX advertised\n    Long-lived Graceful Restart: advertised\n    Route refresh: advertised and received(new)\n    Enhanced Route Refresh: advertised\n    Address Family IPv4 Unicast: advertised and received\n    Address Family IPv6 Unicast: advertised\n    Hostname Capability: advertised (name: gateway2.panclypcwl01.mnc020.mcc714,domain name: n/a) not received\n    Graceful Restart Capability: advertised and received\n      Remote Restart timer is 300 seconds\n      Address families by peer:\n        none\n  Graceful restart information:\n    End-of-RIB send: IPv4 Unicast\n    End-of-RIB received: IPv4 Unicast\n    Local GR Mode: Helper*\n    Remote GR Mode: Helper\n    R bit: False\n    N bit: False\n    Timers:\n      Configured Restart Time(sec): 120\n      Received Restart Time(sec): 300\n    IPv4 Unicast:\n      F bit: False\n      End-of-RIB sent: Yes\n      End-of-RIB sent after update: Yes\n      End-of-RIB received: Yes\n      Timers:\n        Configured Stale Path Time(sec): 360\n    IPv6 Unicast:\n      F bit: False\n      End-of-RIB sent: No\n      End-of-RIB sent after update: No\n      End-of-RIB received: No\n      Timers:\n        Configured Stale Path Time(sec): 360\n  Message statistics:\n    Inq depth is 0\n    Outq depth is 0\n                         Sent       Rcvd\n    Opens:                  3          1\n    Notifications:          0          0\n    Updates:                2          2\n    Keepalives:          7307       7308\n    Route Refresh:          0          0\n    Capability:             0          0\n    Total:               7312       7311\n  Minimum time between advertisement runs is 0 seconds\n\n For address family: IPv4 Unicast\n  Update group 3, subgroup 3\n  Packet Queue length 0\n  Community attribute sent to this neighbor(all)\n  Inbound path policy configured\n  Outbound path policy configured\n  Route map for incoming advertisements is *10.89.97.162-in\n  Route map for outgoing advertisements is *10.89.97.162-out\n  0 accepted prefixes\n\n For address family: IPv6 Unicast\n  Not part of any update group\n  Community attribute sent to this neighbor(all)\n  Inbound path policy configured\n  Outbound path policy configured\n  Route map for incoming advertisements is *10.89.97.162-in\n  Route map for outgoing advertisements is *10.89.97.162-out\n  0 accepted prefixes\n\n  Connections established 1; dropped 0\n  Last reset 2d12h55m,  Waiting for peer OPEN\nLocal host: 10.89.97.165, Local port: 37680\nForeign host: 10.89.97.162, Foreign port: 179\nNexthop: 10.89.97.165\nNexthop global: ::\nNexthop local: ::\nBGP connection: shared network\nBGP Connect Retry Timer in Seconds: 120\nRead thread: on  Write thread: on  FD used: 26\n\n  BFD: Type: single hop\n  Detect Multiplier: 3, Min Rx interval: 300, Min Tx interval: 300\n  Status: Down, Last update: 2:10:03:42\n\nBGP neighbor is 10.89.97.163, remote AS 4200000320, local AS 4200000320, internal link\n  BGP version 4, remote router ID 10.29.90.38, local router ID 172.16.2.2\n  BGP state = Established, up for 2d12h53m\n  Last read 00:00:29, Last write 00:00:03\n  Hold time is 90, keepalive interval is 30 seconds\n  Configured hold time is 90, keepalive interval is 30 seconds\n  Configured conditional advertisements interval is 60 seconds\n  Neighbor capabilities:\n    4 Byte AS: advertised and received\n    Extended Message: advertised\n    AddPath:\n      IPv4 Unicast: RX advertised\n      IPv6 Unicast: RX advertised\n    Long-lived Graceful Restart: advertised\n    Route refresh: advertised and received(new)\n    Enhanced Route Refresh: advertised\n    Address Family IPv4 Unicast: advertised and received\n    Address Family IPv6 Unicast: advertised\n    Hostname Capability: advertised (name: gateway2.panclypcwl01.mnc020.mcc714,domain name: n/a) not received\n    Graceful Restart Capability: advertised and received\n      Remote Restart timer is 300 seconds\n      Address families by peer:\n        none\n  Graceful restart information:\n    End-of-RIB send: IPv4 Unicast\n    End-of-RIB received: IPv4 Unicast\n    Local GR Mode: Helper*\n    Remote GR Mode: Helper\n    R bit: False\n    N bit: False\n    Timers:\n      Configured Restart Time(sec): 120\n      Received Restart Time(sec): 300\n    IPv4 Unicast:\n      F bit: False\n      End-of-RIB sent: Yes\n      End-of-RIB sent after update: Yes\n      End-of-RIB received: Yes\n      Timers:\n        Configured Stale Path Time(sec): 360\n    IPv6 Unicast:\n      F bit: False\n      End-of-RIB sent: No\n      End-of-RIB sent after update: No\n      End-of-RIB received: No\n      Timers:\n        Configured Stale Path Time(sec): 360\n  Message statistics:\n    Inq depth is 0\n    Outq depth is 0\n                         Sent       Rcvd\n    Opens:                  3          1\n    Notifications:          0          0\n    Updates:                2          1\n    Keepalives:          7307       7307\n    Route Refresh:          0          0\n    Capability:             0          0\n    Total:               7312       7309\n  Minimum time between advertisement runs is 0 seconds\n\n For address family: IPv4 Unicast\n  Update group 4, subgroup 4\n  Packet Queue length 0\n  Community attribute sent to this neighbor(all)\n  Inbound path policy configured\n  Outbound path policy configured\n  Route map for incoming advertisements is *10.89.97.163-in\n  Route map for outgoing advertisements is *10.89.97.163-out\n  0 accepted prefixes\n\n For address family: IPv6 Unicast\n  Not part of any update group\n  Community attribute sent to this neighbor(all)\n  Inbound path policy configured\n  Outbound path policy configured\n  Route map for incoming advertisements is *10.89.97.163-in\n  Route map for outgoing advertisements is *10.89.97.163-out\n  0 accepted prefixes\n\n  Connections established 1; dropped 0\n  Last reset 2d12h55m,  Waiting for peer OPEN\nLocal host: 10.89.97.165, Local port: 54688\nForeign host: 10.89.97.163, Foreign port: 179\nNexthop: 10.89.97.165\nNexthop global: ::\nNexthop local: ::\nBGP connection: shared network\nBGP Connect Retry Timer in Seconds: 120\nRead thread: on  Write thread: on  FD used: 25\n\n  BFD: Type: single hop\n  Detect Multiplier: 3, Min Rx interval: 300, Min Tx interval: 300\n  Status: Down, Last update: 2:10:03:42\n\nBGP neighbor is 10.89.147.194, remote AS 4200000320, local AS 4200000320, internal link\n  BGP version 4, remote router ID 0.0.0.0, local router ID 172.16.2.2\n  BGP state = Active\n  Last read 2d10h03m, Last write never\n  Hold time is 90, keepalive interval is 30 seconds\n  Configured hold time is 90, keepalive interval is 30 seconds\n  Configured conditional advertisements interval is 60 seconds\n  Graceful restart information:\n    Local GR Mode: Helper*\n    Remote GR Mode: NotApplicable\n    R bit: False\n    N bit: False\n    Timers:\n      Configured Restart Time(sec): 120\n      Received Restart Time(sec): 0\n  Message statistics:\n    Inq depth is 0\n    Outq depth is 0\n                         Sent       Rcvd\n    Opens:                  0          0\n    Notifications:          0          0\n    Updates:                0          0\n    Keepalives:             0          0\n    Route Refresh:          0          0\n    Capability:             0          0\n    Total:                  0          0\n  Minimum time between advertisement runs is 0 seconds\n\n For address family: IPv4 Unicast\n  Not part of any update group\n  Community attribute sent to this neighbor(all)\n  Inbound path policy configured\n  Outbound path policy configured\n  Route map for incoming advertisements is *10.89.147.194-in\n  Route map for outgoing advertisements is *10.89.147.194-out\n  0 accepted prefixes\n\n For address family: IPv6 Unicast\n  Not part of any update group\n  Community attribute sent to this neighbor(all)\n  Inbound path policy configured\n  Outbound path policy configured\n  Route map for incoming advertisements is *10.89.147.194-in\n  Route map for outgoing advertisements is *10.89.147.194-out\n  0 accepted prefixes\n\n  Connections established 0; dropped 0\n  Last reset 2d10h03m,  Waiting for peer OPEN\nBGP Connect Retry Timer in Seconds: 120\nNext connect timer due in 80 seconds\nRead thread: off  Write thread: off  FD used: -1\n\n  BFD: Type: multi hop\n  Detect Multiplier: 3, Min Rx interval: 300, Min Tx interval: 300\n  Status: Unknown, Last update: never\n\nBGP neighbor is 10.89.147.195, remote AS 4200000320, local AS 4200000320, internal link\n  BGP version 4, remote router ID 0.0.0.0, local router ID 172.16.2.2\n  BGP state = Active\n  Last read 2d10h03m, Last write never\n  Hold time is 90, keepalive interval is 30 seconds\n  Configured hold time is 90, keepalive interval is 30 seconds\n  Configured conditional advertisements interval is 60 seconds\n  Graceful restart information:\n    Local GR Mode: Helper*\n    Remote GR Mode: NotApplicable\n    R bit: False\n    N bit: False\n    Timers:\n      Configured Restart Time(sec): 120\n      Received Restart Time(sec): 0\n  Message statistics:\n    Inq depth is 0\n    Outq depth is 0\n                         Sent       Rcvd\n    Opens:                  0          0\n    Notifications:          0          0\n    Updates:                0          0\n    Keepalives:             0          0\n    Route Refresh:          0          0\n    Capability:             0          0\n    Total:                  0          0\n  Minimum time between advertisement runs is 0 seconds\n\n For address family: IPv4 Unicast\n  Not part of any update group\n  Community attribute sent to this neighbor(all)\n  Inbound path policy configured\n  Outbound path policy configured\n  Route map for incoming advertisements is *10.89.147.195-in\n  Route map for outgoing advertisements is *10.89.147.195-out\n  0 accepted prefixes\n\n For address family: IPv6 Unicast\n  Not part of any update group\n  Community attribute sent to this neighbor(all)\n  Inbound path policy configured\n  Outbound path policy configured\n  Route map for incoming advertisements is *10.89.147.195-in\n  Route map for outgoing advertisements is *10.89.147.195-out\n  0 accepted prefixes\n\n  Connections established 0; dropped 0\n  Last reset 2d10h03m,  Waiting for peer OPEN\nBGP Connect Retry Timer in Seconds: 120\nNext connect timer due in 80 seconds\nRead thread: off  Write thread: off  FD used: -1\n\n  BFD: Type: multi hop\n  Detect Multiplier: 3, Min Rx interval: 300, Min Tx interval: 300\n  Status: Unknown, Last update: never\n</code></pre> <ol> <li>some more additional test to show the local testing here </li> </ol> <pre><code>[root@ncputility ~ pancwl_rc]$ oc get svc -A -o wide |grep -i loadbalan\nncom01pan                                          ncom01pan-citm-ingress                                     LoadBalancer   172.20.138.133   10.89.97.210                           80:32432/TCP,443:32622/TCP,2309:30135/TCP         10d   app=citm-ingress,component=controller,release=ncom01pan-citm-ingress\n[root@ncputility ~ pancwl_rc]$ nslookup 10.89.97.210\n210.97.89.10.in-addr.arpa       name = ncom01.panclyncom01.mnc020.mcc714.\n\n[root@ncputility ~ pancwl_rc]$ curl -k https://ncom01.panclyncom01.mnc020.mcc714/\n^C\n[root@ncputility ~ pancwl_rc]$ ip r get 10.89.97.210\n10.89.97.210 via 10.89.100.65 dev br308 src 10.89.100.66 uid 0\n    cache\n[root@ncputility ~ pancwl_rc]$ tracepath 10.89.97.210\n 1?: [LOCALHOST]                      pmtu 1500\n 1:  _gateway                                              0.184ms\n 1:  _gateway                                              0.290ms\n 2:  no reply\n 3:  10.89.97.129                                          0.284ms asymm  1\n 4:  no reply\n 5:  10.89.97.129                                          0.264ms asymm  1\n 6:  no reply\n 7:  10.89.97.129                                          0.281ms asymm  1\n 8:  no reply\n 9:  10.89.97.129                                          0.279ms asymm  1\n10:  no reply\n11:  10.89.97.129                                          0.269ms asymm  1\n12:  no reply\n13:  10.89.97.129                                          0.280ms asymm  1\n14:  no reply\n15:  10.89.97.129                                          0.234ms asymm  1\n16:  no reply\n17:  10.89.97.129                                          0.297ms asymm  1\n18:  no reply\n19:  10.89.97.129                                          0.328ms asymm  1\n20:  no reply\n21:  10.89.97.129                                          0.292ms asymm  1\n22:  no reply\n23:  10.89.97.129                                          0.336ms asymm  1\n24:  no reply\n25:  10.89.97.129                                          0.332ms asymm  1\n26:  no reply\n27:  10.89.97.129                                          0.330ms asymm  1\n28:  no reply\n29:  10.89.97.129                                          0.332ms asymm  1\n^C\n[root@ncputility ~ pancwl_rc]$ oc get -n metallb-system pods -l component=speaker -o wide\nNAME            READY   STATUS    RESTARTS   AGE   IP            NODE                                  NOMINATED NODE   READINESS GATES\nspeaker-dlrn5   6/6     Running   0          24d   10.89.96.18   gateway2.panclypcwl01.mnc020.mcc714   &lt;none&gt;           &lt;none&gt;\nspeaker-g2g77   6/6     Running   0          24d   10.89.96.19   gateway3.panclypcwl01.mnc020.mcc714   &lt;none&gt;           &lt;none&gt;\nspeaker-jzbw7   6/6     Running   0          24d   10.89.96.17   gateway1.panclypcwl01.mnc020.mcc714   &lt;none&gt;           &lt;none&gt;\nspeaker-pjstl   6/6     Running   0          24d   10.89.96.20   gateway4.panclypcwl01.mnc020.mcc714   &lt;none&gt;           &lt;none&gt;\n[root@ncputility ~ pancwl_rc]$ oc -n metallb-system exec -it -c frr vtysh\nerror: you must specify at least one command for the container\n[root@ncputility ~ pancwl_rc]$ oc -n metallb-system exec -it speaker-dlrn5 -c frr vtysh\nkubectl exec [POD] [COMMAND] is DEPRECATED and will be removed in a future version. Use kubectl exec [POD] -- [COMMAND] instead.\n\nHello, this is FRRouting (version 8.3.1).\nCopyright 1996-2005 Kunihiro Ishiguro, et al.\n\ngateway2.panclypcwl01.mnc020.mcc714# curl\n% Unknown command: curl\ngateway2.panclypcwl01.mnc020.mcc714# exit\n[root@ncputility ~ pancwl_rc]$ oc debug -t node/gateway3.panclypcwl01.mnc020.mcc714\nTemporary namespace openshift-debug-8j8fb is created for debugging node...\nStarting pod/gateway3panclypcwl01mnc020mcc714-debug-qsrhj ...\nTo use host binaries, run `chroot /host`\nPod IP: 10.89.96.19\nIf you don't see a command prompt, try pressing enter.\nsh-5.1# chroot /host\nsh-5.1# ping 10.89.97.163\nPING 10.89.97.163 (10.89.97.163) 56(84) bytes of data.\nFrom 10.89.97.166 icmp_seq=1 Destination Host Unreachable\nFrom 10.89.97.166 icmp_seq=2 Destination Host Unreachable\nFrom 10.89.97.166 icmp_seq=3 Destination Host Unreachable\n^C\n--- 10.89.97.163 ping statistics ---\n4 packets transmitted, 0 received, +3 errors, 100% packet loss, time 3090ms\npipe 3\nsh-5.1# ping 10.89.97.162\nPING 10.89.97.162 (10.89.97.162) 56(84) bytes of data.\nFrom 10.89.97.166 icmp_seq=1 Destination Host Unreachable\nFrom 10.89.97.166 icmp_seq=2 Destination Host Unreachable\nFrom 10.89.97.166 icmp_seq=3 Destination Host Unreachable\nFrom 10.89.97.166 icmp_seq=4 Destination Host Unreachable\n^C\n--- 10.89.97.162 ping statistics ---\n5 packets transmitted, 0 received, +4 errors, 100% packet loss, time 4064ms\npipe 4\nsh-5.1# ip r g 10.89.97.163\n10.89.97.163 via 10.89.97.161 dev vlan104 src 10.89.97.166 uid 0\n    cache\nsh-5.1# ip r g 10.89.97.162\n10.89.97.162 via 10.89.97.161 dev vlan104 src 10.89.97.166 uid 0\n    cache\nsh-5.1#  curl -k https://ncom01.panclyncom01.mnc020.mcc714/\n&lt;!doctype html&gt;&lt;html lang=\"en\"&gt;&lt;head&gt;&lt;meta charset=\"utf-8\"&gt;&lt;meta name=\"viewport\" content=\"width=device-width,initial-scale=1,shrink-to-fit=no\"&gt;&lt;meta name=\"theme-color\" content=\"#000000\"&gt;&lt;link rel=\"manifest\" href=\"/manifest.json\"&gt;&lt;link rel=\"shortcut icon\" href=\"/favicon.ico\"&gt;&lt;title&gt;Nokia Cloud Operations Manager&lt;/title&gt;&lt;link href=\"/static/css/27.2dbd0a91.chunk.css\" rel=\"stylesheet\"&gt;&lt;link href=\"/static/css/main.167b01c4.chunk.css\" rel=\"stylesheet\"&gt;&lt;/head&gt;&lt;body&gt;&lt;noscript&gt;You need to enable JavaScript to run this app.&lt;/noscript&gt;&lt;div id=\"root\"&gt;&lt;/div&gt;&lt;script&gt;!function(e){function t(t){for(var r,o,l=t[0],c=t[1],s=t[2],u=0,d=[];u&lt;l.length;u++)o=l[u],Object.prototype.hasOwnProperty.call(n,o)&amp;&amp;n[o]&amp;&amp;d.push(n[o][0]),n[o]=0;for(r in c)Object.prototype.hasOwnProperty.call(c,r)&amp;&amp;(e[r]=c[r]);for(f&amp;&amp;f(t);d.length;)d.shift()();return i.push.apply(i,s||[]),a()}function a(){for(var e,t=0;t&lt;i.length;t++){for(var a=i[t],r=!0,o=1;o&lt;a.length;o++){var c=a[o];0!==n[c]&amp;&amp;(r=!1)}r&amp;&amp;(i.splice(t--,1),e=l(l.s=a[0]))}return e}var r={},o={25:0},n={25:0},i=[];function l(t){if(r[t])return r[t].exports;var a=r[t]={i:t,l:!1,exports:{}};return e[t].call(a.exports,a,a.exports,l),a.l=!0,a.exports}l.e=function(e){var t=[];o[e]?t.push(o[e]):0!==o[e]&amp;&amp;{2:1,5:1,6:1,7:1,8:1,11:1,12:1,13:1,14:1,15:1,16:1,17:1,18:1,19:1,20:1,21:1,22:1,26:1}[e]&amp;&amp;t.push(o[e]=new Promise((function(t,a){for(var r=\"static/css/\"+({5:\"[AlarmDetailsPage]\",6:\"[AlarmPage]\",7:\"[CatalogDetailsPage]\",8:\"[CatalogTablePage]\",9:\"[CloudFlowLogin]\",10:\"[CloudFlowLogout]\",11:\"[CsvExporterDetailsPage]\",12:\"[DashboardPage]\",13:\"[ExecutionDetailsPage]\",14:\"[JobDetailsPage]\",15:\"[JobsTablePage]\",16:\"[ManagedWorkloadsTablePage]\",17:\"[NSDetailsContainerPage]\",18:\"[OperationsDetailsPage]\",19:\"[ResourceCompositionDetailsPage]\",20:\"[ResourceCompositionsPage]\",21:\"[ResourcesTablePage]\",22:\"[VimDetailsPage]\",23:\"[loginPage]\"}[e]||e)+\".\"+{0:\"31d6cfe0\",1:\"31d6cfe0\",2:\"559272a8\",3:\"31d6cfe0\",4:\"31d6cfe0\",5:\"affca9cb\",6:\"affca9cb\",7:\"91932cb4\",8:\"bf9ba349\",9:\"31d6cfe0\",10:\"31d6cfe0\",11:\"a0ed0644\",12:\"b2dca462\",13:\"b77ab692\",14:\"4ca14441\",15:\"f59232c1\",16:\"c1e044f2\",17:\"398f0759\",18:\"d8693b3e\",19:\"7b4dfbce\",20:\"d9277a02\",21:\"03edf692\",22:\"b6690f64\",23:\"31d6cfe0\",26:\"4391d164\",28:\"31d6cfe0\",29:\"31d6cfe0\"}[e]+\".chunk.css\",n=l.p+r,i=document.getElementsByTagName(\"link\"),c=0;c&lt;i.length;c++){var s=(f=i[c]).getAttribute(\"data-href\")||f.getAttribute(\"href\");if(\"stylesheet\"===f.rel&amp;&amp;(s===r||s===n))return t()}var u=document.getElementsByTagName(\"style\");for(c=0;c&lt;u.length;c++){var f;if((s=(f=u[c]).getAttribute(\"data-href\"))===r||s===n)return t()}var d=document.createElement(\"link\");d.rel=\"stylesheet\",d.type=\"text/css\",d.onload=t,d.onerror=function(t){var r=t&amp;&amp;t.target&amp;&amp;t.target.src||n,i=new Error(\"Loading CSS chunk \"+e+\" failed.\\n(\"+r+\")\");i.code=\"CSS_CHUNK_LOAD_FAILED\",i.request=r,delete o[e],d.parentNode.removeChild(d),a(i)},d.href=n,document.getElementsByTagName(\"head\")[0].appendChild(d)})).then((function(){o[e]=0})));var a=n[e];if(0!==a)if(a)t.push(a[2]);else{var r=new Promise((function(t,r){a=n[e]=[t,r]}));t.push(a[2]=r);var i,c=document.createElement(\"script\");c.charset=\"utf-8\",c.timeout=120,l.nc&amp;&amp;c.setAttribute(\"nonce\",l.nc),c.src=function(e){return l.p+\"static/js/\"+({5:\"[AlarmDetailsPage]\",6:\"[AlarmPage]\",7:\"[CatalogDetailsPage]\",8:\"[CatalogTablePage]\",9:\"[CloudFlowLogin]\",10:\"[CloudFlowLogout]\",11:\"[CsvExporterDetailsPage]\",12:\"[DashboardPage]\",13:\"[ExecutionDetailsPage]\",14:\"[JobDetailsPage]\",15:\"[JobsTablePage]\",16:\"[ManagedWorkloadsTablePage]\",17:\"[NSDetailsContainerPage]\",18:\"[OperationsDetailsPage]\",19:\"[ResourceCompositionDetailsPage]\",20:\"[ResourceCompositionsPage]\",21:\"[ResourcesTablePage]\",22:\"[VimDetailsPage]\",23:\"[loginPage]\"}[e]||e)+\".\"+{0:\"9ea1f8f1\",1:\"d7647e6d\",2:\"49e46739\",3:\"6bb32e31\",4:\"5066ed38\",5:\"3e390b6c\",6:\"88b37376\",7:\"e7438c70\",8:\"f547dc3b\",9:\"c157abf8\",10:\"c3c2011c\",11:\"62ea6190\",12:\"c002dc82\",13:\"f0d807bf\",14:\"cee9c63a\",15:\"737e42f0\",16:\"2aae2df5\",17:\"4a2ec46d\",18:\"4777a959\",19:\"f28d098b\",20:\"7e96e980\",21:\"abc77c4e\",22:\"9d55d492\",23:\"44757b03\",26:\"c04605d3\",28:\"52a01c15\",29:\"f0708fbb\"}[e]+\".chunk.js\"}(e);var s=new Error;i=function(t){c.onerror=c.onload=null,clearTimeout(u);var a=n[e];if(0!==a){if(a){var r=t&amp;&amp;(\"load\"===t.type?\"missing\":t.type),o=t&amp;&amp;t.target&amp;&amp;t.target.src;s.message=\"Loading chunk \"+e+\" failed.\\n(\"+r+\": \"+o+\")\",s.name=\"ChunkLoadError\",s.type=r,s.request=o,a[1](s)}n[e]=void 0}};var u=setTimeout((function(){i({type:\"timeout\",target:c})}),12e4);c.onerror=c.onload=i,document.head.appendChild(c)}return Promise.all(t)},l.m=e,l.c=r,l.d=function(e,t,a){l.o(e,t)||Object.defineProperty(e,t,{enumerable:!0,get:a})},l.r=function(e){\"undefined\"!=typeof Symbol&amp;&amp;Symbol.toStringTag&amp;&amp;Object.defineProperty(e,Symbol.toStringTag,{value:\"Module\"}),Object.defineProperty(e,\"__esModule\",{value:!0})},l.t=function(e,t){if(1&amp;t&amp;&amp;(e=l(e)),8&amp;t)return e;if(4&amp;t&amp;&amp;\"object\"==typeof e&amp;&amp;e&amp;&amp;e.__esModule)return e;var a=Object.create(null);if(l.r(a),Object.defineProperty(a,\"default\",{enumerable:!0,value:e}),2&amp;t&amp;&amp;\"string\"!=typeof e)for(var r in e)l.d(a,r,function(t){return e[t]}.bind(null,r));return a},l.n=function(e){var t=e&amp;&amp;e.__esModule?function(){return e.default}:function(){return e};return l.d(t,\"a\",t),t},l.o=function(e,t){return Object.prototype.hasOwnProperty.call(e,t)},l.p=\"/\",l.oe=function(e){throw console.error(e),e};var c=this.webpackJsonpfrontend=this.webpackJsonpfrontend||[],s=c.push.bind(c);c.push=t,c=c.slice();for(var u=0;u&lt;c.length;u++)t(c[u]);var f=s;a()}([])&lt;/script&gt;&lt;script src=\"/static/js/27.08ffca3e.chunk.js\"&gt;&lt;/script&gt;&lt;script src=\"/static/js/main.90334f28.chunk.js\"&gt;&lt;/script&gt;&lt;/body&gt;&lt;/html&gt;sh-5.1#\n</code></pre>"},{"location":"openshift/projectmanagement/scope/","title":"OpenShift (NCP) Deployment \u2013 Scope of Work","text":""},{"location":"openshift/projectmanagement/scope/#1-design-planning","title":"\ud83d\udcd0 1. Design &amp; Planning","text":"<ul> <li>Prepare the High-Level Design (HLD) for the NCP deployment. (only the NCP position or dedicated document for NCP alone. not solution HLD.)</li> <li>The HLD must be reviewed and approved by Nokia TDL before proceeding to the LLD phase.</li> <li>Develop Low-Level Designs (LLD) for the following clusters:</li> <li>HUB Cluster</li> <li>NMC Cluster</li> <li>NWC Cluster</li> <li>Capture and document application-specific requirements in the LLD, including:</li> <li>Tenants</li> <li>Users</li> <li>Security Context Constraints (SCC)</li> <li>Required application roles, if needed (default <code>network-attach</code> and <code>cbur</code> roles should be documented).</li> <li>Collect all networking information (Metllb, ippool, backwards routes, nmstate networks etc.)</li> <li>NCOM is used for CNF onboarding; no additional roles/SCCs are needed (check with Raj about it.).</li> <li>A Red Hat architect will perform an internal review of the HLD and LLD.</li> <li>Both the HLD and LLD must be formally approved by Nokia TDL before any installation activities commence.</li> <li>The Red Hat DTM must ensure no installation begins before Nokia TDL\u2019s approval of the design documents.</li> <li>Redhat Engineer should be gone through \u201cRedhat entry level criteria\u201d process before touching the infra. </li> <li>The Red Hat DTM must ensure no installation begins before Redhat Entry level criteria process to be fully completed without any gap. </li> </ul>"},{"location":"openshift/projectmanagement/scope/#2-mop-for-deployment","title":"\ud83d\udee0\ufe0f 2. MOP for Deployment","text":"<ul> <li>A detailed Method of Procedure (MOP) must be prepared for the deployment.</li> <li>The Red Hat engineer is responsible for preparing the site specific deployment MOP for Hub, NMC/NWC. </li> <li>A Red Hat architect will prepare the deployment templates using Red Hat\u2019s automation tools.</li> <li>(Check with Raj regarding template creation.)</li> </ul>"},{"location":"openshift/projectmanagement/scope/#3-infrastructure-base-setup","title":"\ud83e\uddf1 3. Infrastructure &amp; Base Setup","text":"<ul> <li>Install the OS on the <code>infra-manager</code> node when it is dedicated to NCP.</li> <li>Deploy the infra-quay application on the <code>infra-manager</code> node.</li> <li>Deploy the Hub Cluster.</li> <li>Deploy all required OpenShift operators on the Hub Cluster.</li> <li>Set up ACM backup on the Hub Cluster before initiating the NMC/NWC cluster deployment.</li> <li>Create and configure the necessary:</li> <li>Users</li> <li>Tenants</li> <li>Roles</li> <li>SCCs   on the Hub Cluster for the NCD Git CNF.</li> </ul>"},{"location":"openshift/projectmanagement/scope/#4-dvts-validation-hub-cluster","title":"\u2705 4. DVTS &amp; Validation \u2013 Hub Cluster","text":"<ul> <li>Complete Hub Cluster DVTS before starting the NMC/NWC cluster deployments.</li> <li>DVTS artifacts must be obtained from Tript via the Red Hat Drive.</li> </ul>"},{"location":"openshift/projectmanagement/scope/#5-gitops-backup","title":"\ud83d\udd01 5. GitOps &amp; Backup","text":"<ul> <li>Deploy the NCD Git server.</li> <li>The Nokia NCD team is responsible for setting up external backup of the Git server prior to the CWL cluster deployment.</li> </ul>"},{"location":"openshift/projectmanagement/scope/#6-cluster-deployment","title":"\ud83d\ude80 6. Cluster Deployment","text":"<ul> <li>Deploy the NMC and NWC Clusters.</li> <li>Complete the end-to-end deployment of the NMC/NWC clusters, including:</li> <li>All scale-outs</li> <li>Common and site-specific site policies</li> <li>MetalLB configuration</li> <li>Backward routing</li> <li>Egress configuration</li> <li>Multus (NMState) networking</li> <li>Tenant creation</li> <li>User creation</li> <li>SCCs and custom roles (as per the approved LLD)</li> <li>Quay proxy cache setup</li> <li>Standard NCP users for NCP, NCOM, and NCD</li> </ul>"},{"location":"openshift/projectmanagement/scope/#7-dvts-validation-nmcnwc-clusters","title":"\u2705 7. DVTS &amp; Validation \u2013 NMC/NWC Clusters","text":"<ul> <li>Complete final DVTS for NMC/NWC Clusters before handing over for CNF onboarding.</li> <li>DVTS artifacts must be obtained from Tript via the Red Hat Drive.</li> <li>Ensure the NCP Criteria Documentation and Checklist are fully completed and verified.</li> </ul>"},{"location":"openshift/projectmanagement/scope/#8-cnf-onboarding","title":"\ud83d\udce4 8. CNF Onboarding","text":"<ul> <li>Red Hat will not engage in CNF onboarding support until the service is formally procured by Nokia.</li> <li>CNF onboarding support (once procured) includes:</li> <li>Running tcpdump for CNF troubleshooting</li> <li>CNF application installation assistance</li> <li>Resolving CNF communication issues</li> </ul>"},{"location":"openshift/projectmanagement/scope/#9-scope-management","title":"\u26a0\ufe0f 9. Scope Management","text":"<ul> <li>Red Hat will not cover any additional tasks outside of this scope without a formal Change Order Request (COR) process.</li> <li>Example requests that require COR:</li> <li>Creating new users or tenants</li> <li>Adding subnets to IP pools</li> <li>Adding/changing/deleting networks/subnets for NMState, MetalLB, backward routes, etc.</li> <li>Handling new application prerequisites not documented in the approved LLD.</li> <li>Handing new changes to the cluster, scale-out, new hostgroup creation, even redeployment, NCP upgrade, Adding/modifying labels, even installing a PP on cluster. Which is not documented in the approved LLD.</li> <li>All such requests must be:</li> <li>Documented in the respective HLD/LLD</li> <li>Approved by the TDL</li> <li>Backed by a Change Order Request (COR) before Red Hat engineers implement changes on the clusters</li> </ul>"},{"location":"openshift/projectmanagement/scope/#10-disclaimer","title":"\u2696\ufe0f 10. Disclaimer","text":"<p>\u26a0\ufe0f Disclaimer: Failure to follow any of the defined steps, guidelines, or approvals outlined in this document may result in strict disciplinary action. All team members are expected to comply with the process to ensure project integrity and success.</p>"},{"location":"openshift/projectmanagement/scope/#11-donts-by-deployment-engineer","title":"\ud83d\udeab 11. Don\u2019ts by Deployment engineer","text":"<ul> <li> <p>Installation activities must not begin until all design documents have been reviewed and approved, and the Red Hat Entry-Level Criteria process has been fully completed with no gaps.</p> </li> <li> <p>If a Red Hat engineer identifies any deviation from the blueprint design\u2014whether at the hardware or software level\u2014it must be escalated to the respective DTM.</p> </li> <li> <p>The Red Hat team does not support workarounds in solution implementation. Only fully supported blueprint configurations for Nokia NCP should be used. The only exception is for non-blueprint certification candidates. also Redhat not recommend any workaround. all issues need to be documented by <code>NCPFM</code> and recommandation should be coming from <code>NCPFM</code>.  </p> </li> <li> <p>Red Hat engineers will not provide recommendations for solution components (e.g., selecting between NCP Quay or NCD Harbor registries for CNF). CNF teams must strictly adhere to the High-Level Design (HLD) documentation.</p> </li> <li> <p>Red Hat teams are not involved in hardware readiness tasks, such as switch configuration, OS installation for the Infra-Manager server, or creating CNF users on Infra-Manager nodes.</p> </li> <li> <p>Red Hat engineers do not grant cluster-admin roles to CNF users, even temporarily. Exceptions apply only to default user IDs associated with NCP, NCD, and NCOM.</p> </li> <li> <p>Red Hat engineers are not involved in CNF onboarding unless it has been officially procured by Nokia.</p> </li> <li> <p>The Red Hat team will not participate in Customer presentations until they have been formally approved by the DTM.</p> </li> <li> <p>Red Hat engineers will not install any third-party software, antivirus, or RPM packages on Hub, NMC/NWC, or Infra-Manager nodes.</p> </li> <li> <p>Red Hat teams do not install or configure DNS or NTP servers as part of the NCP installation.</p> </li> <li> <p>RedHat teams will not provide any major updates to Nokia PM or Customers, all will go via DTM. `</p> </li> <li> <p>RedHat team do not share the credentials or any Exit deliverable artifacts via email. It should be via SP from DTM.</p> </li> </ul>"},{"location":"openshift/storagemanagement/ceph-rebalanceissue/","title":"Ceph Rebalance Issue","text":""},{"location":"openshift/storagemanagement/ceph-rebalanceissue/#ceph-rebalance-issues-and-its-method-to-solve-it","title":"Ceph rebalance issues and it's method to solve it.","text":""},{"location":"openshift/storagemanagement/ceph-rebalanceissue/#ceph-rebalance-issue-when-failed-storagereplacement-node-old-osd-causing-an-stuck-or-rebalance-blocking","title":"ceph rebalance issue, when failed storage/replacement node old OSD causing an stuck or rebalance blocking.","text":"<p>1) Login to OCP CWL cluster</p> <pre><code>[root@dom14npv101-infra-manager ~ vlabrc]# source  /root/raj/vlabrc\nWARNING: Using insecure TLS client config. Setting this option is not supported!\n\nLogin successful.\n\nYou have access to 115 projects, the list has been suppressed. You can list all projects with 'oc projects'\n\nUsing project \"openshift-storage\".\n[root@dom14npv101-infra-manager ~ vlabrc]# oc get nodes\nNAME                                                       STATUS   ROLES                              AGE     VERSION\nncpvnpvlab1-storage-101.ncpvnpvlab1.pnwlab.nsn-rdnet.net   Ready    storage,worker                     3h49m   v1.29.10+67d3387\nncpvnpvlab1-storage-102.ncpvnpvlab1.pnwlab.nsn-rdnet.net   Ready    storage,worker                     149m    v1.29.10+67d3387\nncpvnpvlab1-storage-103.ncpvnpvlab1.pnwlab.nsn-rdnet.net   Ready    storage,worker                     57d     v1.29.10+67d3387\nncpvnpvlab1-storage-201.ncpvnpvlab1.pnwlab.nsn-rdnet.net   Ready    storage,worker                     55d     v1.29.10+67d3387\nncpvnpvlab1-storage-202.ncpvnpvlab1.pnwlab.nsn-rdnet.net   Ready    storage,worker                     57d     v1.29.10+67d3387\nncpvnpvlab1-storage-203.ncpvnpvlab1.pnwlab.nsn-rdnet.net   Ready    storage,worker                     3h48m   v1.29.10+67d3387 \n\noutput omitted\n</code></pre> <p>2) look at the ceph to find ceph status to know, is there any OSD's are missing. </p> <pre><code>bash-5.1$ ceph -s\n  cluster:\n    id:     d6599242-8a82-410c-aa83-c15b31d8f6c7\n    health: HEALTH_OK\n\n  services:\n    mon: 3 daemons, quorum a,b,c (age 4h)\n    mgr: a(active, since 11m), standbys: b\n    mds: 1/1 daemons up, 1 hot standby\n    osd: 46 osds: 46 up (since 10m), 46 in (since 11m)\n    rgw: 1 daemon active (1 hosts, 1 zones)\n\n  data:\n    volumes: 1/1 healthy\n    pools:   12 pools, 585 pgs\n    objects: 38.37k objects, 146 GiB\n    usage:   457 GiB used, 267 TiB / 268 TiB avail\n    pgs:     585 active+clean\n\n  io:\n    client:   135 KiB/s rd, 117 MiB/s wr, 103 op/s rd, 73 op/s wr\n\nbash-5.1$\n</code></pre> <p>3) Let us find which node missing the OSD but counting the osd's on each nodes. </p> <pre><code>bash-5.1$ ceph osd tree\nID   CLASS  WEIGHT     TYPE NAME                                                          STATUS  REWEIGHT  PRI-AFF\n -1         267.80737  root default\n -9          40.75330      host ncpvnpvlab1-storage-101-ncpvnpvlab1-pnwlab-nsn-rdnet-net\n 24    ssd    5.82190          osd.24                                                         up   1.00000  1.00000\n 27    ssd    5.82190          osd.27                                                         up   1.00000  1.00000\n 31    ssd    5.82190          osd.31                                                         up   1.00000  1.00000\n 32    ssd    5.82190          osd.32                                                         up   1.00000  1.00000\n 37    ssd    5.82190          osd.37                                                         up   1.00000  1.00000\n 38    ssd    5.82190          osd.38                                                         up   1.00000  1.00000\n 41    ssd    5.82190          osd.41                                                         up   1.00000  1.00000\n-11          46.57520      host ncpvnpvlab1-storage-102-ncpvnpvlab1-pnwlab-nsn-rdnet-net\n 25    ssd    5.82190          osd.25                                                         up   1.00000  1.00000\n 28    ssd    5.82190          osd.28                                                         up   1.00000  1.00000\n 29    ssd    5.82190          osd.29                                                         up   1.00000  1.00000\n 34    ssd    5.82190          osd.34                                                         up   1.00000  1.00000\n 36    ssd    5.82190          osd.36                                                         up   1.00000  1.00000\n 42    ssd    5.82190          osd.42                                                         up   1.00000  1.00000\n 43    ssd    5.82190          osd.43                                                         up   1.00000  1.00000\n 45    ssd    5.82190          osd.45                                                         up   1.00000  1.00000\n -3          46.57520      host ncpvnpvlab1-storage-103-ncpvnpvlab1-pnwlab-nsn-rdnet-net\n  0    ssd    5.82190          osd.0                                                          up   1.00000  1.00000\n  3    ssd    5.82190          osd.3                                                          up   1.00000  1.00000\n  6    ssd    5.82190          osd.6                                                          up   1.00000  1.00000\n  9    ssd    5.82190          osd.9                                                          up   1.00000  1.00000\n 11    ssd    5.82190          osd.11                                                         up   1.00000  1.00000\n 13    ssd    5.82190          osd.13                                                         up   1.00000  1.00000\n 17    ssd    5.82190          osd.17                                                         up   1.00000  1.00000\n 19    ssd    5.82190          osd.19                                                         up   1.00000  1.00000\n -5          46.57520      host ncpvnpvlab1-storage-201-ncpvnpvlab1-pnwlab-nsn-rdnet-net\n  1    ssd    5.82190          osd.1                                                          up   1.00000  1.00000\n  4    ssd    5.82190          osd.4                                                          up   1.00000  1.00000\n  8    ssd    5.82190          osd.8                                                          up   1.00000  1.00000\n 10    ssd    5.82190          osd.10                                                         up   1.00000  1.00000\n 14    ssd    5.82190          osd.14                                                         up   1.00000  1.00000\n 18    ssd    5.82190          osd.18                                                         up   1.00000  1.00000\n 20    ssd    5.82190          osd.20                                                         up   1.00000  1.00000\n 21    ssd    5.82190          osd.21                                                         up   1.00000  1.00000\n -7          46.57520      host ncpvnpvlab1-storage-202-ncpvnpvlab1-pnwlab-nsn-rdnet-net\n  2    ssd    5.82190          osd.2                                                          up   1.00000  1.00000\n  5    ssd    5.82190          osd.5                                                          up   1.00000  1.00000\n  7    ssd    5.82190          osd.7                                                          up   1.00000  1.00000\n 12    ssd    5.82190          osd.12                                                         up   1.00000  1.00000\n 15    ssd    5.82190          osd.15                                                         up   1.00000  1.00000\n 16    ssd    5.82190          osd.16                                                         up   1.00000  1.00000\n 22    ssd    5.82190          osd.22                                                         up   1.00000  1.00000\n 23    ssd    5.82190          osd.23                                                         up   1.00000  1.00000\n-13          40.75330      host ncpvnpvlab1-storage-203-ncpvnpvlab1-pnwlab-nsn-rdnet-net\n 26    ssd    5.82190          osd.26                                                         up   1.00000  1.00000\n 30    ssd    5.82190          osd.30                                                         up   1.00000  1.00000\n 33    ssd    5.82190          osd.33                                                         up   1.00000  1.00000\n 35    ssd    5.82190          osd.35                                                         up   1.00000  1.00000\n 39    ssd    5.82190          osd.39                                                         up   1.00000  1.00000\n 40    ssd    5.82190          osd.40                                                         up   1.00000  1.00000\n 44    ssd    5.82190          osd.44                                                         up   1.00000  1.00000\nbash-5.1$\n</code></pre> <p>4) check out the ceph health in detail, get better view from here</p> <pre><code>[root@dom14npv101-infra-manager ~ vlabrc]# oc rsh -n openshift-storage pod/$CEPH_TOOL_POD ceph health detail\nHEALTH_WARN 1 filesystem is degraded; 11 osds down; 3 hosts (23 osds) down; Reduced data availability: 247 pgs inactive; Degraded data redundancy: 5609127/17836029 objects degraded (31.448%), 811 pgs degraded, 839 pgs undersized; 1597 slow ops, oldest one blocked for 16139 sec, daemons [osd.0,osd.10,osd.13,osd.19,osd.22,osd.27,osd.29,osd.3,osd.35,osd.36]... have slow ops.\n[WRN] FS_DEGRADED: 1 filesystem is degraded\n    fs ocs-storagecluster-cephfilesystem is degraded\n[WRN] OSD_DOWN: 11 osds down\n    osd.7 (root=default,host=ncpvnpvlab1-storage-102-ncpvnpvlab1-pnwlab-nsn-rdnet-net) is down\n    osd.9 (root=default,host=ncpvnpvlab1-storage-101-ncpvnpvlab1-pnwlab-nsn-rdnet-net) is down\n    osd.28 (root=default,host=ncpvnpvlab1-storage-101-ncpvnpvlab1-pnwlab-nsn-rdnet-net) is down\n    osd.30 (root=default,host=ncpvnpvlab1-storage-203-ncpvnpvlab1-pnwlab-nsn-rdnet-net) is down\n    osd.31 (root=default,host=ncpvnpvlab1-storage-102-ncpvnpvlab1-pnwlab-nsn-rdnet-net) is down\n    osd.33 (root=default,host=ncpvnpvlab1-storage-102-ncpvnpvlab1-pnwlab-nsn-rdnet-net) is down\n    osd.34 (root=default,host=ncpvnpvlab1-storage-101-ncpvnpvlab1-pnwlab-nsn-rdnet-net) is down\n    osd.37 (root=default,host=ncpvnpvlab1-storage-203-ncpvnpvlab1-pnwlab-nsn-rdnet-net) is down\n    osd.40 (root=default,host=ncpvnpvlab1-storage-203-ncpvnpvlab1-pnwlab-nsn-rdnet-net) is down\n    osd.41 (root=default,host=ncpvnpvlab1-storage-101-ncpvnpvlab1-pnwlab-nsn-rdnet-net) is down\n    osd.44 (root=default,host=ncpvnpvlab1-storage-101-ncpvnpvlab1-pnwlab-nsn-rdnet-net) is down\n[WRN] OSD_HOST_DOWN: 3 hosts (23 osds) down\n    host ncpvnpvlab1-storage-101-ncpvnpvlab1-pnwlab-nsn-rdnet-net (root=default) (8 osds) is down\n    host ncpvnpvlab1-storage-203-ncpvnpvlab1-pnwlab-nsn-rdnet-net (root=default) (8 osds) is down\n    host ncpvnpvlab1-storage-102-ncpvnpvlab1-pnwlab-nsn-rdnet-net (root=default) (7 osds) is down\n[WRN] PG_AVAILABILITY: Reduced data availability: 247 pgs inactive\n    pg 9.91 is stuck inactive for 4h, current state unknown, last acting []\n    pg 9.93 is stuck inactive for 4h, current state undersized+degraded+peered, last acting [45]\n    pg 9.97 is stuck inactive for 4h, current state undersized+degraded+peered, last acting [47]\n    pg 9.9e is stuck inactive for 4h, current state unknown, last acting []\n    pg 9.9f is stuck inactive for 4h, current state undersized+degraded+peered, last acting [5]\n    pg 9.a0 is stuck inactive for 4h, current state unknown, last acting []\n    pg 9.a6 is stuck inactive for 4h, current state undersized+degraded+peered, last acting [22]\n    pg 9.ab is stuck inactive for 4h, current state undersized+degraded+peered, last acting [35]\n    pg 9.ae is stuck inactive for 4h, current state undersized+degraded+peered, last acting [25]\n    pg 9.af is stuck inactive for 4h, current state undersized+degraded+peered, last acting [8]\n    pg 9.b2 is stuck inactive for 4h, current state unknown, last acting []\n    pg 9.b3 is stuck inactive for 4h, current state undersized+degraded+peered, last acting [35]\n    pg 9.b4 is stuck inactive for 4h, current state unknown, last acting []\n    pg 9.c5 is stuck inactive for 4h, current state undersized+degraded+peered, last acting [42]\n    pg 9.c6 is stuck inactive for 4h, current state undersized+degraded+peered, last acting [3]\n    pg 9.c7 is stuck inactive for 4h, current state undersized+degraded+peered, last acting [46]\n    pg 9.1e7 is stuck inactive for 4h, current state undersized+degraded+peered, last acting [46]\n    pg 9.1eb is stuck inactive for 4h, current state undersized+degraded+peered, last acting [29]\n    pg 9.1f5 is stuck inactive for 4h, current state undersized+degraded+peered, last acting [0]\n    pg 9.1fa is stuck inactive for 4h, current state undersized+degraded+peered, last acting [22]\n    pg 9.1fb is stuck inactive for 4h, current state unknown, last acting []\n    pg 11.95 is stuck inactive for 4h, current state undersized+degraded+peered, last acting [46]\n    pg 11.98 is stuck inactive for 4h, current state undersized+degraded+peered, last acting [19]\n    pg 11.9b is stuck inactive for 4h, current state undersized+degraded+peered, last acting [32]\n    pg 11.a0 is stuck inactive for 4h, current state undersized+degraded+peered, last acting [36]\n    pg 11.a1 is stuck inactive for 4h, current state unknown, last acting []\n    pg 11.a3 is stuck inactive for 4h, current state undersized+degraded+peered, last acting [3]\n    pg 11.a8 is stuck inactive for 4h, current state undersized+degraded+peered, last acting [23]\n    pg 11.ac is stuck inactive for 4h, current state undersized+degraded+peered, last acting [43]\n    pg 11.b0 is stuck inactive for 4h, current state undersized+degraded+peered, last acting [39]\n    pg 11.b3 is stuck inactive for 4h, current state undersized+degraded+peered, last acting [3]\n    pg 11.b5 is stuck inactive for 4h, current state undersized+degraded+peered, last acting [10]\n    pg 11.b9 is stuck inactive for 4h, current state undersized+degraded+peered, last acting [43]\n    pg 11.bb is stuck inactive for 4h, current state unknown, last acting []\n    pg 11.bc is stuck inactive for 4h, current state undersized+degraded+peered, last acting [42]\n    pg 11.c0 is stuck inactive for 4h, current state undersized+degraded+peered, last acting [15]\n    pg 11.c2 is stuck inactive for 4h, current state undersized+degraded+peered, last acting [3]\n    pg 11.c3 is stuck inactive for 4h, current state undersized+degraded+peered, last acting [3]\n    pg 12.96 is stuck inactive for 4h, current state undersized+degraded+peered, last acting [23]\n    pg 12.9b is stuck inactive for 4h, current state undersized+degraded+peered, last acting [8]\n    pg 12.9c is stuck inactive for 4h, current state undersized+degraded+peered, last acting [38]\n    pg 12.9d is stuck inactive for 4h, current state undersized+degraded+peered, last acting [10]\n    pg 12.ab is stuck inactive for 4h, current state undersized+degraded+peered, last acting [36]\n    pg 12.af is stuck inactive for 4h, current state undersized+degraded+peered, last acting [20]\n    pg 12.b1 is stuck inactive for 4h, current state undersized+degraded+peered, last acting [0]\n    pg 12.b4 is stuck inactive for 4h, current state undersized+degraded+peered, last acting [13]\n    pg 12.b5 is stuck inactive for 4h, current state undersized+degraded+peered, last acting [38]\n    pg 12.be is stuck inactive for 4h, current state undersized+degraded+peered, last acting [3]\n    pg 12.c4 is stuck inactive for 4h, current state undersized+degraded+peered, last acting [23]\n    pg 12.c5 is stuck inactive for 4h, current state undersized+degraded+peered, last acting [39]\n    pg 12.c7 is stuck inactive for 4h, current state undersized+degraded+peered, last acting [13]\n</code></pre> <p>5) remove the OSD's which are part of scaled-in storage node </p> <pre><code>[root@dom14npv101-infra-manager ~ vlabrc]# oc rsh -n openshift-storage pod/$CEPH_TOOL_POD ceph osd tree\nID   CLASS  WEIGHT     TYPE NAME                                                          STATUS  REWEIGHT  PRI-AFF\n -1         273.62927  root default\n -3          46.57520      host ncpvnpvlab1-storage-101-ncpvnpvlab1-pnwlab-nsn-rdnet-net\n  4    ssd    5.82190          osd.4                                                        down         0  1.00000\n  9    ssd    5.82190          osd.9                                                        down   1.00000  1.00000\n 14    ssd    5.82190          osd.14                                                       down         0  1.00000\n 21    ssd    5.82190          osd.21                                                       down         0  1.00000\n 28    ssd    5.82190          osd.28                                                       down   1.00000  1.00000\n 34    ssd    5.82190          osd.34                                                       down   1.00000  1.00000\n 41    ssd    5.82190          osd.41                                                       down   1.00000  1.00000\n 44    ssd    5.82190          osd.44                                                       down   1.00000  1.00000\n ```\n\n6) remove those unwanted OSD's completely. \n</code></pre> <p>oc rsh -n openshift-storage pod/$CEPH_TOOL_POD ceph osd out 4 oc rsh -n openshift-storage pod/$CEPH_TOOL_POD ceph osd out 9 oc rsh -n openshift-storage pod/$CEPH_TOOL_POD ceph osd out 14 oc rsh -n openshift-storage pod/$CEPH_TOOL_POD ceph osd out 21 oc rsh -n openshift-storage pod/$CEPH_TOOL_POD ceph osd out 28 oc rsh -n openshift-storage pod/$CEPH_TOOL_POD ceph osd out 34 oc rsh -n openshift-storage pod/$CEPH_TOOL_POD ceph osd out 41 oc rsh -n openshift-storage pod/$CEPH_TOOL_POD ceph osd out 44</p> <p>osd.4 is already out. osd.9 is already out. osd.14 is already out. osd.21 is already out. marked out osd.28. marked out osd.34. marked out osd.41. marked out osd.44.</p> <pre><code>7) delete it completely. using purge. \n</code></pre> <p>[root@dom14npv101-infra-manager ~ vlabrc]# oc rsh -n openshift-storage pod/$CEPH_TOOL_POD ceph osd purge 4  --yes-i-really-mean-it oc rsh -n openshift-storage pod/$CEPH_TOOL_POD ceph osd purge 9  --yes-i-really-mean-it oc rsh -n openshift-storage pod/$CEPH_TOOL_POD ceph osd purge 14 --yes-i-really-mean-it oc rsh -n openshift-storage pod/$CEPH_TOOL_POD ceph osd purge 21 --yes-i-really-mean-it oc rsh -n openshift-storage pod/$CEPH_TOOL_POD ceph osd purge 28 --yes-i-really-mean-it oc rsh -n openshift-storage pod/$CEPH_TOOL_POD ceph osd purge 34 --yes-i-really-mean-it oc rsh -n openshift-storage pod/$CEPH_TOOL_POD ceph osd purge 41 --yes-i-really-mean-it oc rsh -n openshift-storage pod/$CEPH_TOOL_POD ceph osd purge 44 --yes-i-really-mean-it</p> <p>purged osd.4 purged osd.9 purged osd.14 purged osd.21 purged osd.28 purged osd.34 purged osd.41 purged osd.44</p> <pre><code>8) now check, rebalance should be begins at this point. \n</code></pre> <p>[root@dom14npv101-infra-manager ~ vlabrc]# oc rsh -n openshift-storage pod/$CEPH_TOOL_POD ceph -s   cluster:     id:     a2b0c334-ba7c-4ae1-b3f5-c6d514f19bec     health: HEALTH_WARN             1 filesystem is degraded             Reduced data availability: 203 pgs inactive             Degraded data redundancy: 5434540/17836062 objects degraded (30.469%), 800 pgs degraded, 218 pgs undersized             62 slow ops, oldest one blocked for 24934 sec, daemons [osd.10,osd.13,osd.19,osd.22,osd.35,osd.36,osd.42,osd.45,osd.5] have slow ops.</p> <p>services:     mon: 3 daemons, quorum h,j,k (age 6h)     mgr: a(active, since 6h), standbys: b     mds: 1/1 daemons up, 1 standby     osd: 32 osds: 24 up (since 7h), 24 in (since 119s); 879 remapped pgs</p> <p>data:     volumes: 0/1 healthy, 1 recovering     pools:   12 pools, 1097 pgs     objects: 5.95M objects, 20 TiB     usage:   42 TiB used, 98 TiB / 140 TiB avail     pgs:     3.829% pgs unknown              14.676% pgs not active              5434540/17836062 objects degraded (30.469%)              1857921/17836062 objects misplaced (10.417%)              639 active+undersized+degraded+remapped+backfill_wait              176 active+clean              157 undersized+degraded+remapped+backfill_wait+peered              79  active+remapped+backfill_wait              42  unknown              4   undersized+degraded+remapped+backfilling+peered</p> <p>io:     client:   2.3 MiB/s wr, 0 op/s rd, 6 op/s wr     recovery: 1.4 GiB/s, 0 keys/s, 455 objects/s ```</p>"},{"location":"openshift/storagemanagement/one-OSD-not-created/","title":"Ceph OSD recreate","text":""},{"location":"openshift/storagemanagement/one-OSD-not-created/#one-or-two-osd-not-creating-recreated-post-storage-node-replacement","title":"One or Two OSD not creating recreated post storage node replacement","text":""},{"location":"openshift/storagemanagement/one-OSD-not-created/#login-to-the-cluster-and-make-sure-storage-replacement-finished-fully","title":"Login to the cluster and make sure storage replacement finished fully.","text":"<p>1) Login to OCP CWL cluster</p> <pre><code>[root@dom14npv101-infra-manager ~ vlabrc]# source  /root/raj/vlabrc\nWARNING: Using insecure TLS client config. Setting this option is not supported!\n\nLogin successful.\n\nYou have access to 115 projects, the list has been suppressed. You can list all projects with 'oc projects'\n\nUsing project \"openshift-storage\".\n[root@dom14npv101-infra-manager ~ vlabrc]# oc get nodes\nNAME                                                       STATUS   ROLES                              AGE     VERSION\nncpvnpvlab1-storage-101.ncpvnpvlab1.pnwlab.nsn-rdnet.net   Ready    storage,worker                     3h49m   v1.29.10+67d3387\nncpvnpvlab1-storage-102.ncpvnpvlab1.pnwlab.nsn-rdnet.net   Ready    storage,worker                     149m    v1.29.10+67d3387\nncpvnpvlab1-storage-103.ncpvnpvlab1.pnwlab.nsn-rdnet.net   Ready    storage,worker                     57d     v1.29.10+67d3387\nncpvnpvlab1-storage-201.ncpvnpvlab1.pnwlab.nsn-rdnet.net   Ready    storage,worker                     55d     v1.29.10+67d3387\nncpvnpvlab1-storage-202.ncpvnpvlab1.pnwlab.nsn-rdnet.net   Ready    storage,worker                     57d     v1.29.10+67d3387\nncpvnpvlab1-storage-203.ncpvnpvlab1.pnwlab.nsn-rdnet.net   Ready    storage,worker                     3h48m   v1.29.10+67d3387 \n\noutput omitted\n</code></pre> <p>2) look at the ceph to find ceph status to know, is there any OSD's are missing. </p> <pre><code>bash-5.1$ ceph -s\n  cluster:\n    id:     d6599242-8a82-410c-aa83-c15b31d8f6c7\n    health: HEALTH_OK\n\n  services:\n    mon: 3 daemons, quorum a,b,c (age 4h)\n    mgr: a(active, since 11m), standbys: b\n    mds: 1/1 daemons up, 1 hot standby\n    osd: 46 osds: 46 up (since 10m), 46 in (since 11m)\n    rgw: 1 daemon active (1 hosts, 1 zones)\n\n  data:\n    volumes: 1/1 healthy\n    pools:   12 pools, 585 pgs\n    objects: 38.37k objects, 146 GiB\n    usage:   457 GiB used, 267 TiB / 268 TiB avail\n    pgs:     585 active+clean\n\n  io:\n    client:   135 KiB/s rd, 117 MiB/s wr, 103 op/s rd, 73 op/s wr\n\nbash-5.1$\n</code></pre> <p>3) Let us find which node missing the OSD but counting the osd's on each nodes. </p> <pre><code>bash-5.1$ ceph osd tree\nID   CLASS  WEIGHT     TYPE NAME                                                          STATUS  REWEIGHT  PRI-AFF\n -1         267.80737  root default\n -9          40.75330      host ncpvnpvlab1-storage-101-ncpvnpvlab1-pnwlab-nsn-rdnet-net\n 24    ssd    5.82190          osd.24                                                         up   1.00000  1.00000\n 27    ssd    5.82190          osd.27                                                         up   1.00000  1.00000\n 31    ssd    5.82190          osd.31                                                         up   1.00000  1.00000\n 32    ssd    5.82190          osd.32                                                         up   1.00000  1.00000\n 37    ssd    5.82190          osd.37                                                         up   1.00000  1.00000\n 38    ssd    5.82190          osd.38                                                         up   1.00000  1.00000\n 41    ssd    5.82190          osd.41                                                         up   1.00000  1.00000\n-11          46.57520      host ncpvnpvlab1-storage-102-ncpvnpvlab1-pnwlab-nsn-rdnet-net\n 25    ssd    5.82190          osd.25                                                         up   1.00000  1.00000\n 28    ssd    5.82190          osd.28                                                         up   1.00000  1.00000\n 29    ssd    5.82190          osd.29                                                         up   1.00000  1.00000\n 34    ssd    5.82190          osd.34                                                         up   1.00000  1.00000\n 36    ssd    5.82190          osd.36                                                         up   1.00000  1.00000\n 42    ssd    5.82190          osd.42                                                         up   1.00000  1.00000\n 43    ssd    5.82190          osd.43                                                         up   1.00000  1.00000\n 45    ssd    5.82190          osd.45                                                         up   1.00000  1.00000\n -3          46.57520      host ncpvnpvlab1-storage-103-ncpvnpvlab1-pnwlab-nsn-rdnet-net\n  0    ssd    5.82190          osd.0                                                          up   1.00000  1.00000\n  3    ssd    5.82190          osd.3                                                          up   1.00000  1.00000\n  6    ssd    5.82190          osd.6                                                          up   1.00000  1.00000\n  9    ssd    5.82190          osd.9                                                          up   1.00000  1.00000\n 11    ssd    5.82190          osd.11                                                         up   1.00000  1.00000\n 13    ssd    5.82190          osd.13                                                         up   1.00000  1.00000\n 17    ssd    5.82190          osd.17                                                         up   1.00000  1.00000\n 19    ssd    5.82190          osd.19                                                         up   1.00000  1.00000\n -5          46.57520      host ncpvnpvlab1-storage-201-ncpvnpvlab1-pnwlab-nsn-rdnet-net\n  1    ssd    5.82190          osd.1                                                          up   1.00000  1.00000\n  4    ssd    5.82190          osd.4                                                          up   1.00000  1.00000\n  8    ssd    5.82190          osd.8                                                          up   1.00000  1.00000\n 10    ssd    5.82190          osd.10                                                         up   1.00000  1.00000\n 14    ssd    5.82190          osd.14                                                         up   1.00000  1.00000\n 18    ssd    5.82190          osd.18                                                         up   1.00000  1.00000\n 20    ssd    5.82190          osd.20                                                         up   1.00000  1.00000\n 21    ssd    5.82190          osd.21                                                         up   1.00000  1.00000\n -7          46.57520      host ncpvnpvlab1-storage-202-ncpvnpvlab1-pnwlab-nsn-rdnet-net\n  2    ssd    5.82190          osd.2                                                          up   1.00000  1.00000\n  5    ssd    5.82190          osd.5                                                          up   1.00000  1.00000\n  7    ssd    5.82190          osd.7                                                          up   1.00000  1.00000\n 12    ssd    5.82190          osd.12                                                         up   1.00000  1.00000\n 15    ssd    5.82190          osd.15                                                         up   1.00000  1.00000\n 16    ssd    5.82190          osd.16                                                         up   1.00000  1.00000\n 22    ssd    5.82190          osd.22                                                         up   1.00000  1.00000\n 23    ssd    5.82190          osd.23                                                         up   1.00000  1.00000\n-13          40.75330      host ncpvnpvlab1-storage-203-ncpvnpvlab1-pnwlab-nsn-rdnet-net\n 26    ssd    5.82190          osd.26                                                         up   1.00000  1.00000\n 30    ssd    5.82190          osd.30                                                         up   1.00000  1.00000\n 33    ssd    5.82190          osd.33                                                         up   1.00000  1.00000\n 35    ssd    5.82190          osd.35                                                         up   1.00000  1.00000\n 39    ssd    5.82190          osd.39                                                         up   1.00000  1.00000\n 40    ssd    5.82190          osd.40                                                         up   1.00000  1.00000\n 44    ssd    5.82190          osd.44                                                         up   1.00000  1.00000\nbash-5.1$\n</code></pre> <p>4) assuming here <code>ncpvnpvlab1-storage-101.ncpvnpvlab1.pnwlab.nsn-rdnet.net</code> node missing one OSD here. but have to find which disk is that fail to create an osd. </p> <pre><code>[root@dom14npv101-infra-manager ~ vlabrc]# oc get pods  -o wide |grep -i ncpvnpvlab1-storage-101.ncpvnpvlab1.pnwlab.nsn-rdnet.net |grep -i osd |grep -i running\nrook-ceph-osd-24-6457dffc55-86brj                                 2/2     Running     0               19m     172.31.24.22     ncpvnpvlab1-storage-101.ncpvnpvlab1.pnwlab.nsn-rdnet.net   &lt;none&gt;           &lt;none&gt;\nrook-ceph-osd-27-7597d558fc-kw9l9                                 2/2     Running     0               19m     172.31.24.19     ncpvnpvlab1-storage-101.ncpvnpvlab1.pnwlab.nsn-rdnet.net   &lt;none&gt;           &lt;none&gt;\nrook-ceph-osd-31-5cdf75ff54-bhmlr                                 2/2     Running     0               19m     172.31.24.24     ncpvnpvlab1-storage-101.ncpvnpvlab1.pnwlab.nsn-rdnet.net   &lt;none&gt;           &lt;none&gt;\nrook-ceph-osd-32-6f99fd4845-pqg4m                                 2/2     Running     0               19m     172.31.24.23     ncpvnpvlab1-storage-101.ncpvnpvlab1.pnwlab.nsn-rdnet.net   &lt;none&gt;           &lt;none&gt;\nrook-ceph-osd-37-598b85cf5-mjjzk                                  2/2     Running     0               19m     172.31.24.25     ncpvnpvlab1-storage-101.ncpvnpvlab1.pnwlab.nsn-rdnet.net   &lt;none&gt;           &lt;none&gt;\nrook-ceph-osd-38-6f9cfbc8d9-wtfrd                                 2/2     Running     0               19m     172.31.24.26     ncpvnpvlab1-storage-101.ncpvnpvlab1.pnwlab.nsn-rdnet.net   &lt;none&gt;           &lt;none&gt;\nrook-ceph-osd-41-6b88b44f56-tlv7m                                 2/2     Running     0               19m     172.31.24.27     ncpvnpvlab1-storage-101.ncpvnpvlab1.pnwlab.nsn-rdnet.net   &lt;none&gt;           &lt;none&gt;\n[root@dom14npv101-infra-manager ~ vlabrc]# \n</code></pre>"},{"location":"openshift/storagemanagement/one-OSD-not-created/#find-an-method-to-locate-the-missing-osd-on-that-particular-node","title":"Find an method to locate the missing OSD on that particular node.","text":"<p>5) create to command to grep for osd and it's logical disk location on that node. </p> <pre><code>[root@dom14npv101-infra-manager ~ vlabrc]# oc get pods  -o wide |grep -i ncpvnpvlab1-storage-101.ncpvnpvlab1.pnwlab.nsn-rdnet.net |grep -i osd |grep -i running |awk '{print \"oc logs \"$1 \" |grep -i nvme\"}'\noc logs rook-ceph-osd-24-6457dffc55-86brj |grep -i nvme\noc logs rook-ceph-osd-27-7597d558fc-kw9l9 |grep -i nvme\noc logs rook-ceph-osd-31-5cdf75ff54-bhmlr |grep -i nvme\noc logs rook-ceph-osd-32-6f99fd4845-pqg4m |grep -i nvme\noc logs rook-ceph-osd-37-598b85cf5-mjjzk |grep -i nvme\noc logs rook-ceph-osd-38-6f9cfbc8d9-wtfrd |grep -i nvme\noc logs rook-ceph-osd-41-6b88b44f56-tlv7m |grep -i nvme\n[root@dom14npv101-infra-manager ~ vlabrc]# \n</code></pre> <p>6) run that commands to find the OSD name and it;s corresponding disk logical naming details here. </p> <pre><code>[root@dom14npv101-infra-manager ~ vlabrc]#  oc logs rook-ceph-osd-24-6457dffc55-86brj |grep -i nvme\nDefaulted container \"osd\" out of: osd, log-collector, blkdevmapper (init), activate (init), expand-bluefs (init), chown-container-data-dir (init)\ndebug 2025-05-06T18:36:08.392+0000 7f3d38717640  1 osd.24 185 _collect_metadata nvme2n1:\n[root@dom14npv101-infra-manager ~ vlabrc]# oc logs rook-ceph-osd-27-7597d558fc-kw9l9 |grep -i nvme\nDefaulted container \"osd\" out of: osd, log-collector, blkdevmapper (init), activate (init), expand-bluefs (init), chown-container-data-dir (init)\ndebug 2025-05-06T18:36:04.368+0000 7f793d509640  1 osd.27 181 _collect_metadata nvme7n1:\n[root@dom14npv101-infra-manager ~ vlabrc]# oc logs rook-ceph-osd-31-5cdf75ff54-bhmlr |grep -i nvme\nDefaulted container \"osd\" out of: osd, log-collector, blkdevmapper (init), activate (init), expand-bluefs (init), chown-container-data-dir (init)\ndebug 2025-05-06T18:36:13.727+0000 7fbb2a4ee640  1 osd.31 191 _collect_metadata nvme5n1:\n[root@dom14npv101-infra-manager ~ vlabrc]# oc logs rook-ceph-osd-32-6f99fd4845-pqg4m |grep -i nvme\nDefaulted container \"osd\" out of: osd, log-collector, blkdevmapper (init), activate (init), expand-bluefs (init), chown-container-data-dir (init)\ndebug 2025-05-06T18:36:12.739+0000 7faad8b28640  1 osd.32 190 _collect_metadata nvme6n1:\n[root@dom14npv101-infra-manager ~ vlabrc]# oc logs rook-ceph-osd-37-598b85cf5-mjjzk |grep -i nvme\nDefaulted container \"osd\" out of: osd, log-collector, blkdevmapper (init), activate (init), expand-bluefs (init), chown-container-data-dir (init)\ndebug 2025-05-06T18:36:21.737+0000 7f7fee8fd640  1 osd.37 199 _collect_metadata nvme4n1:\n[root@dom14npv101-infra-manager ~ vlabrc]# oc logs rook-ceph-osd-38-6f9cfbc8d9-wtfrd |grep -i nvme\nDefaulted container \"osd\" out of: osd, log-collector, blkdevmapper (init), activate (init), expand-bluefs (init), chown-container-data-dir (init)\ndebug 2025-05-06T18:36:23.718+0000 7f1f6e8b7640  1 osd.38 201 _collect_metadata nvme0n1:\n[root@dom14npv101-infra-manager ~ vlabrc]# oc logs rook-ceph-osd-41-6b88b44f56-tlv7m |grep -i nvme\nDefaulted container \"osd\" out of: osd, log-collector, blkdevmapper (init), activate (init), expand-bluefs (init), chown-container-data-dir (init)\ndebug 2025-05-06T18:36:27.752+0000 7fe6362c5640  1 osd.41 205 _collect_metadata nvme1n1:\n[root@dom14npv101-infra-manager ~ vlabrc]# \n</code></pre> <p>7) now login to respective storage node via ssh or oc debug so that compare the list osd disk and find out the missing OSD disk and then format that particular drive alone. </p> <pre><code>ssh core@ncpvnpvlab1-storage-101.ncpvnpvlab1.pnwlab.nsn-rdnet.net\nRed Hat Enterprise Linux CoreOS 416.94.202411201433-0\n  Part of OpenShift 4.16, RHCOS is a Kubernetes-native operating system\n  managed by the Machine Config Operator (`clusteroperator/machine-config`).\n\nWARNING: Direct SSH access to machines is not recommended; instead,\nmake configuration changes via `machineconfig` objects:\n  https://docs.openshift.com/container-platform/4.16/architecture/architecture-rhcos.html\n\n---\nLast login: Tue May  6 18:16:30 2025 from 10.203.197.23\n[core@ncpvnpvlab1-storage-101 ~]$ sudos u -= ^C\n[core@ncpvnpvlab1-storage-101 ~]$ sudo su -\nLast login: Tue May  6 18:16:35 UTC 2025 on pts/0\n[root@ncpvnpvlab1-storage-101 ~]# lsblk\nNAME        MAJ:MIN RM   SIZE RO TYPE MOUNTPOINTS\nloop0         7:0    0   5.8T  0 loop\nloop1         7:1    0   5.8T  0 loop\nloop2         7:2    0   5.8T  0 loop\nloop3         7:3    0   5.8T  0 loop\nloop4         7:4    0   5.8T  0 loop\nloop5         7:5    0   5.8T  0 loop\nloop6         7:6    0   5.8T  0 loop\nnbd0         43:0    0     0B  0 disk\nnbd1         43:32   0     0B  0 disk\nnbd2         43:64   0     0B  0 disk\nnbd3         43:96   0     0B  0 disk\nnbd4         43:128  0     0B  0 disk\nnbd5         43:160  0     0B  0 disk\nnbd6         43:192  0     0B  0 disk\nnbd7         43:224  0     0B  0 disk\nnvme3n1     259:1    0 894.2G  0 disk\n\u251c\u2500nvme3n1p1 259:2    0     1M  0 part\n\u251c\u2500nvme3n1p2 259:3    0   127M  0 part\n\u251c\u2500nvme3n1p3 259:4    0   384M  0 part /boot\n\u251c\u2500nvme3n1p4 259:5    0   460G  0 part /var\n\u2502                                     /sysroot/ostree/deploy/rhcos/var\n\u2502                                     /usr\n\u2502                                     /etc\n\u2502                                     /\n\u2502                                     /sysroot\n\u2514\u2500nvme3n1p5 259:6    0 433.7G  0 part\nnvme0n1     259:7    0   5.8T  0 disk\nnvme2n1     259:8    0   5.8T  0 disk\nnvme8n1     259:9    0   5.8T  0 disk\nnvme5n1     259:10   0   5.8T  0 disk\nnvme6n1     259:11   0   5.8T  0 disk\nnvme1n1     259:12   0   5.8T  0 disk\nnvme7n1     259:13   0   5.8T  0 disk\nnvme4n1     259:14   0   5.8T  0 disk\nnbd8         43:256  0     0B  0 disk\nnbd9         43:288  0     0B  0 disk\nnbd10        43:320  0     0B  0 disk\nnbd11        43:352  0     0B  0 disk\nnbd12        43:384  0     0B  0 disk\nnbd13        43:416  0     0B  0 disk\nnbd14        43:448  0     0B  0 disk\nnbd15        43:480  0     0B  0 disk\n[root@ncpvnpvlab1-storage-101 ~]#\n</code></pre> <p>8) on based on output from 6 and compared with output from 7. <code>nvme8n1</code> is missed out. so this drive need to formated. </p> <pre><code>[root@ncpvnpvlab1-storage-101 ~]# wipefs -a -f /dev/nvme8n1\n/dev/nvme8n1: 22 bytes were erased at offset 0x00000000 (ceph_bluestore): 62 6c 75 65 73 74 6f 72 65 20 62 6c 6f 63 6b 20 64 65 76 69 63 65\n[root@ncpvnpvlab1-storage-101 ~]#\nsgdisk -Z /dev/nvme8n1\nCreating new GPT entries in memory.\nGPT data structures destroyed! You may now partition the disk using fdisk or\nother utilities.\n[root@ncpvnpvlab1-storage-101 ~]# exit\nlogout\n[core@ncpvnpvlab1-storage-101 ~]$ exit\nlogout\nConnection to ncpvnpvlab1-storage-101.ncpvnpvlab1.pnwlab.nsn-rdnet.net closed.\n</code></pre> <p>9) wait for lso getting the pv created automationcally. </p> <pre><code>[root@dom14npv101-infra-manager ~ vlabrc]# oc get pv\nNAME                                       CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS      CLAIM                                                              STORAGECLASS          VOLUMEATTRIBUTESCLASS   REASON   AGE\nlocal-pv-1189c26a                          5961Gi     RWO            Delete           Bound       openshift-storage/ocs-deviceset-localblockstorage-1-data-14svnkz   localblockstorage     &lt;unset&gt;                          22s\n</code></pre> <p>10) wait for 4 mins, this ceph OSD will be auto created and ceph status should be having correct list of OSD's here .</p> <pre><code>bash-5.1$ ceph -s\n  cluster:\n    id:     d6599242-8a82-410c-aa83-c15b31d8f6c7\n    health: HEALTH_OK\n\n  services:\n    mon: 3 daemons, quorum a,b,c (age 4h)\n    mgr: a(active, since 67m), standbys: b\n    mds: 1/1 daemons up, 1 hot standby\n    osd: 47 osds: 47 up (since 41m), 47 in (since 42m)\n    rgw: 1 daemon active (1 hosts, 1 zones)\n\n  data:\n    volumes: 1/1 healthy\n    pools:   12 pools, 585 pgs\n    objects: 44.41k objects, 169 GiB\n    usage:   535 GiB used, 279 TiB / 279 TiB avail\n    pgs:     585 active+clean\n\n  io:\n    client:   1023 B/s rd, 224 MiB/s wr, 1 op/s rd, 62 op/s wr\n\nbash-5.1$\n</code></pre>"},{"location":"openshift/storagemanagement/storage-node-replacement/","title":"Storage node replacement","text":""},{"location":"openshift/storagemanagement/storage-node-replacement/#delete-the-storage-node","title":"Delete the storage node","text":"<p>1) Show the initial status of the storage nodes in the managed cluster (output of oc get nodes) and identify which node will be removed, e.g. storage-0.</p> <pre><code>[root@dom14npv101-infra-manager ~ vlabrc]# oc get nodes |grep -i storage\nncpvnpvlab1-storage-101.ncpvnpvlab1.pnwlab.nsn-rdnet.net   Ready    storage,worker                     25d   v1.29.10+67d3387\nncpvnpvlab1-storage-102.ncpvnpvlab1.pnwlab.nsn-rdnet.net   Ready    storage,worker                     27d   v1.29.10+67d3387\nncpvnpvlab1-storage-103.ncpvnpvlab1.pnwlab.nsn-rdnet.net   Ready    storage,worker                     27d   v1.29.10+67d3387\nncpvnpvlab1-storage-201.ncpvnpvlab1.pnwlab.nsn-rdnet.net   Ready    storage,worker                     25d   v1.29.10+67d3387\nncpvnpvlab1-storage-202.ncpvnpvlab1.pnwlab.nsn-rdnet.net   Ready    storage,worker                     27d   v1.29.10+67d3387\nncpvnpvlab1-storage-203.ncpvnpvlab1.pnwlab.nsn-rdnet.net   Ready    storage,worker                     27d   v1.29.10+67d3387\n[root@dom14npv101-infra-manager ~ vlabrc]#\n</code></pre> <p>2) Verify the ceph health status</p> <pre><code>[root@dom14npv101-infra-manager ~ vlabrc]# oc exec -it $(oc get pod -n openshift-storage -l app=rook-ceph-operator -o name) -n openshift-storage -- ceph -s -c /var/lib/rook/openshift-storage/openshift-storage.config\n  cluster:\n    id:     a2b0c334-ba7c-4ae1-b3f5-c6d514f19bec\n    health: HEALTH_OK\n\n  services:\n    mon: 3 daemons, quorum f,g,h (age 21h)\n    mgr: a(active, since 21h), standbys: b\n    mds: 1/1 daemons up, 1 hot standby\n    osd: 48 osds: 48 up (since 21h), 48 in (since 23h)\n    rgw: 1 daemon active (1 hosts, 1 zones)\n\n  data:\n    volumes: 1/1 healthy\n    pools:   12 pools, 1097 pgs\n    objects: 190.32k objects, 393 GiB\n    usage:   1.2 TiB used, 278 TiB / 279 TiB avail\n    pgs:     1097 active+clean\n\n  io:\n    client:   8.7 KiB/s rd, 9.9 MiB/s wr, 11 op/s rd, 22 op/s wr\n\n[root@dom14npv101-infra-manager ~ vlabrc]#\n</code></pre> <p>3) Identify the monitor pod (if any), and OSDs that are running in the node that you need to replace:</p> <pre><code>[root@dom14npv101-infra-manager ~ vlabrc]# oc get pods -n openshift-storage -o wide | grep -i ncpvnpvlab1-storage-101.ncpvnpvlab1.pnwlab.nsn-rdnet.net  |grep -i rook-ceph\nrook-ceph-crashcollector-73c0594e536089af81dd498574227f77-94vtt   1/1     Running   0             21h   172.28.18.41     ncpvnpvlab1-storage-101.ncpvnpvlab1.pnwlab.nsn-rdnet.net   &lt;none&gt;           &lt;none&gt;\nrook-ceph-exporter-73c0594e536089af81dd498574227f77-754b5866njj   1/1     Running   0             21h   172.28.18.42     ncpvnpvlab1-storage-101.ncpvnpvlab1.pnwlab.nsn-rdnet.net   &lt;none&gt;           &lt;none&gt;\nrook-ceph-mds-ocs-storagecluster-cephfilesystem-a-795996f7lvsqs   2/2     Running   0             22h   172.28.18.21     ncpvnpvlab1-storage-101.ncpvnpvlab1.pnwlab.nsn-rdnet.net   &lt;none&gt;           &lt;none&gt;\nrook-ceph-mds-ocs-storagecluster-cephfilesystem-b-78f7bbf8c2hhg   2/2     Running   0             22h   172.28.18.22     ncpvnpvlab1-storage-101.ncpvnpvlab1.pnwlab.nsn-rdnet.net   &lt;none&gt;           &lt;none&gt;\nrook-ceph-mgr-b-5468b7cf-fmwnp                                    4/4     Running   0             22h   172.28.18.7      ncpvnpvlab1-storage-101.ncpvnpvlab1.pnwlab.nsn-rdnet.net   &lt;none&gt;           &lt;none&gt;\nrook-ceph-mon-f-54d858f9cd-m5q76                                  2/2     Running   0             22h   172.28.18.8      ncpvnpvlab1-storage-101.ncpvnpvlab1.pnwlab.nsn-rdnet.net   &lt;none&gt;           &lt;none&gt;\nrook-ceph-operator-7bc4cf5ccd-4lxjr                               1/1     Running   0             22h   172.28.18.39     ncpvnpvlab1-storage-101.ncpvnpvlab1.pnwlab.nsn-rdnet.net   &lt;none&gt;           &lt;none&gt;\nrook-ceph-osd-14-6df66b8b99-nmgvq                                 2/2     Running   0             22h   172.28.18.9      ncpvnpvlab1-storage-101.ncpvnpvlab1.pnwlab.nsn-rdnet.net   &lt;none&gt;           &lt;none&gt;\nrook-ceph-osd-21-86cd6b7f7f-498vh                                 2/2     Running   0             22h   172.28.18.12     ncpvnpvlab1-storage-101.ncpvnpvlab1.pnwlab.nsn-rdnet.net   &lt;none&gt;           &lt;none&gt;\nrook-ceph-osd-28-698bb96856-vmr8t                                 2/2     Running   0             22h   172.28.18.11     ncpvnpvlab1-storage-101.ncpvnpvlab1.pnwlab.nsn-rdnet.net   &lt;none&gt;           &lt;none&gt;\nrook-ceph-osd-34-5f49bdbb85-f528w                                 2/2     Running   0             22h   172.28.18.15     ncpvnpvlab1-storage-101.ncpvnpvlab1.pnwlab.nsn-rdnet.net   &lt;none&gt;           &lt;none&gt;\nrook-ceph-osd-4-7495d5f559-zccrg                                  2/2     Running   0             22h   172.28.18.34     ncpvnpvlab1-storage-101.ncpvnpvlab1.pnwlab.nsn-rdnet.net   &lt;none&gt;           &lt;none&gt;\nrook-ceph-osd-41-689699f766-clfzm                                 2/2     Running   0             22h   172.28.18.17     ncpvnpvlab1-storage-101.ncpvnpvlab1.pnwlab.nsn-rdnet.net   &lt;none&gt;           &lt;none&gt;\nrook-ceph-osd-44-94c7c6565-cz8wg                                  2/2     Running   0             22h   172.28.18.16     ncpvnpvlab1-storage-101.ncpvnpvlab1.pnwlab.nsn-rdnet.net   &lt;none&gt;           &lt;none&gt;\nrook-ceph-osd-9-6b966dc5db-28595                                  2/2     Running   0             22h   172.28.18.18     ncpvnpvlab1-storage-101.ncpvnpvlab1.pnwlab.nsn-rdnet.net   &lt;none&gt;           &lt;none&gt;\nrook-ceph-rgw-ocs-storagecluster-cephobjectstore-a-788d79bdrltz   2/2     Running   0             22h   172.28.18.38     ncpvnpvlab1-storage-101.ncpvnpvlab1.pnwlab.nsn-rdnet.net   &lt;none&gt;           &lt;none&gt;\nrook-ceph-tools-6f854c4bfc-wqhm7                                  1/1     Running   0             22h   172.28.18.30     ncpvnpvlab1-storage-101.ncpvnpvlab1.pnwlab.nsn-rdnet.net   &lt;none&gt;           &lt;none&gt;\n[root@dom14npv101-infra-manager ~ vlabrc]#\n</code></pre> <p>4) Scale down the deployments of the pods identified in the previous step: (mon, osd, crashcollector)</p> <pre><code>[root@dom14npv101-infra-manager ~ vlabrc]# oc -n openshift-storage scale deployment rook-ceph-crashcollector --replicas=0\noc -n openshift-storage scale deployment rook-ceph-mgr-b  --replicas=0\noc -n openshift-storage scale deployment rook-ceph-mon-f  --replicas=0\noc -n openshift-storage scale deployment rook-ceph-osd-14 --replicas=0\noc -n openshift-storage scale deployment rook-ceph-osd-21 --replicas=0\noc -n openshift-storage scale deployment rook-ceph-osd-28 --replicas=0\noc -n openshift-storage scale deployment rook-ceph-osd-34 --replicas=0\noc -n openshift-storage scale deployment rook-ceph-osd-4  --replicas=0\noc -n openshift-storage scale deployment rook-ceph-osd-41 --replicas=0\noc -n openshift-storage scale deployment rook-ceph-osd-44 --replicas=0\noc -n openshift-storage scale deployment rook-ceph-osd-9  --replicas=0\nerror: no objects passed to scale\ndeployment.apps/rook-ceph-mgr-b scaled\ndeployment.apps/rook-ceph-mon-f scaled\ndeployment.apps/rook-ceph-osd-14 scaled\ndeployment.apps/rook-ceph-osd-21 scaled\ndeployment.apps/rook-ceph-osd-28 scaled\ndeployment.apps/rook-ceph-osd-34 scaled\ndeployment.apps/rook-ceph-osd-4 scaled\ndeployment.apps/rook-ceph-osd-41 scaled\ndeployment.apps/rook-ceph-osd-44 scaled\ndeployment.apps/rook-ceph-osd-9 scaled\n[root@dom14npv101-infra-manager ~ vlabrc]#\n</code></pre> <p>5) Add the following Annotation for node deletion in the siteconfig.yaml (add crsuppression and crannotation both)</p> <p>6) To initiate the automated deletion process, begin by deleting the BMH CR of the control plane node that has been previously annotated with the specific \u201ccrAnnotation\u201d.</p> <p>7) Add \u201ccrSuppression\u201d to SiteConfig so that node will be removed from the cluster. Note that you need to keep the \u201ccrAnnotation\u201d on the node.</p> <pre><code>      - hostName: \"ncpvnpvlab1-storage-101.ncpvnpvlab1.pnwlab.nsn-rdnet.net\"\n        role: \"worker\"\n        crSuppression:\n        - BareMetalHost\n        crAnnotations:\n          add:\n          BareMetalHost:\n            bmac.agent-install.openshift.io/remove-agent-and-node-on-delete: true\n</code></pre> <p>8) Git add/commit/push the SiteConfig.yaml, so that ArgoCD syncs the updated SiteConfig to the Hub Cluster      a. The BMH on Hub cluster should start showing updated status that the node is being deprovisioning. This status change indicates that the node is undergoing the deprovisioning process, a necessary step before its complete removal.</p> <pre><code>[root@dom14npv101-infra-manager ~ hubrc]# oc get bmh -n ncpvnpvlab1 |grep -i storage-101\nncpvnpvlab1-storage-101.ncpvnpvlab1.pnwlab.nsn-rdnet.net   deprovisioning              true             60m\n[root@dom14npv101-infra-manager ~ hubrc]#\n</code></pre> <p>9) Cluster Administrators should wait for the BMH to finish deprovisioning and be fully deleted from the cluster environment. After ~10 minutes (this might take longer or shorter depending on your environment to complete the node clean up):     a. The storage node \u201cstorage-0\u201d is powered off     b. The BMH resource of the replaced node is deleted on the Hub Cluster.</p> <pre><code>[root@dom14npv101-infra-manager ~ vlabrc]# oc get bmh -n ncpvnpvlab1 |grep -i storage\nncpvnpvlab1-storage-102.ncpvnpvlab1.pnwlab.nsn-rdnet.net   provisioned              true             27d\nncpvnpvlab1-storage-103.ncpvnpvlab1.pnwlab.nsn-rdnet.net   provisioned              true             27d\nncpvnpvlab1-storage-201.ncpvnpvlab1.pnwlab.nsn-rdnet.net   provisioned              true             25d\nncpvnpvlab1-storage-202.ncpvnpvlab1.pnwlab.nsn-rdnet.net   provisioned              true             27d\nncpvnpvlab1-storage-203.ncpvnpvlab1.pnwlab.nsn-rdnet.net   provisioned              true             27d\n[root@dom14npv101-infra-manager ~ vlabrc]#\n</code></pre> <p>10)  \u201coc get node\u201d on cluster shows that the node \u201cstorage-101\u201d is no longer part of the cluster, only 2 storage nodes remain</p> <p>ncpvnpvlab1-storage-101.ncpvnpvlab1.pnwlab.nsn-rdnet.net  ---&gt; still part of it. </p> <pre><code>[root@dom14npv101-infra-manager ~ vlabrc]# oc get nodes |grep -i storage\nncpvnpvlab1-storage-101.ncpvnpvlab1.pnwlab.nsn-rdnet.net   Ready    storage,worker                     25d   v1.29.10+67d3387\nncpvnpvlab1-storage-102.ncpvnpvlab1.pnwlab.nsn-rdnet.net   Ready    storage,worker                     27d   v1.29.10+67d3387\nncpvnpvlab1-storage-103.ncpvnpvlab1.pnwlab.nsn-rdnet.net   Ready    storage,worker                     27d   v1.29.10+67d3387\nncpvnpvlab1-storage-201.ncpvnpvlab1.pnwlab.nsn-rdnet.net   Ready    storage,worker                     25d   v1.29.10+67d3387\nncpvnpvlab1-storage-202.ncpvnpvlab1.pnwlab.nsn-rdnet.net   Ready    storage,worker                     27d   v1.29.10+67d3387\nncpvnpvlab1-storage-203.ncpvnpvlab1.pnwlab.nsn-rdnet.net   Ready    storage,worker                     27d   v1.29.10+67d3387\n[root@dom14npv101-infra-manager ~ vlabrc]#\n</code></pre>"},{"location":"openshift/troubleshooting/ncomsa/","title":"NCOM Caas Issue","text":""},{"location":"openshift/troubleshooting/ncomsa/#ncom-caas-fluctuate-every-1-hours-once-and-here-is-the-process-to-create-a-service-account-for-ncom-to-resolve-this-issue","title":"NCOM CAAS fluctuate every 1 hours once and here is the process to create a service account for ncom to resolve this issue.","text":"<p>Caas using user/passwd based registation is fluctuate</p>"},{"location":"openshift/troubleshooting/ncomsa/#problem-describe","title":"Problem describe:","text":"<ul> <li>CAAS flucatuate on the ncom, CNF onboarding may fail. in millicom, 30mins once it;s fluctuate.  </li> </ul>"},{"location":"openshift/troubleshooting/ncomsa/#solution-describe","title":"Solution describe:","text":"<ul> <li>The auto discovery mode NCOM uses a token after discovery to integrate with the workload cluster instead of userID/password. This connection appears to be stable and does not fluctuate. So ncom want to use this token based access as a solution to our problem. instead of user id based. </li> </ul>"},{"location":"openshift/troubleshooting/ncomsa/#limitation","title":"limitation:","text":"<ul> <li>based on NCP security hardening, system token may expire within 24hrs. </li> </ul>"},{"location":"openshift/troubleshooting/ncomsa/#solution-steps","title":"Solution steps:","text":"<p>1) You need to be logged in with a user who has cluster-admin privileges:</p> <p>2) Create an service account on the desired project. </p> <pre><code>[root@ncputility ~ pancwl_rc]$ oc project ncom01pan\nNow using project \"ncom01pan\" on server \"https://api.panclypcwl01.mnc020.mcc714:6443\".\n[root@ncputility ~ pancwl_rc]$ \n\n[root@ncputility ~ pancwl_rc]$ oc create serviceaccount ncom-sa\nserviceaccount/ncom-sa created\n[root@ncputility ~ pancwl_rc]$\n</code></pre> <p>3)  Assing <code>cluster-admin</code> role to this <code>SA</code> now. </p> <pre><code>[root@ncputility ~ pancwl_rc]$ oc adm policy add-cluster-role-to-user cluster-admin -z  ncom-sa -n ncom01pan\nclusterrole.rbac.authorization.k8s.io/cluster-admin added: \"ncom-sa\"\n[root@ncputility ~ pancwl_rc]$ \n</code></pre> <p>4) describe sa here, you wont see the token and it's expected on this version OCP. </p> <pre><code>[root@ncputility ~ pancwl_rc]$ oc describe sa ncom-sa\nName:                ncom-sa\nNamespace:           ncom01pan\nLabels:              &lt;none&gt;\nAnnotations:         &lt;none&gt;\nImage pull secrets:  &lt;none&gt;\nMountable secrets:   &lt;none&gt;\nTokens:              &lt;none&gt;\nEvents:              &lt;none&gt;\n[root@ncputility ~ pancwl_rc]$\n</code></pre> <p>5) Now Manually Create a Token for the ServiceAccount. This tells Kubernetes/OpenShift to generate a token for ncom-sa and store it in the secret ncom-sa-secret.</p> <pre><code>[root@ncputility ~ pancwl_rc]$ cat &gt; secret.yaml\napiVersion: v1\nkind: Secret\nmetadata:\n  name: ncom-sa-secret\n  annotations:\n    kubernetes.io/service-account.name: ncom-sa\ntype: kubernetes.io/service-account-token\n\n[root@ncputility ~ pancwl_rc]$\n</code></pre> <p>6) describe sa here again. you can see the secret created. </p> <pre><code>[root@ncputility ~ pancwl_rc]$ oc describe sa ncom-sa\nName:                ncom-sa\nNamespace:           ncom01pan\nLabels:              &lt;none&gt;\nAnnotations:         &lt;none&gt;\nImage pull secrets:  &lt;none&gt;\nMountable secrets:   &lt;none&gt;\nTokens:              ncom-sa-secret\nEvents:              &lt;none&gt;\n[root@ncputility ~ pancwl_rc]$\n</code></pre> <p>7) retrive the token and share with ncom team here</p> <p><code>oc get secret ncom-sa-secret -n ncom01pan -o jsonpath=\"{.data.token}\" | base64 -d</code></p> <pre><code> [root@ncputility ~ pancwl_rc]$ oc get secret ncom-sa-secret -n ncom01pan -o jsonpath=\"{.data.token}\" | base64 -d\n\neyJhbGciOiJSUzI1NiIsImtpZCI6IjBpd2lFcjdSU3ktY25uRDl3YTVhU0M2V0wtZ0pUWXBXM0RzMmpUTFp1N28ifQ.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZX        Rlcy5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJuY29tMDFwYW4iLCJrdWJlcm5ldGVzLmlvL3NlcnZpY2VhY2NvdW50L3NlY3JldC5uYW1lIjoibmNvbS1zYS1zZWNyZXQiLCJrdWJlcm5        ldGVzLmlvL3NlcnZpY2VhY2NvdW50L3NlcnZpY2UtYWNjb3VudC5uYW1lIjoibmNvbS1zYSIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VydmljZS1hY2NvdW50LnVpZCI6IjU0MDli        ZTIyLTA4YTMtNDVkNS1iNGRlLWMyOGNlMGZjYmIwMyIsInN1YiI6InN5c3RlbTpzZXJ2aWNlYWNjb3VudDpuY29tMDFwYW46bmNvbS1zYSJ9.H7EpGT0Le2evHG2UW96OO3-mvY7-Dg6nzVAgr98c        SUALecFyrEC9g-f9lYvgncS4mgQINV7voTaIYQ211AvGn6Adeu-PE3vPObxcIUA5_KJmRbKl7O1TMzqpJmIa5TnDeSLt2xN6D3bn74n1JpaGq3-VhBicA9j0jDbLVTi5EE_JGX2PkpJ-vvMhrnsGF        YzEA7oOAWwyyPMy2RPeEzKYK0bubvQaRLf2T-oZkWgXejWeLV0Z1mU12e75husjrEdu-FEfzyAEU_CQIuPAHHBC7U1OMLAuN_ehImmoLzObSCKqruLxeqIZamr6cNzZAKouc2bcdvLuDWmG3nVZRP        ZRGnRMKPGaFNPnetav0Nq5MvYy4zOAAaDWq1_B5b8iYFxCsycceqZKySe-Z_lmEw1x1lyf2I8Z5fpfaoPW_QxndSuxBrV4h6O9igZpwzoCjwq8vB838vkVMlLIcDGTViLAnLd8pl763-1coSzqsnt        rO_eUUTZICvTzp-dA6QdZOb5SWMXj9-vqvW469rjzgaopSeS7hmUO_6BGMS7O5-nZC4PG-QPISrUKup9eAM62jssTmu2QL4-y3FbY1vjOFvJAbkEwzBhPN-2EPEY9zUu44ZximqFwNkuR4T66u_jG        \n[root@ncputility ~ pancwl_rc]$\n</code></pre>"},{"location":"openshift/troubleshooting/ncomsa/#reference","title":"reference","text":"<ul> <li>Creating an SA and map cluster admin role</li> </ul>"},{"location":"openshift/troubleshooting/nsenter/","title":"Method to use nsenter on the OCP","text":"<p>Please find the method to use nsenter, so that you will not struggle during your deployment. </p> <p>1) Login to the ocp cluster with cluster admin role and find the pod name which you want to login inside the container using nsenter. <code>ncom01pan-caas-plugin-9bd7755bb-bb5fs</code> is selected.</p> <pre><code>[root@ncputility ~ pancwl_rc]$ source /root/pancwlrc\nWARNING: Using insecure TLS client config. Setting this option is not supported!\n\nLogin successful.\n\nYou have access to 119 projects, the list has been suppressed. You can list all projects with 'oc projects'\n\nUsing project \"ncom01pan\".\n[root@ncputility ~ pancwl_rc]$ oc get pods -A -o wide |grep -i ^C\n[root@ncputility ~ pancwl_rc]$ oc get pods -n ncom01pan -o wide |grep -i caas\nncom01pan-caas-plugin-9bd7755bb-bb5fs                       1/1     Running             0          3h15m   172.17.18.34    appworker23.panclypcwl01.mnc020.mcc714   &lt;none&gt;           &lt;none&gt;\nncom01pan-caas-plugin-9bd7755bb-cwzmm                       1/1     Running             0          3h15m   172.18.8.78     appworker16.panclypcwl01.mnc020.mcc714   &lt;none&gt;           &lt;none&gt;\n[root@ncputility ~ pancwl_rc]$\n</code></pre> <p>2) execute to that node where your pod hosted, and this will be indentified from the previous command. </p> <pre><code>[root@ncputility ~ pancwl_rc]$ oc debug -t node/appworker0.panclypcwl01.mnc020.mcc714\nTemporary namespace openshift-debug-vz9qc is created for debugging node...\nStarting pod/appworker0panclypcwl01mnc020mcc714-debug-87f7l ...\nTo use host binaries, run `chroot /host`\nPod IP: 10.89.96.26\nIf you don't see a command prompt, try pressing enter.\nsh-5.1# chroot /host\nsh-5.1#\n</code></pre> <p>3) now find out the container id using crictl command here </p> <pre><code>sh-5.1# crictl ps |grep -i ncom01pan-caas-plugin-7654b86fdb-mz5r7\n2b61910d5eb23       quay-registry.apps.panclyphub01.mnc020.mcc714/ncom01pan/ncom/caas-plugin@sha256:d6d9506d14d756ecafe7d93debcb9eeb498cc805506fb1480002713d17ce64d6   19 minutes ago      Running             cjee-wildfly                         0                   8ca17869e45fa       ncom01pan-caas-plugin-7654b86fdb-mz5r7\n</code></pre> <p>4) find out the pid of the container using inspect command. </p> <pre><code>sh-5.1# crictl inpsect 2b61910d5eb23 |grep -i pid\nNo help topic for 'inpsect'\nsh-5.1# crictl inspect 2b61910d5eb23 |grep -i pid\n    \"pid\": 60545,\n          \"pids\": {\n            \"type\": \"pid\"\n                \"getpid\",\n                \"getppid\",\n                \"pidfd_getfd\",\n                \"pidfd_open\",\n                \"pidfd_send_signal\",\n                \"waitpid\",\n</code></pre> <p>5) now use the toolbox command, since tcpdump is not configured on the host os level. </p> <pre><code>sh-5.1# toolbox\n.toolboxrc file detected, overriding defaults...\nChecking if there is a newer version of quay-registry.apps.panclyphub01.mnc020.mcc714/ocmirror/rhel9/support-tools:latest available...\nContainer 'toolbox-root' already exists. Trying to start...\n(To remove the container and start with a fresh toolbox, run: sudo podman rm 'toolbox-root')\ntoolbox-root\nContainer started successfully. To exit, type 'exit'.\n[root@appworker0 /]# nsenter -t 60545 -n ip a\n1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000\n    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00\n    inet 127.0.0.1/8 scope host lo\n       valid_lft forever preferred_lft forever\n    inet6 ::1/128 scope host\n       valid_lft forever preferred_lft forever\n2: gre0@NONE: &lt;NOARP&gt; mtu 1476 qdisc noop state DOWN group default qlen 1000\n    link/gre 0.0.0.0 brd 0.0.0.0\n3: gretap0@NONE: &lt;BROADCAST,MULTICAST&gt; mtu 1462 qdisc noop state DOWN group default qlen 1000\n    link/ether 00:00:00:00:00:00 brd ff:ff:ff:ff:ff:ff\n4: erspan0@NONE: &lt;BROADCAST,MULTICAST&gt; mtu 1450 qdisc noop state DOWN group default qlen 1000\n    link/ether 00:00:00:00:00:00 brd ff:ff:ff:ff:ff:ff\n5: ip6tnl0@NONE: &lt;NOARP&gt; mtu 1452 qdisc noop state DOWN group default qlen 1000\n    link/tunnel6 :: brd :: permaddr 3eac:e266:df07::\n6: ip6gre0@NONE: &lt;NOARP&gt; mtu 1448 qdisc noop state DOWN group default qlen 1000\n    link/gre6 :: brd :: permaddr 6679:afbc:3648::\n7: eth0@if609: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 8900 qdisc noqueue state UP group default\n    link/ether 0a:58:ac:10:04:0f brd ff:ff:ff:ff:ff:ff link-netns d00bdece-6c79-4840-87d1-10d3103ecdd7\n    inet 172.16.4.15/23 brd 172.16.5.255 scope global eth0\n       valid_lft forever preferred_lft forever\n    inet6 fe80::858:acff:fe10:40f/64 scope link\n       valid_lft forever preferred_lft forever\n[root@appworker0 /]# nsenter -t 60545 -n ping  quay-registry.apps.panclyphub01.mnc020.mcc714\nPING quay-registry.apps.panclyphub01.mnc020.mcc714 (10.89.97.143) 56(84) bytes of data.\nFrom 10.89.97.168 (10.89.97.168) icmp_seq=1 Destination Host Unreachable\nFrom 10.89.97.168 (10.89.97.168) icmp_seq=2 Destination Host Unreachable\nFrom 10.89.97.168 (10.89.97.168) icmp_seq=3 Destination Host Unreachable\nFrom 10.89.97.168 (10.89.97.168) icmp_seq=4 Destination Host Unreachable\n\n^C\n--- quay-registry.apps.panclyphub01.mnc020.mcc714 ping statistics ---\n5 packets transmitted, 0 received, +4 errors, 100% packet loss, time 4089ms\npipe 4\n[root@appworker0 /]# nsenter -t 60545 -n ping  10.89.97.143\nPING 10.89.97.143 (10.89.97.143) 56(84) bytes of data.\nFrom 10.89.97.168 icmp_seq=1 Destination Host Unreachable\nFrom 10.89.97.168 icmp_seq=2 Destination Host Unreachable\nFrom 10.89.97.168 icmp_seq=3 Destination Host Unreachable\nFrom 10.89.97.168 icmp_seq=4 Destination Host Unreachable\n\n^C\n--- 10.89.97.143 ping statistics ---\n4 packets transmitted, 0 received, +4 errors, 100% packet loss, time 3088ms\npipe 4\n[root@appworker0 /]# nsenter -t 60545 -n tracepath 10.89.97.143\n 1?: [LOCALHOST]                      pmtu 8900\n 1:  *.apps.panclyphub01.mnc020.mcc714                     1.683ms asymm  2\n 1:  *.apps.panclyphub01.mnc020.mcc714                     0.878ms asymm  2\n 2:  100.88.0.7                                            1.903ms asymm  3\n 3:  172.17.2.2                                            2.086ms\n 4:  no reply\n 4:  10.89.97.168                                        3087.371ms !H\n     Resume: pmtu 8900\n[root@appworker0 /]#\n</code></pre>"},{"location":"openshift/usermanagement/remove-kubeadmin/","title":"Removing the kubeadmin user","text":""},{"location":"openshift/usermanagement/remove-kubeadmin/#the-kubeadmin-user","title":"The kubeadmin user","text":"<p>OpenShift Container Platform creates a cluster administrator, kubeadmin, after the installation process completes.</p> <p>This user has the cluster-admin role automatically applied and is treated as the root user for the cluster. The password is dynamically generated and unique to your OpenShift Container Platform environment. After installation completes the password is provided in the installation program\u2019s output. For example:</p> <pre><code>INFO Install complete!\nINFO Run 'export KUBECONFIG=&lt;your working directory&gt;/auth/kubeconfig' to manage the cluster with 'oc', the OpenShift CLI.\nINFO The cluster is ready when 'oc login -u kubeadmin -p &lt;provided&gt;' succeeds (wait a few minutes).\nINFO Access the OpenShift web-console here: https://console-openshift-console.apps.demo1.openshift4-beta-abcorp.com\nINFO Login to the console with user: kubeadmin, password: &lt;provided&gt;\n</code></pre>"},{"location":"openshift/usermanagement/remove-kubeadmin/#removing-the-kubeadmin-user_1","title":"Removing the kubeadmin user","text":"<ol> <li>After you define an identity provider and create a new cluster-admin user, you can remove the kubeadmin to improve cluster security.</li> </ol>"},{"location":"openshift/usermanagement/remove-kubeadmin/#warning","title":"Warning","text":"<p><code>If you follow this procedure before another user is a cluster-admin, then OpenShift Container Platform must be reinstalled. It is not possible to undo this command.</code></p>"},{"location":"openshift/usermanagement/remove-kubeadmin/#prerequisites","title":"Prerequisites","text":"<ul> <li>You must have configured at least one identity provider.</li> <li>You must have added the cluster-admin role to a user.</li> <li>You must be logged in as an administrator.</li> </ul>"},{"location":"openshift/usermanagement/remove-kubeadmin/#procedure","title":"Procedure","text":"<ol> <li>Remove the kubeadmin secrets:</li> </ol> <pre><code>oc delete secrets kubeadmin -n kube-system\n</code></pre>"},{"location":"openshift/usermanagement/remove-kubeadmin/#references","title":"References","text":"<ul> <li>Remove kubeadmin id</li> </ul>"},{"location":"openshift/usermanagement/user-management/","title":"OCP User management using htpassword","text":""},{"location":"openshift/usermanagement/user-management/#configure-htpasswd-as-an-identity-provider-for-ocp","title":"Configure HTPASSWD as an identity provider for OCP","text":""},{"location":"openshift/usermanagement/user-management/#add-update-remove-users-for-ocp-using-htpasswd-as-indentity-provider","title":"Add, update, remove users for OCP using htpasswd as indentity provider.","text":""},{"location":"openshift/usermanagement/user-management/#procedure-to-add-an-additional-users","title":"Procedure to <code>Add</code> an additional users","text":"<p>1) Retrieve the htpasswd file from the htpass-secret Secret object and save the file to your file system:</p> <pre><code>[root@dom14npv101-infra-manager ~ hub]# oc get secret htpass-secret -ojsonpath={.data.htpasswd} -n openshift-config | base64 --decode &gt; users.htpasswd\n[root@dom14npv101-infra-manager ~ hub]# cat users.htpasswd\nncpadmin:$2y$05$DYlpXiwFzfyRioO4hRNq1.ZdFLO3yMz3Pl3gs7.yUpEUKeOGoHX9K\n[root@dom14npv101-infra-manager ~ hub]# \n</code></pre> <p>2) Add or remove users from the users.htpasswd file.</p> <pre><code>[root@dom14npv101-infra-manager ~ hub]# htpasswd -bB users.htpasswd nokia nokia@123\nAdding password for user nokia\n[root@dom14npv101-infra-manager ~ hub]# \n</code></pre> <p>3) Replace the htpass-secret Secret object with the updated users in the users.htpasswd file:</p> <pre><code>[root@dom14npv101-infra-manager ~ hub]# oc create secret generic htpass-secret --from-file=htpasswd=users.htpasswd --dry-run=client -o yaml -n openshift-config | oc replace -f -\nsecret/htpass-secret replaced\n[root@dom14npv101-infra-manager ~ hub]# \n</code></pre> <p>4) Wait for all these pods to be restarted </p> <pre><code>[root@dom14npv101-infra-manager ~ hub]# oc get pods -n openshift-authentication -o wide |grep -i oauth\noauth-openshift-f446bd5b-58cps   1/1     Running   0          82s   172.20.2.190   ncpvnpvhub-hubmaster-101.ncpvnpvhub.pnwlab.nsn-rdnet.net   &lt;none&gt;           &lt;none&gt;\noauth-openshift-f446bd5b-k8dqx   0/1     Running   0          27s   172.21.0.241   ncpvnpvhub-hubmaster-103.ncpvnpvhub.pnwlab.nsn-rdnet.net   &lt;none&gt;           &lt;none&gt;\noauth-openshift-f446bd5b-v4n6m   1/1     Running   0          55s   172.20.0.134   ncpvnpvhub-hubmaster-102.ncpvnpvhub.pnwlab.nsn-rdnet.net   &lt;none&gt;           &lt;none&gt;\n[root@dom14npv101-infra-manager ~ hub]# \n</code></pre> <p>5) Validate the login now. </p> <pre><code>[root@dom14npv101-infra-manager ~ hub]# oc login -u nokia -p nokia@123\nWARNING: Using insecure TLS client config. Setting this option is not supported!\n\nLogin successful.\n\nYou don't have any projects. You can try to create a new project, by running\n\n    oc new-project &lt;projectname&gt;\n\n[root@dom14npv101-infra-manager ~ hub]# oc whoami\nnokia\n[root@dom14npv101-infra-manager ~ hub]# \n</code></pre>"},{"location":"openshift/usermanagement/user-management/#procedure-to-update-the-password-of-an-existing-users","title":"Procedure to <code>update</code> the password of an existing users","text":"<p>1) Retrieve the htpasswd file from the htpass-secret Secret object and save the file to your file system:</p> <pre><code>[root@dom14npv101-infra-manager ~ hub]# oc get secret htpass-secret -ojsonpath={.data.htpasswd} -n openshift-config | base64 --decode &gt; users.htpasswd\n[root@dom14npv101-infra-manager ~ hub]# cat users.htpasswd\nncpadmin:$2y$05$DYlpXiwFzfyRioO4hRNq1.ZdFLO3yMz3Pl3gs7.yUpEUKeOGoHX9K\nnokia:$2y$05$SLpgZb3AE.fE7WQEGIMOXesoZdY9pV9OHVpXb5CuITdZO5RVId8Ye\n[root@dom14npv101-infra-manager ~ hub]# \n</code></pre> <p>2) update the users password from the users.htpasswd file.</p> <pre><code>[root@dom14npv101-infra-manager ~ hub]# htpasswd -bB users.htpasswd nokia nokia@1234\nUpdating password for user nokia\n[root@dom14npv101-infra-manager ~ hub]# \n</code></pre> <p>3) Replace the htpass-secret Secret object with the updated users in the users.htpasswd file:</p> <pre><code>[root@dom14npv101-infra-manager ~ hub]# oc create secret generic htpass-secret --from-file=htpasswd=users.htpasswd --dry-run=client -o yaml -n openshift-config | oc replace -f -\nsecret/htpass-secret replaced\n[root@dom14npv101-infra-manager ~ hub]# \n</code></pre> <p>4) Wait for all these pods to be restarted </p> <pre><code>[root@dom14npv101-infra-manager ~ hub]# oc get pods -n openshift-authentication -o wide -w\nNAME                             READY   STATUS    RESTARTS   AGE     IP             NODE                                                       NOMINATED NODE   READINESS GATES\noauth-openshift-f446bd5b-58cps   1/1     Running   0          6m48s   172.20.2.190   ncpvnpvhub-hubmaster-101.ncpvnpvhub.pnwlab.nsn-rdnet.net   &lt;none&gt;           &lt;none&gt;\noauth-openshift-f446bd5b-k8dqx   1/1     Running   0          5m53s   172.21.0.241   ncpvnpvhub-hubmaster-103.ncpvnpvhub.pnwlab.nsn-rdnet.net   &lt;none&gt;           &lt;none&gt;\noauth-openshift-f446bd5b-v4n6m   1/1     Running   0          6m21s   172.20.0.134   ncpvnpvhub-hubmaster-102.ncpvnpvhub.pnwlab.nsn-rdnet.net   &lt;none&gt;           &lt;none&gt;\noauth-openshift-f446bd5b-v4n6m   1/1     Terminating   0          6m24s   172.20.0.134   ncpvnpvhub-hubmaster-102.ncpvnpvhub.pnwlab.nsn-rdnet.net   &lt;none&gt;           &lt;none&gt;\noauth-openshift-6497ccb4f5-d5wxd   0/1     Pending       0          0s      &lt;none&gt;         &lt;none&gt;                                                     &lt;none&gt;           &lt;none&gt;\noauth-openshift-6497ccb4f5-d5wxd   0/1     Pending       0          0s      &lt;none&gt;         &lt;none&gt;                                                     &lt;none&gt;           &lt;none&gt;\n</code></pre> <p>5) Validate the login now. </p> <pre><code>[root@dom14npv101-infra-manager ~ hub]# oc login -u nokia -p nokia@123\nWARNING: Using insecure TLS client config. Setting this option is not supported!\n\nLogin failed (401 Unauthorized)\nVerify you have provided the correct credentials.\n[root@dom14npv101-infra-manager ~ hub]# oc login -u nokia -p nokia@1234\nWARNING: Using insecure TLS client config. Setting this option is not supported!\n\nLogin successful.\n\nYou don't have any projects. You can try to create a new project, by running\n\n    oc new-project &lt;projectname&gt;\n\n[root@dom14npv101-infra-manager ~ hub]# \n</code></pre>"},{"location":"openshift/usermanagement/user-management/#procedure-to-delete-an-user-completely","title":"Procedure to <code>delete</code> an user completely","text":"<p>1) Retrieve the htpasswd file from the htpass-secret Secret object and save the file to your file system:</p> <pre><code>[root@dom14npv101-infra-manager ~ hub]# oc get secret htpass-secret -ojsonpath={.data.htpasswd} -n openshift-config | base64 --decode &gt; users.htpasswd\n[root@dom14npv101-infra-manager ~ hub]# cat users.htpasswd\nncpadmin:$2y$05$DYlpXiwFzfyRioO4hRNq1.ZdFLO3yMz3Pl3gs7.yUpEUKeOGoHX9K\nnokia:$2y$05$SLpgZb3AE.fE7WQEGIMOXesoZdY9pV9OHVpXb5CuITdZO5RVId8Ye\n[root@dom14npv101-infra-manager ~ hub]# \n</code></pre> <p>2) remove users from the users.htpasswd file.</p> <pre><code>[root@dom14npv101-infra-manager ~ hub]# htpasswd -D users.htpasswd nokia\nDeleting password for user nokia\n[root@dom14npv101-infra-manager ~ hub]#\n</code></pre> <p>3) Replace the htpass-secret Secret object with the updated users in the users.htpasswd file:</p> <pre><code>[root@dom14npv101-infra-manager ~ hub]# oc create secret generic htpass-secret --from-file=htpasswd=users.htpasswd --dry-run=client -o yaml -n openshift-config | oc replace -f -\nsecret/htpass-secret replaced\n[root@dom14npv101-infra-manager ~ hub]# \n</code></pre> <p>4) Wait for all these pods to be restarted </p> <pre><code>[root@dom14npv101-infra-manager ~ hub]# oc get pods -n openshift-authentication -o wide -w\nNAME                             READY   STATUS    RESTARTS   AGE     IP             NODE                                                       NOMINATED NODE   READINESS GATES\noauth-openshift-f446bd5b-58cps   1/1     Running   0          6m48s   172.20.2.190   ncpvnpvhub-hubmaster-101.ncpvnpvhub.pnwlab.nsn-rdnet.net   &lt;none&gt;           &lt;none&gt;\noauth-openshift-f446bd5b-k8dqx   1/1     Running   0          5m53s   172.21.0.241   ncpvnpvhub-hubmaster-103.ncpvnpvhub.pnwlab.nsn-rdnet.net   &lt;none&gt;           &lt;none&gt;\noauth-openshift-f446bd5b-v4n6m   1/1     Running   0          6m21s   172.20.0.134   ncpvnpvhub-hubmaster-102.ncpvnpvhub.pnwlab.nsn-rdnet.net   &lt;none&gt;           &lt;none&gt;\noauth-openshift-f446bd5b-v4n6m   1/1     Terminating   0          6m24s   172.20.0.134   ncpvnpvhub-hubmaster-102.ncpvnpvhub.pnwlab.nsn-rdnet.net   &lt;none&gt;           &lt;none&gt;\noauth-openshift-6497ccb4f5-d5wxd   0/1     Pending       0          0s      &lt;none&gt;         &lt;none&gt;                                                     &lt;none&gt;           &lt;none&gt;\noauth-openshift-6497ccb4f5-d5wxd   0/1     Pending       0          0s      &lt;none&gt;         &lt;none&gt;                                                     &lt;none&gt;           &lt;none&gt;\n</code></pre> <p>5) If you removed one or more users, you must additionally remove existing resources for each user.</p> <pre><code>a) Delete the User object:\n\n```\n[root@dom14npv101-infra-manager ~ hub]# oc get user \nNAME       UID                                    FULL NAME   IDENTITIES\nncpadmin   8e728883-a17f-4bde-8b0f-2eab78ccc6c3               my_htpasswd_provider:ncpadmin\nnokia      7e44ba3a-8d91-435f-800f-380a8e87f0d1               my_htpasswd_provider:nokia\n[root@dom14npv101-infra-manager ~ hub]# oc delete user nokia \nuser.user.openshift.io \"nokia\" deleted\n[root@dom14npv101-infra-manager ~ hub]#\n\n```\nb) Be sure to remove the user, otherwise the user can continue using their token as long as it has not expired.\n\n```\n[root@dom14npv101-infra-manager ~ hub]#  oc get identity \nNAME                            IDP NAME               IDP USER NAME   USER NAME   USER UID\nmy_htpasswd_provider:ncpadmin   my_htpasswd_provider   ncpadmin        ncpadmin    8e728883-a17f-4bde-8b0f-2eab78ccc6c3\nmy_htpasswd_provider:nokia      my_htpasswd_provider   nokia           nokia       7e44ba3a-8d91-435f-800f-380a8e87f0d1\n[root@dom14npv101-infra-manager ~ hub]# oc delete identity my_htpasswd_provider:nokia  \nidentity.user.openshift.io \"my_htpasswd_provider:nokia\" deleted\n[root@dom14npv101-infra-manager ~ hub]# \n```\n</code></pre> <p>5) Validate the login and it should not work. </p> <pre><code>[root@dom14npv101-infra-manager ~ hub]# oc login -u nokia -p nokia@1234\nWARNING: Using insecure TLS client config. Setting this option is not supported!\n\nLogin failed (401 Unauthorized)\nVerify you have provided the correct credentials.\n[root@dom14npv101-infra-manager ~ hub]# \n</code></pre>"},{"location":"openshift/usermanagement/user-management/#disclaimer","title":"Disclaimer","text":"<p>All these procedures are collected from redhat link given below. </p>"},{"location":"openshift/usermanagement/user-management/#references","title":"References","text":"<ul> <li> <p>Configuring an htpasswd identity provider </p> </li> <li> <p>To configure an HTPasswd identity provider in OpenShift 4</p> </li> <li> <p>Remove kubeadmin id</p> </li> </ul>"}]}