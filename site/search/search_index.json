{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to NCP OpenShift Notes","text":"<p>Your comprehensive troubleshooting guide to OpenShift installation, backup, storage, troubleshooting, and more.</p> <p></p>"},{"location":"#documentation-overview","title":"\ud83d\udcda Documentation Overview","text":"<ul> <li>Git Helper</li> <li>OpenShift Backup &amp; Restore</li> <li>CNF Onboarding Support</li> <li>Deployment Guides</li> <li>Maintenance Procedures</li> <li>Networking Configurations</li> <li>Storage Management</li> <li>Tools Management</li> <li>User Management</li> <li>Troubleshooting and Workarounds</li> </ul> <p>   Logout </p>"},{"location":"access-denied/","title":"Access Denied","text":"<p>You do not have permission to view this site.</p> <p>If you believe this is a mistake, please contact the administrator at venkatapathiraj.ravichandran.ext@nokia.com.</p>"},{"location":"git-helper/ncd-git-backup/","title":"Backing up Nokia Git Server","text":"<p>this procedure, works for NCD 24.9 mp1 pp1. </p>"},{"location":"git-helper/ncd-git-backup/#instructions-on-backing-up-nokia-git-server","title":"Instructions on backing up Nokia Git Server","text":""},{"location":"git-helper/ncd-git-backup/#backup-utility-will-generate-the-following-outputs","title":"backup utility will generate the following outputs","text":"<ul> <li>database.sql file containing the PostgreSQL database content</li> <li>Git repository data</li> <li>Kubernetes secrets required for the upgrade</li> <li> <p>backup_information.yml generated by the backup tool</p> </li> <li> <p>Example of backup_information.yml</p> </li> </ul> <pre><code>:db_version: 20240508085441\n:backup_created_at: 2024-06-13 13:57:08 +0000\n:gitlab_version: 17.0.1\n:tar_version: tar (GNU tar) 1.34\n:installation_type: gitlab-helm-chart\n:skipped: builds,pages,registry,uploads,artifacts,lfs,packages,\nexternal_diffs,terraform_state,pages,ci_secure_files\n:repositories_server_side: false\n</code></pre>"},{"location":"git-helper/ncd-git-backup/#prerequisites","title":"Prerequisites","text":"<ul> <li>To back up Nokia Git Server and the necessary Helm plugins for Backup and Recovery (CBUR), CBUR must be installed on your cluster.</li> <li>CBUR must be enabled in the Nokia Git Server installation.</li> </ul>"},{"location":"git-helper/ncd-git-backup/#remove-plugins-for-helm-backup-and-restore","title":"remove plugins for helm (backup and restore)","text":"<ol> <li>login to bastion host and login to cluster here .</li> </ol> <pre><code>[root@ncputility ~]# source /root/panhubrc\nWARNING: Using insecure TLS client config. Setting this option is not supported!\n\nLogin successful.\n\nYou have access to 101 projects, the list has been suppressed. You can list all projects with 'oc projects'\n\nUsing project \"default\".\n[root@ncputility ~ panhub_rc]$\n</code></pre> <ol> <li>list of plugins installed on helm here.</li> </ol> <pre><code>[root@ncputility ~ panhub_rc]$ helm plugin list\nNAME    VERSION DESCRIPTION\nbackup  0.1.2   backup/restore releases in a namespace to/from a file\n[root@ncputility ~ panhub_rc]$\n</code></pre> <ol> <li>remove using following command on helm <code>#helm plugin remove backup</code></li> </ol> <pre><code>[root@ncputility ~ panhub_rc]$ helm plugin remove backup\nUninstalled plugin: backup\n[root@ncputility ~ panhub_rc]$\n</code></pre>"},{"location":"git-helper/ncd-git-backup/#install-plugins-for-helm-backup-and-restore","title":"Install plugins for helm (backup and restore)","text":"<ol> <li>login to bastion host and login to cluster here .</li> </ol> <pre><code>[root@ncputility ~]# source /root/panhubrc\nWARNING: Using insecure TLS client config. Setting this option is not supported!\n\nLogin successful.\n\nYou have access to 101 projects, the list has been suppressed. You can list all projects with 'oc projects'\n\nUsing project \"default\".\n[root@ncputility ~ panhub_rc]$\n</code></pre> <ol> <li>list of plugins installed on helm here.</li> </ol> <pre><code>[root@ncputility ~ panhub_rc]$ helm plugin list\nNAME    VERSION DESCRIPTION\n[root@ncputility ~ panhub_rc]$\n</code></pre> <ol> <li>Install the latest version of the Helm backup and restore plugins.</li> </ol> <p>For the Helm backup and restore plugins, use the version that is compatible with the respective Backup and Recovery chart release.</p> <pre><code>[root@ncputility ~ panhub_rc]$ export HELM_HOME=$HOME/.helm\n[root@ncputility ~ panhub_rc]$ cd\n[root@ncputility ~ panhub_rc]$ WORK_DIR=`mktemp -d`\n[root@ncputility ~ panhub_rc]$ mkdir -p $WORK_DIR/backup\n[root@ncputility ~ panhub_rc]$ mkdir -p $WORK_DIR/restore\n[root@ncputility ~ panhub_rc]$ tar xf /root/ncd/NCD_24.9_Git_Server_ORB-RC/ncd-git-server-product/INSTALL_MEDIA/backup-3.7.4.tgz -C $WORK_DIR/backup\n[root@ncputility ~ panhub_rc]$ tar xf /root/ncd/NCD_24.9_Git_Server_ORB-RC/ncd-git-server-product/INSTALL_MEDIA/restore-3.7.4.tgz -C $WORK_DIR/restore\n[root@ncputility ~ panhub_rc]$ helm plugin install $WORK_DIR/backup\nInstalled plugin: backup\n[root@ncputility ~ panhub_rc]$ helm plugin install $WORK_DIR/restore\nInstalled plugin: restore\n[root@ncputility ~ panhub_rc]$ rm -rf $WORK_DIR\n[root@ncputility ~ panhub_rc]$\n</code></pre> <pre><code>a. check the status of installed helm plugins.\n\n```\n[root@ncputility ~ panhub_rc]$ helm plugin list\nNAME    VERSION DESCRIPTION\nbackup  3.7.4   Plugin responsible for the backup of helm releases and k8s namespaces with CBUR.\nrestore 3.7.4   Plugin responsible for restoring helm releases and k8s namespaces with CBUR.\n[root@ncputility ~ panhub_rc]$ \n```\n</code></pre> <ol> <li>Configure RBAC for CRDs. (only when you deploy using tenant credentials)</li> </ol> <p>For tenants only using Backup and Recovery, permission to read, create, modify and delete BrPolicy and BrHook is sufficient. However, tenants installing their own namespaces for Backup and Recovery also need permission to read brpolices/status.</p> <pre><code>kind: ClusterRole\napiVersion: rbac.authorization.k8s.io/v1\nmetadata:\n  name: admin:brpolices\n  labels:\n    rbac.authorization.k8s.io/aggregate-to-admin: \"true\"\n    rbac.authorization.k8s.io/aggregate-to-edit: \"true\"\nrules:\n- apiGroups: [\"cbur.csf.nokia.com\"]\n  resources: [\"brpolices\", \"brhooks\"]\n  verbs: [\"get\", \"list\", \"watch\", \"create\", \"update\", \"patch\", \"delete\"]\n- apiGroups: [\"cbur.csf.nokia.com\"]\n  resources: [\"brpolices/status\"]\n  verbs: [\"get\", \"list\", \"watch\", \"update\", \"patch\"]\n---\nkind: ClusterRole\napiVersion: rbac.authorization.k8s.io/v1\nmetadata:\n  name: view:brpolices\n  labels:\n    rbac.authorization.k8s.io/aggregate-to-view: \"true\"\nrules:\n- apiGroups: [\"cbur.csf.nokia.com\"]\n  resources: [\"brpolices\", \"brhooks\", \"brpolices/status\"]\n  verbs: [\"get\", \"list\", \"watch\"]\n</code></pre>"},{"location":"git-helper/ncd-git-backup/#backup-nokia-git-server","title":"Backup Nokia Git Server","text":"<p>Notes:</p> <pre><code>The CBUR master service (CBUR_MASTER_SERVICE) must be specified in either of the following formats:\nhttp(s)://dns_name:port\nhttp(s)://ipv4:port\nhttp(s)://[ipv6]:port\nhttp(s)://dns_name\nhttp(s)://ingress/ingresspath\n</code></pre> <p>here is the command to run the backup of git server </p> <pre><code>helm backup -t ncd-git -a none -n paclypancdgit01 -x http://172.20.8.113:80\n\nhelm backup -n paclypancdgit01 -t ncd-git -a none -x http://172.20.8.113:80\n</code></pre> <ol> <li>login to bastion host and login to cluster here .</li> </ol> <pre><code>[root@ncputility ~]# source /root/panhubrc\nWARNING: Using insecure TLS client config. Setting this option is not supported!\n\nLogin successful.\n\nYou have access to 101 projects, the list has been suppressed. You can list all projects with 'oc projects'\n\nUsing project \"default\".\n[root@ncputility ~ panhub_rc]$\n</code></pre> <ol> <li>Find the <code>namespace</code> and <code>release</code> name of git server and cbut details from here 'helm list -A '</li> </ol> <pre><code>[root@ncputility ~ panhub_rc]$ helm list  -A\nNAME            NAMESPACE               REVISION        UPDATED                                 STATUS          CHART                                   APP VERSION\ncbur-crds       paclypancdcbur01        1               2025-02-27 14:18:04.313208995 -0500 EST deployed        cbur-crds-2.6.0                         2.6.0\nncd-cbur        paclypancdcbur01        1               2025-02-27 14:24:56.578242699 -0500 EST deployed        cbur-1.18.1                             1.13.1\nncd-git         paclypancdgit01         1               2025-02-27 14:45:25.489107042 -0500 EST deployed        ncd-git-server-24.9.1-7.g30f1acf        17.3.3\nncd-postgresql  paclypancddb01          1               2025-02-27 14:30:27.65639258 -0500 EST  deployed        postgresql-ha-24.9.1-1009.g19e2a92      24.9.1-1009.g19e2a92\nncd-redis       paclypancddb01          1               2025-02-27 14:37:11.651840036 -0500 EST deployed        ncd-redis-24.9.1-1009.g19e2a92          24.9.1-1009.g19e2a92\n[root@ncputility ~ panhub_rc]$\n</code></pre> <ol> <li>trigger the backup job using following command here </li> </ol> <p>Uri should be having <code>cbur</code> structure name in it </p> <pre><code>[root@ncputility ~ panhub_rc]$ helm backup -n paclypancdgit01 -a none -t ncd-git -x http://cbur.apps.panclyphub01.mnc020.mcc714/cbur\n{\n  \"kind\": \"Status\",\n  \"apiVersion\": \"v1\",\n  \"metadata\": {},\n  \"status\": \"Success\",\n  \"message\": \"backupHelmRelease task = a80ed466-254c-4644-8d19-9a0cdf813fbe is on!\",\n  \"reason\": \"\",\n  \"details\": {\n    \"a80ed466-254c-4644-8d19-9a0cdf813fbe\": {\n      \"name\": \"ncd-git\",\n      \"namespace\": \"paclypancdgit01\",\n      \"timestamp\": \"20250402104554\",\n      \"backup_data\": {},\n      \"helm_version\": 3,\n      \"request\": \"backupHelmRelease\"\n    }\n  },\n  \"code\": 202\n}\n[root@ncputility ~ panhub_rc]$ \n</code></pre> <ol> <li>here is an example with <code>--verbose</code> mode  (optinal for troubleshooting)</li> </ol> <pre><code>[root@ncputility ~ panhub_rc]$ helm backup -n paclypancdgit01 -a none -t ncd-git -x http://cbur.apps.panclyphub01.mnc020.mcc714/cbur --verbose\n*******\n+ exit_func 0\n+ '[' -d /tmp/certs.1974896 ']'\n+ rm -rf /tmp/certs.1974896\n+ exit 0\n[root@ncputility ~ panhub_rc]$\n</code></pre> <ol> <li>check the status of the backup using cbur br policy status</li> </ol> <pre><code>[root@ncputility ~ panhub_rc]$ kubectl -n paclypancdgit01 get brpolices.cbur.csf.nokia.com ncd-git-toolbox\nNAME              AGE\nncd-git-toolbox   33d\n[root@ncputility ~ panhub_rc]$ kubectl -n paclypancdgit01 get brpolices.cbur.csf.nokia.com ncd-git-toolbox -o yaml\n[root@ncputility ~ panhub_rc]$\n</code></pre> <ol> <li>look for backup files exported. , right now backup files are saved locally on the gitserver(paclypancdgit01) <code>namespace</code> within <code>ncd-git-toolbox</code> pod. </li> </ol> <p>but we need this backup files to be saved outside the cluster, like sftp server or something, so that we can restore it on new git server.</p> <pre><code>[root@ncputility ~ panhub_rc]$ oc -n paclypancdgit01 exec -it ncd-git-toolbox-56b88dd4c7-27hb5 -- bash\nDefaulted container \"toolbox\" out of: toolbox, cbura-sidecar, certificates (init), configure (init)\ngit@ncd-git-toolbox-56b88dd4c7-27hb5:/$ cd /srv/gitlab/tmp/backups\ngit@ncd-git-toolbox-56b88dd4c7-27hb5:/srv/gitlab/tmp/backups$ ls\nbackup_information.yml  db  repositories\ngit@ncd-git-toolbox-56b88dd4c7-27hb5:/srv/gitlab/tmp/backups$ ls -la\ntotal 4\ndrwxr-sr-x. 4 git  git  66 Apr  2 10:46 .\ndrwxrwsrwx. 3 root git  21 Apr  2 10:46 ..\n-rw-r--r--. 1 git  git 318 Apr  2 10:46 backup_information.yml\ndrwxr-sr-x. 2 git  git  29 Apr  2 10:46 db\ndrwx--S---. 4 git  git  38 Apr  2 10:46 repositories\ngit@ncd-git-toolbox-56b88dd4c7-27hb5:/srv/gitlab/tmp/backups$\n</code></pre>"},{"location":"git-helper/ncd-git-backup/#creating-user-for-remote-backup","title":"Creating user for remote backup","text":"<ol> <li>login to the <code>linux</code> host and used <code>useradd</code> and <code>passwd</code> commands to create <code>user and password</code> here </li> </ol> <pre><code>[root@ncputility ~ panhub_rc]$ useradd gitserverbackup\n[root@ncputility ~ panhub_rc]$ passwd gitserverbackup\nChanging password for user gitserverbackup.\nNew password:\nBAD PASSWORD: The password is shorter than 8 characters\nRetype new password:\npasswd: all authentication tokens updated successfully.\n[root@ncputility ~ panhub_rc]$\n</code></pre> <ol> <li>create a public key on the new user authrazation file.</li> </ol> <pre><code>[root@ncputility ~ pancwl_rc]$ su - gitserverbackup\n[gitserverbackup@ncputility ~]$ pwd\n/home/gitserverbackup\n[gitserverbackup@ncputility ~]$ \n[gitserverbackup@ncputility ~]$ ssh-keygen -t rsa\nGenerating public/private rsa key pair.\nEnter file in which to save the key (/home/gitserverbackup/.ssh/id_rsa):\nCreated directory '/home/gitserverbackup/.ssh'.\nEnter passphrase (empty for no passphrase):\nEnter same passphrase again:\nYour identification has been saved in /home/gitserverbackup/.ssh/id_rsa\nYour public key has been saved in /home/gitserverbackup/.ssh/id_rsa.pub\nThe key fingerprint is:\nSHA256:oF5K3Ky1tLDmb5ALID4FXyLPnlTtX97ZpJyFE0r3Z2A gitserverbackup@ncputility.panclyphub01.mnc020.mcc714\nThe key's randomart image is:\n+---[RSA 3072]----+\n|      .          |\n| o . o .   . oE  |\n|  * + o   . o.+. |\n|o  B + o   o o +o|\n|o.+ *.* S o o O..|\n| o.=oO o . . * . |\n|  ..*oo          |\n|   o. .          |\n|    .o.          |\n+----[SHA256]-----+\n[gitserverbackup@ncputility ~]$\n</code></pre> <ol> <li>take copy of the cbur public using following oc command</li> </ol> <pre><code>[root@ncputility ~ panhub_rc]$ helm list -A\nNAME            NAMESPACE               REVISION        UPDATED                                 STATUS          CHART                                   APP VERSION\ncbur-crds       paclypancdcbur01        1               2025-02-27 14:18:04.313208995 -0500 EST deployed        cbur-crds-2.6.0                         2.6.0\nncd-cbur        paclypancdcbur01        2               2025-04-02 04:17:30.755392541 -0500 EST deployed        cbur-1.18.1                             1.13.1\nncd-git         paclypancdgit01         1               2025-02-27 14:45:25.489107042 -0500 EST deployed        ncd-git-server-24.9.1-7.g30f1acf        17.3.3\nncd-postgresql  paclypancddb01          1               2025-02-27 14:30:27.65639258 -0500 EST  deployed        postgresql-ha-24.9.1-1009.g19e2a92      24.9.1-1009.g19e2a92\nncd-redis       paclypancddb01          1               2025-02-27 14:37:11.651840036 -0500 EST deployed        ncd-redis-24.9.1-1009.g19e2a92          24.9.1-1009.g19e2a92\n[root@ncputility ~ panhub_rc]$\n[root@ncputility ~ panhub_rc]$ oc get secret -n paclypancdcbur01 cburm-ssh-public-key -o jsonpath={.data.ssh_public_key} | base64 -d\nssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQDUH73f7kf8ey2UEkgeN/IJbEBDJPgricRlsD7d7pB0RAQGLx/fNZT15VRX2qOvLydsTbtcxz28Uzryy5zAmAB9z0zGuYKaSo80bS7bXjIsKc71fGD6NvvfSBBLQ1GCk0mFIjn06XmkRJOgqtgOvq66HQEcGSJQ6jq3NQzGERe+VrCk1VWbyfr0vtqqasmKChgr0dAh+0f07lUdpbR9XzEnOG20LNCAcffEBPXXccSEz/huPHOV0Kjfe7rtaKj5ZoIkFlFETPTz4HoKPlZcfxp/s94yICXk++TiI9+mF2SVYuEUWqx00p1DTa79dmUncTaz1c5nshaq4bNdJcPkoDdp[root@ncputility ~ panhub_rc]$\n</code></pre> <ol> <li>here i am using ssh-copy-id command to auto create an authzation file </li> </ol> <pre><code>[gitserverbackup@ncputility ~]$ ssh-copy-id localhost\n/usr/bin/ssh-copy-id: INFO: Source of key(s) to be installed: \"/home/gitserverbackup/.ssh/id_rsa.pub\"\nThe authenticity of host 'localhost (::1)' can't be established.\nED25519 key fingerprint is SHA256:KXMGNVqIYHOtvcd+VqGq/5d/t5UWWGKxPVkuffCOD9I.\nThis key is not known by any other names\nAre you sure you want to continue connecting (yes/no/[fingerprint])? yes\n/usr/bin/ssh-copy-id: INFO: attempting to log in with the new key(s), to filter out any that are already installed\n/usr/bin/ssh-copy-id: INFO: 1 key(s) remain to be installed -- if you are prompted now it is to install the new keys\ngitserverbackup@localhost's password:\n\nNumber of key(s) added: 1\n\nNow try logging into the machine, with:   \"ssh 'localhost'\"\nand check to make sure that only the key(s) you wanted were added.\n\n[gitserverbackup@ncputility ~]$ cd .ssh/\n[gitserverbackup@ncputility .ssh]$ ll\ntotal 20\n-rw-------. 1 gitserverbackup gitserverbackup  607 Apr  2 08:14 authorized_keys\n-rw-------. 1 gitserverbackup gitserverbackup 2655 Apr  2 08:14 id_rsa\n-rw-r--r--. 1 gitserverbackup gitserverbackup  607 Apr  2 08:14 id_rsa.pub\n-rw-------. 1 gitserverbackup gitserverbackup  825 Apr  2 08:14 known_hosts\n-rw-r--r--. 1 gitserverbackup gitserverbackup   91 Apr  2 08:14 known_hosts.old\n[gitserverbackup@ncputility .ssh]$ vi authorized_keys   #&lt;- this updated based on step.3 output&gt;\n[gitserverbackup@ncputility .ssh]$\n</code></pre>"},{"location":"git-helper/ncd-git-backup/#creating-remote-backup-on-ncd-git","title":"Creating remote backup on NCD git","text":"<ol> <li>Create an git server backup on a remote server, so create a secret with sftp details. </li> </ol> <pre><code>[root@ncputility ~ panhub_rc]$ oc create secret generic bastionhostpan \\\n  --namespace=paclypancdcbur01 \\\n  --from-literal=port=\"22\" \\\n  --from-literal=host=\"10.89.100.66\" \\\n  --from-literal=mode=\"sftp\" \\\n  --from-literal=username=\"gitserverbackup\" \\\n  --from-literal=path=\"/home/gitserverbackup\" \\\n  --from-literal=strictHostKeyChecking=\"no\" \\\n  --from-literal=hostKey=\"\"\nsecret/bastionhostpan created\n[root@ncputility ~ panhub_rc]$\n</code></pre> <ol> <li>make sure, it's avaiable via get command. </li> </ol> <pre><code>[root@ncputility ~ panhub_rc]$ oc  get secret -n paclypancdcbur01\nNAME                              TYPE                             DATA   AGE\nbastionhostpan                    Opaque                           7      10s\ncbur-redis                        Opaque                           1      33d\ncburm-ssh-public-key              Opaque                           1      33d\nmy-pull-secret                    kubernetes.io/dockerconfigjson   1      33d\nsh.helm.release.v1.cbur-crds.v1   helm.sh/release.v1               1      33d\nsh.helm.release.v1.ncd-cbur.v1    helm.sh/release.v1               1      33d\nsh.helm.release.v1.ncd-cbur.v2    helm.sh/release.v1               1      4h14m\nsh.helm.release.v1.ncd-cbur.v3    helm.sh/release.v1               1      14m\n[root@ncputility ~ panhub_rc]$\n</code></pre>"},{"location":"git-helper/ncd-git-backup/#update-on-the-git-cbur-side","title":"update on the git cbur side.","text":"<ol> <li>get the backup of cbur repo values file. and update the ssh:credentialName </li> </ol> <pre><code>[root@ncputility ~ panhub_rc]$ helm get values ncd-cbur -n paclypancdcbur01 &gt;  rr.yaml\n[root@ncputility ~ panhub_rc]$\n\nSSH:\n  credentialName: bastionhostpan\n\n[root@ncputility ~ panhub_rc]$ helm upgrade ncd-cbur -n paclypancdcbur01 /root/ncd/NCD_24.9_Git_Server_ORB-RC/ncd-git-server-product/helmcharts/cbur-1.18.1.tgz -f rr.yaml --debug\n</code></pre> <ol> <li>Similar to git cbur modification, need to update the git=server chart as well. </li> </ol> <pre><code>[root@ncputility ~ panhub_rc]$ helm get values ncd-git -n paclypancdgit01 &gt; rr.yaml\n[root@ncputility ~ panhub_rc]$\n\nupdate:\n\ncbur:\n  brPolicy:\n    apiVersion: cbur.csf.nokia.com/v1\n    spec:\n      autoEnableCron: false\n      autoUpdateCron: false\n      backend:\n        mode: sftp\n      cronSpec: 0 0 * * *\n      dataEncryption:\n        enable: false\n      ignoreFileChanged: false\n      maxiCopy: 3\n      weight: 5\n\n[root@ncputility ~ panhub_rc]$  helm upgrade ncd-git -n paclypancdgit01 /root/ncd/NCD_24.9_Git_Server_ORB-RC/ncd-git-server-product/helmcharts/ncd-git-server-24.9.1-7.g30f1acf.tgz -f rr.yaml --debug --timeout 20m\n</code></pre> <ol> <li>validate there is no pods are in pending or crashloop error here .</li> </ol>"},{"location":"git-helper/ncd-git-backup/#trigger-the-backup-on-remote-desination","title":"Trigger the backup on remote desination","text":"<ol> <li>using follow method to trigger  the backup </li> </ol> <pre><code>[root@ncputility ~ panhub_rc]$ helm backup -n paclypancdgit01 -a none -t ncd-git -x http://cbur.apps.panclyphub01.mnc020.mcc714/cbur --backend sftp --verbose\n\n[root@ncputility ~ panhub_rc]$\n</code></pre> <ol> <li>login to that SFTP server and check for backup files.</li> </ol> <pre><code>[gitserverbackup@ncputility 20250402134019_SFTP_paclypancdgit01_ncd-git-toolbox]$ pwd\n/home/gitserverbackup/BACKUP/sftp/paclypancdgit01/DEPLOYMENT_ncd-git-toolbox/20250402134019_SFTP_paclypancdgit01_ncd-git-toolbox\n[gitserverbackup@ncputility 20250402134019_SFTP_paclypancdgit01_ncd-git-toolbox]$ ll\ntotal 2476\n-rw-r--r--. 1 gitserverbackup gitserverbackup    7846 Apr  2 08:40 20250402134019_SFTP_paclypancdgit01_ncd-git-toolbox_Secrets.tar.gz\n-rw-r--r--. 1 gitserverbackup gitserverbackup 2523279 Apr  2 08:41 20250402134019_SFTP_paclypancdgit01_ncd-git-toolbox_volume.tar.gz\n[gitserverbackup@ncputility 20250402134019_SFTP_paclypancdgit01_ncd-git-toolbox]$\n</code></pre>"},{"location":"git-helper/ncd-git-backup/#troubleshooting","title":"Troubleshooting","text":""},{"location":"git-helper/ncd-git-backup/#certificate-error","title":"Certificate error","text":"<ol> <li>copy crt file used during installation.</li> </ol> <pre><code>[root@ncputility denmark]# cp -rp ca.crt /etc/pki/ca-trust/source/anchors/\n[root@ncputility denmark]# sudo update-ca-trust\n[root@ncputility denmark]#\n</code></pre>"},{"location":"git-helper/ncd-git-backup/#backup-job-failure","title":"backup job failure","text":"<ol> <li>if Error looks like this, then problem on the namespace spoce. </li> </ol> <pre><code>[root@ncputility ~ hn_hub_rc]$ helm backup -n hnevocdgit01 -a none -t ncd-git -x http://cbur.apps.hnevocphub01.mnc002.mcc708/cbur\n{\n  \"kind\": \"Status\",\n  \"apiVersion\": \"v1\",\n  \"metadata\": {},\n  \"status\": \"Failure\",\n  \"message\": \"Failed to get helm release ncd-git in namespace hnevocdgit01\",\n  \"reason\": \"\",\n  \"details\": {},\n  \"code\": 500\n}\n[root@ncputility ~ hn_hub_rc]$ \n</code></pre> <ol> <li>to resolve this issue, get the values file and update the namespace scope list.  include git-server namespace too. </li> </ol> <pre><code>[root@ncputility ~ hn_hub_rc]$ helm get values ncd-cbur  -n hnevocdcbur01 &gt; rr.yaml\n</code></pre> <ol> <li>retry it, it will work now</li> </ol> <pre><code>[root@ncputility ~ hn_hub_rc]$ helm backup -n hnevocdgit01 -a none -t ncd-git -x http://cbur.apps.hnevocphub01.mnc002.mcc708/cbur\n{\n  \"kind\": \"Status\",\n  \"apiVersion\": \"v1\",\n  \"metadata\": {},\n  \"status\": \"Success\",\n  \"message\": \"backupHelmRelease task = cc857932-d83d-4e2d-9493-1252595da3f2 is on!\",\n  \"reason\": \"\",\n  \"details\": {\n    \"cc857932-d83d-4e2d-9493-1252595da3f2\": {\n      \"name\": \"ncd-git\",\n      \"namespace\": \"hnevocdgit01\",\n      \"timestamp\": \"20250403005800\",\n      \"backup_data\": {},\n      \"helm_version\": 3,\n      \"request\": \"backupHelmRelease\"\n    }\n  },\n  \"code\": 202\n}\n[root@ncputility ~ hn_hub_rc]$\n</code></pre>"},{"location":"git-helper/ncd-git-backup/#reference","title":"Reference","text":"<ol> <li>NCDFM tickets</li> <li>https://jiradc2.ext.net.nokia.com/browse/NCDFM-3023</li> </ol>"},{"location":"git-helper/ncd-git-restore/","title":"Backing up Nokia Git Server","text":"<p>this procedure, works for NCD 24.9 mp1 pp1. </p>"},{"location":"git-helper/ncd-git-restore/#instructions-on-backing-up-nokia-git-server","title":"Instructions on backing up Nokia Git Server","text":""},{"location":"git-helper/readme/","title":"This document to help pushing you git code without any issues","text":"<ol> <li>Set Your Name and Email Globally (One-Time Setup)     <code>If the issue is related to your identity (name and email), configure it using:</code></li> </ol> <pre><code># git config --global user.name \"venkatapathiraj Ravichandran\"\n\n# git config --global user.email \"rajurraju400@gmail.com\"\n</code></pre> <p>This will apply to all repositories on your system.</p> <ol> <li> <p>Enable Git Credential Caching</p> <p>a. If the issue is related to authentication (username/password), you can cache your credentials using Git's credential helper. Here's how:     For a Temporary Cache (Default 15 Minutes)</p> </li> </ol> <pre><code>#git config --global credential.helper cache\n</code></pre> <pre><code>b. Git will store your credentials in memory for 15 minutes.\n\nc. You can adjust the cache timeout (in seconds) like this:\n</code></pre> <pre><code>git config --global credential.helper 'cache --timeout=3600'\n</code></pre> <ol> <li> <p>For a Persistent Cache</p> <p>a. To store your credentials securely and never be prompted again:</p> </li> </ol> <pre><code>git config --global credential.helper store\n</code></pre> <pre><code>b. Your credentials will be stored in a plain-text file (~/.git-credentials).\n\nc. Be cautious with this method if you're using a shared machine.\n</code></pre> <ol> <li>Configure Git to Skip SSL Verification (Not Recommended for Production)</li> </ol> <p>If you're in a controlled environment where you trust the source, and you want to bypass SSL verification temporarily, you can configure Git to skip SSL verification:</p> <pre><code>git config --global http.sslVerify false\n</code></pre>"},{"location":"openshift/CNF-onboarding-support/Failtoconfigure_nncp/","title":"Failed to configure or Degraded","text":""},{"location":"openshift/CNF-onboarding-support/Failtoconfigure_nncp/#troubleshooting-failed-to-configure-nncp-error-in-ocp","title":"Troubleshooting failed to configure nncp Error in OCP","text":"<p>When encountering a <code>failed to configure nncp</code> error in OpenShift, it typically indicates an issue with applying the Node Network Configuration Policy (NNCP) using the Kubernetes NMState Operator. also this will make application installation failed due to missing master interface. </p>"},{"location":"openshift/CNF-onboarding-support/Failtoconfigure_nncp/#common-causes","title":"Common Causes","text":""},{"location":"openshift/CNF-onboarding-support/Failtoconfigure_nncp/#invalid-nncp-yaml","title":"Invalid NNCP YAML","text":"<ul> <li>Syntax errors</li> <li>Incorrect interface names or missing fields</li> </ul>"},{"location":"openshift/CNF-onboarding-support/Failtoconfigure_nncp/#nmstate-operator-issues","title":"NMState Operator Issues","text":"<ul> <li>Operator not installed</li> <li>Operator pods in failed states</li> </ul>"},{"location":"openshift/CNF-onboarding-support/Failtoconfigure_nncp/#node-labeling-mismatch","title":"Node Labeling Mismatch","text":"<ul> <li>NNCP uses nodeSelector that doesn\u2019t match any node</li> </ul>"},{"location":"openshift/CNF-onboarding-support/Failtoconfigure_nncp/#conflicting-policies","title":"Conflicting Policies","text":"<ul> <li>Multiple NNCPs modifying the same interface</li> </ul>"},{"location":"openshift/CNF-onboarding-support/Failtoconfigure_nncp/#insufficient-permissions","title":"Insufficient Permissions","text":"<ul> <li>NMState may lack required privileges (not applicable for CWL, since we are using blueprinted templates.)</li> </ul>"},{"location":"openshift/CNF-onboarding-support/Failtoconfigure_nncp/#node-issues","title":"Node Issues","text":"<ul> <li>Node is <code>NotReady</code> or cordoned</li> </ul>"},{"location":"openshift/CNF-onboarding-support/Failtoconfigure_nncp/#troubleshooting-steps","title":"Troubleshooting Steps","text":"<p>1) Check NNCP Status</p> <pre><code>oc get nncp\noc describe nncp &lt;nncp-name&gt;\n</code></pre>"},{"location":"openshift/CNF-onboarding-support/image-tls-issue/","title":"Container Image download issue due to TLS problem.","text":""},{"location":"openshift/CNF-onboarding-support/image-tls-issue/#creating-tls-certificate-on-the-ocp-cluster-as-additional-tls-newly","title":"Creating tls certificate on the OCP cluster as additional tls <code>newly</code>","text":""},{"location":"openshift/CNF-onboarding-support/image-tls-issue/#this-step-will-resolve-issue-on-linux-os-levelbastion-host-level","title":"This step will resolve issue on Linux OS level(bastion host level)","text":"<p>In order to be able to push images (for example) from the infra node to this Quay, the rootCA certificate shall be put to the trusted list of the client. The Quay is exposed using the OCP\u2019s default ingress controller, route, so its rootCA shall be fetched (if it was not swapped already).</p> <p>1) from bastion host, login to respective cluster and find the respective CWL cluster quay url. </p> <pre><code>[root@dom14npv101-infra-manager ~ vlabrc]# oc get route -A |grep -i quay\nquay-registry              quay-registry-quay                          quay-registry.apps.ncpvnpvlab1.pnwlab.nsn-rdnet.net                                                             quay-registry-quay-app                             http         edge/Redirect          None\nquay-registry              quay-registry-quay-builder                  quay-registry-quay-builder-quay-registry.apps.ncpvnpvlab1.pnwlab.nsn-rdnet.net                                  quay-registry-quay-app                             grpc         edge/Redirect          None\n[root@dom14npv101-infra-manager ~ vlabrc]# \n</code></pre> <p>2) first locally resolve it, so make dir and download the cert (this is resolve on that particular linux server.)</p> <pre><code>[root@dom14npv101-infra-manager ~ vlabrc]# sudo mkdir -p /etc/containers/certs.d/quay-registry.apps.ncpvnpvlab1.pnwlab.nsn-rdnet.net\n[root@dom14npv101-infra-manager ~ vlabrc]#\n</code></pre> <p>3) create the ingress certificate to local host </p> <pre><code>[root@dom14npv101-infra-manager ~ vlabrc]# oc get secrets -n openshift-ingress-operator router-ca \\\n-o jsonpath=\"{.data['tls\\.crt']}\" | base64 -d | \\\nsudo tee -a \\\n/etc/containers/certs.d/quay-registry.apps.ncpvnpvlab1.pnwlab.nsn-rdnet.net/ca.crt\n-----BEGIN CERTIFICATE-----\nMIIDDDCCAfSgAwIBAgIBATANBgkqhkiG9w0BAQsFADAmMSQwIgYDVQQDDBtpbmdy\nZXNzLW9wZXJhdG9yQDE3NDE2MjI1OTkwHhcNMjUwMzEwMTYwMzE4WhcNMjcwMzEw\nMTYwMzE5WjAmMSQwIgYDVQQDDBtpbmdyZXNzLW9wZXJhdG9yQDE3NDE2MjI1OTkw\nggEiMA0GCSqGSIb3DQEBAQUAA4IBDwAwggEKAoIBAQCtDCDqA6Cx+MvulZtYNleK\nT3e4OVlQrR8zytgIQIvlLpeGoqsoNWEe/ZukJ35fco6LA42RcIiI+F5aI6zZ4+F8\n81ZEpsSRBx8VUMsZCdkb5mA8tl5vJjV+GC1tRlohk6KWqYoR5VJVLbggFer95efv\nxyny/BCYXmU2CSHSmRQRnwAI6cX0K7QgEB0kMHaFjEta16UnwzKdhNbaj5rn0aTm\nhLSGYLvPMx9RVZswjqOqrju0Aovfv9ZzzE++e6+KH9/jZr2HepK62ZZdGznbmnzu\nAl0D+ILaVj8DiFpcwUIaSaRxUVlphAUmm530GLbKrBdQGSsWcJTo4ixf8R2wYo69\nAgMBAAGjRTBDMA4GA1UdDwEB/wQEAwICpDASBgNVHRMBAf8ECDAGAQH/AgEAMB0G\nA1UdDgQWBBQMuq1odXMd4OlIU4vG8Kfu/aLltjANBgkqhkiG9w0BAQsFAAOCAQEA\nBVerN+fxay+kk9uei+bQIpryakFstJ5ApuB1wDKgLY3LucwbzXhaE48i9TEOoNlB\n32ugNpShYQoOyVMMAvQNQG69HNu0KDJHYGDMAs4seGIsMwqityS6Zgv8T3xo176g\nmR0y74yiK1ImtnUAaAPt7NNFflhpZafzhY24k4L3AVNEjMKI9B2SgUJAscmXkNIZ\nDri+EILpba6MzmeLdE3sVTaOIRberr6yTKbZQskaci+twaO7r83hD3E3xwGJB823\nZu+B2i/txKbBrFeKUpppfrg7zCsyqM1UwFtenuj1yj3qECJVwe1Lr8SctrzpJc+J\nryyB1JeEPQWwewI1j7QXqg==\n-----END CERTIFICATE-----\n[root@dom14npv101-infra-manager ~ vlabrc]#\n</code></pre>"},{"location":"openshift/CNF-onboarding-support/image-tls-issue/#on-the-ocp-cluster-level-changes-tls-creation","title":"On the OCP cluster level changes, TLS Creation","text":"<p>1) Similarly if images would be pulled on the HUB cluster, its ingress\u2019 rootCA shall be put into the image.config.openshift.io/cluster CR, as the registry is using self-signed certificate by default and treated as an insecure registry, more precisely the OCP\u2019s ingress (if it is not swapped yet). In order to overcome issues the following commands:</p> <pre><code> [root@dom14npv101-infra-manager ~ vlabrc]# oc get secrets -n openshift-ingress-operator router-ca \\\n-o jsonpath=\"{.data['tls\\.crt']}\" | base64 -d &gt; ingress_ca.crt\n[root@dom14npv101-infra-manager ~ vlabrc]# \n</code></pre> <p>2) create an cm to enforce the certificate here.</p> <pre><code>[root@dom14npv101-infra-manager ~ vlabrc]# oc create configmap registry-cas -n openshift-config \\\n--from-file=quay-registry.apps.ncpvnpvlab1.pnwlab.nsn-rdnet.net=ingress_ca.crt\nconfigmap/registry-cas created\n[root@dom14npv101-infra-manager ~ vlabrc]# \n</code></pre> <p>3) now create an image patch to cluster iamge config. </p> <pre><code>[root@dom14npv101-infra-manager ~ vlabrc]# oc patch image.config.openshift.io/cluster --patch '{\"spec\":\n{\"additionalTrustedCA\":{\"name\":\"registry-cas\"}}}' \\\n--type=merge\nimage.config.openshift.io/cluster patched\n[root@dom14npv101-infra-manager ~ vlabrc]#\n</code></pre>"},{"location":"openshift/CNF-onboarding-support/image-tls-issue/#creating-tls-certificate-on-the-ocp-cluster-as-additional-tls-add","title":"Creating tls certificate on the OCP cluster as additional tls <code>ADD</code>","text":"<p>1) oc apply is the preferred way to update an existing resource, including ConfigMaps. This command will update the ConfigMap with the new data while keeping existing data intact. If you don\u2019t have a YAML file, you can first export the current ConfigMap to a file, edit it, and then apply the changes:</p> <pre><code>oc get configmap registry-cas -n openshift-config -o yaml &gt; registry-cas.yaml\n</code></pre> <p>2) now try to prepare file for new certificate with correct file intension using <code>--dry-run</code></p> <pre><code>oc create configmap registry-cas -n openshift-config \\\n--from-file=harbor.ncdvnpv.ncpvnpvmgt.pnwlab.nsn-rdnet.net=ingress_ca.crt \\\n--dry-run=client -o yaml \n</code></pre> <p>3) Now try to copy the harbor and it's certificate line's and then add it as second list of the certificate on the first yaml file and later apply it. </p> <pre><code>vi registry-cas.yaml\n\noc apply -f registry-cas.yaml\n</code></pre>"},{"location":"openshift/CNF-onboarding-support/kubeconfig/","title":"Kubeconfig overlapping","text":""},{"location":"openshift/CNF-onboarding-support/kubeconfig/#method-to-avoid-ocp-user-profile-overlapping-issue","title":"Method to avoid OCP user profile overlapping issue.","text":"<p>this can resolved by create an rc file each cluster/user access. and source before running oc command can prevant. secret will be kubeconfig file be exported to uniq file each new rc files. </p>"},{"location":"openshift/CNF-onboarding-support/kubeconfig/#problem-describe","title":"Problem describe","text":"<ul> <li>when two user or two different shell try to login with two different users or two different clusters, most recent login will be overlapped with old one. example:  both screen will be having same login to user/cluster.</li> </ul>"},{"location":"openshift/CNF-onboarding-support/kubeconfig/#create-an-rc-for-hub-cluster","title":"Create an RC for hub cluster.","text":"<p>1) login to linux machine and create a rc file for hub cluster </p> <ul> <li><code>KUBECONFIG</code> variable should unique between the clusters. so that token will be saved on that particular sheel profile.</li> <li><code>PS1</code> this variable should be unique name and can be used as shell reference.</li> </ul> <pre><code>[root@ncputility ~ pancwl_rc]$ cat /root/panhubrc\nexport KUBECONFIG=~/.kube/hubconfig\n\noc login -u kubeadmin -p NdnkM-SWpwe-SE4Ba-rNSbR https://api.panclyphub01.mnc020.mcc714:6443\nPS1=\"[\\u@\\h ~ panhub_rc]$ \"\n[root@ncputility ~ pancwl_rc]$\n</code></pre> <p>2) To login to this cluster try </p> <pre><code>[root@ncputility ~ pancwl_rc]$ source /root/panhubrc\nWARNING: Using insecure TLS client config. Setting this option is not supported!\n\nLogin successful.\n\nYou have access to 103 projects, the list has been suppressed. You can list all projects with 'oc projects'\n\nUsing project \"panclypcwl01\".\n[root@ncputility ~ panhub_rc]$\n</code></pre>"},{"location":"openshift/CNF-onboarding-support/kubeconfig/#create-an-rc-for-cwl-cluster","title":"Create an RC for CWL cluster.","text":"<p>1) login to linux machine and create a rc file for CWL cluster </p> <ul> <li><code>KUBECONFIG</code> variable should unique between the clusters. so that token will be saved on that particular shell profile.</li> <li><code>PS1</code> this variable should be unique name and can be used as shell reference.</li> </ul> <pre><code>[root@ncputility ~ pancwl_rc]$ cat /root/pancwlrc\nexport KUBECONFIG=~/.kube/cwlconfig\n\noc login -u kubeadmin -p Y59eh-d8Poa-sSdyp-9zBvL https://api.panclypcwl01.mnc020.mcc714:6443\nPS1=\"[\\u@\\h ~ pancwl_rc]$ \"\n\n[root@ncputility ~ pancwl_rc]$\n</code></pre> <p>2) to login to this cluster try </p> <pre><code>[root@ncputility ~ pancwl_rc]$ source /root/pancwlrc\nWARNING: Using insecure TLS client config. Setting this option is not supported!\n\nLogin successful.\n\nYou have access to 117 projects, the list has been suppressed. You can list all projects with 'oc projects'\n\nUsing project \"default\".\n[root@ncputility ~ pancwl_rc]$\n</code></pre>"},{"location":"openshift/CNF-onboarding-support/oclogin-tls-issue/","title":"Oc login ssl error on OCP","text":"<p>To login to NWC cluster, oauth-serving-cert (second certificate) to be copied to /etc/pki/ca-trust/source/anchors/ and then run sudo update-ca-trust extract command. It should be done for each NWC cluster.</p> <p>1) To get the certificate, execute below command on NWC cluster.</p> <pre><code>[root@ncputility ~ panhub_rc]$ oc get configmaps -n openshift-config-managed oauth-serving-cert -o yaml\napiVersion: v1\ndata:\n  ca-bundle.crt: |2\n\n    -----BEGIN CERTIFICATE-----\n    MIIDeTCCAmGgAwIBAgIIItt373u4MZMwDQYJKoZIhvcNAQELBQAwJjEkMCIGA1UE\n    AwwbaW5ncmVzcy1vcGVyYXRvckAxNzQwMDc0NTQ0MB4XDTI1MDIyMDE4MDIyM1oX\n    DTI3MDIyMDE4MDIyNFowLDEqMCgGA1UEAwwhKi5hcHBzLnBhbmNseXBodWIwMS5t\n    bmMwMjAubWNjNzE0MIIBIjANBgkqhkiG9w0BAQEFAAOCAQ8AMIIBCgKCAQEAoNne\n    91mbAMQEjKAyOBWp0wGYpELbNKgYtgUGqL0zteOA3WI+opnJnpTPztlqr4Xqddyw\n    EAuPS7gjUqplbwJROO0lQ+sDSVqfOCC7wvrkd6pI/0jxWPK9WgnSZt1lmiJ9L0Rs\n    s7H5iVdUq8hvIfI9ZUzvr2BUGi9StdABRFoxk1R+BF6yRRiQnxhyqhYjPOuzV4GM\n    blfDAvo3yqFoMOHo0DTZQXcRLQnbt2a3ApPHcsLgyBjTmOMlPilRSHtVFivQP2Pd\n    VRhZGSsAANk7aJyCvHZ+oMo0DLUqmBgikHpgm9TAv6M+oX0kbfdqfMci8sEF7Vqj\n    9fK5l19t+zZaXTnH3QIDAQABo4GkMIGhMA4GA1UdDwEB/wQEAwIFoDATBgNVHSUE\n    DDAKBggrBgEFBQcDATAMBgNVHRMBAf8EAjAAMB0GA1UdDgQWBBTsyQRlbeyo2H/V\n    f887YPhF8jVixDAfBgNVHSMEGDAWgBTZS/SzkBfiHQ5/Gy+b4g1XJOHkojAsBgNV\n    HREEJTAjgiEqLmFwcHMucGFuY2x5cGh1YjAxLm1uYzAyMC5tY2M3MTQwDQYJKoZI\n    hvcNAQELBQADggEBAE6kBjPoA3RJI09pYfUzQlEyKQrnudNTu+O61ZspCPvafp4s\n    4py8hyS/pzkp7611KfmJnzXjiBjw6qzcE5lye4coO5vwplYDbZUTCn9bz30+2g1O\n    wpA1ZOLLTHet11+i0FG0m4AJq4OXEjHuA1K2+AyfzG0TsogT88WstndoNPtGrYWJ\n    pj1kQYbVqwBtCU/jhKbXycEQ+UdeICRuNp5FbBcJ/ZrxJRmJa/zUkT6tHWMvzlsI\n    IPrTpL7BEkoRtWPYhcW4gL70XgkmahuX2bssG7C2IJxN2DNTvmFsMSWHzQNc7AaD\n    ND+v4E+mn2zKhzdQyOB4Mx6H4LH5cEHZsLfVal8=\n    -----END CERTIFICATE-----\n\n    -----BEGIN CERTIFICATE-----\n    MIIDDDCCAfSgAwIBAgIBATANBgkqhkiG9w0BAQsFADAmMSQwIgYDVQQDDBtpbmdy\n    ZXNzLW9wZXJhdG9yQDE3NDAwNzQ1NDQwHhcNMjUwMjIwMTgwMjIzWhcNMjcwMjIw\n    MTgwMjI0WjAmMSQwIgYDVQQDDBtpbmdyZXNzLW9wZXJhdG9yQDE3NDAwNzQ1NDQw\n    ggEiMA0GCSqGSIb3DQEBAQUAA4IBDwAwggEKAoIBAQDjXWFSVPshwihivZhaTrB7\n    0boUOw2j3Ut/J6eSm+JA+xVl05L4XD8+C8VSst+f32Pe42Lso2hrovY1dT7IBX3g\n    6S9Nnd2iRc9vC/qHcQkGQ6krIYSQ48aCH7UuahCpTqxEp+MwmhCTQngN2maTJBe3\n    0E6K2IL4zXSi0Iuj08BOnH/w4pJxeWhyngXDf2SeA88EmU3juHLshHrAND84uou2\n    /sOStIAhFwU+o887dSgx7iERy+6nczSDB9Qvq11cUSS4RPC82bNnxcc+BrynDwZ/\n    eggs0OEaj/1cHu/svKZHX9gUKrqz80wF8YGZLLgI2oPAf2VJorvtkJAErzgttroF\n    AgMBAAGjRTBDMA4GA1UdDwEB/wQEAwICpDASBgNVHRMBAf8ECDAGAQH/AgEAMB0G\n    A1UdDgQWBBTZS/SzkBfiHQ5/Gy+b4g1XJOHkojANBgkqhkiG9w0BAQsFAAOCAQEA\n    UwPhAbzTWZIlBsMHAL+8jvxM8qxc6HDhayAD4gbCE65vHYgSizost02vRfpOPQq1\n    D6HM8JjifS3KHd6E6chdTbrHI0W8pMJJPon5akCJf/uGeGDl+2wKfmVC6UoV7hC3\n    pcUzm3JKwsNJbjS5rxL8f5a8bNdIFfLQKuyRpnVX2CsNHvh+WJzynQ+PUJ6zCa7y\n    x5AJxca2PTnBKRoVTAyumT1suluI9f4GRYnxTE/qIKRZRs+uT3kIl/N9VX+GbjGb\n    pPszJ+p6N6Arl1BqJP1DdLin2IFGZL39pTyifm5GP+Vou2aHPuHZDVoCdxsFKup+\n    gUY2KeKz0UManwubPQNnKA==\n    -----END CERTIFICATE-----\nkind: ConfigMap\nmetadata:\n  annotations:\n    openshift.io/owning-component: apiserver-auth\n  creationTimestamp: \"2025-02-20T18:02:31Z\"\n  name: oauth-serving-cert\n  namespace: openshift-config-managed\n  resourceVersion: \"73869405\"\n  uid: a08c0137-033d-41d0-9c79-8f345662140c\n[root@ncputility ~ panhub_rc]$\n</code></pre> <p>2) Then copy second certificate and put it in a file. Move this file to <code>/etc/pki/ca-trust/source/anchors/</code>Post that, run <code>update-ca-trust extrace</code>command.</p> <p>3) Create separate linux user and do oc login kubeadmin, password and api url (find from the kubeconfig) file.</p>"},{"location":"openshift/CNF-onboarding-support/proxy-cache-pod/","title":"Proxy-cache quay based pod creation","text":""},{"location":"openshift/CNF-onboarding-support/proxy-cache-pod/#proxy-cache-tls-issue","title":"proxy-cache TLS issue.","text":"<p>0) Login in to CWL cluster and make sure mcp and nodes are looks good at this point, if any issue, need to be resolved first. </p> <pre><code>[root@ncputility ~ pancwl_rc]$ oc get nodes\nNAME                                     STATUS   ROLES                              AGE   VERSION\nappworker0.panclypcwl01.mnc020.mcc714    Ready    appworker,appworker-mcp-a,worker   38d   v1.29.6+aba1e8d\nappworker1.panclypcwl01.mnc020.mcc714    Ready    appworker,appworker-mcp-a,worker   38d   v1.29.6+aba1e8d\nappworker10.panclypcwl01.mnc020.mcc714   Ready    appworker,appworker-mcp-b,worker   37d   v1.29.6+aba1e8d\nappworker11.panclypcwl01.mnc020.mcc714   Ready    appworker,appworker-mcp-b,worker   37d   v1.29.6+aba1e8d\nappworker12.panclypcwl01.mnc020.mcc714   Ready    appworker,appworker-mcp-b,worker   37d   v1.29.6+aba1e8d\nappworker13.panclypcwl01.mnc020.mcc714   Ready    appworker,appworker-mcp-b,worker   37d   v1.29.6+aba1e8d\nappworker14.panclypcwl01.mnc020.mcc714   Ready    appworker,appworker-mcp-b,worker   37d   v1.29.6+aba1e8d\nappworker15.panclypcwl01.mnc020.mcc714   Ready    appworker,appworker-mcp-b,worker   37d   v1.29.6+aba1e8d\nappworker16.panclypcwl01.mnc020.mcc714   Ready    appworker,appworker-mcp-b,worker   37d   v1.29.6+aba1e8d\nappworker17.panclypcwl01.mnc020.mcc714   Ready    appworker,appworker-mcp-b,worker   37d   v1.29.6+aba1e8d\nappworker19.panclypcwl01.mnc020.mcc714   Ready    appworker,appworker-mcp-c,worker   37d   v1.29.6+aba1e8d\nappworker2.panclypcwl01.mnc020.mcc714    Ready    appworker,appworker-mcp-a,worker   30d   v1.29.6+aba1e8d\nappworker20.panclypcwl01.mnc020.mcc714   Ready    appworker,appworker-mcp-c,worker   37d   v1.29.6+aba1e8d\nappworker21.panclypcwl01.mnc020.mcc714   Ready    appworker,appworker-mcp-c,worker   37d   v1.29.6+aba1e8d\nappworker22.panclypcwl01.mnc020.mcc714   Ready    appworker,appworker-mcp-c,worker   37d   v1.29.6+aba1e8d\nappworker23.panclypcwl01.mnc020.mcc714   Ready    appworker,appworker-mcp-c,worker   37d   v1.29.6+aba1e8d\nappworker24.panclypcwl01.mnc020.mcc714   Ready    appworker,appworker-mcp-c,worker   37d   v1.29.6+aba1e8d\nappworker25.panclypcwl01.mnc020.mcc714   Ready    appworker,appworker-mcp-c,worker   37d   v1.29.6+aba1e8d\nappworker26.panclypcwl01.mnc020.mcc714   Ready    appworker,appworker-mcp-c,worker   37d   v1.29.6+aba1e8d\nappworker27.panclypcwl01.mnc020.mcc714   Ready    appworker,appworker-mcp-d,worker   37d   v1.29.6+aba1e8d\nappworker28.panclypcwl01.mnc020.mcc714   Ready    appworker,appworker-mcp-d,worker   37d   v1.29.6+aba1e8d\nappworker29.panclypcwl01.mnc020.mcc714   Ready    appworker,appworker-mcp-d,worker   37d   v1.29.6+aba1e8d\nappworker3.panclypcwl01.mnc020.mcc714    Ready    appworker,appworker-mcp-a,worker   38d   v1.29.6+aba1e8d\nappworker30.panclypcwl01.mnc020.mcc714   Ready    appworker,appworker-mcp-d,worker   37d   v1.29.6+aba1e8d\nappworker31.panclypcwl01.mnc020.mcc714   Ready    appworker,appworker-mcp-d,worker   37d   v1.29.6+aba1e8d\nappworker32.panclypcwl01.mnc020.mcc714   Ready    appworker,appworker-mcp-d,worker   37d   v1.29.6+aba1e8d\nappworker33.panclypcwl01.mnc020.mcc714   Ready    appworker,appworker-mcp-d,worker   37d   v1.29.6+aba1e8d\nappworker34.panclypcwl01.mnc020.mcc714   Ready    appworker,appworker-mcp-d,worker   37d   v1.29.6+aba1e8d\nappworker4.panclypcwl01.mnc020.mcc714    Ready    appworker,appworker-mcp-a,worker   38d   v1.29.6+aba1e8d\nappworker5.panclypcwl01.mnc020.mcc714    Ready    appworker,appworker-mcp-a,worker   38d   v1.29.6+aba1e8d\nappworker6.panclypcwl01.mnc020.mcc714    Ready    appworker,appworker-mcp-a,worker   38d   v1.29.6+aba1e8d\nappworker7.panclypcwl01.mnc020.mcc714    Ready    appworker,appworker-mcp-a,worker   37d   v1.29.6+aba1e8d\nappworker9.panclypcwl01.mnc020.mcc714    Ready    appworker,appworker-mcp-b,worker   37d   v1.29.6+aba1e8d\ngateway1.panclypcwl01.mnc020.mcc714      Ready    gateway,gateway-mcp-a,worker       38d   v1.29.6+aba1e8d\ngateway2.panclypcwl01.mnc020.mcc714      Ready    gateway,gateway-mcp-a,worker       38d   v1.29.6+aba1e8d\ngateway3.panclypcwl01.mnc020.mcc714      Ready    gateway,gateway-mcp-b,worker       38d   v1.29.6+aba1e8d\ngateway4.panclypcwl01.mnc020.mcc714      Ready    gateway,gateway-mcp-b,worker       38d   v1.29.6+aba1e8d\nmaster0.panclypcwl01.mnc020.mcc714       Ready    control-plane,master,monitor       38d   v1.29.6+aba1e8d\nmaster1.panclypcwl01.mnc020.mcc714       Ready    control-plane,master,monitor       38d   v1.29.6+aba1e8d\nmaster2.panclypcwl01.mnc020.mcc714       Ready    control-plane,master,monitor       38d   v1.29.6+aba1e8d\nstorage0.panclypcwl01.mnc020.mcc714      Ready    storage,worker                     38d   v1.29.6+aba1e8d\nstorage1.panclypcwl01.mnc020.mcc714      Ready    storage,worker                     38d   v1.29.6+aba1e8d\nstorage2.panclypcwl01.mnc020.mcc714      Ready    storage,worker                     38d   v1.29.6+aba1e8d\nstorage3.panclypcwl01.mnc020.mcc714      Ready    storage,worker                     38d   v1.29.6+aba1e8d\nstorage4.panclypcwl01.mnc020.mcc714      Ready    storage,worker                     38d   v1.29.6+aba1e8d\n[root@ncputility ~ pancwl_rc]$ oc get mcp\nNAME              CONFIG                                                      UPDATED   UPDATING   DEGRADED   MACHINECOUNT   READYMACHINECOUNT   UPDATEDMACHINECOUNT   DEGRADEDMACHINECOUNT   AGE\nappworker-mcp-a   rendered-appworker-mcp-a-0e9dd6df593dcd5016bbe7d601119bf4   True      False      False      8              8                   8                     0                      37d\nappworker-mcp-b   rendered-appworker-mcp-b-0e9dd6df593dcd5016bbe7d601119bf4   True      False      False      9              9                   9                     0                      37d\nappworker-mcp-c   rendered-appworker-mcp-c-5afb864664d3b10530b54b3153a1a61e   True      False      False      8              8                   8                     0                      29h\nappworker-mcp-d   rendered-appworker-mcp-d-5afb864664d3b10530b54b3153a1a61e   True      False      False      8              8                   8                     0                      29h\ngateway-mcp-a     rendered-gateway-mcp-a-c81254a16575de9053ae543c4f1ba3fb     True      False      False      2              2                   2                     0                      37d\ngateway-mcp-b     rendered-gateway-mcp-b-3be41ecbbe09004c35ca04a4309cabf0     True      False      False      2              2                   2                     0                      29h\nmaster            rendered-master-114f60e6be691323222ea11e72de0bcf            True      False      False      3              3                   3                     0                      38d\nstorage           rendered-storage-dc2d8a34080bce1400e11bb1fb098693           True      False      False      5              5                   5                     0                      37d\nworker            rendered-worker-68cb1df39185f7ad80fda7915e4c5a42            True      False      False      0              0                   0                     0                      38d\n[root@ncputility ~ pancwl_rc]$\n</code></pre> <p>1) login to hub cluster and run this command </p> <pre><code>[root@ncputility ~ pancwl_rc]$ source /root/panhubrc\nWARNING: Using insecure TLS client config. Setting this option is not supported!\n\nLogin successful.\n\nYou have access to 105 projects, the list has been suppressed. You can list all projects with 'oc projects'\n\nUsing project \"default\".\n[root@ncputility ~ panhub_rc]$ oc get secret -n openshift-ingress-operator router-ca -o \"jsonpath={.data['tls\\.crt']}\" | base64 -d\n-----BEGIN CERTIFICATE-----\nMIIDDDCCAfSgAwIBAgIBATANBgkqhkiG9w0BAQsFADAmMSQwIgYDVQQDDBtpbmdy\nZXNzLW9wZXJhdG9yQDE3NDAwNzQ1NDQwHhcNMjUwMjIwMTgwMjIzWhcNMjcwMjIw\nMTgwMjI0WjAmMSQwIgYDVQQDDBtpbmdyZXNzLW9wZXJhdG9yQDE3NDAwNzQ1NDQw\nggEiMA0GCSqGSIb3DQEBAQUAA4IBDwAwggEKAoIBAQDjXWFSVPshwihivZhaTrB7\n0boUOw2j3Ut/J6eSm+JA+xVl05L4XD8+C8VSst+f32Pe42Lso2hrovY1dT7IBX3g\n6S9Nnd2iRc9vC/qHcQkGQ6krIYSQ48aCH7UuahCpTqxEp+MwmhCTQngN2maTJBe3\n0E6K2IL4zXSi0Iuj08BOnH/w4pJxeWhyngXDf2SeA88EmU3juHLshHrAND84uou2\n/sOStIAhFwU+o887dSgx7iERy+6nczSDB9Qvq11cUSS4RPC82bNnxcc+BrynDwZ/\neggs0OEaj/1cHu/svKZHX9gUKrqz80wF8YGZLLgI2oPAf2VJorvtkJAErzgttroF\nAgMBAAGjRTBDMA4GA1UdDwEB/wQEAwICpDASBgNVHRMBAf8ECDAGAQH/AgEAMB0G\nA1UdDgQWBBTZS/SzkBfiHQ5/Gy+b4g1XJOHkojANBgkqhkiG9w0BAQsFAAOCAQEA\nUwPhAbzTWZIlBsMHAL+8jvxM8qxc6HDhayAD4gbCE65vHYgSizost02vRfpOPQq1\nD6HM8JjifS3KHd6E6chdTbrHI0W8pMJJPon5akCJf/uGeGDl+2wKfmVC6UoV7hC3\npcUzm3JKwsNJbjS5rxL8f5a8bNdIFfLQKuyRpnVX2CsNHvh+WJzynQ+PUJ6zCa7y\nx5AJxca2PTnBKRoVTAyumT1suluI9f4GRYnxTE/qIKRZRs+uT3kIl/N9VX+GbjGb\npPszJ+p6N6Arl1BqJP1DdLin2IFGZL39pTyifm5GP+Vou2aHPuHZDVoCdxsFKup+\ngUY2KeKz0UManwubPQNnKA==\n-----END CERTIFICATE-----\n[root@ncputility ~ panhub_rc]$\n</code></pre> <p>2) login to workload cluster and run this command </p> <pre><code>[root@ncputility ~ pancwl_rc]$ source /root/pancwlrc\nWARNING: Using insecure TLS client config. Setting this option is not supported!\n\nLogin successful.\n\nYou have access to 115 projects, the list has been suppressed. You can list all projects with 'oc projects'\n\nUsing project \"nokia\".\n[root@ncputility ~ pancwl_rc]$ oc get cm -n openshift-config user-ca-bundle -o yaml\napiVersion: v1\ndata:\n  ca-bundle.crt: |\n    -----BEGIN CERTIFICATE-----\n    MIIDDDCCAfSgAwIBAgIBATANBgkqhkiG9w0BAQsFADAmMSQwIgYDVQQDDBtpbmdy\n    ZXNzLW9wZXJhdG9yQDE3NDAwNzQ1NDQwHhcNMjUwMjIwMTgwMjIzWhcNMjcwMjIw\n    MTgwMjI0WjAmMSQwIgYDVQQDDBtpbmdyZXNzLW9wZXJhdG9yQDE3NDAwNzQ1NDQw\n    ggEiMA0GCSqGSIb3DQEBAQUAA4IBDwAwggEKAoIBAQDjXWFSVPshwihivZhaTrB7\n    0boUOw2j3Ut/J6eSm+JA+xVl05L4XD8+C8VSst+f32Pe42Lso2hrovY1dT7IBX3g\n    6S9Nnd2iRc9vC/qHcQkGQ6krIYSQ48aCH7UuahCpTqxEp+MwmhCTQngN2maTJBe3\n    0E6K2IL4zXSi0Iuj08BOnH/w4pJxeWhyngXDf2SeA88EmU3juHLshHrAND84uou2\n    /sOStIAhFwU+o887dSgx7iERy+6nczSDB9Qvq11cUSS4RPC82bNnxcc+BrynDwZ/\n    eggs0OEaj/1cHu/svKZHX9gUKrqz80wF8YGZLLgI2oPAf2VJorvtkJAErzgttroF\n    AgMBAAGjRTBDMA4GA1UdDwEB/wQEAwICpDASBgNVHRMBAf8ECDAGAQH/AgEAMB0G\n    A1UdDgQWBBTZS/SzkBfiHQ5/Gy+b4g1XJOHkojANBgkqhkiG9w0BAQsFAAOCAQEA\n    UwPhAbzTWZIlBsMHAL+8jvxM8qxc6HDhayAD4gbCE65vHYgSizost02vRfpOPQq1\n    D6HM8JjifS3KHd6E6chdTbrHI0W8pMJJPon5akCJf/uGeGDl+2wKfmVC6UoV7hC3\n    pcUzm3JKwsNJbjS5rxL8f5a8bNdIFfLQKuyRpnVX2CsNHvh+WJzynQ+PUJ6zCa7y\n    x5AJxca2PTnBKRoVTAyumT1suluI9f4GRYnxTE/qIKRZRs+uT3kIl/N9VX+GbjGb\n    pPszJ+p6N6Arl1BqJP1DdLin2IFGZL39pTyifm5GP+Vou2aHPuHZDVoCdxsFKup+\n    gUY2KeKz0UManwubPQNnKA==\n    -----END CERTIFICATE-----\nkind: ConfigMap\nmetadata:\n  annotations:\n    openshift.io/owning-component: End User\n  creationTimestamp: \"2025-03-03T08:56:26Z\"\n  name: user-ca-bundle\n  namespace: openshift-config\n  resourceVersion: \"43318895\"\n  uid: 6624e5ad-93b5-418e-8c1e-7a91c724a760\n[root@ncputility ~ pancwl_rc]$ oc get proxy cluster -o yaml\napiVersion: config.openshift.io/v1\nkind: Proxy\nmetadata:\n  creationTimestamp: \"2025-03-03T08:54:59Z\"\n  generation: 2\n  name: cluster\n  resourceVersion: \"48312416\"\n  uid: ceaebe02-9bd3-4361-847e-1b880ebb85de\nspec:\n  trustedCA:\n    name: user-ca-bundle   &lt;---------------------------- this should be patched. if missing. \nstatus: {}\n[root@ncputility ~ pancwl_rc]$\n</code></pre> <p>3) login to workload cluster add tls of hub to cwl. </p> <pre><code>[root@ncputility ~ pancwl_rc]$ oc patch proxy cluster --patch '{\"spec\":{\"trustedCA\":{\"name\":\"user-ca-bundle\"}}}' --type=merge\n[root@ncputility ~ pancwl_rc]$ oc get proxy cluster -o yaml\napiVersion: config.openshift.io/v1\nkind: Proxy\nmetadata:\n  creationTimestamp: \"2025-03-03T08:54:59Z\"\n  generation: 2\n  name: cluster\n  resourceVersion: \"48312416\"\n  uid: ceaebe02-9bd3-4361-847e-1b880ebb85de\nspec:\n  trustedCA:\n    name: user-ca-bundle\nstatus: {}\n[root@ncputility ~ pancwl_rc]$\n</code></pre>"},{"location":"openshift/CNF-onboarding-support/proxy-cache-pod/#configure-the-proxy-cache-on-the-registry-level","title":"Configure the proxy cache on the registry level.","text":""},{"location":"openshift/CNF-onboarding-support/proxy-cache-pod/#hub-quay-configiration","title":"hub quay configiration","text":"<p>1) Open up the hub quay url </p>"},{"location":"openshift/CNF-onboarding-support/proxy-cache-pod/#cwl-quay-configuration","title":"CWL quay configuration","text":"<p>1) Open CWL quay login via super account and created an user called cnfowners also fix the passwd.</p> <p> </p> <p>2) Create an org as same as hub quay. </p> <p></p> <p>3) Set the proxy cache configuration for the organization</p> <p></p> <p>4) create robot account and default permission</p> <p></p> <p>5) Create a new team for image pull user</p> <p></p> <p>6) Set default permission for the pull user (optional)</p> <p> \u2192 Default Permissions \u2192 + Create Default Permission <p>7) Extend the global image pull secret</p> <p>During the Managed cluster installation, the global pull secret is configured. If the 2nd Hub Quay account and the cache account are not prepared in advance, these accounts need to be added. In case of mirrored registries, only the global pull secret can be used. It is not possible to add project specific pull secrets. For more information, see chapter Image configuration resources in document Images, available in OpenShift Container Platform Product documentation.</p>"},{"location":"openshift/CNF-onboarding-support/proxy-cache-pod/#testing-pod-creation-using-proxy-cache-quay","title":"Testing pod creation using <code>proxy-cache</code> quay.","text":"<p>1) Login the namespace with cluster admin access to grand rights for an scc. </p> <pre><code>[root@ncputility ~ pancwl_rc]$ source  /root/pancwlrc\nWARNING: Using insecure TLS client config. Setting this option is not supported!\n\nLogin successful.\n\nYou have access to 116 projects, the list has been suppressed. You can list all projects with 'oc projects'\n\nUsing project \"default\".\n[root@ncputility ~ pancwl_rc]$ \n</code></pre> <p>2) Grand admin rights to project. if missed during project creation phase. </p> <pre><code>[root@ncputility ~ pancwl_rc]$ oc policy add-role-to-user admin  nokia  -n nokia\nclusterrole.rbac.authorization.k8s.io/admin added: \"nokia\"\n[root@ncputility ~ pancwl_rc]$ \n</code></pre> <p>3) Also grand scc role to default service account via anyuid. </p> <pre><code>[root@ncputility ~ pancwl_rc]$ oc adm policy add-scc-to-user anyuid -z default -n nokia\nclusterrole.rbac.authorization.k8s.io/system:openshift:scc:anyuid added: \"default\"\n[root@ncputility ~ pancwl_rc]$ \n</code></pre> <p>4) login to cnf tenant here .</p> <pre><code>[root@ncputility ~ pancwl_rc]$ oc login -u nokia -p nokia@123\nWARNING: Using insecure TLS client config. Setting this option is not supported!\n\nLogin successful.\n\nYou have one project on this server: \"nokia\"\n\nUsing project \"nokia\".\n[root@ncputility ~ pancwl_rc]$ \n</code></pre> <p>5) run an pod using proxy-cache url </p> <pre><code>[root@ncputility ~ pancwl_rc]$  oc run podpingtest3 --image=ephemeral.url/cnfimages/testimage01:latest --restart=Never  -- tail -f /dev/null\npod/podpingtest3 created\n[root@ncputility ~ pancwl_rc]$ oc get pods\nNAME           READY   STATUS              RESTARTS   AGE\npodpingtest3   0/1     ContainerCreating   0          4s\n[root@ncputility ~ pancwl_rc]$ oc get pods\nNAME           READY   STATUS              RESTARTS   AGE\npodpingtest3   0/1     ContainerCreating   0          6s\n[root@ncputility ~ pancwl_rc]$ oc get pods\nNAME           READY   STATUS              RESTARTS   AGE\npodpingtest3   0/1     ContainerCreating   0          8s\n[root@ncputility ~ pancwl_rc]$ oc get pods\nNAME           READY   STATUS              RESTARTS   AGE\npodpingtest3   0/1     ContainerCreating   0          9s\n[root@ncputility ~ pancwl_rc]$ oc get pods\nNAME           READY   STATUS              RESTARTS   AGE\npodpingtest3   0/1     ContainerCreating   0          11s\n[root@ncputility ~ pancwl_rc]$ oc get pods\nNAME           READY   STATUS              RESTARTS   AGE\npodpingtest3   0/1     ContainerCreating   0          13s\n[root@ncputility ~ pancwl_rc]$ \n</code></pre> <p>6) validate the pod status and make sure it's getting the image via proxy-cache. </p> <pre><code>[root@ncputility ~ pancwl_rc]$ oc get pods\nNAME           READY   STATUS    RESTARTS   AGE\npodpingtest3   1/1     Running   0          5m1s\n[root@ncputility ~ pancwl_rc]$\n\n\n[root@ncputility ~ pancwl_rc]$ oc describe pod podpingtest3\nName:             podpingtest3\nNamespace:        nokia\nPriority:         0\nService Account:  default\nNode:             appworker9.panclypcwl01.mnc020.mcc714/10.89.96.35\nStart Time:       Fri, 04 Apr 2025 08:04:47 -0500\nLabels:           run=podpingtest3\nAnnotations:      k8s.ovn.org/pod-networks:\n                    {\"default\":{\"ip_addresses\":[\"172.19.21.252/23\"],\"mac_address\":\"0a:58:ac:13:15:fc\",\"gateway_ips\":[\"172.19.20.1\"],\"routes\":[{\"dest\":\"172.16....\n                  k8s.v1.cni.cncf.io/network-status:\n                    [{\n                        \"name\": \"ovn-kubernetes\",\n                        \"interface\": \"eth0\",\n                        \"ips\": [\n                            \"172.19.21.252\"\n                        ],\n                        \"mac\": \"0a:58:ac:13:15:fc\",\n                        \"default\": true,\n                        \"dns\": {}\n                    }]\n                  openshift.io/scc: anyuid\nStatus:           Running\nIP:               172.19.21.252\nIPs:\n  IP:  172.19.21.252\nContainers:\n  podpingtest3:\n    Container ID:  cri-o://749055ef608c6f30be42248c63889fd85377928389dae2e29eed50919cc2ee79\n    Image:         ephemeral.url/cnfimages/testimage01:latest\n    Image ID:      ephemeral.url/cnfimages/testimage01@sha256:32666e0234f88377a91de56bb78f2d4f8df45b4f99c1c2dc9ee1d134c84f4753\n    Port:          &lt;none&gt;\n    Host Port:     &lt;none&gt;\n    Args:\n      tail\n      -f\n      /dev/null\n    State:          Running\n      Started:      Fri, 04 Apr 2025 08:05:05 -0500\n    Ready:          True\n    Restart Count:  0\n    Environment:    &lt;none&gt;\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-mp46w (ro)\nConditions:\n  Type                        Status\n  PodReadyToStartContainers   True\n  Initialized                 True\n  Ready                       True\n  ContainersReady             True\n  PodScheduled                True\nVolumes:\n  kube-api-access-mp46w:\n    Type:                    Projected (a volume that contains injected data from multiple sources)\n    TokenExpirationSeconds:  3607\n    ConfigMapName:           kube-root-ca.crt\n    ConfigMapOptional:       &lt;nil&gt;\n    DownwardAPI:             true\n    ConfigMapName:           openshift-service-ca.crt\n    ConfigMapOptional:       &lt;nil&gt;\nQoS Class:                   BestEffort\nNode-Selectors:              &lt;none&gt;\nTolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 60s\n                             node.kubernetes.io/unreachable:NoExecute op=Exists for 60s\nEvents:\n  Type    Reason          Age    From               Message\n  ----    ------          ----   ----               -------\n  Normal  Scheduled       6m5s   default-scheduler  Successfully assigned nokia/podpingtest3 to appworker9.panclypcwl01.mnc020.mcc714\n  Normal  AddedInterface  6m4s   multus             Add eth0 [172.19.21.252/23] from ovn-kubernetes\n  Normal  Pulling         6m4s   kubelet            Pulling image \"ephemeral.url/cnfimages/testimage01:latest\"\n  Normal  Pulled          5m47s  kubelet            Successfully pulled image \"ephemeral.url/cnfimages/testimage01:latest\" in 17.521s (17.521s including waiting)\n  Normal  Created         5m47s  kubelet            Created container podpingtest3\n  Normal  Started         5m47s  kubelet            Started container podpingtest3\n[root@ncputility ~ pancwl_rc]$\n</code></pre> <p></p>"},{"location":"openshift/CNF-onboarding-support/proxy-cache-pod/#cnf-image-upload-using-pod-command","title":"CNF image upload using pod command","text":"<p>1) login to hub quay using cnfowners and org as cnfimages. </p> <pre><code>[root@ncputility ~ pancwl_rc]$ podman login quay-registry.apps.panclyphub01.mnc020.mcc714 -u cnfowners -p cnfowners\nLogin Succeeded!\n[root@ncputility ~ pancwl_rc]$\n</code></pre> <p>2) load the container images to log registry </p> <pre><code>[root@ncputility ~ pancwl_rc]$ podman load -i &lt;filename&gt;.tar^C\n[root@ncputility ~ pancwl_rc]$\n</code></pre> <p>3) tag the image to your registry here </p> <pre><code>[root@ncputility ~ pancwl_rc]$ podman tag quay-registry.apps.panclyphub01.mnc020.mcc714/cnfimages/testimage01 quay-registry.apps.panclyphub01.mnc020.mcc714/cnfimagesnew/testimage01:latest\n[root@ncputility ~ pancwl_rc]$\n</code></pre> <p>4) push the image to remore registry using podman push command here .</p> <pre><code>[root@ncputility ~ pancwl_rc]$ podman push quay-registry.apps.panclyphub01.mnc020.mcc714/cnfimagesnew/testimage01:latest\nGetting image source signatures\nCopying blob 1af69dabfc93 done   |\nCopying blob 53f86715cdba done   |\nCopying blob b6361360b38a done   |\nCopying config d39b33df22 done   |\nWriting manifest to image destination\n[root@ncputility ~ pancwl_rc]$\n</code></pre>"},{"location":"openshift/CNF-onboarding-support/rolebinding/","title":"Role Allocation for CNF user","text":""},{"location":"openshift/CNF-onboarding-support/rolebinding/#openshift-container-platform-ocp-role-defination","title":"OpenShift Container Platform (OCP) role defination","text":"<p>In OpenShift Container Platform (OCP), roles define a set of permissions and are a core part of Role-Based Access Control (RBAC). OCP uses Kubernetes RBAC with some OpenShift-specific enhancements. You can define different types of roles depending on your access and security needs.</p>"},{"location":"openshift/CNF-onboarding-support/rolebinding/#types-of-roles-in-ocp","title":"Types of Roles in OCP:","text":""},{"location":"openshift/CNF-onboarding-support/rolebinding/#clusterrole","title":"ClusterRole","text":"<ul> <li> <p>Scope: Cluster-wide.</p> </li> <li> <p>Purpose: Grants permissions across the entire cluster or to non-namespaced resources (e.g., nodes, persistent volumes).</p> </li> <li> <p>Use Cases:     Granting administrators full access across all projects.     Giving access to cluster-wide resources like nodes or storage classes.</p> </li> </ul>"},{"location":"openshift/CNF-onboarding-support/rolebinding/#if-you-use-a-clusterrole","title":"If You Use a ClusterRole:","text":"<p>Scope: Cluster-wide, but can still be bound to specific namespaces via RoleBinding.</p>"},{"location":"openshift/CNF-onboarding-support/rolebinding/#implication","title":"Implication:","text":"<ul> <li> <p>You can reuse the same ClusterRole and bind it to multiple users and namespaces.</p> </li> <li> <p>Just create separate RoleBindings in each namespace for each user.</p> </li> </ul>"},{"location":"openshift/CNF-onboarding-support/rolebinding/#role","title":"Role","text":"<ul> <li> <p>Scope: Namespaced.</p> </li> <li> <p>Purpose: Grants permissions only within a specific namespace (project).</p> </li> <li> <p>Use Cases:</p> <p>Application developers working within a single namespace. CI/CD pipelines running in isolated namespaces.</p> </li> </ul>"},{"location":"openshift/CNF-onboarding-support/rolebinding/#if-you-use-a-role-namespaced","title":"If You Use a Role (namespaced):","text":"<p>Scope: One namespace only.</p>"},{"location":"openshift/CNF-onboarding-support/rolebinding/#implication_1","title":"Implication:","text":"<ul> <li> <p>You can\u2019t reuse the same Role in another namespace.</p> </li> <li> <p>You\u2019d have to create a new Role (with the same rules) in each namespace where you want the same access.</p> </li> </ul>"},{"location":"openshift/CNF-onboarding-support/rolebinding/#binding-roles","title":"Binding Roles","text":"<p>You assign roles to users or groups through bindings:</p> <ul> <li> <p>RoleBinding: Assigns a Role to a user/group/service account within a specific namespace.</p> </li> <li> <p>ClusterRoleBinding: Assigns a ClusterRole to a user/group/service account cluster-wide or within a namespace (via a RoleBinding referencing a ClusterRole).</p> </li> </ul>"},{"location":"openshift/CNF-onboarding-support/rolebinding/#implementation-for-network-attachement-role","title":"Implementation for Network attachement role:","text":"<p>1) login to cluster with admin privilage and then create a new role (clusterrole) with granding access to network attachements.</p> <pre><code>[root@ncputility ~ pancwl_rc]$ oc create clusterrole net-attach-def-cluster-role \\\n  --verb=create,get,list,watch,update,patch,delete \\\n  --resource=network-attachment-definitions.k8s.cni.cncf.io\n\nclusterrole.rbac.authorization.k8s.io/net-attach-def-cluster-role created\n</code></pre> <p>2) Run this step for below cnf users requesting for access to network attachment definition and assign to their NS.</p> <pre><code>[root@ncputility ~ pancwl_rc]$ oc create rolebinding net-attach-def-rolebinding \\\n  --clusterrole=net-attach-def-cluster-role \\\n  --user=paclypamrf01 \\\n  --namespace=paclypamrf01\nrolebinding.rbac.authorization.k8s.io/net-attach-def-rolebinding created\n</code></pre> <p>3) Then validate is that user having access to it. </p> <pre><code>[root@ncputility ~ pancwl_rc]$ oc auth can-i create network-attachment-definitions.k8s.cni.cncf.io --as=paclypamrf01 -n paclypamrf01\nyes\n[root@ncputility ~ pancwl_rc]$\n</code></pre>"},{"location":"openshift/CNF-onboarding-support/rolebinding/#implementation-for-cbur-role","title":"Implementation for Cbur role:","text":"<p>1) login to cluster with admin privilage and then create a new role (clusterrole) with granding access to cbur br polices.</p> <p>code snippet: <code># cat  &gt; cburrole.yaml</code></p> <pre><code>apiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRole\nmetadata:\n  name: ncd-cbur-role\nrules:\n  - apiGroups: [\"cbur.bcmt.local\"]\n    resources: [\"brpolices\"]\n    verbs: [\"create\", \"get\", \"list\", \"watch\", \"update\", \"patch\"]\n  - apiGroups: [\"cbur.csf.nokia.com\"]\n    resources: [\"brhooks\", \"brpolices\"]\n    verbs: [\"create\", \"get\", \"list\", \"watch\", \"update\", \"patch\"]\n</code></pre> <p>1.1) here is the command executed on this platform. just give ctrl+c at last. right after that just apply it. </p> <pre><code>[root@ncputility ~ panhub_rc]$ cat  &gt; cburrole.yaml\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRole\nmetadata:\n  name: ncd-cbur-role\nrules:\n  - apiGroups: [\"cbur.bcmt.local\"]\n    resources: [\"brpolices\"]\n    verbs: [\"create\", \"get\", \"list\", \"watch\", \"update\", \"patch\"]\n  - apiGroups: [\"cbur.csf.nokia.com\"]\n    resources: [\"brhooks\", \"brpolices\"]\n    verbs: [\"create\", \"get\", \"list\", \"watch\", \"update\", \"patch\"]\n^C\n[root@ncputility ~ panhub_rc]$ oc apply  -f cburrole.yaml\nclusterrole.rbac.authorization.k8s.io/ncd-cbur-role created\n[root@ncputility ~ panhub_rc]$\n</code></pre> <p>2) Run this step for below cnf users requesting for access to cbur br polices and assign to their NS.</p> <pre><code>[root@ncputility ~ panhub_rc]$ oc create rolebinding ncd-cbur-role-binding \\\n  --clusterrole=ncd-cbur-role \\\n  --user=nokia \\\n  --namespace=test01\nrolebinding.rbac.authorization.k8s.io/ncd-cbur-role-binding created\n[root@ncputility ~ panhub_rc]$\n</code></pre> <p>3) Then validate is that user having access to it. </p> <pre><code>[root@ncputility ~ panhub_rc]$ oc auth can-i create brpolices --as=nokia -n test01\nyes\n[root@ncputility ~ panhub_rc]$\n</code></pre>"},{"location":"openshift/backup-restore/ACM-localbackup/","title":"ACM Local Backup","text":""},{"location":"openshift/backup-restore/ACM-localbackup/#backup-hub-clusters-with-red-hat-advanced-cluster-management-for-kubernetes","title":"Backup Hub Clusters with Red Hat Advanced Cluster Management for Kubernetes","text":""},{"location":"openshift/backup-restore/ACM-localbackup/#redhat-acm-introduction","title":"Redhat ACM introduction","text":"<p>Red Hat Advanced Cluster Management for Kubernetes (RHACM) defines two main types of clusters: hub clusters and managed clusters. </p> <ul> <li>The hub cluster is the main cluster with RHACM installed on it. You can create, manage, and monitor other Kubernetes clusters with the hub cluster.</li> <li>The managed clusters are Kubernetes clusters that are managed by the hub cluster. You can create some clusters by using the RHACM hub cluster, and you can also import existing clusters to be managed by the hub cluster.</li> </ul>"},{"location":"openshift/backup-restore/ACM-localbackup/#prerequisites","title":"Prerequisites","text":"<p>MultiClusterHub resource is created and displays the status of Running; the MultiClusterHub resource is automatically created when you install the RHACM Operator</p> <p>The cluster backup and restore operator chart is not installed automatically. Enable the cluster-backup operator on the hub cluster. Edit the MultiClusterHub resource and set the cluster-backup to true. This installs the OADP operator in the same namespace with the backup chart. </p>"},{"location":"openshift/backup-restore/ACM-localbackup/#how-it-works","title":"How it works","text":"<p>The cluster backup and restore operator runs on the hub cluster and depends on the OADP Operator to create a connection to a backup storage location on the hub cluster. The OADP Operator also installs Velero, which is the component used to backup and restore user created hub resources.</p> <p>The cluster backup and restore operator is installed using the cluster-backup-chart file. The cluster backup and restore operator chart is not installed automatically. Starting with RHACM version 2.5, the cluster backup and restore operator chart is installed by setting the cluster-backup option to true on the MultiClusterHub resource.</p> <p>The cluster backup and restore operator chart automatically installs the OADP Operator in the same namespace with the backup chart. If you have previously installed and used the OADP Operator on your hub cluster, you should uninstall the version since the backup chart works now with the operator that is installed in the chart namespace. This should not affect your old backups and previous work. Just use the same storage location for the DataProtectionApplication resource, which is owned by the OADP Operator and installed with the backup chart; you should have access to the same backup data as the previous operator. The only difference is that Velero backup resources are now loaded in the new OADP Operator namespace on your hub cluster.</p>"},{"location":"openshift/backup-restore/ACM-localbackup/#implementation","title":"Implementation","text":"<p>1) You need to be logged in with a user who has cluster-admin privileges:</p> <pre><code>[root@ncputility ~ pancwl_rc]$ source /root/panhubrc\nWARNING: Using insecure TLS client config. Setting this option is not supported!\n\nLogin successful.\n\nYou have access to 103 projects, the list has been suppressed. You can list all projects with 'oc projects'\n\nUsing project \"panclypcwl01\".\n[root@ncputility ~ panhub_rc]$\n</code></pre> <p>2) Add a new annotation to the <code>multiclusterhub</code> CR and enable <code>cluster-backup</code>for triggering to deploy the OADP</p> <p>Active hub</p> <pre><code>[root@ncputility ~ panhub_rc]$ oc edit multiclusterhubs.operator.open-cluster-management.io -n open-cluster-management multiclusterhub\nmulticlusterhub.operator.open-cluster-management.io/multiclusterhub edited\n[root@ncputility ~ panhub_rc]$ \n</code></pre> <p>2.1) sample syntax are attached here</p> <pre><code>oc edit multiclusterhubs.operator.open-cluster-management.io -n open-cluster-management multiclusterhub\n\napiVersion: operator.open-cluster-management.io/v1\nkind: MultiClusterHub\nmetadata:\n  annotations:\n    installer.open-cluster-management.io/oadp-subscription-spec: '{\"source\": \"cs-redhat-operator-index-acm-oadp-1-4-0\"}'\n\ncluster-backup\nenabled: true\n</code></pre> <p>On the Hub cluster, the value of source has to be the name of the catalogsource of the Infra manager node's Quay that is pointing to the organization where the operators of 24.7 were mirrored.</p> <p>3) Create BucketClass</p> <p>passive cluster or CWL cluster. </p> <pre><code>[root@ncputility ~ pancwl_rc]$ cat bucketclass-noobaa-default-backing-store.yaml\napiVersion: noobaa.io/v1alpha1\nkind: BucketClass\nmetadata:\n  name: bucketclass-noobaa-default-backing-store\n  namespace: openshift-storage\nspec:\n  placementPolicy:\n    tiers:\n    - backingStores:\n      - noobaa-default-backing-store\n      placement: Spread\n[root@ncputility ~ pancwl_rc]$\n</code></pre> <p>oc apply -f bucketclass-noobaa-default-backing-store.yaml</p> <p>4) Create Object bucket</p> <p>passive cluster or CWL cluster. </p> <pre><code>[root@ncputility ~ pancwl_rc]$ cat ObjectBucketClaim.yaml\napiVersion: objectbucket.io/v1alpha1\nkind: ObjectBucketClaim\nmetadata:\n  name: acm-backups\n  namespace: openshift-storage\nspec:\n  generateBucketName: acm-backups\n  storageClassName: openshift-storage.noobaa.io\n  additionalConfig:\n      bucketclass: bucketclass-noobaa-default-backing-store\n[root@ncputility ~ pancwl_rc]$\n\n# spec refers to the BucketClass created earlier.\n</code></pre> <p>oc apply -f ObjectBucketClaim.yaml</p> <p>a) Use the following commands to gather values for later steps</p> <pre><code>BUCKET_NAME=$(oc get -n openshift-storage configmap acm-backups -o jsonpath='{.data.BUCKET_NAME}')\nACCESS_KEY_ID=$(oc get -n openshift-storage secret acm-backups -o jsonpath='{.data.AWS_ACCESS_KEY_ID}' | base64 -d)\nSECRET_ACCESS_KEY=$(oc get -n openshift-storage secret acm-backups -o jsonpath='{.data.AWS_SECRET_ACCESS_KEY}' | base64 -d)\n\necho \"BUCKET_NAME=$BUCKET_NAME\"\necho \"ACCESS_KEY_ID=$ACCESS_KEY_ID\"\necho \"SECRET_ACCESS_KEY=$SECRET_ACCESS_KEY\"\n</code></pre> <p>5) Create secret.txt file</p> <pre><code>[root@ncputility ~ pancwl_rc]$ cat secret.txt\n[default]\naws_access_key_id = hOJGxRsZDKOaBgRRXqf5\naws_secret_access_key = KuOAAqyTDM434if+EM16Ajp7Le66MN1+KirdzXKt\n[root@ncputility ~ pancwl_rc]$\n\n#aws_access_key_id and aws_secret_access_key provide access to Object bucket.\n</code></pre> <p>6) Apply the following command on secret.txt</p> <pre><code>base64 -w 0 secret.txt\n</code></pre> <p>Expected output: </p> <pre><code>W2RlZmF1bHRdCmF3c19hY2Nlc3Nfa2V5X2lkID0gWkQ3YnJrRWRUa0lheTBwVUphVlMKYXdzX3NlY3JldF9hY2Nlc3Nfa2V5ID0gZmlvZ0JYek40elpES1lBNVR6cHcrWUhTM3FEeWYwa0tJWlNyekt0agoK\n</code></pre> <p>7) Create secret.yaml file</p> <p>both clusters: </p> <pre><code>[root@ncputility ~ pancwl_rc]$ cat secret.yaml\napiVersion: v1\ndata:\n  cloud: W2RlZmF1bHRdCmF3c19hY2Nlc3Nfa2V5X2lkID0gaE9KR3hSc1pES09hQmdSUlhxZjUKYXdzX3NlY3JldF9hY2Nlc3Nfa2V5ID0gS3VPQUFxeVRETTQzNGlmK0VNMTZBanA3TGU2Nk1OMStLaXJkelhLdAo=\nkind: Secret\nmetadata:\n  name: cloud-credentials\n  namespace: open-cluster-management-backup\ntype: Opaque\n[root@ncputility ~ pancwl_rc]$\n\n\n#The value of cloud under data block is the base64 encoded output of the secret.txt.\n</code></pre> <p>8) Apply secret.yaml </p> <pre><code>[root@dom14npv101-infra-manager ~ hubrc]# oc apply -f secret.yaml\nsecret/cloud-credentials created\n[root@dom14npv101-infra-manager ~ hubrc]#\n</code></pre> <p>9) Create DataProtectionApplication custom resource</p> <p>9.1) s3 interface can be accessed on the standby cluster via s3Url. The value of s3Url can be checked with the following command:</p> <p>oc get routes -n openshift-storage | grep s3-openshift-storage | awk '{print $2}'</p> <p>9.2) caCert is a root certificate in base64 format of the standby hub cluster's default ingress controller which is used for exposing s3 service. The value of caCert can be checked with the following command:</p> <p>oc get secrets -n openshift-ingress-operator router-ca -o jsonpath=\"{.data['tls.crt']}\"</p> <p>9.3) The value of credential is the name of the secret which was created in secret.yaml. The value of key is the name of the value created insecret.yaml under data block.</p> <pre><code>[root@ncputility ~ pancwl_rc]$ cat DataProtectionApplication.yaml\nkind: DataProtectionApplication\napiVersion: oadp.openshift.io/v1alpha1\nmetadata:\n  name: velero-acm-backup\n  namespace: open-cluster-management-backup\nspec:\n  backupLocations:\n    - velero:\n        config:\n          profile: default\n          region: none\n          s3ForcePathStyle: \"true\"\n          s3Url: \"https://s3-openshift-storage.apps.hnevocphub01.mnc002.mcc708\"\n        default: true\n        provider: aws\n        objectStorage:\n          bucket: acm-backups-3885ebaa-8c8b-4985-b9c7-0c25d631de14\n          prefix: velero\n          caCert: \"LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSURERENDQWZTZ0F3SUJBZ0lCQVRBTkJna3Foa2lHOXcwQkFRc0ZBREFtTVNRd0lnWURWUVFEREJ0cGJtZHkKWlhOekxXOXdaWEpoZEc5eVFERTNORFV4TlRNM09UZ3dIaGNOTWpVd05ESXdNVEkxTmpNM1doY05NamN3TkRJdwpNVEkxTmpNNFdqQW1NU1F3SWdZRFZRUUREQnRwYm1keVpYTnpMVzl3WlhKaGRHOXlRREUzTkRVeE5UTTNPVGd3CmdnRWlNQTBHQ1NxR1NJYjNEUUVCQVFVQUE0SUJEd0F3Z2dFS0FvSUJBUUM4aXhWMDZnM0ozZ01lUE04d0JEVTEKRFdMNWhyVW9DeXh5SThkTFNob2trSDNtdTF1RFNCeEJ2blBQYnZhRUs1aFR1RFhjQ3dSaUVNYmZPcFFjUnZzdApQT2dVSU05T3RhY3lwWVZTeGVhSUNQZnNpMlBzd0VaNElWNWMwZEM1SVVSc2hTTTFXaksxckpvanZYQmE3c0c3CjhpQXRCMnJ0K1FwUkhJRzVSYVpnZVdXQVhrMXVPdWlTazBSLzJ3ajZsbDJmZ01NYklOSGluNXdseU1YQldjOHYKaHJTc3VjR2wrL2pXRVl2QWdXKzFrRW5YRDgvclpqUTN6bExVVTRpdzNvNlBMRFlrKzhjYm5DQnNMQjh2RWlqWQppbFRzQ1N3TjdwUk5CVGNqcWxLYVpqWVZ2Mk9QYi80WjF4TG1aekFxWGRhc2pSRW5XNXcyZGtrZy9RNlgxWmZGCkFnTUJBQUdqUlRCRE1BNEdBMVVkRHdFQi93UUVBd0lDcERBU0JnTlZIUk1CQWY4RUNEQUdBUUgvQWdFQU1CMEcKQTFVZERnUVdCQlRsdFFLZ2NjYjZ4bE5BZUt5SFpjY1BZT1A1b0RBTkJna3Foa2lHOXcwQkFRc0ZBQU9DQVFFQQpnZDYyWVJkRHhNTm5HcCtDWStRaXBZRlMxMkNRMUVwVXcwMEVIUno0b2k0UG5PQzcrazlGbHVZeW9vWWNZa1QyCkNGTGp4K3hOODE2bkFVRWFMQ25Ia2NjK2JCSlhuNmU0LzFqWUZGdFFjRnhJeEZVdXByb3dNY21ZeGxBbHlBQ1oKZU5ac1V1Tk4xWVh1aDl2Z3MrRVE2a1VNSjFYSVNPeVp3RDNIbVZSaW16RHN6MUo4MDRLSlhqVnRURlNEbXR1WQpBbGxLWnF2czdqWGJWTUlxY1VtUmFQclluRm8xRDBXcFFGMnFLRGtUL1RDdnFuS3ROR2x5dHVCZmp1YittL0U0Ck1yOExTdXFjTGlhcGhFQS9BM3pwNHNaVWxVbmVCaksrRnkzTkwwdnprcjZxZGVjbG1YSHU5bzdtMnV4UlBKR2UKTm9vb240cmdYU25SSnB5VForcEhyUT09Ci0tLS0tRU5EIENFUlRJRklDQVRFLS0tLS0K\"\n        credential:\n          key: cloud\n          name: cloud-credentials\n  configuration:\n    nodeAgent:\n      enable: true\n      uploaderType: restic\n    velero:\n      featureFlags:\n        - EnableCSI\n      defaultPlugins:\n        - openshift\n        - aws\n        - csi\n[root@ncputility ~ pancwl_rc]$\n\n\n#bucket is the value checked earlier with BUCKET_NAME command.\n</code></pre> <p><code>[root@dom14npv101-infra-manager ~ hubrc]# oc apply -f DataProtectionApplication.yaml dataprotectionapplication.oadp.openshift.io/velero-acm-backup created [root@dom14npv101-infra-manager ~ hubrc]#</code></p> <p>10) validate that resouce creation is completed successfully. </p> <pre><code>[root@dom14npv101-infra-manager ~ hubrc]# oc get backupstoragelocations.velero.io -n open-cluster-management-backup\nNAME                  PHASE       LAST VALIDATED   AGE   DEFAULT\nvelero-acm-backup-1   Available   22s              33s   true\n[root@dom14npv101-infra-manager ~ hubrc]#\n</code></pre> <p>11) Check the resources that are application projects from GitOps</p> <p>hub cluster: </p> <pre><code>[root@dom14npv101-infra-manager ~ hubrc]# oc get appprojects.argoproj.io -n openshift-gitops | grep -v default\nNAME                  AGE\nsite-config-project   59d\nsite-policy-project   59d\n</code></pre> <p>12) Add label velero.io/exclude-from-backup: \"true\" manually on both Hub clusters</p> <pre><code>[root@dom14npv101-infra-manager ~ hubrc]# oc get appprojects.argoproj.io -n openshift-gitops -l velero.io/exclude-from-backup\nNAME                  AGE\nsite-config-project   59d\nsite-policy-project   59d\n</code></pre> <p>13) Add label velero.io/exclude-from-backup: \"true\" manually on both Hub clusters</p> <pre><code>[root@dom14npv101-infra-manager ~ hubrc]# oc get applications.argoproj.io -n openshift-gitops -l velero.io/exclude-from-backup\nNAME                        SYNC STATUS   HEALTH STATUS\nncpvnpvlab1-site-configs    Synced        Healthy\nncpvnpvlab1-site-policies   Synced        Healthy\nncpvnpvmgt-site-configs     Synced        Healthy\nncpvnpvmgt-site-policies    Synced        Healthy\n[root@dom14npv101-infra-manager ~ hubrc]#\n</code></pre> <p>14) A backup is made everyday at 10 PM.</p> <pre><code>[root@dom14npv101-infra-manager ~ hubrc]# cat &gt; backup.yaml\napiVersion: cluster.open-cluster-management.io/v1beta1\nkind: BackupSchedule\nmetadata:\n  name: schedule-acm\n  namespace: open-cluster-management-backup\nspec:\n  veleroSchedule: 0 22 * * *\n  veleroTtl: 120h\n^C\n[root@dom14npv101-infra-manager ~ hubrc]# date\nThu May  1 06:04:49 PM UTC 2025\n[root@dom14npv101-infra-manager ~ hubrc]# vi backup.yaml\n[root@dom14npv101-infra-manager ~ hubrc]# oc apply  -f  backup.yaml\nbackupschedule.cluster.open-cluster-management.io/schedule-acm created\n[root@dom14npv101-infra-manager ~ hubrc]# \n</code></pre>"},{"location":"openshift/backup-restore/ACM-localbackup/#status-of-backup-job-here","title":"Status of backup job here:","text":"<p>1) checking the status of the backup </p> <pre><code>[root@dom14npv101-infra-manager ~ hubrc]# oc get backupstoragelocations.velero.io -A\nNAMESPACE                        NAME                  PHASE       LAST VALIDATED   AGE     DEFAULT\nopen-cluster-management-backup   velero-acm-backup-1   Available   4s               5m15s   true\n[root@dom14npv101-infra-manager ~ hubrc]# \n</code></pre> <p>2) describe the status of the output job </p> <pre><code>[root@dom14npv101-infra-manager ~ hubrc]#  oc describe backupstoragelocations.velero.io velero-acm-backup-1 -n open-cluster-management-backup\nName:         velero-acm-backup-1\nNamespace:    open-cluster-management-backup\nLabels:       app.kubernetes.io/component=bsl\n              app.kubernetes.io/instance=velero-acm-backup-1\n              app.kubernetes.io/managed-by=oadp-operator\n              app.kubernetes.io/name=oadp-operator-velero\n              openshift.io/oadp=True\n              openshift.io/oadp-registry=True\nAnnotations:  &lt;none&gt;\nAPI Version:  velero.io/v1\nKind:         BackupStorageLocation\nMetadata:\n  Creation Timestamp:  2025-05-01T18:02:31Z\n  Generation:          13\n  Owner References:\n    API Version:           oadp.openshift.io/v1alpha1\n    Block Owner Deletion:  true\n    Controller:            true\n    Kind:                  DataProtectionApplication\n    Name:                  velero-acm-backup\n    UID:                   89734e12-0144-4376-954d-cb13cde17515\n  Resource Version:        142419914\n  UID:                     52b6b341-5e36-4e02-bc98-15622be65673\nSpec:\n  Config:\n    Checksum Algorithm:\n    Profile:             default\n    Region:              none\n    s3ForcePathStyle:    true\n    s3Url:               https://s3-openshift-storage.apps.ncpvnpvmgt.pnwlab.nsn-rdnet.net\n  Credential:\n    Key:    cloud\n    Name:   cloud-credentials\n  Default:  true\n  Object Storage:\n    Bucket:   acm-backups-bcf1990d-0846-4a6d-8518-67db81ebee63\n    Ca Cert:  LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSURERENDQWZTZ0F3SUJBZ0lCQVRBTkJna3Foa2lHOXcwQkFRc0ZBREFtTVNRd0lnWURWUVFEREJ0cGJtZHkKWlhOekxXOXdaWEpoZEc5eVFERTNOREV4TXpJek16RXdIaGNOTWpVd016QTBNak0xTWpF        d1doY05NamN3TXpBMApNak0xTWpFeFdqQW1NU1F3SWdZRFZRUUREQnRwYm1keVpYTnpMVzl3WlhKaGRHOXlRREUzTkRFeE16SXpNekV3CmdnRWlNQTBHQ1NxR1NJYjNEUUVCQVFVQUE0SUJEd0F3Z2dFS0FvSUJBUURLOUhrUkd4VWNPRnZjcHVZc1dLdWQKd2ltMFBWc3B3VU        1LNXgyejhBQWJ1eGhuOTRFWThWbk9tSkJUVkhqRnIvbmI0SjRzYldzN3JBbXZINWpjbTdzbApMYW1wWU1uUWlSamtrdE5FSVV3Rm9LVm9VU3ZRSWtiTHJlZk1hcjJYSENOOEU0dHVHeXA3U1c1YjBMRjc4cUY5CktIY0ttMXBDVi9NR3lyU1RtMkx0SmtBcnM5d0lsL0ZPYmox        UEcvUmsvQThtRHZhalBmSUVGbU8yMHduWEQ5bWcKZDVIVk1ZVzkyWWRWVDZPR0FWMEZUNCtJNzNibEdyK3pqQUJzMklxTnUzQ3h0cHlucXMvVVV6RGFodGRvc2ZxSgpWeHhaTUFaZlRENTk0UUtzMFZhamR2aTU1Z1pPejVBQXRLSG96Rm85TWk5dklXblpwR3Z0T2xINnIyVE        ZmSytmCkFnTUJBQUdqUlRCRE1BNEdBMVVkRHdFQi93UUVBd0lDcERBU0JnTlZIUk1CQWY4RUNEQUdBUUgvQWdFQU1CMEcKQTFVZERnUVdCQlRJNVRqeVp2aUJ1NE0wNzd6Mk9PY0lDRmkwMURBTkJna3Foa2lHOXcwQkFRc0ZBQU9DQVFFQQpuNG96eWZha0sySUFqb3dFSlZE        bFNMMlp4YVJuWmJFcjVLanhJbjhiQ0tjaUdBK0h2UURuY0UzK1BzSTJDZGxpCjhXQlR4ZnJ3aFoxTVZ2YjVySmlsTXpZUklQamJaanUrbitaNlB0SGJYMEZDb2Q0elpaYkZBOFQvNlZXUkJSSmUKL3VMaXU3VVRENjJQRDgydVlNSmJFTDNTa1V6b1U5T2NXMSt1S1R3UG56NG        VFblVvNzVnbVlUWnBybkhKSEttNAowUVljTlF6RndHR3JnQnNuTy8wRW94Z2Roa09keENlWFBqRUpURnZXRTdpRjhYWTlKQVZKMVpCcStuZUNoMmRCCkowRGRkaGZjMDhtUnpHbStkVXRyeCtZMnRoSXBxWVUreTN1WDZaM09TcDVNeFIvQzMvelRGR2pqK3pKUmNIOTIKUmtH        c2V5bGNCYXpYbmVvWXgzdEVxUT09Ci0tLS0tRU5EIENFUlRJRklDQVRFLS0tLS0K\n    Prefix:   velero\n  Provider:   aws\nStatus:\n  Last Synced Time:      2025-05-01T18:07:42Z\n  Last Validation Time:  2025-05-01T18:07:42Z\n  Phase:                 Available\nEvents:\n  Type    Reason                           Age    From            Message\n  ----    ------                           ----   ----            -------\n  Normal  BackupStorageLocationReconciled  5m33s  DPA-controller  performed created on backupstoragelocation open-cluster-management-backup/velero-acm-backup-1\n</code></pre>"},{"location":"openshift/backup-restore/ACM-localbackup/#access-the-backup-jobs","title":"Access the backup jobs","text":"<p>1) status of open-cluster-manager pods here</p> <pre><code>[root@dom14npv101-infra-manager ~ hubrc]# oc get pods -n  open-cluster-management-backup\nNAME                                                 READY   STATUS    RESTARTS   AGE\ncluster-backup-chart-clusterbackup-698656f7f-cqxxj   1/1     Running   0          24d\ncluster-backup-chart-clusterbackup-698656f7f-dvdz8   1/1     Running   0          24d\nnode-agent-brq4q                                     1/1     Running   0          6m6s\nnode-agent-j9xm8                                     1/1     Running   0          6m6s\nnode-agent-mlrwk                                     1/1     Running   0          6m6s\nnode-agent-nkb7v                                     1/1     Running   0          6m6s\nnode-agent-sgqxw                                     1/1     Running   0          6m6s\nopenshift-adp-controller-manager-74f799649f-v2slw    1/1     Running   0          24d\nvelero-549fbfb95d-s966g                              1/1     Running   0          6m6s\n[root@dom14npv101-infra-manager ~ hubrc]# \n</code></pre> <p>2) status of job from velero binary</p> <pre><code>[root@dom14npv101-infra-manager ~ hubrc]# oc -n open-cluster-management-backup exec -it velero-549fbfb95d-s966g -- ./velero backup-location get\nDefaulted container \"velero\" out of: velero, openshift-velero-plugin (init), velero-plugin-for-aws (init)\nNAME                  PROVIDER   BUCKET/PREFIX                                             PHASE       LAST VALIDATED                  ACCESS MODE   DEFAULT\nvelero-acm-backup-1   aws        acm-backups-bcf1990d-0846-4a6d-8518-67db81ebee63/velero   Available   2025-05-01 18:08:42 +0000 UTC   ReadWrite     true\n[root@dom14npv101-infra-manager ~ hubrc]#\n</code></pre> <p>3) Look at the final status of the backup jobs</p> <pre><code> [root@dom14npv101-infra-manager ~ hubrc]# oc -n open-cluster-management-backup exec -it velero-549fbfb95d-s966g -- ./velero backup get\nDefaulted container \"velero\" out of: velero, openshift-velero-plugin (init), velero-plugin-for-aws (init)\nNAME                                            STATUS      ERRORS   WARNINGS   CREATED                         EXPIRES   STORAGE LOCATION      SELECTOR\nacm-credentials-schedule-20250501180616         Completed   0        0          2025-05-01 18:06:17 +0000 UTC   4d        velero-acm-backup-1   &lt;none&gt;\nacm-managed-clusters-schedule-20250501180616    Completed   0        0          2025-05-01 18:06:18 +0000 UTC   4d        velero-acm-backup-1   &lt;none&gt;\nacm-resources-generic-schedule-20250501180616   Completed   0        0          2025-05-01 18:06:20 +0000 UTC   4d        velero-acm-backup-1   cluster.open-cluster-management.io/backup\nacm-resources-schedule-20250501180616           Completed   0        0          2025-05-01 18:06:29 +0000 UTC   4d        velero-acm-backup-1   !cluster.open-cluster-management.io/backup,!policy.open-cluste        r-management.io/root-policy\nacm-validation-policy-schedule-20250501180616   Completed   0        0          2025-05-01 18:06:30 +0000 UTC   1d        velero-acm-backup-1   &lt;none&gt;\n[root@dom14npv101-infra-manager ~ hubrc]# date\nThu May  1 06:09:39 PM UTC 2025\n[root@dom14npv101-infra-manager ~ hubrc]# \n</code></pre>"},{"location":"openshift/backup-restore/AWS-S3/","title":"AWS S3 Backup","text":"<p>[root@dom14npv101-infra-manager ~ hubrc]# oc -n open-cluster-management-backup exec -it velero-549fbfb95d-s966g -- ./velero backup-location get Defaulted container \"velero\" out of: velero, openshift-velero-plugin (init), velero-plugin-for-aws (init) NAME                  PROVIDER   BUCKET/PREFIX                                             PHASE       LAST VALIDATED                  ACCESS MODE   DEFAULT velero-acm-backup-1   aws        acm-backups-bcf1990d-0846-4a6d-8518-67db81ebee63/velero   Available   2025-05-01 18:20:42 +0000 UTC   ReadWrite     true [root@dom14npv101-infra-manager ~ hubrc]# oc -n open-cluster-management-backup exec -it velero-549fbfb95d-s966g -- ./velero backup get Defaulted container \"velero\" out of: velero, openshift-velero-plugin (init), velero-plugin-for-aws (init) NAME                                            STATUS      ERRORS   WARNINGS   CREATED                         EXPIRES   STORAGE LOCATION      SELECTOR acm-credentials-schedule-20250501180616         Completed   0        0          2025-05-01 18:06:17 +0000 UTC   4d        velero-acm-backup-1    acm-managed-clusters-schedule-20250501180616    Completed   0        0          2025-05-01 18:06:18 +0000 UTC   4d        velero-acm-backup-1    acm-resources-generic-schedule-20250501180616   Completed   0        0          2025-05-01 18:06:20 +0000 UTC   4d        velero-acm-backup-1   cluster.open-cluster-management.io/backup acm-resources-schedule-20250501180616           Completed   0        0          2025-05-01 18:06:29 +0000 UTC   4d        velero-acm-backup-1   !cluster.open-cluster-management.io/backup,!policy.open-cluste        r-management.io/root-policy acm-validation-policy-schedule-20250501180616   Completed   0        0          2025-05-01 18:06:30 +0000 UTC   23h       velero-acm-backup-1    [root@dom14npv101-infra-manager ~ hubrc]# oc -n open-cluster-management-backup exec -it velero-549fbfb95d-s966g -- ./velero backup get Defaulted container \"velero\" out of: velero, openshift-velero-plugin (init), velero-plugin-for-aws (init) NAME                                            STATUS      ERRORS   WARNINGS   CREATED                         EXPIRES   STORAGE LOCATION      SELECTOR acm-credentials-schedule-20250501180616         Completed   0        0          2025-05-01 18:06:17 +0000 UTC   4d        velero-acm-backup-1    acm-managed-clusters-schedule-20250501180616    Completed   0        0          2025-05-01 18:06:18 +0000 UTC   4d        velero-acm-backup-1    acm-resources-generic-schedule-20250501180616   Completed   0        0          2025-05-01 18:06:20 +0000 UTC   4d        velero-acm-backup-1   cluster.open-cluster-management.io/backup acm-resources-schedule-20250501180616           Completed   0        0          2025-05-01 18:06:29 +0000 UTC   4d        velero-acm-backup-1   !cluster.open-cluster-management.io/backup,!policy.open-cluste        r-management.io/root-policy acm-validation-policy-schedule-20250501180616   Completed   0        0          2025-05-01 18:06:30 +0000 UTC   22h       velero-acm-backup-1    [root@dom14npv101-infra-manager ~ hubrc]# curl \"https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip\" -o \"awscliv2.zip\"   % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current                                  Dload  Upload   Total   Spent    Left  Speed 100 67.2M  100 67.2M    0     0  20.2M      0  0:00:03  0:00:03 --:--:-- 20.2M [root@dom14npv101-infra-manager ~ hubrc]# unzip awscliv2.zip Archive:  awscliv2.zip    creating: aws/    creating: aws/dist/   inflating: aws/install   inflating: aws/README.md   inflating: aws/THIRD_PARTY_LICENSES    creating: aws/dist/awscli/    creating: aws/dist/cryptography/    creating: aws/dist/docutils/    creating: aws/dist/lib-dynload/   inflating: aws/dist/aws   inflating: aws/dist/aws_completer   inflating: aws/dist/libpython3.13.so.1.0   inflating: aws/dist/_awscrt.abi3.so   inflating: aws/dist/_cffi_backend.cpython-313-x86_64-linux-gnu.so   inflating: aws/dist/_ruamel_yaml.cpython-313-x86_64-linux-gnu.so   inflating: aws/dist/libz.so.1   inflating: aws/dist/liblzma.so.5   inflating: aws/dist/libbz2.so.1   inflating: aws/dist/libffi.so.6   inflating: aws/dist/libuuid.so.1 <p>[root@dom14npv101-infra-manager ~ hubrc]# sudo ./aws/install You can now run: /usr/local/bin/aws --version [root@dom14npv101-infra-manager ~ hubrc]# aws --version aws-cli/2.27.6 Python/3.13.2 Linux/5.14.0-427.13.1.el9_4.x86_64 exe/x86_64.rhel.9 [root@dom14npv101-infra-manager ~ hubrc]# </p> <p>root@dom14npv101-infra-manager ~ hubrc]# aws configure --profile oadp AWS Access Key ID [None]: I35Rzin4xFf58GaCkJlC AWS Secret Access Key [None]: vaimKLwBnO5glSe+f4AKgwJTtYEMOPMnfon540LZ Default region name [None]: Default output format [None]: [root@dom14npv101-infra-manager ~ hubrc]# aws --endpoint-url $AWS_ENDPOINT_URL s3 ls s3://acm-backups-bcf1990d-0846-4a6d-8518-67db81ebee63/velero/</p> <p>SSL validation failed for https://s3-openshift-storage.apps.ncpvnpvmgt.pnwlab.nsn-rdnet.net:443/acm-backups-bcf1990d-0846-4a6d-8518-67db81ebee63?list-type=2&amp;prefix=velero%2F&amp;delimiter=%2F&amp;encoding-type=url         [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: self signed certificate in certificate chain (_ssl.c:1028)</p> <p>[root@dom14npv101-infra-manager ~ hubrc]# [root@dom14npv101-infra-manager ~ hubrc]# aws --endpoint-url \"$AWS_ENDPOINT_URL\" --no-verify-ssl s3 ls s3://acm-backups-bcf1990d-0846-4a6d-8518-67db81ebee63/velero/ urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 's3-openshift-storage.apps.ncpvnpvmgt.pnwlab.nsn-rdnet.net'. Adding certificate verification is strongl        y advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings                            PRE backups/ [root@dom14npv101-infra-manager ~ hubrc]# aws --endpoint-url \"$AWS_ENDPOINT_URL\" --no-verify-ssl s3 ls s3://acm-backups-bcf1990d-0846-4a6d-8518-67db81ebee63/velero/backups/ urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 's3-openshift-storage.apps.ncpvnpvmgt.pnwlab.nsn-rdnet.net'. Adding certificate verification is strongl        y advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings                            PRE acm-credentials-schedule-20250501180616/                            PRE acm-managed-clusters-schedule-20250501180616/                            PRE acm-resources-generic-schedule-20250501180616/                            PRE acm-resources-schedule-20250501180616/                            PRE acm-validation-policy-schedule-20250501180616/ [root@dom14npv101-infra-manager ~ hubrc]# aws --endpoint-url \"$AWS_ENDPOINT_URL\" --no-verify-ssl s3 ls s3://acm-backups-bcf1990d-0846-4a6d-8518-67db81ebee63/velero/backups/acm-credentials-schedule-202505011        80616/ urllib3/connectionpool.py:1064: InsecureRequestWarning: Unverified HTTPS request is being made to host 's3-openshift-storage.apps.ncpvnpvmgt.pnwlab.nsn-rdnet.net'. Adding certificate verification is strongl        y advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings 2025-05-01 18:06:18         29 acm-credentials-schedule-20250501180616-csi-volumesnapshotclasses.json.gz 2025-05-01 18:06:18         27 acm-credentials-schedule-20250501180616-csi-volumesnapshotcontents.json.gz 2025-05-01 18:06:18         29 acm-credentials-schedule-20250501180616-csi-volumesnapshots.json.gz 2025-05-01 18:06:18         27 acm-credentials-schedule-20250501180616-itemoperations.json.gz 2025-05-01 18:06:18      11604 acm-credentials-schedule-20250501180616-logs.gz 2025-05-01 18:06:18         29 acm-credentials-schedule-20250501180616-podvolumebackups.json.gz 2025-05-01 18:06:18        327 acm-credentials-schedule-20250501180616-resource-list.json.gz 2025-05-01 18:06:18         49 acm-credentials-schedule-20250501180616-results.gz 2025-05-01 18:06:18         27 acm-credentials-schedule-20250501180616-volumeinfo.json.gz 2025-05-01 18:06:18         29 acm-credentials-schedule-20250501180616-volumesnapshots.json.gz 2025-05-01 18:06:18      46601 acm-credentials-schedule-20250501180616.tar.gz 2025-05-01 18:06:18       4428 velero-backup.json [root@dom14npv101-infra-manager ~ hubrc]#</p>"},{"location":"openshift/backup-restore/etcd-backup/","title":"Control plane backup","text":""},{"location":"openshift/backup-restore/etcd-backup/#backing-up-etcd","title":"Backing up etcd","text":"<p>etcd is the key-value store for OpenShift Container Platform, which persists the state of all resource objects.</p> <p>Back up your cluster\u2019s etcd data regularly and store in a secure location ideally outside the OpenShift Container Platform environment. Do not take an etcd backup before the first certificate rotation completes, which occurs 24 hours after installation, otherwise the backup will contain expired certificates. It is also recommended to take etcd backups during non-peak usage hours because the etcd snapshot has a high I/O cost.</p> <p>Be sure to take an etcd backup after you upgrade your cluster. This is important because when you restore your cluster, you must use an etcd backup that was taken from the same z-stream release. For example, an OpenShift Container Platform 4.y.z cluster must use an etcd backup that was taken from 4.y.z.</p>"},{"location":"openshift/backup-restore/etcd-backup/#backing-up-etcd-data","title":"Backing up etcd data","text":""},{"location":"openshift/backup-restore/etcd-backup/#prerequisites","title":"Prerequisites","text":"<ul> <li>You have access to the cluster as a user with the cluster-admin role.</li> <li>You have checked whether the cluster-wide proxy is enabled.</li> </ul>"},{"location":"openshift/backup-restore/etcd-backup/#procedure","title":"Procedure","text":"<ol> <li>Start a debug session for a control plane node:</li> </ol> <pre><code>oc debug node/&lt;node_name&gt;\n</code></pre> <ol> <li>Change your root directory to /host:</li> </ol> <pre><code>chroot /host\n</code></pre> <ol> <li> <p>If the cluster-wide proxy is enabled, be sure that you have exported the NO_PROXY, HTTP_PROXY, and HTTPS_PROXY environment variables. (optional)</p> </li> <li> <p>Run the cluster-backup.sh script and pass in the location to save the backup to.</p> </li> </ol> <pre><code>sh-4.4# /usr/local/bin/cluster-backup.sh /home/core/assets/backup\n</code></pre> <p><code>Example script output</code></p> <pre><code>found latest kube-apiserver: /etc/kubernetes/static-pod-resources/kube-apiserver-pod-6\nfound latest kube-controller-manager: /etc/kubernetes/static-pod-resources/kube-controller-manager-pod-7\nfound latest kube-scheduler: /etc/kubernetes/static-pod-resources/kube-scheduler-pod-6\nfound latest etcd: /etc/kubernetes/static-pod-resources/etcd-pod-3\nede95fe6b88b87ba86a03c15e669fb4aa5bf0991c180d3c6895ce72eaade54a1\netcdctl version: 3.4.14\nAPI version: 3.4\n{\"level\":\"info\",\"ts\":1624647639.0188997,\"caller\":\"snapshot/v3_snapshot.go:119\",\"msg\":\"created temporary db file\",\"path\":\"/home/core/assets/backup/snapshot_2021-06-25_190035.db.part\"}\n{\"level\":\"info\",\"ts\":\"2021-06-25T19:00:39.030Z\",\"caller\":\"clientv3/maintenance.go:200\",\"msg\":\"opened snapshot stream; downloading\"}\n{\"level\":\"info\",\"ts\":1624647639.0301006,\"caller\":\"snapshot/v3_snapshot.go:127\",\"msg\":\"fetching snapshot\",\"endpoint\":\"https://10.0.0.5:2379\"}\n{\"level\":\"info\",\"ts\":\"2021-06-25T19:00:40.215Z\",\"caller\":\"clientv3/maintenance.go:208\",\"msg\":\"completed snapshot read; closing\"}\n{\"level\":\"info\",\"ts\":1624647640.6032252,\"caller\":\"snapshot/v3_snapshot.go:142\",\"msg\":\"fetched snapshot\",\"endpoint\":\"https://10.0.0.5:2379\",\"size\":\"114 MB\",\"took\":1.584090459}\n{\"level\":\"info\",\"ts\":1624647640.6047094,\"caller\":\"snapshot/v3_snapshot.go:152\",\"msg\":\"saved\",\"path\":\"/home/core/assets/backup/snapshot_2021-06-25_190035.db\"}\nSnapshot saved at /home/core/assets/backup/snapshot_2021-06-25_190035.db\n{\"hash\":3866667823,\"revision\":31407,\"totalKey\":12828,\"totalSize\":114446336}\nsnapshot db and kube resources are successfully saved to /home/core/assets/backup\n</code></pre>"},{"location":"openshift/backup-restore/etcd-backup/#automated-steps-to-run-and-copy-the-file-locally","title":"automated steps to run and copy the file locally","text":"<pre><code>backupdir=/tmp/backup-etcd/\n\nservername=$(oc get nodes |grep -i master|awk '{print $1}'|head -1)\n\n[ -d $backupdir/${servername} ] || mkdir -p $backupdir/${servername}\n\nexecout=$(oc debug -t node/$servername -- chroot /host bash -c '/usr/local/bin/cluster-backup.sh /home/core/assets/backup') \n# filename=$(echo $execout | awk -F'path\":\"' '{print $2}' | awk -F'\"' '{print $1}')   #&lt;--- not working\nsleep 20\n\noc debug -t node/$servername -- chroot /host bash -c 'chown core:core /home/core/assets/backup/*.db'\n\nscp -rp core@$servername:${filename} $backupdir/${servername}/\n</code></pre>"},{"location":"openshift/backup-restore/etcd-backup/#reference","title":"Reference","text":"<ul> <li>ETCD backup and restore</li> </ul>"},{"location":"openshift/backup-restore/etcd-restore/","title":"Replacing an unhealthy etcd member","text":""},{"location":"openshift/backup-restore/etcd-restore/#reference","title":"Reference","text":"<ul> <li>ETCD backup and restore</li> </ul>"},{"location":"openshift/deployment/readme/","title":"Deployment","text":"<p>update</p>"},{"location":"openshift/maintenace/reboot-nodes/","title":"Rebooting nodes on the openshift cluster","text":"<p>steps to show, boot on master, worker/gateway and storage. </p>"},{"location":"openshift/maintenace/reboot-nodes/#master-node-reboot","title":"Master node reboot","text":"<p>1) login to right cluster using cluster-admin based role. </p> <pre><code>[root@dom14npv101-infra-manager ~ management]# source  /root/raj/managementrc\nWARNING: Using insecure TLS client config. Setting this option is not supported!\n\nLogin successful.\n\nYou have access to 99 projects, the list has been suppressed. You can list all projects with 'oc projects'\n\nUsing project \"default\".\n[root@dom14npv101-infra-manager ~ management]# \n</code></pre> <p>2) get the list of master nodes and also check the node status. </p> <pre><code>[root@dom14npv101-infra-manager ~ management]# oc get nodes |grep -i master\nncpvnpvmgt-master-101.ncpvnpvmgt.pnwlab.nsn-rdnet.net    Ready    control-plane,master,monitor       32d   v1.29.10+67d3387\nncpvnpvmgt-master-102.ncpvnpvmgt.pnwlab.nsn-rdnet.net    Ready    control-plane,master,monitor       32d   v1.29.10+67d3387\nncpvnpvmgt-master-103.ncpvnpvmgt.pnwlab.nsn-rdnet.net    Ready    control-plane,master,monitor       32d   v1.29.10+67d3387\n[root@dom14npv101-infra-manager ~ management]#\n</code></pre> <p>3) drain the node completely </p> <pre><code>node=ncpvnpvmgt-master-101.ncpvnpvmgt.pnwlab.nsn-rdnet.net\noc adm drain $node --ignore-daemonsets --delete-emptydir-data --force \noc adm drain $node --ignore-daemonsets --delete-emptydir-data --force --disable-eviction \n</code></pre> <p>3.1) Check for any pod got stuck at termination phase or failed to evicate due to pod distrubation policy etc. </p> <p>3.2) those stuck pods should be fully removed/deleted to move to next step here. </p> <p>4) trigger the shutdown now. </p> <pre><code>oc debug node/$node -- chroot /host shutdown -r +1 \n</code></pre> <p>5) once this server came up, waiting for server to be in ready state </p> <pre><code>oc wait --for=condition=Ready node/$node --timeout=800s \n</code></pre> <p>6) uncordon the node and make it schedulable now. </p> <pre><code>oc adm uncordon $node \n</code></pre>"},{"location":"openshift/maintenace/reboot-nodes/#workergateway-node-reboot","title":"Worker/Gateway node reboot","text":"<p>1) login to right cluster using cluster-admin based role. </p> <pre><code>[root@dom14npv101-infra-manager ~ management]# source  /root/raj/managementrc\nWARNING: Using insecure TLS client config. Setting this option is not supported!\n\nLogin successful.\n\nYou have access to 99 projects, the list has been suppressed. You can list all projects with 'oc projects'\n\nUsing project \"default\".\n[root@dom14npv101-infra-manager ~ management]# \n</code></pre> <p>2) get the list of worker/gateway nodes and also check the node status. </p> <pre><code>[root@dom14npv101-infra-manager ~ management]# oc get nodes |grep -i master\nncpvnpvmgt-master-101.ncpvnpvmgt.pnwlab.nsn-rdnet.net    Ready    control-plane,master,monitor       32d   v1.29.10+67d3387\nncpvnpvmgt-master-102.ncpvnpvmgt.pnwlab.nsn-rdnet.net    Ready    control-plane,master,monitor       32d   v1.29.10+67d3387\nncpvnpvmgt-master-103.ncpvnpvmgt.pnwlab.nsn-rdnet.net    Ready    control-plane,master,monitor       32d   v1.29.10+67d3387\n[root@dom14npv101-infra-manager ~ management]#\n</code></pre> <p>3) drain the node completely </p> <pre><code>node=ncpvnpvmgt-master-101.ncpvnpvmgt.pnwlab.nsn-rdnet.net\noc adm drain $node --ignore-daemonsets --delete-emptydir-data --force \noc adm drain $node --ignore-daemonsets --delete-emptydir-data --force --disable-eviction \n</code></pre> <p>3.1) Check for any pod got stuck at termination phase or failed to evicate due to pod distrubation policy etc. </p> <p>3.2) those stuck pods should be fully removed/deleted to move to next step here. </p> <p>4) trigger the shutdown now. </p> <pre><code>oc debug node/$node -- chroot /host shutdown -r +1 \n</code></pre> <p>5) once this server came up, waiting for server to be in ready state </p> <pre><code>oc wait --for=condition=Ready node/$node --timeout=800s \n</code></pre> <p>6) uncordon the node and make it schedulable now. </p> <pre><code>oc adm uncordon $node \n</code></pre>"},{"location":"openshift/maintenace/reboot-nodes/#storage-node-reboot","title":"Storage node reboot","text":"<p>1) login to right cluster using cluster-admin based role. </p> <pre><code>[root@dom14npv101-infra-manager ~ management]# source  /root/raj/managementrc\nWARNING: Using insecure TLS client config. Setting this option is not supported!\n\nLogin successful.\n\nYou have access to 99 projects, the list has been suppressed. You can list all projects with 'oc projects'\n\nUsing project \"default\".\n[root@dom14npv101-infra-manager ~ management]# \n</code></pre> <p>2) get the list of storage nodes and also check the ceph status. </p> <pre><code>[root@dom14npv101-infra-manager ~ vlabrc]# oc get no |grep -i storage\nncpvnpvlab1-storage-101.ncpvnpvlab1.pnwlab.nsn-rdnet.net   Ready                      storage,worker                     24d   v1.29.10+67d3387\nncpvnpvlab1-storage-102.ncpvnpvlab1.pnwlab.nsn-rdnet.net   Ready                      storage,worker                     26d   v1.29.10+67d3387\nncpvnpvlab1-storage-103.ncpvnpvlab1.pnwlab.nsn-rdnet.net   Ready                      storage,worker                     26d   v1.29.10+67d3387\nncpvnpvlab1-storage-201.ncpvnpvlab1.pnwlab.nsn-rdnet.net   Ready                      storage,worker                     24d   v1.29.10+67d3387\nncpvnpvlab1-storage-202.ncpvnpvlab1.pnwlab.nsn-rdnet.net   Ready                      storage,worker                     26d   v1.29.10+67d3387\nncpvnpvlab1-storage-203.ncpvnpvlab1.pnwlab.nsn-rdnet.net   Ready                      storage,worker                     26d   v1.29.10+67d3387\n[root@dom14npv101-infra-manager ~ vlabrc]# \n\n[root@dom14npv101-infra-manager ~ vlabrc]# oc exec -it $(oc get pod -n openshift-storage -l app=rook-ceph-operator -o name) -n openshift-storage -- ceph -s -c /var/lib/rook/openshift-storage/openshift-storage.config\n</code></pre> <p>3) define a variable called and value as respective storage node, easy run of reboot</p> <pre><code>[root@dom14npv101-infra-manager ~ vlabrc]# node=ncpvnpvlab1-storage-203.ncpvnpvlab1.pnwlab.nsn-rdnet.net\n</code></pre> <p>4) get the list ceph storage related pods </p> <pre><code>[root@dom14npv101-infra-manager ~ vlabrc]# oc -n openshift-storage get po -o wide | grep -e mon -e osd | grep  ${node}|grep -iv complete|awk '{print $1}'\nrook-ceph-osd-12-589c6485c9-ptqs7\nrook-ceph-osd-16-74cbc54b4f-tg8lw\nrook-ceph-osd-2-7fcc4ff664-hjc2w\nrook-ceph-osd-26-67d86fbf5d-lqdv8\nrook-ceph-osd-30-78fb4f588b-474jn\nrook-ceph-osd-37-bd549c676-jn5ld\nrook-ceph-osd-40-5b4ddb6d7d-7qd8z\nrook-ceph-osd-6-5b64cf8d49-n55zf\n[root@dom14npv101-infra-manager ~ vlabrc]# \n</code></pre> <p>5) use notepad++ to create these following commands to remove those pods on that host. </p> <pre><code>[root@dom14npv101-infra-manager ~ vlabrc]# oc -n openshift-storage scale deployment rook-ceph-osd-12 --replicas=0\noc -n openshift-storage scale deployment rook-ceph-osd-16 --replicas=0\noc -n openshift-storage scale deployment rook-ceph-osd-2  --replicas=0\noc -n openshift-storage scale deployment rook-ceph-osd-26 --replicas=0\noc -n openshift-storage scale deployment rook-ceph-osd-30 --replicas=0\noc -n openshift-storage scale deployment rook-ceph-osd-37 --replicas=0\noc -n openshift-storage scale deployment rook-ceph-osd-40 --replicas=0\noc -n openshift-storage scale deployment rook-ceph-osd-6  --replicas=0\ndeployment.apps/rook-ceph-osd-12 scaled\ndeployment.apps/rook-ceph-osd-16 scaled\ndeployment.apps/rook-ceph-osd-2 scaled\ndeployment.apps/rook-ceph-osd-26 scaled\ndeployment.apps/rook-ceph-osd-30 scaled\ndeployment.apps/rook-ceph-osd-37 scaled\ndeployment.apps/rook-ceph-osd-40 scaled\ndeployment.apps/rook-ceph-osd-6 scaled\n[root@dom14npv101-infra-manager ~ vlabrc]#\n</code></pre> <p>6) Completely drain that storage node, using oc adm command </p> <pre><code>[root@dom14npv101-infra-manager ~ vlabrc]#oc adm drain ${node} --delete-emptydir-data --ignore-daemonsets=true --timeout=500s --force\nnode/ncpvnpvlab1-storage-203.ncpvnpvlab1.pnwlab.nsn-rdnet.net cordoned\nWarning: ignoring DaemonSet-managed Pods: openshift-cluster-node-tuning-operator/tuned-kqqfn, openshift-dns/dns-default-27nmr, openshift-dns/node-resolver-4dcng, openshift-image-registry/node-ca-2xtbk, openshift-ingress-canary/ingress-canary-285m2, openshift-local-storage/diskmaker-discovery-zmmwj, openshift-local-storage/diskmaker-manager-dvpjq, openshift-logging/collector-7g5zv, openshift-machine-config-operator/machine-config-daemon-c7c4v, openshift-monitoring/node-exporter-gsh5l, openshift-multus/multus-6q7c8, openshift-multus/multus-additional-cni-plugins-pwq85, openshift-multus/network-metrics-daemon-f6flp, openshift-multus/whereabouts-reconciler-fjpmv, openshift-network-diagnostics/network-check-target-9h7cj, openshift-network-operator/iptables-alerter-km8jq, openshift-nmstate/nmstate-handler-6hmz9, openshift-operators/istio-cni-node-v2-5-4qpnk, openshift-ovn-kubernetes/ovnkube-node-w672f, openshift-storage/csi-cephfsplugin-ck4mf, openshift-storage/csi-rbdplugin-gkmjm\nevicting pod openshift-storage/rook-ceph-osd-6-5b64cf8d49-n55zf\nevicting pod openshift-compliance/openscap-pod-24dd5998a72563f75be98757ffbe02e424df617e\nevicting pod openshift-storage/rook-ceph-crashcollector-9c7a6e51d7dc201a808c754612468a82-j84n5\nevicting pod openshift-storage/rook-ceph-mds-ocs-storagecluster-cephfilesystem-b-78f7bbf8bn4rl\nevicting pod openshift-storage/rook-ceph-mgr-b-5468b7cf-nx4d4\nevicting pod openshift-storage/rook-ceph-osd-40-5b4ddb6d7d-7qd8z\nevicting pod openshift-storage/rook-ceph-exporter-9c7a6e51d7dc201a808c754612468a82-c854b4r2tn4\nevicting pod openshift-compliance/openscap-pod-5f54688367da49304dfd83a2e0d564582b073f2b\npod/openscap-pod-5f54688367da49304dfd83a2e0d564582b073f2b evicted\npod/openscap-pod-24dd5998a72563f75be98757ffbe02e424df617e evicted\npod/rook-ceph-crashcollector-9c7a6e51d7dc201a808c754612468a82-j84n5 evicted\npod/rook-ceph-mgr-b-5468b7cf-nx4d4 evicted\npod/rook-ceph-exporter-9c7a6e51d7dc201a808c754612468a82-c854b4r2tn4 evicted\npod/rook-ceph-mds-ocs-storagecluster-cephfilesystem-b-78f7bbf8bn4rl evicted\nnode/ncpvnpvlab1-storage-203.ncpvnpvlab1.pnwlab.nsn-rdnet.net drained\n[root@dom14npv101-infra-manager ~ vlabrc]#\n</code></pre> <p>7) reboot the respective storage node using following command. </p> <pre><code>[root@dom14npv101-infra-manager ~ vlabrc]#oc debug node/${node} -- chroot /host systemctl reboot \n</code></pre> <p>8) waiting for node to be fully up. then check the kubelet status </p> <pre><code>[root@dom14npv101-infra-manager ~ vlabrc]# oc get no |grep -i storage\nncpvnpvlab1-storage-101.ncpvnpvlab1.pnwlab.nsn-rdnet.net   Ready                      storage,worker                     24d   v1.29.10+67d3387\nncpvnpvlab1-storage-102.ncpvnpvlab1.pnwlab.nsn-rdnet.net   Ready                      storage,worker                     26d   v1.29.10+67d3387\nncpvnpvlab1-storage-103.ncpvnpvlab1.pnwlab.nsn-rdnet.net   Ready                      storage,worker                     26d   v1.29.10+67d3387\nncpvnpvlab1-storage-201.ncpvnpvlab1.pnwlab.nsn-rdnet.net   Ready                      storage,worker                     24d   v1.29.10+67d3387\nncpvnpvlab1-storage-202.ncpvnpvlab1.pnwlab.nsn-rdnet.net   Ready                      storage,worker                     26d   v1.29.10+67d3387\nncpvnpvlab1-storage-203.ncpvnpvlab1.pnwlab.nsn-rdnet.net   Ready,sechd-disable        storage,worker                     26d   v1.29.10+67d3387\n[root@dom14npv101-infra-manager ~ vlabrc]# \n</code></pre> <p>9) uncordon the storage node.</p> <pre><code>[root@dom14npv101-infra-manager ~ vlabrc]# oc adm uncordon ncpvnpvlab1-storage-203.ncpvnpvlab1.pnwlab.nsn-rdnet.net \n</code></pre> <p>10) check the ceph health and wait for all osd to be fully up.  give 10mins</p> <pre><code>[root@dom14npv101-infra-manager ~ vlabrc]# oc exec -it $(oc get pod -n openshift-storage -l app=rook-ceph-operator -o name) -n openshift-storage -- ceph -s -c /var/lib/rook/openshift-storage/openshift-storage.config\n</code></pre>"},{"location":"openshift/networking/metalb-troubleshooting/","title":"metalb configuration troubleshooting","text":"<p>1)  login to cluster </p> <pre><code>[root@ncputility ~ pancwl_rc]$ source /root/pancwlrc\nWARNING: Using insecure TLS client config. Setting this option is not supported!\n\nLogin successful.\n\nYou have access to 113 projects, the list has been suppressed. You can list all projects with 'oc projects'\n\nUsing project \"ncd01pan\".\n</code></pre> <p>2) check for <code>metallb-system</code> namespace exist</p> <pre><code>[root@ncputility ~ pancwl_rc]$ oc get ns | grep -i metallb\nmetallb-system                                     Active   26d\n[root@ncputility ~ pancwl_rc]$\n</code></pre> <p>3) check the status of the pods on the <code>metallb-system</code> namespace here. </p> <pre><code>[root@ncputility ~ pancwl_rc]$ oc get pods -n metallb-system\nNAME                                                   READY   STATUS    RESTARTS   AGE\ncontroller-5785bc85cb-qpk8h                            2/2     Running   0          24d\nmetallb-operator-controller-manager-7bf5d8978d-clpd7   1/1     Running   0          26d\nmetallb-operator-webhook-server-86784c6c8c-49ncp       1/1     Running   0          26d\nspeaker-dlrn5                                          6/6     Running   0          24d\nspeaker-g2g77                                          6/6     Running   0          24d\nspeaker-jzbw7                                          6/6     Running   0          24d\nspeaker-pjstl                                          6/6     Running   0          24d\n</code></pre> <p>4) check for <code>bfdprofile</code> on this cluster </p> <p>here transmit interval and receiver interval should be equal from local and remote end. </p> <pre><code> oc -n metallb-system get BFDProfile -o wide\nNAME                                PASSIVE MODE   TRANSMIT INTERVAL   RECEIVE INTERVAL   MULTIPLIER\nncp-metallb-oam-pa-hn-bfd-profile   true           300                 300                3\nncp-metallb-oam-pa-ni-bfd-profile   true           300                 300                3\nncp-metallb-oam-pa-pa-bfd-profile   true           300                 300                3\nncp-metallb-oam-pa-sv-bfd-profile   true           300                 300                3\n</code></pre> <p>5) make sure, desination having the backward route configured here .</p> <pre><code>[root@ncputility ~ pancwl_rc]$ oc -n metallb-system get nncp -o wide\nNAME                                           STATUS      REASON\nbackward-route-for-oam-pa-pa-metallb-vlan104   Available   SuccessfullyConfigured\nncp-metallb-oam-pa-hn-route-for-switches-105   Available   SuccessfullyConfigured\nncp-metallb-oam-pa-ni-route-for-switches-107   Available   SuccessfullyConfigured\nncp-metallb-oam-pa-pa-route-for-switches-104   Available   SuccessfullyConfigured\nncp-metallb-oam-pa-sv-route-for-switches-106   Available   SuccessfullyConfigured\ntenant-bond-bgp-oam-vlan104-gateway-0          Available   SuccessfullyConfigured\ntenant-bond-bgp-oam-vlan104-gateway-1          Available   SuccessfullyConfigured\ntenant-bond-bgp-oam-vlan104-gateway-2          Available   SuccessfullyConfigured\n** output omitted **\ntenantvlan-373                                 Available   SuccessfullyConfigured\ntenantvlan-374                                 Available   SuccessfullyConfigured\n</code></pre> <p>6) check for <code>ipaddresspool</code> exist here, so that application can create their <code>service</code> as <code>loadbalancer</code> here . </p> <pre><code>[root@ncputility ~ pancwl_rc]$ oc -n metallb-system get IPAddressPool -o wide\nNAME                                AUTO ASSIGN   AVOID BUGGY IPS   ADDRESSES\nncp-metallb-oam-pa-hn-addresspool   false         false             [\"10.89.147.128/28\"]\nncp-metallb-oam-pa-ni-addresspool   false         false             [\"10.86.10.16/28\"]\nncp-metallb-oam-pa-pa-addresspool   false         false             [\"10.89.101.128/27\",\"10.89.97.208/28\"]\nncp-metallb-oam-pa-sv-addresspool   false         false             [\"10.85.186.240/28\"]\n</code></pre> <p>7) check for <code>bgppeer</code> are up on the metallb speakers thats important for this communication</p> <pre><code>[root@ncputility ~ pancwl_rc]$ oc -n metallb-system get BGPPeer -o wide\nNAME                               ADDRESS         ASN          BFD PROFILE                         MULTI HOPS\nncp-metallb-oam-pa-hn-bgp-peer-1   10.89.147.194   4200000320   ncp-metallb-oam-pa-hn-bfd-profile\nncp-metallb-oam-pa-hn-bgp-peer-2   10.89.147.195   4200000320   ncp-metallb-oam-pa-hn-bfd-profile\nncp-metallb-oam-pa-ni-bgp-peer-1   10.86.10.98     4200000320   ncp-metallb-oam-pa-ni-bfd-profile\nncp-metallb-oam-pa-ni-bgp-peer-2   10.86.10.99     4200000320   ncp-metallb-oam-pa-ni-bfd-profile\nncp-metallb-oam-pa-pa-bgp-peer-1   10.89.97.162    4200000320   ncp-metallb-oam-pa-pa-bfd-profile\nncp-metallb-oam-pa-pa-bgp-peer-2   10.89.97.163    4200000320   ncp-metallb-oam-pa-pa-bfd-profile\nncp-metallb-oam-pa-sv-bgp-peer-1   10.85.187.34    4200000320   ncp-metallb-oam-pa-sv-bfd-profile\nncp-metallb-oam-pa-sv-bgp-peer-2   10.85.187.35    4200000320   ncp-metallb-oam-pa-sv-bfd-profile\n</code></pre> <p>8) check for <code>BGPAdvertisement</code> are created on this cluster and it should be in the <code>metallb-system</code> namespace. </p> <pre><code>[root@ncputility ~ pancwl_rc]$ oc -n metallb-system get BGPAdvertisement -o wide\nNAME                                      IPADDRESSPOOLS                          IPADDRESSPOOL SELECTORS   PEERS                                                                     NODE SELECTORS\nncp-metallb-oam-pa-hn-bgp-advertisement   [\"ncp-metallb-oam-pa-hn-addresspool\"]                             [\"ncp-metallb-oam-pa-hn-bgp-peer-1\",\"ncp-metallb-oam-pa-hn-bgp-peer-2\"]\nncp-metallb-oam-pa-ni-bgp-advertisement   [\"ncp-metallb-oam-pa-ni-addresspool\"]                             [\"ncp-metallb-oam-pa-ni-bgp-peer-1\",\"ncp-metallb-oam-pa-ni-bgp-peer-2\"]\nncp-metallb-oam-pa-pa-bgp-advertisement   [\"ncp-metallb-oam-pa-pa-addresspool\"]                             [\"ncp-metallb-oam-pa-pa-bgp-peer-1\",\"ncp-metallb-oam-pa-pa-bgp-peer-2\"]\nncp-metallb-oam-pa-sv-bgp-advertisement   [\"ncp-metallb-oam-pa-sv-addresspool\"]                             [\"ncp-metallb-oam-pa-sv-bgp-peer-1\",\"ncp-metallb-oam-pa-sv-bgp-peer-2\"]\n</code></pre> <p>10) No, error from the container logs on this namespace .</p> <pre><code>[root@ncputility ~ pancwl_rc]$ oc -n metallb-system logs -l component=speaker\nDefaulted container \"speaker\" out of: speaker, frr, reloader, frr-metrics, kube-rbac-proxy, kube-rbac-proxy-frr, cp-frr-files (init), cp-reloader (init), cp-metrics (init)\nDefaulted container \"speaker\" out of: speaker, frr, reloader, frr-metrics, kube-rbac-proxy, kube-rbac-proxy-frr, cp-frr-files (init), cp-reloader (init), cp-metrics (init)\nDefaulted container \"speaker\" out of: speaker, frr, reloader, frr-metrics, kube-rbac-proxy, kube-rbac-proxy-frr, cp-frr-files (init), cp-reloader (init), cp-metrics (init)\nDefaulted container \"speaker\" out of: speaker, frr, reloader, frr-metrics, kube-rbac-proxy, kube-rbac-proxy-frr, cp-frr-files (init), cp-reloader (init), cp-metrics (init)\n{\"caller\":\"node_controller.go:46\",\"controller\":\"NodeReconciler\",\"level\":\"info\",\"start reconcile\":\"/gateway4.panclypcwl01.mnc020.mcc714\",\"ts\":\"2025-03-31T04:44:07Z\"}\n{\"caller\":\"node_controller.go:69\",\"controller\":\"NodeReconciler\",\"end reconcile\":\"/gateway4.panclypcwl01.mnc020.mcc714\",\"level\":\"info\",\"ts\":\"2025-03-31T04:44:07Z\"}\n{\"caller\":\"node_controller.go:46\",\"controller\":\"NodeReconciler\",\"level\":\"info\",\"start reconcile\":\"/gateway3.panclypcwl01.mnc020.mcc714\",\"ts\":\"2025-03-31T04:44:12Z\"}\n{\"caller\":\"node_controller.go:69\",\"controller\":\"NodeReconciler\",\"end reconcile\":\"/gateway3.panclypcwl01.mnc020.mcc714\",\"level\":\"info\",\"ts\":\"2025-03-31T04:44:12Z\"}\n{\"caller\":\"node_controller.go:46\",\"controller\":\"NodeReconciler\",\"level\":\"info\",\"start reconcile\":\"/appworker16.panclypcwl01.mnc020.mcc714\",\"ts\":\"2025-03-31T04:44:21Z\"}\n{\"caller\":\"node_controller.go:69\",\"controller\":\"NodeReconciler\",\"end reconcile\":\"/appworker16.panclypcwl01.mnc020.mcc714\",\"level\":\"info\",\"ts\":\"2025-03-31T04:44:21Z\"}\n{\"caller\":\"node_controller.go:46\",\"controller\":\"NodeReconciler\",\"level\":\"info\",\"start reconcile\":\"/appworker20.panclypcwl01.mnc020.mcc714\",\"ts\":\"2025-03-31T04:44:34Z\"}\n{\"caller\":\"node_controller.go:69\",\"controller\":\"NodeReconciler\",\"end reconcile\":\"/appworker20.panclypcwl01.mnc020.mcc714\",\"level\":\"info\",\"ts\":\"2025-03-31T04:44:34Z\"}\n{\"caller\":\"node_controller.go:46\",\"controller\":\"NodeReconciler\",\"level\":\"info\",\"start reconcile\":\"/appworker28.panclypcwl01.mnc020.mcc714\",\"ts\":\"2025-03-31T04:44:44Z\"}\n{\"caller\":\"node_controller.go:69\",\"controller\":\"NodeReconciler\",\"end reconcile\":\"/appworker28.panclypcwl01.mnc020.mcc714\",\"level\":\"info\",\"ts\":\"2025-03-31T04:44:44Z\"}\n{\"caller\":\"node_controller.go:69\",\"controller\":\"NodeReconciler\",\"end reconcile\":\"/gateway4.panclypcwl01.mnc020.mcc714\",\"level\":\"info\",\"ts\":\"2025-03-31T04:44:07Z\"}\n{\"caller\":\"node_controller.go:46\",\"controller\":\"NodeReconciler\",\"level\":\"info\",\"start reconcile\":\"/gateway3.panclypcwl01.mnc020.mcc714\",\"ts\":\"2025-03-31T04:44:12Z\"}\n{\"caller\":\"speakerlist.go:274\",\"level\":\"info\",\"msg\":\"triggering discovery\",\"op\":\"memberDiscovery\",\"ts\":\"2025-03-31T04:44:12Z\"}\n{\"caller\":\"node_controller.go:69\",\"controller\":\"NodeReconciler\",\"end reconcile\":\"/gateway3.panclypcwl01.mnc020.mcc714\",\"level\":\"info\",\"ts\":\"2025-03-31T04:44:12Z\"}\n{\"caller\":\"node_controller.go:46\",\"controller\":\"NodeReconciler\",\"level\":\"info\",\"start reconcile\":\"/appworker16.panclypcwl01.mnc020.mcc714\",\"ts\":\"2025-03-31T04:44:21Z\"}\n{\"caller\":\"node_controller.go:69\",\"controller\":\"NodeReconciler\",\"end reconcile\":\"/appworker16.panclypcwl01.mnc020.mcc714\",\"level\":\"info\",\"ts\":\"2025-03-31T04:44:21Z\"}\n{\"caller\":\"node_controller.go:46\",\"controller\":\"NodeReconciler\",\"level\":\"info\",\"start reconcile\":\"/appworker20.panclypcwl01.mnc020.mcc714\",\"ts\":\"2025-03-31T04:44:34Z\"}\n{\"caller\":\"node_controller.go:69\",\"controller\":\"NodeReconciler\",\"end reconcile\":\"/appworker20.panclypcwl01.mnc020.mcc714\",\"level\":\"info\",\"ts\":\"2025-03-31T04:44:34Z\"}\n{\"caller\":\"node_controller.go:46\",\"controller\":\"NodeReconciler\",\"level\":\"info\",\"start reconcile\":\"/appworker28.panclypcwl01.mnc020.mcc714\",\"ts\":\"2025-03-31T04:44:44Z\"}\n{\"caller\":\"node_controller.go:69\",\"controller\":\"NodeReconciler\",\"end reconcile\":\"/appworker28.panclypcwl01.mnc020.mcc714\",\"level\":\"info\",\"ts\":\"2025-03-31T04:44:44Z\"}\n{\"caller\":\"node_controller.go:46\",\"controller\":\"NodeReconciler\",\"level\":\"info\",\"start reconcile\":\"/gateway4.panclypcwl01.mnc020.mcc714\",\"ts\":\"2025-03-31T04:44:07Z\"}\n{\"caller\":\"node_controller.go:69\",\"controller\":\"NodeReconciler\",\"end reconcile\":\"/gateway4.panclypcwl01.mnc020.mcc714\",\"level\":\"info\",\"ts\":\"2025-03-31T04:44:07Z\"}\n{\"caller\":\"node_controller.go:46\",\"controller\":\"NodeReconciler\",\"level\":\"info\",\"start reconcile\":\"/gateway3.panclypcwl01.mnc020.mcc714\",\"ts\":\"2025-03-31T04:44:12Z\"}\n{\"caller\":\"node_controller.go:69\",\"controller\":\"NodeReconciler\",\"end reconcile\":\"/gateway3.panclypcwl01.mnc020.mcc714\",\"level\":\"info\",\"ts\":\"2025-03-31T04:44:12Z\"}\n{\"caller\":\"node_controller.go:46\",\"controller\":\"NodeReconciler\",\"level\":\"info\",\"start reconcile\":\"/appworker16.panclypcwl01.mnc020.mcc714\",\"ts\":\"2025-03-31T04:44:21Z\"}\n{\"caller\":\"node_controller.go:69\",\"controller\":\"NodeReconciler\",\"end reconcile\":\"/appworker16.panclypcwl01.mnc020.mcc714\",\"level\":\"info\",\"ts\":\"2025-03-31T04:44:21Z\"}\n{\"caller\":\"node_controller.go:46\",\"controller\":\"NodeReconciler\",\"level\":\"info\",\"start reconcile\":\"/appworker20.panclypcwl01.mnc020.mcc714\",\"ts\":\"2025-03-31T04:44:34Z\"}\n{\"caller\":\"node_controller.go:69\",\"controller\":\"NodeReconciler\",\"end reconcile\":\"/appworker20.panclypcwl01.mnc020.mcc714\",\"level\":\"info\",\"ts\":\"2025-03-31T04:44:34Z\"}\n{\"caller\":\"node_controller.go:46\",\"controller\":\"NodeReconciler\",\"level\":\"info\",\"start reconcile\":\"/appworker28.panclypcwl01.mnc020.mcc714\",\"ts\":\"2025-03-31T04:44:44Z\"}\n{\"caller\":\"node_controller.go:69\",\"controller\":\"NodeReconciler\",\"end reconcile\":\"/appworker28.panclypcwl01.mnc020.mcc714\",\"level\":\"info\",\"ts\":\"2025-03-31T04:44:44Z\"}\n{\"caller\":\"speakerlist.go:274\",\"level\":\"info\",\"msg\":\"triggering discovery\",\"op\":\"memberDiscovery\",\"ts\":\"2025-03-31T04:44:07Z\"}\n{\"caller\":\"node_controller.go:69\",\"controller\":\"NodeReconciler\",\"end reconcile\":\"/gateway4.panclypcwl01.mnc020.mcc714\",\"level\":\"info\",\"ts\":\"2025-03-31T04:44:07Z\"}\n{\"caller\":\"node_controller.go:46\",\"controller\":\"NodeReconciler\",\"level\":\"info\",\"start reconcile\":\"/gateway3.panclypcwl01.mnc020.mcc714\",\"ts\":\"2025-03-31T04:44:12Z\"}\n{\"caller\":\"node_controller.go:69\",\"controller\":\"NodeReconciler\",\"end reconcile\":\"/gateway3.panclypcwl01.mnc020.mcc714\",\"level\":\"info\",\"ts\":\"2025-03-31T04:44:12Z\"}\n{\"caller\":\"node_controller.go:46\",\"controller\":\"NodeReconciler\",\"level\":\"info\",\"start reconcile\":\"/appworker16.panclypcwl01.mnc020.mcc714\",\"ts\":\"2025-03-31T04:44:21Z\"}\n{\"caller\":\"node_controller.go:69\",\"controller\":\"NodeReconciler\",\"end reconcile\":\"/appworker16.panclypcwl01.mnc020.mcc714\",\"level\":\"info\",\"ts\":\"2025-03-31T04:44:21Z\"}\n{\"caller\":\"node_controller.go:46\",\"controller\":\"NodeReconciler\",\"level\":\"info\",\"start reconcile\":\"/appworker20.panclypcwl01.mnc020.mcc714\",\"ts\":\"2025-03-31T04:44:34Z\"}\n{\"caller\":\"node_controller.go:69\",\"controller\":\"NodeReconciler\",\"end reconcile\":\"/appworker20.panclypcwl01.mnc020.mcc714\",\"level\":\"info\",\"ts\":\"2025-03-31T04:44:34Z\"}\n{\"caller\":\"node_controller.go:46\",\"controller\":\"NodeReconciler\",\"level\":\"info\",\"start reconcile\":\"/appworker28.panclypcwl01.mnc020.mcc714\",\"ts\":\"2025-03-31T04:44:44Z\"}\n{\"caller\":\"node_controller.go:69\",\"controller\":\"NodeReconciler\",\"end reconcile\":\"/appworker28.panclypcwl01.mnc020.mcc714\",\"level\":\"info\",\"ts\":\"2025-03-31T04:44:44Z\"}\n</code></pre> <p>11)  just showing an backwards route here for comparison  and your destination should be exist here . </p> <pre><code>[root@ncputility ~ pancwl_rc]$ oc get nncp -A -o yaml\napiVersion: v1\nitems:\n- apiVersion: nmstate.io/v1\n  kind: NodeNetworkConfigurationPolicy\n  metadata:\n    annotations:\n      kubectl.kubernetes.io/last-applied-configuration: |\n        {\"apiVersion\":\"nmstate.io/v1\",\"kind\":\"NodeNetworkConfigurationPolicy\",\"metadata\":{\"annotations\":{},\"name\":\"backward-route-for-oam-pa-pa-metallb-vlan104\"},\"spec\":{\"desiredState\":{\"routes\":{\"config\":[{\"destination\":\"10.89.100.66/32\",\"metric\":150,\"next-hop-address\":\"10.89.97.161\",\"next-hop-interface\":\"vlan104\",\"table-id\":254},{\"destination\":\"10.89.27.4/32\",\"metric\":150,\"next-hop-address\":\"10.89.97.161\",\"next-hop-interface\":\"vlan104\",\"table-id\":254}]}},\"nodeSelector\":{\"node-role.kubernetes.io/gateway\":\"\"}}}\n      nmstate.io/webhook-mutating-timestamp: \"1743107464714866687\"\n    creationTimestamp: \"2025-03-27T20:31:04Z\"\n    generation: 1\n    name: backward-route-for-oam-pa-pa-metallb-vlan104\n    resourceVersion: \"36077334\"\n    uid: d72a67aa-44d0-403f-8ed5-29a49994eaf6\n  spec:\n    desiredState:\n      routes:\n        config:\n        - destination: 10.89.100.66/32\n          metric: 150\n          next-hop-address: 10.89.97.161\n          next-hop-interface: vlan104\n          table-id: 254\n        - destination: 10.89.27.4/32\n          metric: 150\n          next-hop-address: 10.89.97.161\n          next-hop-interface: vlan104\n          table-id: 254\n    nodeSelector:\n      node-role.kubernetes.io/gateway: \"\"\n  status:\n    conditions:\n    - lastHeartbeatTime: \"2025-03-27T20:32:10Z\"\n      lastTransitionTime: \"2025-03-27T20:32:10Z\"\n      message: 4/4 nodes successfully configured\n      reason: SuccessfullyConfigured\n      status: \"True\"\n      type: Available\n    - lastHeartbeatTime: \"2025-03-27T20:32:10Z\"\n      lastTransitionTime: \"2025-03-27T20:32:10Z\"\n      reason: SuccessfullyConfigured\n      status: \"False\"\n      type: Degraded\n    - lastHeartbeatTime: \"2025-03-27T20:32:10Z\"\n      lastTransitionTime: \"2025-03-27T20:32:10Z\"\n      reason: ConfigurationProgressing\n      status: \"False\"\n      type: Progressing\n    lastUnavailableNodeCountUpdate: \"2025-03-27T20:32:09Z\"\n</code></pre> <ol> <li>Check the status of local and remote configuration by login into the metallb-system namespace. </li> </ol> <p><code>bdf and bgp should be fully up</code></p> <pre><code>[root@ncputility ~ pancwl_rc]$ oc get -n metallb-system pods -l component=speaker -o wide\nNAME            READY   STATUS    RESTARTS   AGE   IP            NODE                                  NOMINATED NODE   READINESS GATES\nspeaker-dlrn5   6/6     Running   0          24d   10.89.96.18   gateway2.panclypcwl01.mnc020.mcc714   &lt;none&gt;           &lt;none&gt;\nspeaker-g2g77   6/6     Running   0          24d   10.89.96.19   gateway3.panclypcwl01.mnc020.mcc714   &lt;none&gt;           &lt;none&gt;\nspeaker-jzbw7   6/6     Running   0          24d   10.89.96.17   gateway1.panclypcwl01.mnc020.mcc714   &lt;none&gt;           &lt;none&gt;\nspeaker-pjstl   6/6     Running   0          24d   10.89.96.20   gateway4.panclypcwl01.mnc020.mcc714   &lt;none&gt;           &lt;none&gt;\n</code></pre> <pre><code>[root@ncputility ~ pancwl_rc]$ oc exec -n metallb-system  speaker-dlrn5 -c frr -- vtysh -c \"show running-config\"\nBuilding configuration...\n\nCurrent configuration:\n!\nfrr version 8.3.1\nfrr defaults traditional\nhostname gateway2.panclypcwl01.mnc020.mcc714\nlog file /etc/frr/frr.log informational\nlog timestamp precision 3\nservice integrated-vtysh-config\n!\nrouter bgp 4200000320\n no bgp ebgp-requires-policy\n no bgp default ipv4-unicast\n no bgp network import-check\n neighbor 10.85.187.34 remote-as 4200000320\n neighbor 10.85.187.34 bfd\n neighbor 10.85.187.34 bfd profile ncp-metallb-oam-pa-sv-bfd-profile\n neighbor 10.85.187.34 timers 30 90\n neighbor 10.85.187.35 remote-as 4200000320\n neighbor 10.85.187.35 bfd\n neighbor 10.85.187.35 bfd profile ncp-metallb-oam-pa-sv-bfd-profile\n neighbor 10.85.187.35 timers 30 90\n neighbor 10.86.10.98 remote-as 4200000320\n neighbor 10.86.10.98 bfd\n neighbor 10.86.10.98 bfd profile ncp-metallb-oam-pa-ni-bfd-profile\n neighbor 10.86.10.98 timers 30 90\n neighbor 10.86.10.99 remote-as 4200000320\n neighbor 10.86.10.99 bfd\n neighbor 10.86.10.99 bfd profile ncp-metallb-oam-pa-ni-bfd-profile\n neighbor 10.86.10.99 timers 30 90\n neighbor 10.89.97.162 remote-as 4200000320\n neighbor 10.89.97.162 bfd\n neighbor 10.89.97.162 bfd profile ncp-metallb-oam-pa-pa-bfd-profile\n neighbor 10.89.97.162 timers 30 90\n neighbor 10.89.97.163 remote-as 4200000320\n neighbor 10.89.97.163 bfd\n neighbor 10.89.97.163 bfd profile ncp-metallb-oam-pa-pa-bfd-profile\n neighbor 10.89.97.163 timers 30 90\n neighbor 10.89.147.194 remote-as 4200000320\n neighbor 10.89.147.194 bfd\n neighbor 10.89.147.194 bfd profile ncp-metallb-oam-pa-hn-bfd-profile\n neighbor 10.89.147.194 timers 30 90\n neighbor 10.89.147.195 remote-as 4200000320\n neighbor 10.89.147.195 bfd\n neighbor 10.89.147.195 bfd profile ncp-metallb-oam-pa-hn-bfd-profile\n neighbor 10.89.147.195 timers 30 90\n !\n address-family ipv4 unicast\n  network 10.89.97.210/32\n  neighbor 10.85.187.34 activate\n  neighbor 10.85.187.34 route-map 10.85.187.34-in in\n  neighbor 10.85.187.34 route-map 10.85.187.34-out out\n  neighbor 10.85.187.35 activate\n  neighbor 10.85.187.35 route-map 10.85.187.35-in in\n  neighbor 10.85.187.35 route-map 10.85.187.35-out out\n  neighbor 10.86.10.98 activate\n  neighbor 10.86.10.98 route-map 10.86.10.98-in in\n  neighbor 10.86.10.98 route-map 10.86.10.98-out out\n  neighbor 10.86.10.99 activate\n  neighbor 10.86.10.99 route-map 10.86.10.99-in in\n  neighbor 10.86.10.99 route-map 10.86.10.99-out out\n  neighbor 10.89.97.162 activate\n  neighbor 10.89.97.162 route-map 10.89.97.162-in in\n  neighbor 10.89.97.162 route-map 10.89.97.162-out out\n  neighbor 10.89.97.163 activate\n  neighbor 10.89.97.163 route-map 10.89.97.163-in in\n  neighbor 10.89.97.163 route-map 10.89.97.163-out out\n  neighbor 10.89.147.194 activate\n  neighbor 10.89.147.194 route-map 10.89.147.194-in in\n  neighbor 10.89.147.194 route-map 10.89.147.194-out out\n  neighbor 10.89.147.195 activate\n  neighbor 10.89.147.195 route-map 10.89.147.195-in in\n  neighbor 10.89.147.195 route-map 10.89.147.195-out out\n exit-address-family\n !\n address-family ipv6 unicast\n  neighbor 10.85.187.34 activate\n  neighbor 10.85.187.34 route-map 10.85.187.34-in in\n  neighbor 10.85.187.34 route-map 10.85.187.34-out out\n  neighbor 10.85.187.35 activate\n  neighbor 10.85.187.35 route-map 10.85.187.35-in in\n  neighbor 10.85.187.35 route-map 10.85.187.35-out out\n  neighbor 10.86.10.98 activate\n  neighbor 10.86.10.98 route-map 10.86.10.98-in in\n  neighbor 10.86.10.98 route-map 10.86.10.98-out out\n  neighbor 10.86.10.99 activate\n  neighbor 10.86.10.99 route-map 10.86.10.99-in in\n  neighbor 10.86.10.99 route-map 10.86.10.99-out out\n  neighbor 10.89.97.162 activate\n  neighbor 10.89.97.162 route-map 10.89.97.162-in in\n  neighbor 10.89.97.162 route-map 10.89.97.162-out out\n  neighbor 10.89.97.163 activate\n  neighbor 10.89.97.163 route-map 10.89.97.163-in in\n  neighbor 10.89.97.163 route-map 10.89.97.163-out out\n  neighbor 10.89.147.194 activate\n  neighbor 10.89.147.194 route-map 10.89.147.194-in in\n  neighbor 10.89.147.194 route-map 10.89.147.194-out out\n  neighbor 10.89.147.195 activate\n  neighbor 10.89.147.195 route-map 10.89.147.195-in in\n  neighbor 10.89.147.195 route-map 10.89.147.195-out out\n exit-address-family\nexit\n!\nip prefix-list 10.89.97.162-pl-ipv4 seq 1 permit 10.89.97.210/32\nip prefix-list 10.89.97.163-pl-ipv4 seq 1 permit 10.89.97.210/32\nip prefix-list 10.85.187.34-pl-ipv4 seq 1 deny any\nip prefix-list 10.85.187.35-pl-ipv4 seq 1 deny any\nip prefix-list 10.86.10.98-pl-ipv4 seq 1 deny any\nip prefix-list 10.86.10.99-pl-ipv4 seq 1 deny any\nip prefix-list 10.89.147.194-pl-ipv4 seq 1 deny any\nip prefix-list 10.89.147.195-pl-ipv4 seq 1 deny any\n!\nipv6 prefix-list 10.89.97.162-pl-ipv4 seq 2 deny any\nipv6 prefix-list 10.89.97.163-pl-ipv4 seq 2 deny any\nipv6 prefix-list 10.85.187.34-pl-ipv4 seq 2 deny any\nipv6 prefix-list 10.85.187.35-pl-ipv4 seq 2 deny any\nipv6 prefix-list 10.86.10.98-pl-ipv4 seq 2 deny any\nipv6 prefix-list 10.86.10.99-pl-ipv4 seq 2 deny any\nipv6 prefix-list 10.89.147.194-pl-ipv4 seq 2 deny any\nipv6 prefix-list 10.89.147.195-pl-ipv4 seq 2 deny any\n!\nroute-map 10.85.187.34-in deny 20\nexit\n!\nroute-map 10.85.187.34-out permit 1\n match ip address prefix-list 10.85.187.34-pl-ipv4\nexit\n!\nroute-map 10.85.187.34-out permit 2\n match ipv6 address prefix-list 10.85.187.34-pl-ipv4\nexit\n!\nroute-map 10.85.187.35-in deny 20\nexit\n!\nroute-map 10.85.187.35-out permit 1\n match ip address prefix-list 10.85.187.35-pl-ipv4\nexit\n!\nroute-map 10.85.187.35-out permit 2\n match ipv6 address prefix-list 10.85.187.35-pl-ipv4\nexit\n!\nroute-map 10.86.10.98-in deny 20\nexit\n!\nroute-map 10.86.10.98-out permit 1\n match ip address prefix-list 10.86.10.98-pl-ipv4\nexit\n!\nroute-map 10.86.10.98-out permit 2\n match ipv6 address prefix-list 10.86.10.98-pl-ipv4\nexit\n!\nroute-map 10.86.10.99-in deny 20\nexit\n!\nroute-map 10.86.10.99-out permit 1\n match ip address prefix-list 10.86.10.99-pl-ipv4\nexit\n!\nroute-map 10.86.10.99-out permit 2\n match ipv6 address prefix-list 10.86.10.99-pl-ipv4\nexit\n!\nroute-map 10.89.147.194-in deny 20\nexit\n!\nroute-map 10.89.147.194-out permit 1\n match ip address prefix-list 10.89.147.194-pl-ipv4\nexit\n!\nroute-map 10.89.147.194-out permit 2\n match ipv6 address prefix-list 10.89.147.194-pl-ipv4\nexit\n!\nroute-map 10.89.147.195-in deny 20\nexit\n!\nroute-map 10.89.147.195-out permit 1\n match ip address prefix-list 10.89.147.195-pl-ipv4\nexit\n!\nroute-map 10.89.147.195-out permit 2\n match ipv6 address prefix-list 10.89.147.195-pl-ipv4\nexit\n!\nroute-map 10.89.97.162-in deny 20\nexit\n!\nroute-map 10.89.97.162-out permit 1\n match ip address prefix-list 10.89.97.162-pl-ipv4\nexit\n!\nroute-map 10.89.97.162-out permit 2\n match ipv6 address prefix-list 10.89.97.162-pl-ipv4\nexit\n!\nroute-map 10.89.97.163-in deny 20\nexit\n!\nroute-map 10.89.97.163-out permit 1\n match ip address prefix-list 10.89.97.163-pl-ipv4\nexit\n!\nroute-map 10.89.97.163-out permit 2\n match ipv6 address prefix-list 10.89.97.163-pl-ipv4\nexit\n!\nip nht resolve-via-default\n!\nipv6 nht resolve-via-default\n!\nbfd\n profile ncp-metallb-oam-pa-hn-bfd-profile\n  passive-mode\n exit\n !\n profile ncp-metallb-oam-pa-ni-bfd-profile\n  passive-mode\n exit\n !\n profile ncp-metallb-oam-pa-pa-bfd-profile\n  passive-mode\n exit\n !\n profile ncp-metallb-oam-pa-sv-bfd-profile\n  passive-mode\n exit\n !\nexit\n!\nend\n</code></pre> <pre><code>[root@ncputility ~ pancwl_rc]$ oc exec -n metallb-system  speaker-dlrn5 -c frr -- vtysh -c \"show bgp summary\"\n\nIPv4 Unicast Summary (VRF default):\nBGP router identifier 172.16.2.2, local AS number 4200000320 vrf-id 0\nBGP table version 1\nRIB entries 1, using 192 bytes of memory\nPeers 8, using 5788 KiB of memory\n\nNeighbor        V         AS   MsgRcvd   MsgSent   TblVer  InQ OutQ  Up/Down State/PfxRcd   PfxSnt Desc\n10.85.187.34    4 4200000320         0         0        0    0    0    never       Active        0 N/A\n10.85.187.35    4 4200000320         0         0        0    0    0    never       Active        0 N/A\n10.86.10.98     4 4200000320         0         0        0    0    0    never       Active        0 N/A\n10.86.10.99     4 4200000320         0         0        0    0    0    never       Active        0 N/A\n10.89.97.162    4 4200000320      7306      7307        0    0    0 2d12h50m            0        1 N/A\n10.89.97.163    4 4200000320      7305      7307        0    0    0 2d12h50m            0        1 N/A\n10.89.147.194   4 4200000320         0         0        0    0    0    never       Active        0 N/A\n10.89.147.195   4 4200000320         0         0        0    0    0    never       Active        0 N/A\n\nTotal number of neighbors 8\n\nIPv6 Unicast Summary (VRF default):\nBGP router identifier 172.16.2.2, local AS number 4200000320 vrf-id 0\nBGP table version 0\nRIB entries 0, using 0 bytes of memory\nPeers 8, using 5788 KiB of memory\n\nNeighbor        V         AS   MsgRcvd   MsgSent   TblVer  InQ OutQ  Up/Down State/PfxRcd   PfxSnt Desc\n10.85.187.34    4 4200000320         0         0        0    0    0    never       Active        0 N/A\n10.85.187.35    4 4200000320         0         0        0    0    0    never       Active        0 N/A\n10.86.10.98     4 4200000320         0         0        0    0    0    never       Active        0 N/A\n10.86.10.99     4 4200000320         0         0        0    0    0    never       Active        0 N/A\n10.89.97.162    4 4200000320      7306      7307        0    0    0 2d12h50m        NoNeg    NoNeg N/A\n10.89.97.163    4 4200000320      7305      7307        0    0    0 2d12h50m        NoNeg    NoNeg N/A\n10.89.147.194   4 4200000320         0         0        0    0    0    never       Active        0 N/A\n10.89.147.195   4 4200000320         0         0        0    0    0    never       Active        0 N/A\n\nTotal number of neighbors 8\n</code></pre> <pre><code>[root@ncputility ~ pancwl_rc]$ oc exec -n metallb-system  speaker-dlrn5 -c frr -- vtysh -c \"show bfd peers brief\"\nSession count: 2\nSessionId  LocalAddress                             PeerAddress                             Status\n=========  ============                             ===========                             ======\n62069252   10.89.97.165                             10.89.97.162                            down\n3159552171 10.89.97.165                             10.89.97.163                            down\n</code></pre> <pre><code>[root@ncputility ~ pancwl_rc]$ oc exec -n metallb-system  speaker-dlrn5 -c frr -- vtysh -c \"show bfd peer\"\nBFD Peers:\n        peer 10.89.97.162 local-address 10.89.97.165 vrf default interface vlan104\n                ID: 62069252\n                Remote ID: 0\n                Passive mode\n                Status: down\n                Downtime: 2 day(s), 10 hour(s), 3 minute(s), 16 second(s)\n                Diagnostics: ok\n                Remote diagnostics: ok\n                Peer Type: dynamic\n                Local timers:\n                        Detect-multiplier: 3\n                        Receive interval: 300ms\n                        Transmission interval: 300ms\n                        Echo receive interval: 50ms\n                        Echo transmission interval: disabled\n                Remote timers:\n                        Detect-multiplier: 3\n                        Receive interval: 1000ms\n                        Transmission interval: 1000ms\n                        Echo receive interval: disabled\n\n        peer 10.89.97.163 local-address 10.89.97.165 vrf default interface vlan104\n                ID: 3159552171\n                Remote ID: 0\n                Passive mode\n                Status: down\n                Downtime: 2 day(s), 10 hour(s), 3 minute(s), 16 second(s)\n                Diagnostics: ok\n                Remote diagnostics: ok\n                Peer Type: dynamic\n                Local timers:\n                        Detect-multiplier: 3\n                        Receive interval: 300ms\n                        Transmission interval: 300ms\n                        Echo receive interval: 50ms\n                        Echo transmission interval: disabled\n                Remote timers:\n                        Detect-multiplier: 3\n                        Receive interval: 1000ms\n                        Transmission interval: 1000ms\n                        Echo receive interval: disabled\n</code></pre> <pre><code>[root@ncputility ~ pancwl_rc]$ oc exec -n metallb-system  speaker-dlrn5 -c frr -- vtysh -c \"show ip bgp neighbors\"\nBGP neighbor is 10.85.187.34, remote AS 4200000320, local AS 4200000320, internal link\n  BGP version 4, remote router ID 0.0.0.0, local router ID 172.16.2.2\n  BGP state = Active\n  Last read 2d10h03m, Last write never\n  Hold time is 90, keepalive interval is 30 seconds\n  Configured hold time is 90, keepalive interval is 30 seconds\n  Configured conditional advertisements interval is 60 seconds\n  Graceful restart information:\n    Local GR Mode: Helper*\n    Remote GR Mode: NotApplicable\n    R bit: False\n    N bit: False\n    Timers:\n      Configured Restart Time(sec): 120\n      Received Restart Time(sec): 0\n  Message statistics:\n    Inq depth is 0\n    Outq depth is 0\n                         Sent       Rcvd\n    Opens:                  0          0\n    Notifications:          0          0\n    Updates:                0          0\n    Keepalives:             0          0\n    Route Refresh:          0          0\n    Capability:             0          0\n    Total:                  0          0\n  Minimum time between advertisement runs is 0 seconds\n\n For address family: IPv4 Unicast\n  Not part of any update group\n  Community attribute sent to this neighbor(all)\n  Inbound path policy configured\n  Outbound path policy configured\n  Route map for incoming advertisements is *10.85.187.34-in\n  Route map for outgoing advertisements is *10.85.187.34-out\n  0 accepted prefixes\n\n For address family: IPv6 Unicast\n  Not part of any update group\n  Community attribute sent to this neighbor(all)\n  Inbound path policy configured\n  Outbound path policy configured\n  Route map for incoming advertisements is *10.85.187.34-in\n  Route map for outgoing advertisements is *10.85.187.34-out\n  0 accepted prefixes\n\n  Connections established 0; dropped 0\n  Last reset 2d10h03m,  Waiting for peer OPEN\nBGP Connect Retry Timer in Seconds: 120\nNext connect timer due in 80 seconds\nRead thread: off  Write thread: off  FD used: -1\n\n  BFD: Type: multi hop\n  Detect Multiplier: 3, Min Rx interval: 300, Min Tx interval: 300\n  Status: Unknown, Last update: never\n\nBGP neighbor is 10.85.187.35, remote AS 4200000320, local AS 4200000320, internal link\n  BGP version 4, remote router ID 0.0.0.0, local router ID 172.16.2.2\n  BGP state = Active\n  Last read 2d10h03m, Last write never\n  Hold time is 90, keepalive interval is 30 seconds\n  Configured hold time is 90, keepalive interval is 30 seconds\n  Configured conditional advertisements interval is 60 seconds\n  Graceful restart information:\n    Local GR Mode: Helper*\n    Remote GR Mode: NotApplicable\n    R bit: False\n    N bit: False\n    Timers:\n      Configured Restart Time(sec): 120\n      Received Restart Time(sec): 0\n  Message statistics:\n    Inq depth is 0\n    Outq depth is 0\n                         Sent       Rcvd\n    Opens:                  0          0\n    Notifications:          0          0\n    Updates:                0          0\n    Keepalives:             0          0\n    Route Refresh:          0          0\n    Capability:             0          0\n    Total:                  0          0\n  Minimum time between advertisement runs is 0 seconds\n\n For address family: IPv4 Unicast\n  Not part of any update group\n  Community attribute sent to this neighbor(all)\n  Inbound path policy configured\n  Outbound path policy configured\n  Route map for incoming advertisements is *10.85.187.35-in\n  Route map for outgoing advertisements is *10.85.187.35-out\n  0 accepted prefixes\n\n For address family: IPv6 Unicast\n  Not part of any update group\n  Community attribute sent to this neighbor(all)\n  Inbound path policy configured\n  Outbound path policy configured\n  Route map for incoming advertisements is *10.85.187.35-in\n  Route map for outgoing advertisements is *10.85.187.35-out\n  0 accepted prefixes\n\n  Connections established 0; dropped 0\n  Last reset 2d10h03m,  Waiting for peer OPEN\nBGP Connect Retry Timer in Seconds: 120\nNext connect timer due in 80 seconds\nRead thread: off  Write thread: off  FD used: -1\n\n  BFD: Type: multi hop\n  Detect Multiplier: 3, Min Rx interval: 300, Min Tx interval: 300\n  Status: Unknown, Last update: never\n\nBGP neighbor is 10.86.10.98, remote AS 4200000320, local AS 4200000320, internal link\n  BGP version 4, remote router ID 0.0.0.0, local router ID 172.16.2.2\n  BGP state = Active\n  Last read 2d10h03m, Last write never\n  Hold time is 90, keepalive interval is 30 seconds\n  Configured hold time is 90, keepalive interval is 30 seconds\n  Configured conditional advertisements interval is 60 seconds\n  Graceful restart information:\n    Local GR Mode: Helper*\n    Remote GR Mode: NotApplicable\n    R bit: False\n    N bit: False\n    Timers:\n      Configured Restart Time(sec): 120\n      Received Restart Time(sec): 0\n  Message statistics:\n    Inq depth is 0\n    Outq depth is 0\n                         Sent       Rcvd\n    Opens:                  0          0\n    Notifications:          0          0\n    Updates:                0          0\n    Keepalives:             0          0\n    Route Refresh:          0          0\n    Capability:             0          0\n    Total:                  0          0\n  Minimum time between advertisement runs is 0 seconds\n\n For address family: IPv4 Unicast\n  Not part of any update group\n  Community attribute sent to this neighbor(all)\n  Inbound path policy configured\n  Outbound path policy configured\n  Route map for incoming advertisements is *10.86.10.98-in\n  Route map for outgoing advertisements is *10.86.10.98-out\n  0 accepted prefixes\n\n For address family: IPv6 Unicast\n  Not part of any update group\n  Community attribute sent to this neighbor(all)\n  Inbound path policy configured\n  Outbound path policy configured\n  Route map for incoming advertisements is *10.86.10.98-in\n  Route map for outgoing advertisements is *10.86.10.98-out\n  0 accepted prefixes\n\n  Connections established 0; dropped 0\n  Last reset 2d10h03m,  Waiting for peer OPEN\nBGP Connect Retry Timer in Seconds: 120\nNext connect timer due in 80 seconds\nRead thread: off  Write thread: off  FD used: -1\n\n  BFD: Type: multi hop\n  Detect Multiplier: 3, Min Rx interval: 300, Min Tx interval: 300\n  Status: Unknown, Last update: never\n\nBGP neighbor is 10.86.10.99, remote AS 4200000320, local AS 4200000320, internal link\n  BGP version 4, remote router ID 0.0.0.0, local router ID 172.16.2.2\n  BGP state = Active\n  Last read 2d10h03m, Last write never\n  Hold time is 90, keepalive interval is 30 seconds\n  Configured hold time is 90, keepalive interval is 30 seconds\n  Configured conditional advertisements interval is 60 seconds\n  Graceful restart information:\n    Local GR Mode: Helper*\n    Remote GR Mode: NotApplicable\n    R bit: False\n    N bit: False\n    Timers:\n      Configured Restart Time(sec): 120\n      Received Restart Time(sec): 0\n  Message statistics:\n    Inq depth is 0\n    Outq depth is 0\n                         Sent       Rcvd\n    Opens:                  0          0\n    Notifications:          0          0\n    Updates:                0          0\n    Keepalives:             0          0\n    Route Refresh:          0          0\n    Capability:             0          0\n    Total:                  0          0\n  Minimum time between advertisement runs is 0 seconds\n\n For address family: IPv4 Unicast\n  Not part of any update group\n  Community attribute sent to this neighbor(all)\n  Inbound path policy configured\n  Outbound path policy configured\n  Route map for incoming advertisements is *10.86.10.99-in\n  Route map for outgoing advertisements is *10.86.10.99-out\n  0 accepted prefixes\n\n For address family: IPv6 Unicast\n  Not part of any update group\n  Community attribute sent to this neighbor(all)\n  Inbound path policy configured\n  Outbound path policy configured\n  Route map for incoming advertisements is *10.86.10.99-in\n  Route map for outgoing advertisements is *10.86.10.99-out\n  0 accepted prefixes\n\n  Connections established 0; dropped 0\n  Last reset 2d10h03m,  Waiting for peer OPEN\nBGP Connect Retry Timer in Seconds: 120\nNext connect timer due in 80 seconds\nRead thread: off  Write thread: off  FD used: -1\n\n  BFD: Type: multi hop\n  Detect Multiplier: 3, Min Rx interval: 300, Min Tx interval: 300\n  Status: Unknown, Last update: never\n\nBGP neighbor is 10.89.97.162, remote AS 4200000320, local AS 4200000320, internal link\n  BGP version 4, remote router ID 10.29.90.34, local router ID 172.16.2.2\n  BGP state = Established, up for 2d12h53m\n  Last read 00:00:00, Last write 00:00:03\n  Hold time is 90, keepalive interval is 30 seconds\n  Configured hold time is 90, keepalive interval is 30 seconds\n  Configured conditional advertisements interval is 60 seconds\n  Neighbor capabilities:\n    4 Byte AS: advertised and received\n    Extended Message: advertised\n    AddPath:\n      IPv4 Unicast: RX advertised\n      IPv6 Unicast: RX advertised\n    Long-lived Graceful Restart: advertised\n    Route refresh: advertised and received(new)\n    Enhanced Route Refresh: advertised\n    Address Family IPv4 Unicast: advertised and received\n    Address Family IPv6 Unicast: advertised\n    Hostname Capability: advertised (name: gateway2.panclypcwl01.mnc020.mcc714,domain name: n/a) not received\n    Graceful Restart Capability: advertised and received\n      Remote Restart timer is 300 seconds\n      Address families by peer:\n        none\n  Graceful restart information:\n    End-of-RIB send: IPv4 Unicast\n    End-of-RIB received: IPv4 Unicast\n    Local GR Mode: Helper*\n    Remote GR Mode: Helper\n    R bit: False\n    N bit: False\n    Timers:\n      Configured Restart Time(sec): 120\n      Received Restart Time(sec): 300\n    IPv4 Unicast:\n      F bit: False\n      End-of-RIB sent: Yes\n      End-of-RIB sent after update: Yes\n      End-of-RIB received: Yes\n      Timers:\n        Configured Stale Path Time(sec): 360\n    IPv6 Unicast:\n      F bit: False\n      End-of-RIB sent: No\n      End-of-RIB sent after update: No\n      End-of-RIB received: No\n      Timers:\n        Configured Stale Path Time(sec): 360\n  Message statistics:\n    Inq depth is 0\n    Outq depth is 0\n                         Sent       Rcvd\n    Opens:                  3          1\n    Notifications:          0          0\n    Updates:                2          2\n    Keepalives:          7307       7308\n    Route Refresh:          0          0\n    Capability:             0          0\n    Total:               7312       7311\n  Minimum time between advertisement runs is 0 seconds\n\n For address family: IPv4 Unicast\n  Update group 3, subgroup 3\n  Packet Queue length 0\n  Community attribute sent to this neighbor(all)\n  Inbound path policy configured\n  Outbound path policy configured\n  Route map for incoming advertisements is *10.89.97.162-in\n  Route map for outgoing advertisements is *10.89.97.162-out\n  0 accepted prefixes\n\n For address family: IPv6 Unicast\n  Not part of any update group\n  Community attribute sent to this neighbor(all)\n  Inbound path policy configured\n  Outbound path policy configured\n  Route map for incoming advertisements is *10.89.97.162-in\n  Route map for outgoing advertisements is *10.89.97.162-out\n  0 accepted prefixes\n\n  Connections established 1; dropped 0\n  Last reset 2d12h55m,  Waiting for peer OPEN\nLocal host: 10.89.97.165, Local port: 37680\nForeign host: 10.89.97.162, Foreign port: 179\nNexthop: 10.89.97.165\nNexthop global: ::\nNexthop local: ::\nBGP connection: shared network\nBGP Connect Retry Timer in Seconds: 120\nRead thread: on  Write thread: on  FD used: 26\n\n  BFD: Type: single hop\n  Detect Multiplier: 3, Min Rx interval: 300, Min Tx interval: 300\n  Status: Down, Last update: 2:10:03:42\n\nBGP neighbor is 10.89.97.163, remote AS 4200000320, local AS 4200000320, internal link\n  BGP version 4, remote router ID 10.29.90.38, local router ID 172.16.2.2\n  BGP state = Established, up for 2d12h53m\n  Last read 00:00:29, Last write 00:00:03\n  Hold time is 90, keepalive interval is 30 seconds\n  Configured hold time is 90, keepalive interval is 30 seconds\n  Configured conditional advertisements interval is 60 seconds\n  Neighbor capabilities:\n    4 Byte AS: advertised and received\n    Extended Message: advertised\n    AddPath:\n      IPv4 Unicast: RX advertised\n      IPv6 Unicast: RX advertised\n    Long-lived Graceful Restart: advertised\n    Route refresh: advertised and received(new)\n    Enhanced Route Refresh: advertised\n    Address Family IPv4 Unicast: advertised and received\n    Address Family IPv6 Unicast: advertised\n    Hostname Capability: advertised (name: gateway2.panclypcwl01.mnc020.mcc714,domain name: n/a) not received\n    Graceful Restart Capability: advertised and received\n      Remote Restart timer is 300 seconds\n      Address families by peer:\n        none\n  Graceful restart information:\n    End-of-RIB send: IPv4 Unicast\n    End-of-RIB received: IPv4 Unicast\n    Local GR Mode: Helper*\n    Remote GR Mode: Helper\n    R bit: False\n    N bit: False\n    Timers:\n      Configured Restart Time(sec): 120\n      Received Restart Time(sec): 300\n    IPv4 Unicast:\n      F bit: False\n      End-of-RIB sent: Yes\n      End-of-RIB sent after update: Yes\n      End-of-RIB received: Yes\n      Timers:\n        Configured Stale Path Time(sec): 360\n    IPv6 Unicast:\n      F bit: False\n      End-of-RIB sent: No\n      End-of-RIB sent after update: No\n      End-of-RIB received: No\n      Timers:\n        Configured Stale Path Time(sec): 360\n  Message statistics:\n    Inq depth is 0\n    Outq depth is 0\n                         Sent       Rcvd\n    Opens:                  3          1\n    Notifications:          0          0\n    Updates:                2          1\n    Keepalives:          7307       7307\n    Route Refresh:          0          0\n    Capability:             0          0\n    Total:               7312       7309\n  Minimum time between advertisement runs is 0 seconds\n\n For address family: IPv4 Unicast\n  Update group 4, subgroup 4\n  Packet Queue length 0\n  Community attribute sent to this neighbor(all)\n  Inbound path policy configured\n  Outbound path policy configured\n  Route map for incoming advertisements is *10.89.97.163-in\n  Route map for outgoing advertisements is *10.89.97.163-out\n  0 accepted prefixes\n\n For address family: IPv6 Unicast\n  Not part of any update group\n  Community attribute sent to this neighbor(all)\n  Inbound path policy configured\n  Outbound path policy configured\n  Route map for incoming advertisements is *10.89.97.163-in\n  Route map for outgoing advertisements is *10.89.97.163-out\n  0 accepted prefixes\n\n  Connections established 1; dropped 0\n  Last reset 2d12h55m,  Waiting for peer OPEN\nLocal host: 10.89.97.165, Local port: 54688\nForeign host: 10.89.97.163, Foreign port: 179\nNexthop: 10.89.97.165\nNexthop global: ::\nNexthop local: ::\nBGP connection: shared network\nBGP Connect Retry Timer in Seconds: 120\nRead thread: on  Write thread: on  FD used: 25\n\n  BFD: Type: single hop\n  Detect Multiplier: 3, Min Rx interval: 300, Min Tx interval: 300\n  Status: Down, Last update: 2:10:03:42\n\nBGP neighbor is 10.89.147.194, remote AS 4200000320, local AS 4200000320, internal link\n  BGP version 4, remote router ID 0.0.0.0, local router ID 172.16.2.2\n  BGP state = Active\n  Last read 2d10h03m, Last write never\n  Hold time is 90, keepalive interval is 30 seconds\n  Configured hold time is 90, keepalive interval is 30 seconds\n  Configured conditional advertisements interval is 60 seconds\n  Graceful restart information:\n    Local GR Mode: Helper*\n    Remote GR Mode: NotApplicable\n    R bit: False\n    N bit: False\n    Timers:\n      Configured Restart Time(sec): 120\n      Received Restart Time(sec): 0\n  Message statistics:\n    Inq depth is 0\n    Outq depth is 0\n                         Sent       Rcvd\n    Opens:                  0          0\n    Notifications:          0          0\n    Updates:                0          0\n    Keepalives:             0          0\n    Route Refresh:          0          0\n    Capability:             0          0\n    Total:                  0          0\n  Minimum time between advertisement runs is 0 seconds\n\n For address family: IPv4 Unicast\n  Not part of any update group\n  Community attribute sent to this neighbor(all)\n  Inbound path policy configured\n  Outbound path policy configured\n  Route map for incoming advertisements is *10.89.147.194-in\n  Route map for outgoing advertisements is *10.89.147.194-out\n  0 accepted prefixes\n\n For address family: IPv6 Unicast\n  Not part of any update group\n  Community attribute sent to this neighbor(all)\n  Inbound path policy configured\n  Outbound path policy configured\n  Route map for incoming advertisements is *10.89.147.194-in\n  Route map for outgoing advertisements is *10.89.147.194-out\n  0 accepted prefixes\n\n  Connections established 0; dropped 0\n  Last reset 2d10h03m,  Waiting for peer OPEN\nBGP Connect Retry Timer in Seconds: 120\nNext connect timer due in 80 seconds\nRead thread: off  Write thread: off  FD used: -1\n\n  BFD: Type: multi hop\n  Detect Multiplier: 3, Min Rx interval: 300, Min Tx interval: 300\n  Status: Unknown, Last update: never\n\nBGP neighbor is 10.89.147.195, remote AS 4200000320, local AS 4200000320, internal link\n  BGP version 4, remote router ID 0.0.0.0, local router ID 172.16.2.2\n  BGP state = Active\n  Last read 2d10h03m, Last write never\n  Hold time is 90, keepalive interval is 30 seconds\n  Configured hold time is 90, keepalive interval is 30 seconds\n  Configured conditional advertisements interval is 60 seconds\n  Graceful restart information:\n    Local GR Mode: Helper*\n    Remote GR Mode: NotApplicable\n    R bit: False\n    N bit: False\n    Timers:\n      Configured Restart Time(sec): 120\n      Received Restart Time(sec): 0\n  Message statistics:\n    Inq depth is 0\n    Outq depth is 0\n                         Sent       Rcvd\n    Opens:                  0          0\n    Notifications:          0          0\n    Updates:                0          0\n    Keepalives:             0          0\n    Route Refresh:          0          0\n    Capability:             0          0\n    Total:                  0          0\n  Minimum time between advertisement runs is 0 seconds\n\n For address family: IPv4 Unicast\n  Not part of any update group\n  Community attribute sent to this neighbor(all)\n  Inbound path policy configured\n  Outbound path policy configured\n  Route map for incoming advertisements is *10.89.147.195-in\n  Route map for outgoing advertisements is *10.89.147.195-out\n  0 accepted prefixes\n\n For address family: IPv6 Unicast\n  Not part of any update group\n  Community attribute sent to this neighbor(all)\n  Inbound path policy configured\n  Outbound path policy configured\n  Route map for incoming advertisements is *10.89.147.195-in\n  Route map for outgoing advertisements is *10.89.147.195-out\n  0 accepted prefixes\n\n  Connections established 0; dropped 0\n  Last reset 2d10h03m,  Waiting for peer OPEN\nBGP Connect Retry Timer in Seconds: 120\nNext connect timer due in 80 seconds\nRead thread: off  Write thread: off  FD used: -1\n\n  BFD: Type: multi hop\n  Detect Multiplier: 3, Min Rx interval: 300, Min Tx interval: 300\n  Status: Unknown, Last update: never\n</code></pre> <ol> <li>some more additional test to show the local testing here </li> </ol> <pre><code>[root@ncputility ~ pancwl_rc]$ oc get svc -A -o wide |grep -i loadbalan\nncom01pan                                          ncom01pan-citm-ingress                                     LoadBalancer   172.20.138.133   10.89.97.210                           80:32432/TCP,443:32622/TCP,2309:30135/TCP         10d   app=citm-ingress,component=controller,release=ncom01pan-citm-ingress\n[root@ncputility ~ pancwl_rc]$ nslookup 10.89.97.210\n210.97.89.10.in-addr.arpa       name = ncom01.panclyncom01.mnc020.mcc714.\n\n[root@ncputility ~ pancwl_rc]$ curl -k https://ncom01.panclyncom01.mnc020.mcc714/\n^C\n[root@ncputility ~ pancwl_rc]$ ip r get 10.89.97.210\n10.89.97.210 via 10.89.100.65 dev br308 src 10.89.100.66 uid 0\n    cache\n[root@ncputility ~ pancwl_rc]$ tracepath 10.89.97.210\n 1?: [LOCALHOST]                      pmtu 1500\n 1:  _gateway                                              0.184ms\n 1:  _gateway                                              0.290ms\n 2:  no reply\n 3:  10.89.97.129                                          0.284ms asymm  1\n 4:  no reply\n 5:  10.89.97.129                                          0.264ms asymm  1\n 6:  no reply\n 7:  10.89.97.129                                          0.281ms asymm  1\n 8:  no reply\n 9:  10.89.97.129                                          0.279ms asymm  1\n10:  no reply\n11:  10.89.97.129                                          0.269ms asymm  1\n12:  no reply\n13:  10.89.97.129                                          0.280ms asymm  1\n14:  no reply\n15:  10.89.97.129                                          0.234ms asymm  1\n16:  no reply\n17:  10.89.97.129                                          0.297ms asymm  1\n18:  no reply\n19:  10.89.97.129                                          0.328ms asymm  1\n20:  no reply\n21:  10.89.97.129                                          0.292ms asymm  1\n22:  no reply\n23:  10.89.97.129                                          0.336ms asymm  1\n24:  no reply\n25:  10.89.97.129                                          0.332ms asymm  1\n26:  no reply\n27:  10.89.97.129                                          0.330ms asymm  1\n28:  no reply\n29:  10.89.97.129                                          0.332ms asymm  1\n^C\n[root@ncputility ~ pancwl_rc]$ oc get -n metallb-system pods -l component=speaker -o wide\nNAME            READY   STATUS    RESTARTS   AGE   IP            NODE                                  NOMINATED NODE   READINESS GATES\nspeaker-dlrn5   6/6     Running   0          24d   10.89.96.18   gateway2.panclypcwl01.mnc020.mcc714   &lt;none&gt;           &lt;none&gt;\nspeaker-g2g77   6/6     Running   0          24d   10.89.96.19   gateway3.panclypcwl01.mnc020.mcc714   &lt;none&gt;           &lt;none&gt;\nspeaker-jzbw7   6/6     Running   0          24d   10.89.96.17   gateway1.panclypcwl01.mnc020.mcc714   &lt;none&gt;           &lt;none&gt;\nspeaker-pjstl   6/6     Running   0          24d   10.89.96.20   gateway4.panclypcwl01.mnc020.mcc714   &lt;none&gt;           &lt;none&gt;\n[root@ncputility ~ pancwl_rc]$ oc -n metallb-system exec -it -c frr vtysh\nerror: you must specify at least one command for the container\n[root@ncputility ~ pancwl_rc]$ oc -n metallb-system exec -it speaker-dlrn5 -c frr vtysh\nkubectl exec [POD] [COMMAND] is DEPRECATED and will be removed in a future version. Use kubectl exec [POD] -- [COMMAND] instead.\n\nHello, this is FRRouting (version 8.3.1).\nCopyright 1996-2005 Kunihiro Ishiguro, et al.\n\ngateway2.panclypcwl01.mnc020.mcc714# curl\n% Unknown command: curl\ngateway2.panclypcwl01.mnc020.mcc714# exit\n[root@ncputility ~ pancwl_rc]$ oc debug -t node/gateway3.panclypcwl01.mnc020.mcc714\nTemporary namespace openshift-debug-8j8fb is created for debugging node...\nStarting pod/gateway3panclypcwl01mnc020mcc714-debug-qsrhj ...\nTo use host binaries, run `chroot /host`\nPod IP: 10.89.96.19\nIf you don't see a command prompt, try pressing enter.\nsh-5.1# chroot /host\nsh-5.1# ping 10.89.97.163\nPING 10.89.97.163 (10.89.97.163) 56(84) bytes of data.\nFrom 10.89.97.166 icmp_seq=1 Destination Host Unreachable\nFrom 10.89.97.166 icmp_seq=2 Destination Host Unreachable\nFrom 10.89.97.166 icmp_seq=3 Destination Host Unreachable\n^C\n--- 10.89.97.163 ping statistics ---\n4 packets transmitted, 0 received, +3 errors, 100% packet loss, time 3090ms\npipe 3\nsh-5.1# ping 10.89.97.162\nPING 10.89.97.162 (10.89.97.162) 56(84) bytes of data.\nFrom 10.89.97.166 icmp_seq=1 Destination Host Unreachable\nFrom 10.89.97.166 icmp_seq=2 Destination Host Unreachable\nFrom 10.89.97.166 icmp_seq=3 Destination Host Unreachable\nFrom 10.89.97.166 icmp_seq=4 Destination Host Unreachable\n^C\n--- 10.89.97.162 ping statistics ---\n5 packets transmitted, 0 received, +4 errors, 100% packet loss, time 4064ms\npipe 4\nsh-5.1# ip r g 10.89.97.163\n10.89.97.163 via 10.89.97.161 dev vlan104 src 10.89.97.166 uid 0\n    cache\nsh-5.1# ip r g 10.89.97.162\n10.89.97.162 via 10.89.97.161 dev vlan104 src 10.89.97.166 uid 0\n    cache\nsh-5.1#  curl -k https://ncom01.panclyncom01.mnc020.mcc714/\n&lt;!doctype html&gt;&lt;html lang=\"en\"&gt;&lt;head&gt;&lt;meta charset=\"utf-8\"&gt;&lt;meta name=\"viewport\" content=\"width=device-width,initial-scale=1,shrink-to-fit=no\"&gt;&lt;meta name=\"theme-color\" content=\"#000000\"&gt;&lt;link rel=\"manifest\" href=\"/manifest.json\"&gt;&lt;link rel=\"shortcut icon\" href=\"/favicon.ico\"&gt;&lt;title&gt;Nokia Cloud Operations Manager&lt;/title&gt;&lt;link href=\"/static/css/27.2dbd0a91.chunk.css\" rel=\"stylesheet\"&gt;&lt;link href=\"/static/css/main.167b01c4.chunk.css\" rel=\"stylesheet\"&gt;&lt;/head&gt;&lt;body&gt;&lt;noscript&gt;You need to enable JavaScript to run this app.&lt;/noscript&gt;&lt;div id=\"root\"&gt;&lt;/div&gt;&lt;script&gt;!function(e){function t(t){for(var r,o,l=t[0],c=t[1],s=t[2],u=0,d=[];u&lt;l.length;u++)o=l[u],Object.prototype.hasOwnProperty.call(n,o)&amp;&amp;n[o]&amp;&amp;d.push(n[o][0]),n[o]=0;for(r in c)Object.prototype.hasOwnProperty.call(c,r)&amp;&amp;(e[r]=c[r]);for(f&amp;&amp;f(t);d.length;)d.shift()();return i.push.apply(i,s||[]),a()}function a(){for(var e,t=0;t&lt;i.length;t++){for(var a=i[t],r=!0,o=1;o&lt;a.length;o++){var c=a[o];0!==n[c]&amp;&amp;(r=!1)}r&amp;&amp;(i.splice(t--,1),e=l(l.s=a[0]))}return e}var r={},o={25:0},n={25:0},i=[];function l(t){if(r[t])return r[t].exports;var a=r[t]={i:t,l:!1,exports:{}};return e[t].call(a.exports,a,a.exports,l),a.l=!0,a.exports}l.e=function(e){var t=[];o[e]?t.push(o[e]):0!==o[e]&amp;&amp;{2:1,5:1,6:1,7:1,8:1,11:1,12:1,13:1,14:1,15:1,16:1,17:1,18:1,19:1,20:1,21:1,22:1,26:1}[e]&amp;&amp;t.push(o[e]=new Promise((function(t,a){for(var r=\"static/css/\"+({5:\"[AlarmDetailsPage]\",6:\"[AlarmPage]\",7:\"[CatalogDetailsPage]\",8:\"[CatalogTablePage]\",9:\"[CloudFlowLogin]\",10:\"[CloudFlowLogout]\",11:\"[CsvExporterDetailsPage]\",12:\"[DashboardPage]\",13:\"[ExecutionDetailsPage]\",14:\"[JobDetailsPage]\",15:\"[JobsTablePage]\",16:\"[ManagedWorkloadsTablePage]\",17:\"[NSDetailsContainerPage]\",18:\"[OperationsDetailsPage]\",19:\"[ResourceCompositionDetailsPage]\",20:\"[ResourceCompositionsPage]\",21:\"[ResourcesTablePage]\",22:\"[VimDetailsPage]\",23:\"[loginPage]\"}[e]||e)+\".\"+{0:\"31d6cfe0\",1:\"31d6cfe0\",2:\"559272a8\",3:\"31d6cfe0\",4:\"31d6cfe0\",5:\"affca9cb\",6:\"affca9cb\",7:\"91932cb4\",8:\"bf9ba349\",9:\"31d6cfe0\",10:\"31d6cfe0\",11:\"a0ed0644\",12:\"b2dca462\",13:\"b77ab692\",14:\"4ca14441\",15:\"f59232c1\",16:\"c1e044f2\",17:\"398f0759\",18:\"d8693b3e\",19:\"7b4dfbce\",20:\"d9277a02\",21:\"03edf692\",22:\"b6690f64\",23:\"31d6cfe0\",26:\"4391d164\",28:\"31d6cfe0\",29:\"31d6cfe0\"}[e]+\".chunk.css\",n=l.p+r,i=document.getElementsByTagName(\"link\"),c=0;c&lt;i.length;c++){var s=(f=i[c]).getAttribute(\"data-href\")||f.getAttribute(\"href\");if(\"stylesheet\"===f.rel&amp;&amp;(s===r||s===n))return t()}var u=document.getElementsByTagName(\"style\");for(c=0;c&lt;u.length;c++){var f;if((s=(f=u[c]).getAttribute(\"data-href\"))===r||s===n)return t()}var d=document.createElement(\"link\");d.rel=\"stylesheet\",d.type=\"text/css\",d.onload=t,d.onerror=function(t){var r=t&amp;&amp;t.target&amp;&amp;t.target.src||n,i=new Error(\"Loading CSS chunk \"+e+\" failed.\\n(\"+r+\")\");i.code=\"CSS_CHUNK_LOAD_FAILED\",i.request=r,delete o[e],d.parentNode.removeChild(d),a(i)},d.href=n,document.getElementsByTagName(\"head\")[0].appendChild(d)})).then((function(){o[e]=0})));var a=n[e];if(0!==a)if(a)t.push(a[2]);else{var r=new Promise((function(t,r){a=n[e]=[t,r]}));t.push(a[2]=r);var i,c=document.createElement(\"script\");c.charset=\"utf-8\",c.timeout=120,l.nc&amp;&amp;c.setAttribute(\"nonce\",l.nc),c.src=function(e){return l.p+\"static/js/\"+({5:\"[AlarmDetailsPage]\",6:\"[AlarmPage]\",7:\"[CatalogDetailsPage]\",8:\"[CatalogTablePage]\",9:\"[CloudFlowLogin]\",10:\"[CloudFlowLogout]\",11:\"[CsvExporterDetailsPage]\",12:\"[DashboardPage]\",13:\"[ExecutionDetailsPage]\",14:\"[JobDetailsPage]\",15:\"[JobsTablePage]\",16:\"[ManagedWorkloadsTablePage]\",17:\"[NSDetailsContainerPage]\",18:\"[OperationsDetailsPage]\",19:\"[ResourceCompositionDetailsPage]\",20:\"[ResourceCompositionsPage]\",21:\"[ResourcesTablePage]\",22:\"[VimDetailsPage]\",23:\"[loginPage]\"}[e]||e)+\".\"+{0:\"9ea1f8f1\",1:\"d7647e6d\",2:\"49e46739\",3:\"6bb32e31\",4:\"5066ed38\",5:\"3e390b6c\",6:\"88b37376\",7:\"e7438c70\",8:\"f547dc3b\",9:\"c157abf8\",10:\"c3c2011c\",11:\"62ea6190\",12:\"c002dc82\",13:\"f0d807bf\",14:\"cee9c63a\",15:\"737e42f0\",16:\"2aae2df5\",17:\"4a2ec46d\",18:\"4777a959\",19:\"f28d098b\",20:\"7e96e980\",21:\"abc77c4e\",22:\"9d55d492\",23:\"44757b03\",26:\"c04605d3\",28:\"52a01c15\",29:\"f0708fbb\"}[e]+\".chunk.js\"}(e);var s=new Error;i=function(t){c.onerror=c.onload=null,clearTimeout(u);var a=n[e];if(0!==a){if(a){var r=t&amp;&amp;(\"load\"===t.type?\"missing\":t.type),o=t&amp;&amp;t.target&amp;&amp;t.target.src;s.message=\"Loading chunk \"+e+\" failed.\\n(\"+r+\": \"+o+\")\",s.name=\"ChunkLoadError\",s.type=r,s.request=o,a[1](s)}n[e]=void 0}};var u=setTimeout((function(){i({type:\"timeout\",target:c})}),12e4);c.onerror=c.onload=i,document.head.appendChild(c)}return Promise.all(t)},l.m=e,l.c=r,l.d=function(e,t,a){l.o(e,t)||Object.defineProperty(e,t,{enumerable:!0,get:a})},l.r=function(e){\"undefined\"!=typeof Symbol&amp;&amp;Symbol.toStringTag&amp;&amp;Object.defineProperty(e,Symbol.toStringTag,{value:\"Module\"}),Object.defineProperty(e,\"__esModule\",{value:!0})},l.t=function(e,t){if(1&amp;t&amp;&amp;(e=l(e)),8&amp;t)return e;if(4&amp;t&amp;&amp;\"object\"==typeof e&amp;&amp;e&amp;&amp;e.__esModule)return e;var a=Object.create(null);if(l.r(a),Object.defineProperty(a,\"default\",{enumerable:!0,value:e}),2&amp;t&amp;&amp;\"string\"!=typeof e)for(var r in e)l.d(a,r,function(t){return e[t]}.bind(null,r));return a},l.n=function(e){var t=e&amp;&amp;e.__esModule?function(){return e.default}:function(){return e};return l.d(t,\"a\",t),t},l.o=function(e,t){return Object.prototype.hasOwnProperty.call(e,t)},l.p=\"/\",l.oe=function(e){throw console.error(e),e};var c=this.webpackJsonpfrontend=this.webpackJsonpfrontend||[],s=c.push.bind(c);c.push=t,c=c.slice();for(var u=0;u&lt;c.length;u++)t(c[u]);var f=s;a()}([])&lt;/script&gt;&lt;script src=\"/static/js/27.08ffca3e.chunk.js\"&gt;&lt;/script&gt;&lt;script src=\"/static/js/main.90334f28.chunk.js\"&gt;&lt;/script&gt;&lt;/body&gt;&lt;/html&gt;sh-5.1#\n</code></pre>"},{"location":"openshift/storagemanagement/ceph-rebalanceissue/","title":"Ceph Rebalance Issue","text":""},{"location":"openshift/storagemanagement/ceph-rebalanceissue/#ceph-rebalance-issues-and-its-method-to-solve-it","title":"Ceph rebalance issues and it's method to solve it.","text":""},{"location":"openshift/storagemanagement/ceph-rebalanceissue/#ceph-rebalance-issue-when-failed-storagereplacement-node-old-osd-causing-an-stuck-or-rebalance-blocking","title":"ceph rebalance issue, when failed storage/replacement node old OSD causing an stuck or rebalance blocking.","text":"<p>1) Login to OCP CWL cluster</p> <pre><code>[root@dom14npv101-infra-manager ~ vlabrc]# source  /root/raj/vlabrc\nWARNING: Using insecure TLS client config. Setting this option is not supported!\n\nLogin successful.\n\nYou have access to 115 projects, the list has been suppressed. You can list all projects with 'oc projects'\n\nUsing project \"openshift-storage\".\n[root@dom14npv101-infra-manager ~ vlabrc]# oc get nodes\nNAME                                                       STATUS   ROLES                              AGE     VERSION\nncpvnpvlab1-storage-101.ncpvnpvlab1.pnwlab.nsn-rdnet.net   Ready    storage,worker                     3h49m   v1.29.10+67d3387\nncpvnpvlab1-storage-102.ncpvnpvlab1.pnwlab.nsn-rdnet.net   Ready    storage,worker                     149m    v1.29.10+67d3387\nncpvnpvlab1-storage-103.ncpvnpvlab1.pnwlab.nsn-rdnet.net   Ready    storage,worker                     57d     v1.29.10+67d3387\nncpvnpvlab1-storage-201.ncpvnpvlab1.pnwlab.nsn-rdnet.net   Ready    storage,worker                     55d     v1.29.10+67d3387\nncpvnpvlab1-storage-202.ncpvnpvlab1.pnwlab.nsn-rdnet.net   Ready    storage,worker                     57d     v1.29.10+67d3387\nncpvnpvlab1-storage-203.ncpvnpvlab1.pnwlab.nsn-rdnet.net   Ready    storage,worker                     3h48m   v1.29.10+67d3387 \n\noutput omitted\n</code></pre> <p>2) look at the ceph to find ceph status to know, is there any OSD's are missing. </p> <pre><code>bash-5.1$ ceph -s\n  cluster:\n    id:     d6599242-8a82-410c-aa83-c15b31d8f6c7\n    health: HEALTH_OK\n\n  services:\n    mon: 3 daemons, quorum a,b,c (age 4h)\n    mgr: a(active, since 11m), standbys: b\n    mds: 1/1 daemons up, 1 hot standby\n    osd: 46 osds: 46 up (since 10m), 46 in (since 11m)\n    rgw: 1 daemon active (1 hosts, 1 zones)\n\n  data:\n    volumes: 1/1 healthy\n    pools:   12 pools, 585 pgs\n    objects: 38.37k objects, 146 GiB\n    usage:   457 GiB used, 267 TiB / 268 TiB avail\n    pgs:     585 active+clean\n\n  io:\n    client:   135 KiB/s rd, 117 MiB/s wr, 103 op/s rd, 73 op/s wr\n\nbash-5.1$\n</code></pre> <p>3) Let us find which node missing the OSD but counting the osd's on each nodes. </p> <pre><code>bash-5.1$ ceph osd tree\nID   CLASS  WEIGHT     TYPE NAME                                                          STATUS  REWEIGHT  PRI-AFF\n -1         267.80737  root default\n -9          40.75330      host ncpvnpvlab1-storage-101-ncpvnpvlab1-pnwlab-nsn-rdnet-net\n 24    ssd    5.82190          osd.24                                                         up   1.00000  1.00000\n 27    ssd    5.82190          osd.27                                                         up   1.00000  1.00000\n 31    ssd    5.82190          osd.31                                                         up   1.00000  1.00000\n 32    ssd    5.82190          osd.32                                                         up   1.00000  1.00000\n 37    ssd    5.82190          osd.37                                                         up   1.00000  1.00000\n 38    ssd    5.82190          osd.38                                                         up   1.00000  1.00000\n 41    ssd    5.82190          osd.41                                                         up   1.00000  1.00000\n-11          46.57520      host ncpvnpvlab1-storage-102-ncpvnpvlab1-pnwlab-nsn-rdnet-net\n 25    ssd    5.82190          osd.25                                                         up   1.00000  1.00000\n 28    ssd    5.82190          osd.28                                                         up   1.00000  1.00000\n 29    ssd    5.82190          osd.29                                                         up   1.00000  1.00000\n 34    ssd    5.82190          osd.34                                                         up   1.00000  1.00000\n 36    ssd    5.82190          osd.36                                                         up   1.00000  1.00000\n 42    ssd    5.82190          osd.42                                                         up   1.00000  1.00000\n 43    ssd    5.82190          osd.43                                                         up   1.00000  1.00000\n 45    ssd    5.82190          osd.45                                                         up   1.00000  1.00000\n -3          46.57520      host ncpvnpvlab1-storage-103-ncpvnpvlab1-pnwlab-nsn-rdnet-net\n  0    ssd    5.82190          osd.0                                                          up   1.00000  1.00000\n  3    ssd    5.82190          osd.3                                                          up   1.00000  1.00000\n  6    ssd    5.82190          osd.6                                                          up   1.00000  1.00000\n  9    ssd    5.82190          osd.9                                                          up   1.00000  1.00000\n 11    ssd    5.82190          osd.11                                                         up   1.00000  1.00000\n 13    ssd    5.82190          osd.13                                                         up   1.00000  1.00000\n 17    ssd    5.82190          osd.17                                                         up   1.00000  1.00000\n 19    ssd    5.82190          osd.19                                                         up   1.00000  1.00000\n -5          46.57520      host ncpvnpvlab1-storage-201-ncpvnpvlab1-pnwlab-nsn-rdnet-net\n  1    ssd    5.82190          osd.1                                                          up   1.00000  1.00000\n  4    ssd    5.82190          osd.4                                                          up   1.00000  1.00000\n  8    ssd    5.82190          osd.8                                                          up   1.00000  1.00000\n 10    ssd    5.82190          osd.10                                                         up   1.00000  1.00000\n 14    ssd    5.82190          osd.14                                                         up   1.00000  1.00000\n 18    ssd    5.82190          osd.18                                                         up   1.00000  1.00000\n 20    ssd    5.82190          osd.20                                                         up   1.00000  1.00000\n 21    ssd    5.82190          osd.21                                                         up   1.00000  1.00000\n -7          46.57520      host ncpvnpvlab1-storage-202-ncpvnpvlab1-pnwlab-nsn-rdnet-net\n  2    ssd    5.82190          osd.2                                                          up   1.00000  1.00000\n  5    ssd    5.82190          osd.5                                                          up   1.00000  1.00000\n  7    ssd    5.82190          osd.7                                                          up   1.00000  1.00000\n 12    ssd    5.82190          osd.12                                                         up   1.00000  1.00000\n 15    ssd    5.82190          osd.15                                                         up   1.00000  1.00000\n 16    ssd    5.82190          osd.16                                                         up   1.00000  1.00000\n 22    ssd    5.82190          osd.22                                                         up   1.00000  1.00000\n 23    ssd    5.82190          osd.23                                                         up   1.00000  1.00000\n-13          40.75330      host ncpvnpvlab1-storage-203-ncpvnpvlab1-pnwlab-nsn-rdnet-net\n 26    ssd    5.82190          osd.26                                                         up   1.00000  1.00000\n 30    ssd    5.82190          osd.30                                                         up   1.00000  1.00000\n 33    ssd    5.82190          osd.33                                                         up   1.00000  1.00000\n 35    ssd    5.82190          osd.35                                                         up   1.00000  1.00000\n 39    ssd    5.82190          osd.39                                                         up   1.00000  1.00000\n 40    ssd    5.82190          osd.40                                                         up   1.00000  1.00000\n 44    ssd    5.82190          osd.44                                                         up   1.00000  1.00000\nbash-5.1$\n</code></pre> <p>4) check out the ceph health in detail, get better view from here</p> <pre><code>[root@dom14npv101-infra-manager ~ vlabrc]# oc rsh -n openshift-storage pod/$CEPH_TOOL_POD ceph health detail\nHEALTH_WARN 1 filesystem is degraded; 11 osds down; 3 hosts (23 osds) down; Reduced data availability: 247 pgs inactive; Degraded data redundancy: 5609127/17836029 objects degraded (31.448%), 811 pgs degraded, 839 pgs undersized; 1597 slow ops, oldest one blocked for 16139 sec, daemons [osd.0,osd.10,osd.13,osd.19,osd.22,osd.27,osd.29,osd.3,osd.35,osd.36]... have slow ops.\n[WRN] FS_DEGRADED: 1 filesystem is degraded\n    fs ocs-storagecluster-cephfilesystem is degraded\n[WRN] OSD_DOWN: 11 osds down\n    osd.7 (root=default,host=ncpvnpvlab1-storage-102-ncpvnpvlab1-pnwlab-nsn-rdnet-net) is down\n    osd.9 (root=default,host=ncpvnpvlab1-storage-101-ncpvnpvlab1-pnwlab-nsn-rdnet-net) is down\n    osd.28 (root=default,host=ncpvnpvlab1-storage-101-ncpvnpvlab1-pnwlab-nsn-rdnet-net) is down\n    osd.30 (root=default,host=ncpvnpvlab1-storage-203-ncpvnpvlab1-pnwlab-nsn-rdnet-net) is down\n    osd.31 (root=default,host=ncpvnpvlab1-storage-102-ncpvnpvlab1-pnwlab-nsn-rdnet-net) is down\n    osd.33 (root=default,host=ncpvnpvlab1-storage-102-ncpvnpvlab1-pnwlab-nsn-rdnet-net) is down\n    osd.34 (root=default,host=ncpvnpvlab1-storage-101-ncpvnpvlab1-pnwlab-nsn-rdnet-net) is down\n    osd.37 (root=default,host=ncpvnpvlab1-storage-203-ncpvnpvlab1-pnwlab-nsn-rdnet-net) is down\n    osd.40 (root=default,host=ncpvnpvlab1-storage-203-ncpvnpvlab1-pnwlab-nsn-rdnet-net) is down\n    osd.41 (root=default,host=ncpvnpvlab1-storage-101-ncpvnpvlab1-pnwlab-nsn-rdnet-net) is down\n    osd.44 (root=default,host=ncpvnpvlab1-storage-101-ncpvnpvlab1-pnwlab-nsn-rdnet-net) is down\n[WRN] OSD_HOST_DOWN: 3 hosts (23 osds) down\n    host ncpvnpvlab1-storage-101-ncpvnpvlab1-pnwlab-nsn-rdnet-net (root=default) (8 osds) is down\n    host ncpvnpvlab1-storage-203-ncpvnpvlab1-pnwlab-nsn-rdnet-net (root=default) (8 osds) is down\n    host ncpvnpvlab1-storage-102-ncpvnpvlab1-pnwlab-nsn-rdnet-net (root=default) (7 osds) is down\n[WRN] PG_AVAILABILITY: Reduced data availability: 247 pgs inactive\n    pg 9.91 is stuck inactive for 4h, current state unknown, last acting []\n    pg 9.93 is stuck inactive for 4h, current state undersized+degraded+peered, last acting [45]\n    pg 9.97 is stuck inactive for 4h, current state undersized+degraded+peered, last acting [47]\n    pg 9.9e is stuck inactive for 4h, current state unknown, last acting []\n    pg 9.9f is stuck inactive for 4h, current state undersized+degraded+peered, last acting [5]\n    pg 9.a0 is stuck inactive for 4h, current state unknown, last acting []\n    pg 9.a6 is stuck inactive for 4h, current state undersized+degraded+peered, last acting [22]\n    pg 9.ab is stuck inactive for 4h, current state undersized+degraded+peered, last acting [35]\n    pg 9.ae is stuck inactive for 4h, current state undersized+degraded+peered, last acting [25]\n    pg 9.af is stuck inactive for 4h, current state undersized+degraded+peered, last acting [8]\n    pg 9.b2 is stuck inactive for 4h, current state unknown, last acting []\n    pg 9.b3 is stuck inactive for 4h, current state undersized+degraded+peered, last acting [35]\n    pg 9.b4 is stuck inactive for 4h, current state unknown, last acting []\n    pg 9.c5 is stuck inactive for 4h, current state undersized+degraded+peered, last acting [42]\n    pg 9.c6 is stuck inactive for 4h, current state undersized+degraded+peered, last acting [3]\n    pg 9.c7 is stuck inactive for 4h, current state undersized+degraded+peered, last acting [46]\n    pg 9.1e7 is stuck inactive for 4h, current state undersized+degraded+peered, last acting [46]\n    pg 9.1eb is stuck inactive for 4h, current state undersized+degraded+peered, last acting [29]\n    pg 9.1f5 is stuck inactive for 4h, current state undersized+degraded+peered, last acting [0]\n    pg 9.1fa is stuck inactive for 4h, current state undersized+degraded+peered, last acting [22]\n    pg 9.1fb is stuck inactive for 4h, current state unknown, last acting []\n    pg 11.95 is stuck inactive for 4h, current state undersized+degraded+peered, last acting [46]\n    pg 11.98 is stuck inactive for 4h, current state undersized+degraded+peered, last acting [19]\n    pg 11.9b is stuck inactive for 4h, current state undersized+degraded+peered, last acting [32]\n    pg 11.a0 is stuck inactive for 4h, current state undersized+degraded+peered, last acting [36]\n    pg 11.a1 is stuck inactive for 4h, current state unknown, last acting []\n    pg 11.a3 is stuck inactive for 4h, current state undersized+degraded+peered, last acting [3]\n    pg 11.a8 is stuck inactive for 4h, current state undersized+degraded+peered, last acting [23]\n    pg 11.ac is stuck inactive for 4h, current state undersized+degraded+peered, last acting [43]\n    pg 11.b0 is stuck inactive for 4h, current state undersized+degraded+peered, last acting [39]\n    pg 11.b3 is stuck inactive for 4h, current state undersized+degraded+peered, last acting [3]\n    pg 11.b5 is stuck inactive for 4h, current state undersized+degraded+peered, last acting [10]\n    pg 11.b9 is stuck inactive for 4h, current state undersized+degraded+peered, last acting [43]\n    pg 11.bb is stuck inactive for 4h, current state unknown, last acting []\n    pg 11.bc is stuck inactive for 4h, current state undersized+degraded+peered, last acting [42]\n    pg 11.c0 is stuck inactive for 4h, current state undersized+degraded+peered, last acting [15]\n    pg 11.c2 is stuck inactive for 4h, current state undersized+degraded+peered, last acting [3]\n    pg 11.c3 is stuck inactive for 4h, current state undersized+degraded+peered, last acting [3]\n    pg 12.96 is stuck inactive for 4h, current state undersized+degraded+peered, last acting [23]\n    pg 12.9b is stuck inactive for 4h, current state undersized+degraded+peered, last acting [8]\n    pg 12.9c is stuck inactive for 4h, current state undersized+degraded+peered, last acting [38]\n    pg 12.9d is stuck inactive for 4h, current state undersized+degraded+peered, last acting [10]\n    pg 12.ab is stuck inactive for 4h, current state undersized+degraded+peered, last acting [36]\n    pg 12.af is stuck inactive for 4h, current state undersized+degraded+peered, last acting [20]\n    pg 12.b1 is stuck inactive for 4h, current state undersized+degraded+peered, last acting [0]\n    pg 12.b4 is stuck inactive for 4h, current state undersized+degraded+peered, last acting [13]\n    pg 12.b5 is stuck inactive for 4h, current state undersized+degraded+peered, last acting [38]\n    pg 12.be is stuck inactive for 4h, current state undersized+degraded+peered, last acting [3]\n    pg 12.c4 is stuck inactive for 4h, current state undersized+degraded+peered, last acting [23]\n    pg 12.c5 is stuck inactive for 4h, current state undersized+degraded+peered, last acting [39]\n    pg 12.c7 is stuck inactive for 4h, current state undersized+degraded+peered, last acting [13]\n</code></pre> <p>5) remove the OSD's which are part of scaled-in storage node </p> <pre><code>[root@dom14npv101-infra-manager ~ vlabrc]# oc rsh -n openshift-storage pod/$CEPH_TOOL_POD ceph osd tree\nID   CLASS  WEIGHT     TYPE NAME                                                          STATUS  REWEIGHT  PRI-AFF\n -1         273.62927  root default\n -3          46.57520      host ncpvnpvlab1-storage-101-ncpvnpvlab1-pnwlab-nsn-rdnet-net\n  4    ssd    5.82190          osd.4                                                        down         0  1.00000\n  9    ssd    5.82190          osd.9                                                        down   1.00000  1.00000\n 14    ssd    5.82190          osd.14                                                       down         0  1.00000\n 21    ssd    5.82190          osd.21                                                       down         0  1.00000\n 28    ssd    5.82190          osd.28                                                       down   1.00000  1.00000\n 34    ssd    5.82190          osd.34                                                       down   1.00000  1.00000\n 41    ssd    5.82190          osd.41                                                       down   1.00000  1.00000\n 44    ssd    5.82190          osd.44                                                       down   1.00000  1.00000\n ```\n\n6) remove those unwanted OSD's completely. \n</code></pre> <p>oc rsh -n openshift-storage pod/$CEPH_TOOL_POD ceph osd out 4 oc rsh -n openshift-storage pod/$CEPH_TOOL_POD ceph osd out 9 oc rsh -n openshift-storage pod/$CEPH_TOOL_POD ceph osd out 14 oc rsh -n openshift-storage pod/$CEPH_TOOL_POD ceph osd out 21 oc rsh -n openshift-storage pod/$CEPH_TOOL_POD ceph osd out 28 oc rsh -n openshift-storage pod/$CEPH_TOOL_POD ceph osd out 34 oc rsh -n openshift-storage pod/$CEPH_TOOL_POD ceph osd out 41 oc rsh -n openshift-storage pod/$CEPH_TOOL_POD ceph osd out 44</p> <p>osd.4 is already out. osd.9 is already out. osd.14 is already out. osd.21 is already out. marked out osd.28. marked out osd.34. marked out osd.41. marked out osd.44.</p> <pre><code>7) delete it completely. using purge. \n</code></pre> <p>[root@dom14npv101-infra-manager ~ vlabrc]# oc rsh -n openshift-storage pod/$CEPH_TOOL_POD ceph osd purge 4  --yes-i-really-mean-it oc rsh -n openshift-storage pod/$CEPH_TOOL_POD ceph osd purge 9  --yes-i-really-mean-it oc rsh -n openshift-storage pod/$CEPH_TOOL_POD ceph osd purge 14 --yes-i-really-mean-it oc rsh -n openshift-storage pod/$CEPH_TOOL_POD ceph osd purge 21 --yes-i-really-mean-it oc rsh -n openshift-storage pod/$CEPH_TOOL_POD ceph osd purge 28 --yes-i-really-mean-it oc rsh -n openshift-storage pod/$CEPH_TOOL_POD ceph osd purge 34 --yes-i-really-mean-it oc rsh -n openshift-storage pod/$CEPH_TOOL_POD ceph osd purge 41 --yes-i-really-mean-it oc rsh -n openshift-storage pod/$CEPH_TOOL_POD ceph osd purge 44 --yes-i-really-mean-it</p> <p>purged osd.4 purged osd.9 purged osd.14 purged osd.21 purged osd.28 purged osd.34 purged osd.41 purged osd.44</p> <pre><code>8) now check, rebalance should be begins at this point. \n</code></pre> <p>[root@dom14npv101-infra-manager ~ vlabrc]# oc rsh -n openshift-storage pod/$CEPH_TOOL_POD ceph -s   cluster:     id:     a2b0c334-ba7c-4ae1-b3f5-c6d514f19bec     health: HEALTH_WARN             1 filesystem is degraded             Reduced data availability: 203 pgs inactive             Degraded data redundancy: 5434540/17836062 objects degraded (30.469%), 800 pgs degraded, 218 pgs undersized             62 slow ops, oldest one blocked for 24934 sec, daemons [osd.10,osd.13,osd.19,osd.22,osd.35,osd.36,osd.42,osd.45,osd.5] have slow ops.</p> <p>services:     mon: 3 daemons, quorum h,j,k (age 6h)     mgr: a(active, since 6h), standbys: b     mds: 1/1 daemons up, 1 standby     osd: 32 osds: 24 up (since 7h), 24 in (since 119s); 879 remapped pgs</p> <p>data:     volumes: 0/1 healthy, 1 recovering     pools:   12 pools, 1097 pgs     objects: 5.95M objects, 20 TiB     usage:   42 TiB used, 98 TiB / 140 TiB avail     pgs:     3.829% pgs unknown              14.676% pgs not active              5434540/17836062 objects degraded (30.469%)              1857921/17836062 objects misplaced (10.417%)              639 active+undersized+degraded+remapped+backfill_wait              176 active+clean              157 undersized+degraded+remapped+backfill_wait+peered              79  active+remapped+backfill_wait              42  unknown              4   undersized+degraded+remapped+backfilling+peered</p> <p>io:     client:   2.3 MiB/s wr, 0 op/s rd, 6 op/s wr     recovery: 1.4 GiB/s, 0 keys/s, 455 objects/s ```</p>"},{"location":"openshift/storagemanagement/one-OSD-not-created/","title":"Ceph OSD recreate","text":""},{"location":"openshift/storagemanagement/one-OSD-not-created/#one-or-two-osd-not-creating-recreated-post-storage-node-replacement","title":"One or Two OSD not creating recreated post storage node replacement","text":""},{"location":"openshift/storagemanagement/one-OSD-not-created/#login-to-the-cluster-and-make-sure-storage-replacement-finished-fully","title":"Login to the cluster and make sure storage replacement finished fully.","text":"<p>1) Login to OCP CWL cluster</p> <pre><code>[root@dom14npv101-infra-manager ~ vlabrc]# source  /root/raj/vlabrc\nWARNING: Using insecure TLS client config. Setting this option is not supported!\n\nLogin successful.\n\nYou have access to 115 projects, the list has been suppressed. You can list all projects with 'oc projects'\n\nUsing project \"openshift-storage\".\n[root@dom14npv101-infra-manager ~ vlabrc]# oc get nodes\nNAME                                                       STATUS   ROLES                              AGE     VERSION\nncpvnpvlab1-storage-101.ncpvnpvlab1.pnwlab.nsn-rdnet.net   Ready    storage,worker                     3h49m   v1.29.10+67d3387\nncpvnpvlab1-storage-102.ncpvnpvlab1.pnwlab.nsn-rdnet.net   Ready    storage,worker                     149m    v1.29.10+67d3387\nncpvnpvlab1-storage-103.ncpvnpvlab1.pnwlab.nsn-rdnet.net   Ready    storage,worker                     57d     v1.29.10+67d3387\nncpvnpvlab1-storage-201.ncpvnpvlab1.pnwlab.nsn-rdnet.net   Ready    storage,worker                     55d     v1.29.10+67d3387\nncpvnpvlab1-storage-202.ncpvnpvlab1.pnwlab.nsn-rdnet.net   Ready    storage,worker                     57d     v1.29.10+67d3387\nncpvnpvlab1-storage-203.ncpvnpvlab1.pnwlab.nsn-rdnet.net   Ready    storage,worker                     3h48m   v1.29.10+67d3387 \n\noutput omitted\n</code></pre> <p>2) look at the ceph to find ceph status to know, is there any OSD's are missing. </p> <pre><code>bash-5.1$ ceph -s\n  cluster:\n    id:     d6599242-8a82-410c-aa83-c15b31d8f6c7\n    health: HEALTH_OK\n\n  services:\n    mon: 3 daemons, quorum a,b,c (age 4h)\n    mgr: a(active, since 11m), standbys: b\n    mds: 1/1 daemons up, 1 hot standby\n    osd: 46 osds: 46 up (since 10m), 46 in (since 11m)\n    rgw: 1 daemon active (1 hosts, 1 zones)\n\n  data:\n    volumes: 1/1 healthy\n    pools:   12 pools, 585 pgs\n    objects: 38.37k objects, 146 GiB\n    usage:   457 GiB used, 267 TiB / 268 TiB avail\n    pgs:     585 active+clean\n\n  io:\n    client:   135 KiB/s rd, 117 MiB/s wr, 103 op/s rd, 73 op/s wr\n\nbash-5.1$\n</code></pre> <p>3) Let us find which node missing the OSD but counting the osd's on each nodes. </p> <pre><code>bash-5.1$ ceph osd tree\nID   CLASS  WEIGHT     TYPE NAME                                                          STATUS  REWEIGHT  PRI-AFF\n -1         267.80737  root default\n -9          40.75330      host ncpvnpvlab1-storage-101-ncpvnpvlab1-pnwlab-nsn-rdnet-net\n 24    ssd    5.82190          osd.24                                                         up   1.00000  1.00000\n 27    ssd    5.82190          osd.27                                                         up   1.00000  1.00000\n 31    ssd    5.82190          osd.31                                                         up   1.00000  1.00000\n 32    ssd    5.82190          osd.32                                                         up   1.00000  1.00000\n 37    ssd    5.82190          osd.37                                                         up   1.00000  1.00000\n 38    ssd    5.82190          osd.38                                                         up   1.00000  1.00000\n 41    ssd    5.82190          osd.41                                                         up   1.00000  1.00000\n-11          46.57520      host ncpvnpvlab1-storage-102-ncpvnpvlab1-pnwlab-nsn-rdnet-net\n 25    ssd    5.82190          osd.25                                                         up   1.00000  1.00000\n 28    ssd    5.82190          osd.28                                                         up   1.00000  1.00000\n 29    ssd    5.82190          osd.29                                                         up   1.00000  1.00000\n 34    ssd    5.82190          osd.34                                                         up   1.00000  1.00000\n 36    ssd    5.82190          osd.36                                                         up   1.00000  1.00000\n 42    ssd    5.82190          osd.42                                                         up   1.00000  1.00000\n 43    ssd    5.82190          osd.43                                                         up   1.00000  1.00000\n 45    ssd    5.82190          osd.45                                                         up   1.00000  1.00000\n -3          46.57520      host ncpvnpvlab1-storage-103-ncpvnpvlab1-pnwlab-nsn-rdnet-net\n  0    ssd    5.82190          osd.0                                                          up   1.00000  1.00000\n  3    ssd    5.82190          osd.3                                                          up   1.00000  1.00000\n  6    ssd    5.82190          osd.6                                                          up   1.00000  1.00000\n  9    ssd    5.82190          osd.9                                                          up   1.00000  1.00000\n 11    ssd    5.82190          osd.11                                                         up   1.00000  1.00000\n 13    ssd    5.82190          osd.13                                                         up   1.00000  1.00000\n 17    ssd    5.82190          osd.17                                                         up   1.00000  1.00000\n 19    ssd    5.82190          osd.19                                                         up   1.00000  1.00000\n -5          46.57520      host ncpvnpvlab1-storage-201-ncpvnpvlab1-pnwlab-nsn-rdnet-net\n  1    ssd    5.82190          osd.1                                                          up   1.00000  1.00000\n  4    ssd    5.82190          osd.4                                                          up   1.00000  1.00000\n  8    ssd    5.82190          osd.8                                                          up   1.00000  1.00000\n 10    ssd    5.82190          osd.10                                                         up   1.00000  1.00000\n 14    ssd    5.82190          osd.14                                                         up   1.00000  1.00000\n 18    ssd    5.82190          osd.18                                                         up   1.00000  1.00000\n 20    ssd    5.82190          osd.20                                                         up   1.00000  1.00000\n 21    ssd    5.82190          osd.21                                                         up   1.00000  1.00000\n -7          46.57520      host ncpvnpvlab1-storage-202-ncpvnpvlab1-pnwlab-nsn-rdnet-net\n  2    ssd    5.82190          osd.2                                                          up   1.00000  1.00000\n  5    ssd    5.82190          osd.5                                                          up   1.00000  1.00000\n  7    ssd    5.82190          osd.7                                                          up   1.00000  1.00000\n 12    ssd    5.82190          osd.12                                                         up   1.00000  1.00000\n 15    ssd    5.82190          osd.15                                                         up   1.00000  1.00000\n 16    ssd    5.82190          osd.16                                                         up   1.00000  1.00000\n 22    ssd    5.82190          osd.22                                                         up   1.00000  1.00000\n 23    ssd    5.82190          osd.23                                                         up   1.00000  1.00000\n-13          40.75330      host ncpvnpvlab1-storage-203-ncpvnpvlab1-pnwlab-nsn-rdnet-net\n 26    ssd    5.82190          osd.26                                                         up   1.00000  1.00000\n 30    ssd    5.82190          osd.30                                                         up   1.00000  1.00000\n 33    ssd    5.82190          osd.33                                                         up   1.00000  1.00000\n 35    ssd    5.82190          osd.35                                                         up   1.00000  1.00000\n 39    ssd    5.82190          osd.39                                                         up   1.00000  1.00000\n 40    ssd    5.82190          osd.40                                                         up   1.00000  1.00000\n 44    ssd    5.82190          osd.44                                                         up   1.00000  1.00000\nbash-5.1$\n</code></pre> <p>4) assuming here <code>ncpvnpvlab1-storage-101.ncpvnpvlab1.pnwlab.nsn-rdnet.net</code> node missing one OSD here. but have to find which disk is that fail to create an osd. </p> <pre><code>[root@dom14npv101-infra-manager ~ vlabrc]# oc get pods  -o wide |grep -i ncpvnpvlab1-storage-101.ncpvnpvlab1.pnwlab.nsn-rdnet.net |grep -i osd |grep -i running\nrook-ceph-osd-24-6457dffc55-86brj                                 2/2     Running     0               19m     172.31.24.22     ncpvnpvlab1-storage-101.ncpvnpvlab1.pnwlab.nsn-rdnet.net   &lt;none&gt;           &lt;none&gt;\nrook-ceph-osd-27-7597d558fc-kw9l9                                 2/2     Running     0               19m     172.31.24.19     ncpvnpvlab1-storage-101.ncpvnpvlab1.pnwlab.nsn-rdnet.net   &lt;none&gt;           &lt;none&gt;\nrook-ceph-osd-31-5cdf75ff54-bhmlr                                 2/2     Running     0               19m     172.31.24.24     ncpvnpvlab1-storage-101.ncpvnpvlab1.pnwlab.nsn-rdnet.net   &lt;none&gt;           &lt;none&gt;\nrook-ceph-osd-32-6f99fd4845-pqg4m                                 2/2     Running     0               19m     172.31.24.23     ncpvnpvlab1-storage-101.ncpvnpvlab1.pnwlab.nsn-rdnet.net   &lt;none&gt;           &lt;none&gt;\nrook-ceph-osd-37-598b85cf5-mjjzk                                  2/2     Running     0               19m     172.31.24.25     ncpvnpvlab1-storage-101.ncpvnpvlab1.pnwlab.nsn-rdnet.net   &lt;none&gt;           &lt;none&gt;\nrook-ceph-osd-38-6f9cfbc8d9-wtfrd                                 2/2     Running     0               19m     172.31.24.26     ncpvnpvlab1-storage-101.ncpvnpvlab1.pnwlab.nsn-rdnet.net   &lt;none&gt;           &lt;none&gt;\nrook-ceph-osd-41-6b88b44f56-tlv7m                                 2/2     Running     0               19m     172.31.24.27     ncpvnpvlab1-storage-101.ncpvnpvlab1.pnwlab.nsn-rdnet.net   &lt;none&gt;           &lt;none&gt;\n[root@dom14npv101-infra-manager ~ vlabrc]# \n</code></pre>"},{"location":"openshift/storagemanagement/one-OSD-not-created/#find-an-method-to-locate-the-missing-osd-on-that-particular-node","title":"Find an method to locate the missing OSD on that particular node.","text":"<p>5) create to command to grep for osd and it's logical disk location on that node. </p> <pre><code>[root@dom14npv101-infra-manager ~ vlabrc]# oc get pods  -o wide |grep -i ncpvnpvlab1-storage-101.ncpvnpvlab1.pnwlab.nsn-rdnet.net |grep -i osd |grep -i running |awk '{print \"oc logs \"$1 \" |grep -i nvme\"}'\noc logs rook-ceph-osd-24-6457dffc55-86brj |grep -i nvme\noc logs rook-ceph-osd-27-7597d558fc-kw9l9 |grep -i nvme\noc logs rook-ceph-osd-31-5cdf75ff54-bhmlr |grep -i nvme\noc logs rook-ceph-osd-32-6f99fd4845-pqg4m |grep -i nvme\noc logs rook-ceph-osd-37-598b85cf5-mjjzk |grep -i nvme\noc logs rook-ceph-osd-38-6f9cfbc8d9-wtfrd |grep -i nvme\noc logs rook-ceph-osd-41-6b88b44f56-tlv7m |grep -i nvme\n[root@dom14npv101-infra-manager ~ vlabrc]# \n</code></pre> <p>6) run that commands to find the OSD name and it;s corresponding disk logical naming details here. </p> <pre><code>[root@dom14npv101-infra-manager ~ vlabrc]#  oc logs rook-ceph-osd-24-6457dffc55-86brj |grep -i nvme\nDefaulted container \"osd\" out of: osd, log-collector, blkdevmapper (init), activate (init), expand-bluefs (init), chown-container-data-dir (init)\ndebug 2025-05-06T18:36:08.392+0000 7f3d38717640  1 osd.24 185 _collect_metadata nvme2n1:\n[root@dom14npv101-infra-manager ~ vlabrc]# oc logs rook-ceph-osd-27-7597d558fc-kw9l9 |grep -i nvme\nDefaulted container \"osd\" out of: osd, log-collector, blkdevmapper (init), activate (init), expand-bluefs (init), chown-container-data-dir (init)\ndebug 2025-05-06T18:36:04.368+0000 7f793d509640  1 osd.27 181 _collect_metadata nvme7n1:\n[root@dom14npv101-infra-manager ~ vlabrc]# oc logs rook-ceph-osd-31-5cdf75ff54-bhmlr |grep -i nvme\nDefaulted container \"osd\" out of: osd, log-collector, blkdevmapper (init), activate (init), expand-bluefs (init), chown-container-data-dir (init)\ndebug 2025-05-06T18:36:13.727+0000 7fbb2a4ee640  1 osd.31 191 _collect_metadata nvme5n1:\n[root@dom14npv101-infra-manager ~ vlabrc]# oc logs rook-ceph-osd-32-6f99fd4845-pqg4m |grep -i nvme\nDefaulted container \"osd\" out of: osd, log-collector, blkdevmapper (init), activate (init), expand-bluefs (init), chown-container-data-dir (init)\ndebug 2025-05-06T18:36:12.739+0000 7faad8b28640  1 osd.32 190 _collect_metadata nvme6n1:\n[root@dom14npv101-infra-manager ~ vlabrc]# oc logs rook-ceph-osd-37-598b85cf5-mjjzk |grep -i nvme\nDefaulted container \"osd\" out of: osd, log-collector, blkdevmapper (init), activate (init), expand-bluefs (init), chown-container-data-dir (init)\ndebug 2025-05-06T18:36:21.737+0000 7f7fee8fd640  1 osd.37 199 _collect_metadata nvme4n1:\n[root@dom14npv101-infra-manager ~ vlabrc]# oc logs rook-ceph-osd-38-6f9cfbc8d9-wtfrd |grep -i nvme\nDefaulted container \"osd\" out of: osd, log-collector, blkdevmapper (init), activate (init), expand-bluefs (init), chown-container-data-dir (init)\ndebug 2025-05-06T18:36:23.718+0000 7f1f6e8b7640  1 osd.38 201 _collect_metadata nvme0n1:\n[root@dom14npv101-infra-manager ~ vlabrc]# oc logs rook-ceph-osd-41-6b88b44f56-tlv7m |grep -i nvme\nDefaulted container \"osd\" out of: osd, log-collector, blkdevmapper (init), activate (init), expand-bluefs (init), chown-container-data-dir (init)\ndebug 2025-05-06T18:36:27.752+0000 7fe6362c5640  1 osd.41 205 _collect_metadata nvme1n1:\n[root@dom14npv101-infra-manager ~ vlabrc]# \n</code></pre> <p>7) now login to respective storage node via ssh or oc debug so that compare the list osd disk and find out the missing OSD disk and then format that particular drive alone. </p> <pre><code>ssh core@ncpvnpvlab1-storage-101.ncpvnpvlab1.pnwlab.nsn-rdnet.net\nRed Hat Enterprise Linux CoreOS 416.94.202411201433-0\n  Part of OpenShift 4.16, RHCOS is a Kubernetes-native operating system\n  managed by the Machine Config Operator (`clusteroperator/machine-config`).\n\nWARNING: Direct SSH access to machines is not recommended; instead,\nmake configuration changes via `machineconfig` objects:\n  https://docs.openshift.com/container-platform/4.16/architecture/architecture-rhcos.html\n\n---\nLast login: Tue May  6 18:16:30 2025 from 10.203.197.23\n[core@ncpvnpvlab1-storage-101 ~]$ sudos u -= ^C\n[core@ncpvnpvlab1-storage-101 ~]$ sudo su -\nLast login: Tue May  6 18:16:35 UTC 2025 on pts/0\n[root@ncpvnpvlab1-storage-101 ~]# lsblk\nNAME        MAJ:MIN RM   SIZE RO TYPE MOUNTPOINTS\nloop0         7:0    0   5.8T  0 loop\nloop1         7:1    0   5.8T  0 loop\nloop2         7:2    0   5.8T  0 loop\nloop3         7:3    0   5.8T  0 loop\nloop4         7:4    0   5.8T  0 loop\nloop5         7:5    0   5.8T  0 loop\nloop6         7:6    0   5.8T  0 loop\nnbd0         43:0    0     0B  0 disk\nnbd1         43:32   0     0B  0 disk\nnbd2         43:64   0     0B  0 disk\nnbd3         43:96   0     0B  0 disk\nnbd4         43:128  0     0B  0 disk\nnbd5         43:160  0     0B  0 disk\nnbd6         43:192  0     0B  0 disk\nnbd7         43:224  0     0B  0 disk\nnvme3n1     259:1    0 894.2G  0 disk\n\u251c\u2500nvme3n1p1 259:2    0     1M  0 part\n\u251c\u2500nvme3n1p2 259:3    0   127M  0 part\n\u251c\u2500nvme3n1p3 259:4    0   384M  0 part /boot\n\u251c\u2500nvme3n1p4 259:5    0   460G  0 part /var\n\u2502                                     /sysroot/ostree/deploy/rhcos/var\n\u2502                                     /usr\n\u2502                                     /etc\n\u2502                                     /\n\u2502                                     /sysroot\n\u2514\u2500nvme3n1p5 259:6    0 433.7G  0 part\nnvme0n1     259:7    0   5.8T  0 disk\nnvme2n1     259:8    0   5.8T  0 disk\nnvme8n1     259:9    0   5.8T  0 disk\nnvme5n1     259:10   0   5.8T  0 disk\nnvme6n1     259:11   0   5.8T  0 disk\nnvme1n1     259:12   0   5.8T  0 disk\nnvme7n1     259:13   0   5.8T  0 disk\nnvme4n1     259:14   0   5.8T  0 disk\nnbd8         43:256  0     0B  0 disk\nnbd9         43:288  0     0B  0 disk\nnbd10        43:320  0     0B  0 disk\nnbd11        43:352  0     0B  0 disk\nnbd12        43:384  0     0B  0 disk\nnbd13        43:416  0     0B  0 disk\nnbd14        43:448  0     0B  0 disk\nnbd15        43:480  0     0B  0 disk\n[root@ncpvnpvlab1-storage-101 ~]#\n</code></pre> <p>8) on based on output from 6 and compared with output from 7. <code>nvme8n1</code> is missed out. so this drive need to formated. </p> <pre><code>[root@ncpvnpvlab1-storage-101 ~]# wipefs -a -f /dev/nvme8n1\n/dev/nvme8n1: 22 bytes were erased at offset 0x00000000 (ceph_bluestore): 62 6c 75 65 73 74 6f 72 65 20 62 6c 6f 63 6b 20 64 65 76 69 63 65\n[root@ncpvnpvlab1-storage-101 ~]#\nsgdisk -Z /dev/nvme8n1\nCreating new GPT entries in memory.\nGPT data structures destroyed! You may now partition the disk using fdisk or\nother utilities.\n[root@ncpvnpvlab1-storage-101 ~]# exit\nlogout\n[core@ncpvnpvlab1-storage-101 ~]$ exit\nlogout\nConnection to ncpvnpvlab1-storage-101.ncpvnpvlab1.pnwlab.nsn-rdnet.net closed.\n</code></pre> <p>9) wait for lso getting the pv created automationcally. </p> <pre><code>[root@dom14npv101-infra-manager ~ vlabrc]# oc get pv\nNAME                                       CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS      CLAIM                                                              STORAGECLASS          VOLUMEATTRIBUTESCLASS   REASON   AGE\nlocal-pv-1189c26a                          5961Gi     RWO            Delete           Bound       openshift-storage/ocs-deviceset-localblockstorage-1-data-14svnkz   localblockstorage     &lt;unset&gt;                          22s\n</code></pre> <p>10) wait for 4 mins, this ceph OSD will be auto created and ceph status should be having correct list of OSD's here .</p> <pre><code>bash-5.1$ ceph -s\n  cluster:\n    id:     d6599242-8a82-410c-aa83-c15b31d8f6c7\n    health: HEALTH_OK\n\n  services:\n    mon: 3 daemons, quorum a,b,c (age 4h)\n    mgr: a(active, since 67m), standbys: b\n    mds: 1/1 daemons up, 1 hot standby\n    osd: 47 osds: 47 up (since 41m), 47 in (since 42m)\n    rgw: 1 daemon active (1 hosts, 1 zones)\n\n  data:\n    volumes: 1/1 healthy\n    pools:   12 pools, 585 pgs\n    objects: 44.41k objects, 169 GiB\n    usage:   535 GiB used, 279 TiB / 279 TiB avail\n    pgs:     585 active+clean\n\n  io:\n    client:   1023 B/s rd, 224 MiB/s wr, 1 op/s rd, 62 op/s wr\n\nbash-5.1$\n</code></pre>"},{"location":"openshift/storagemanagement/storage-node-replacement/","title":"Storage node replacement","text":""},{"location":"openshift/storagemanagement/storage-node-replacement/#delete-the-storage-node","title":"Delete the storage node","text":"<p>1) Show the initial status of the storage nodes in the managed cluster (output of oc get nodes) and identify which node will be removed, e.g. storage-0.</p> <pre><code>[root@dom14npv101-infra-manager ~ vlabrc]# oc get nodes |grep -i storage\nncpvnpvlab1-storage-101.ncpvnpvlab1.pnwlab.nsn-rdnet.net   Ready    storage,worker                     25d   v1.29.10+67d3387\nncpvnpvlab1-storage-102.ncpvnpvlab1.pnwlab.nsn-rdnet.net   Ready    storage,worker                     27d   v1.29.10+67d3387\nncpvnpvlab1-storage-103.ncpvnpvlab1.pnwlab.nsn-rdnet.net   Ready    storage,worker                     27d   v1.29.10+67d3387\nncpvnpvlab1-storage-201.ncpvnpvlab1.pnwlab.nsn-rdnet.net   Ready    storage,worker                     25d   v1.29.10+67d3387\nncpvnpvlab1-storage-202.ncpvnpvlab1.pnwlab.nsn-rdnet.net   Ready    storage,worker                     27d   v1.29.10+67d3387\nncpvnpvlab1-storage-203.ncpvnpvlab1.pnwlab.nsn-rdnet.net   Ready    storage,worker                     27d   v1.29.10+67d3387\n[root@dom14npv101-infra-manager ~ vlabrc]#\n</code></pre> <p>2) Verify the ceph health status</p> <pre><code>[root@dom14npv101-infra-manager ~ vlabrc]# oc exec -it $(oc get pod -n openshift-storage -l app=rook-ceph-operator -o name) -n openshift-storage -- ceph -s -c /var/lib/rook/openshift-storage/openshift-storage.config\n  cluster:\n    id:     a2b0c334-ba7c-4ae1-b3f5-c6d514f19bec\n    health: HEALTH_OK\n\n  services:\n    mon: 3 daemons, quorum f,g,h (age 21h)\n    mgr: a(active, since 21h), standbys: b\n    mds: 1/1 daemons up, 1 hot standby\n    osd: 48 osds: 48 up (since 21h), 48 in (since 23h)\n    rgw: 1 daemon active (1 hosts, 1 zones)\n\n  data:\n    volumes: 1/1 healthy\n    pools:   12 pools, 1097 pgs\n    objects: 190.32k objects, 393 GiB\n    usage:   1.2 TiB used, 278 TiB / 279 TiB avail\n    pgs:     1097 active+clean\n\n  io:\n    client:   8.7 KiB/s rd, 9.9 MiB/s wr, 11 op/s rd, 22 op/s wr\n\n[root@dom14npv101-infra-manager ~ vlabrc]#\n</code></pre> <p>3) Identify the monitor pod (if any), and OSDs that are running in the node that you need to replace:</p> <pre><code>[root@dom14npv101-infra-manager ~ vlabrc]# oc get pods -n openshift-storage -o wide | grep -i ncpvnpvlab1-storage-101.ncpvnpvlab1.pnwlab.nsn-rdnet.net  |grep -i rook-ceph\nrook-ceph-crashcollector-73c0594e536089af81dd498574227f77-94vtt   1/1     Running   0             21h   172.28.18.41     ncpvnpvlab1-storage-101.ncpvnpvlab1.pnwlab.nsn-rdnet.net   &lt;none&gt;           &lt;none&gt;\nrook-ceph-exporter-73c0594e536089af81dd498574227f77-754b5866njj   1/1     Running   0             21h   172.28.18.42     ncpvnpvlab1-storage-101.ncpvnpvlab1.pnwlab.nsn-rdnet.net   &lt;none&gt;           &lt;none&gt;\nrook-ceph-mds-ocs-storagecluster-cephfilesystem-a-795996f7lvsqs   2/2     Running   0             22h   172.28.18.21     ncpvnpvlab1-storage-101.ncpvnpvlab1.pnwlab.nsn-rdnet.net   &lt;none&gt;           &lt;none&gt;\nrook-ceph-mds-ocs-storagecluster-cephfilesystem-b-78f7bbf8c2hhg   2/2     Running   0             22h   172.28.18.22     ncpvnpvlab1-storage-101.ncpvnpvlab1.pnwlab.nsn-rdnet.net   &lt;none&gt;           &lt;none&gt;\nrook-ceph-mgr-b-5468b7cf-fmwnp                                    4/4     Running   0             22h   172.28.18.7      ncpvnpvlab1-storage-101.ncpvnpvlab1.pnwlab.nsn-rdnet.net   &lt;none&gt;           &lt;none&gt;\nrook-ceph-mon-f-54d858f9cd-m5q76                                  2/2     Running   0             22h   172.28.18.8      ncpvnpvlab1-storage-101.ncpvnpvlab1.pnwlab.nsn-rdnet.net   &lt;none&gt;           &lt;none&gt;\nrook-ceph-operator-7bc4cf5ccd-4lxjr                               1/1     Running   0             22h   172.28.18.39     ncpvnpvlab1-storage-101.ncpvnpvlab1.pnwlab.nsn-rdnet.net   &lt;none&gt;           &lt;none&gt;\nrook-ceph-osd-14-6df66b8b99-nmgvq                                 2/2     Running   0             22h   172.28.18.9      ncpvnpvlab1-storage-101.ncpvnpvlab1.pnwlab.nsn-rdnet.net   &lt;none&gt;           &lt;none&gt;\nrook-ceph-osd-21-86cd6b7f7f-498vh                                 2/2     Running   0             22h   172.28.18.12     ncpvnpvlab1-storage-101.ncpvnpvlab1.pnwlab.nsn-rdnet.net   &lt;none&gt;           &lt;none&gt;\nrook-ceph-osd-28-698bb96856-vmr8t                                 2/2     Running   0             22h   172.28.18.11     ncpvnpvlab1-storage-101.ncpvnpvlab1.pnwlab.nsn-rdnet.net   &lt;none&gt;           &lt;none&gt;\nrook-ceph-osd-34-5f49bdbb85-f528w                                 2/2     Running   0             22h   172.28.18.15     ncpvnpvlab1-storage-101.ncpvnpvlab1.pnwlab.nsn-rdnet.net   &lt;none&gt;           &lt;none&gt;\nrook-ceph-osd-4-7495d5f559-zccrg                                  2/2     Running   0             22h   172.28.18.34     ncpvnpvlab1-storage-101.ncpvnpvlab1.pnwlab.nsn-rdnet.net   &lt;none&gt;           &lt;none&gt;\nrook-ceph-osd-41-689699f766-clfzm                                 2/2     Running   0             22h   172.28.18.17     ncpvnpvlab1-storage-101.ncpvnpvlab1.pnwlab.nsn-rdnet.net   &lt;none&gt;           &lt;none&gt;\nrook-ceph-osd-44-94c7c6565-cz8wg                                  2/2     Running   0             22h   172.28.18.16     ncpvnpvlab1-storage-101.ncpvnpvlab1.pnwlab.nsn-rdnet.net   &lt;none&gt;           &lt;none&gt;\nrook-ceph-osd-9-6b966dc5db-28595                                  2/2     Running   0             22h   172.28.18.18     ncpvnpvlab1-storage-101.ncpvnpvlab1.pnwlab.nsn-rdnet.net   &lt;none&gt;           &lt;none&gt;\nrook-ceph-rgw-ocs-storagecluster-cephobjectstore-a-788d79bdrltz   2/2     Running   0             22h   172.28.18.38     ncpvnpvlab1-storage-101.ncpvnpvlab1.pnwlab.nsn-rdnet.net   &lt;none&gt;           &lt;none&gt;\nrook-ceph-tools-6f854c4bfc-wqhm7                                  1/1     Running   0             22h   172.28.18.30     ncpvnpvlab1-storage-101.ncpvnpvlab1.pnwlab.nsn-rdnet.net   &lt;none&gt;           &lt;none&gt;\n[root@dom14npv101-infra-manager ~ vlabrc]#\n</code></pre> <p>4) Scale down the deployments of the pods identified in the previous step: (mon, osd, crashcollector)</p> <pre><code>[root@dom14npv101-infra-manager ~ vlabrc]# oc -n openshift-storage scale deployment rook-ceph-crashcollector --replicas=0\noc -n openshift-storage scale deployment rook-ceph-mgr-b  --replicas=0\noc -n openshift-storage scale deployment rook-ceph-mon-f  --replicas=0\noc -n openshift-storage scale deployment rook-ceph-osd-14 --replicas=0\noc -n openshift-storage scale deployment rook-ceph-osd-21 --replicas=0\noc -n openshift-storage scale deployment rook-ceph-osd-28 --replicas=0\noc -n openshift-storage scale deployment rook-ceph-osd-34 --replicas=0\noc -n openshift-storage scale deployment rook-ceph-osd-4  --replicas=0\noc -n openshift-storage scale deployment rook-ceph-osd-41 --replicas=0\noc -n openshift-storage scale deployment rook-ceph-osd-44 --replicas=0\noc -n openshift-storage scale deployment rook-ceph-osd-9  --replicas=0\nerror: no objects passed to scale\ndeployment.apps/rook-ceph-mgr-b scaled\ndeployment.apps/rook-ceph-mon-f scaled\ndeployment.apps/rook-ceph-osd-14 scaled\ndeployment.apps/rook-ceph-osd-21 scaled\ndeployment.apps/rook-ceph-osd-28 scaled\ndeployment.apps/rook-ceph-osd-34 scaled\ndeployment.apps/rook-ceph-osd-4 scaled\ndeployment.apps/rook-ceph-osd-41 scaled\ndeployment.apps/rook-ceph-osd-44 scaled\ndeployment.apps/rook-ceph-osd-9 scaled\n[root@dom14npv101-infra-manager ~ vlabrc]#\n</code></pre> <p>5) Add the following Annotation for node deletion in the siteconfig.yaml (add crsuppression and crannotation both)</p> <p>6) To initiate the automated deletion process, begin by deleting the BMH CR of the control plane node that has been previously annotated with the specific \u201ccrAnnotation\u201d.</p> <p>7) Add \u201ccrSuppression\u201d to SiteConfig so that node will be removed from the cluster. Note that you need to keep the \u201ccrAnnotation\u201d on the node.</p> <pre><code>      - hostName: \"ncpvnpvlab1-storage-101.ncpvnpvlab1.pnwlab.nsn-rdnet.net\"\n        role: \"worker\"\n        crSuppression:\n        - BareMetalHost\n        crAnnotations:\n          add:\n          BareMetalHost:\n            bmac.agent-install.openshift.io/remove-agent-and-node-on-delete: true\n</code></pre> <p>8) Git add/commit/push the SiteConfig.yaml, so that ArgoCD syncs the updated SiteConfig to the Hub Cluster      a. The BMH on Hub cluster should start showing updated status that the node is being deprovisioning. This status change indicates that the node is undergoing the deprovisioning process, a necessary step before its complete removal.</p> <pre><code>[root@dom14npv101-infra-manager ~ hubrc]# oc get bmh -n ncpvnpvlab1 |grep -i storage-101\nncpvnpvlab1-storage-101.ncpvnpvlab1.pnwlab.nsn-rdnet.net   deprovisioning              true             60m\n[root@dom14npv101-infra-manager ~ hubrc]#\n</code></pre> <p>9) Cluster Administrators should wait for the BMH to finish deprovisioning and be fully deleted from the cluster environment. After ~10 minutes (this might take longer or shorter depending on your environment to complete the node clean up):     a. The storage node \u201cstorage-0\u201d is powered off     b. The BMH resource of the replaced node is deleted on the Hub Cluster.</p> <pre><code>[root@dom14npv101-infra-manager ~ vlabrc]# oc get bmh -n ncpvnpvlab1 |grep -i storage\nncpvnpvlab1-storage-102.ncpvnpvlab1.pnwlab.nsn-rdnet.net   provisioned              true             27d\nncpvnpvlab1-storage-103.ncpvnpvlab1.pnwlab.nsn-rdnet.net   provisioned              true             27d\nncpvnpvlab1-storage-201.ncpvnpvlab1.pnwlab.nsn-rdnet.net   provisioned              true             25d\nncpvnpvlab1-storage-202.ncpvnpvlab1.pnwlab.nsn-rdnet.net   provisioned              true             27d\nncpvnpvlab1-storage-203.ncpvnpvlab1.pnwlab.nsn-rdnet.net   provisioned              true             27d\n[root@dom14npv101-infra-manager ~ vlabrc]#\n</code></pre> <p>10)  \u201coc get node\u201d on cluster shows that the node \u201cstorage-101\u201d is no longer part of the cluster, only 2 storage nodes remain</p> <p>ncpvnpvlab1-storage-101.ncpvnpvlab1.pnwlab.nsn-rdnet.net  ---&gt; still part of it. </p> <pre><code>[root@dom14npv101-infra-manager ~ vlabrc]# oc get nodes |grep -i storage\nncpvnpvlab1-storage-101.ncpvnpvlab1.pnwlab.nsn-rdnet.net   Ready    storage,worker                     25d   v1.29.10+67d3387\nncpvnpvlab1-storage-102.ncpvnpvlab1.pnwlab.nsn-rdnet.net   Ready    storage,worker                     27d   v1.29.10+67d3387\nncpvnpvlab1-storage-103.ncpvnpvlab1.pnwlab.nsn-rdnet.net   Ready    storage,worker                     27d   v1.29.10+67d3387\nncpvnpvlab1-storage-201.ncpvnpvlab1.pnwlab.nsn-rdnet.net   Ready    storage,worker                     25d   v1.29.10+67d3387\nncpvnpvlab1-storage-202.ncpvnpvlab1.pnwlab.nsn-rdnet.net   Ready    storage,worker                     27d   v1.29.10+67d3387\nncpvnpvlab1-storage-203.ncpvnpvlab1.pnwlab.nsn-rdnet.net   Ready    storage,worker                     27d   v1.29.10+67d3387\n[root@dom14npv101-infra-manager ~ vlabrc]#\n</code></pre>"},{"location":"openshift/tools-management-ts/ts-tools/","title":"This documentation help to setup some troubleshooting tools installation on the openshift infra.","text":""},{"location":"openshift/tools-management-ts/ts-tools/#tcpdump","title":"TCPDUMP","text":"<p>1) login to node via ssh or debug utitiy </p> <pre><code>[root@ncputility ~ pancwl_rc]$ ssh core@gateway2.panclypcwl01.mnc020.mcc714\nRed Hat Enterprise Linux CoreOS 416.94.202407081958-0\n  Part of OpenShift 4.16, RHCOS is a Kubernetes-native operating system\n  managed by the Machine Config Operator (`clusteroperator/machine-config`).\n\nWARNING: Direct SSH access to machines is not recommended; instead,\nmake configuration changes via `machineconfig` objects:\n  https://docs.openshift.com/container-platform/4.16/architecture/architecture-rhcos.html\n\n---\n[core@gateway2 ~]$\n</code></pre> <p>2) become root and update the toolbox rc file here.     a. use your hub cluster quay to avoid ssl certificate trust error. </p> <pre><code>[root@gateway2 ~]# cat /root/.toolboxrc\n#REGISTRY=ncputility.panclyphub01.mnc020.mcc714:8443/ocmirror/rhel9\nREGISTRY=quay-registry.apps.panclyphub01.mnc020.mcc714/ocmirror/rhel9\nIMAGE=support-tools:latest\n[root@gateway2 ~]#\n</code></pre> <p>3) now trigger toolbox command to execute into a shell which contains all required tools like tcpdump, sosreport etc. </p> <pre><code>[root@gateway2 ~]# toolbox\n.toolboxrc file detected, overriding defaults...\nTrying to pull quay-registry.apps.panclyphub01.mnc020.mcc714/ocmirror/rhel9/support-tools:latest...\nGetting image source signatures\nCopying blob f5e6502d2728 done   |\nCopying blob ebc7dc32a098 done   |\nCopying config affd08d3be done   |\nWriting manifest to image destination\naffd08d3bead20c55f40f08270d477b1524d9d7a2db25235956c7858755ef5f3\nSpawning a container 'toolbox-root' with image 'quay-registry.apps.panclyphub01.mnc020.mcc714/ocmirror/rhel9/support-tools:latest'\nDetected RUN label in the container image. Using that as the default...\n6bdff24c2e5da044965e2cec8eea58c3d86668f3a5bbe1e2d34495e956fdf0d7\ntoolbox-root\nContainer started successfully. To exit, type 'exit'.\n[root@gateway2 /]#\n</code></pre> <p>4) now run tcpdump command against any linux network interface. </p> <pre><code>[root@gateway2 /]# tcpdump -i vlan104 host 10.89.97.162 -n\ndropped privs to tcpdump\ntcpdump: verbose output suppressed, use -v[v]... for full protocol decode\nlistening on vlan104, link-type EN10MB (Ethernet), snapshot length 262144 bytes\n11:09:46.575071 ARP, Request who-has 10.89.97.167 tell 10.89.97.162, length 42\n11:09:46.847283 IP 10.89.97.162.49152 &gt; 10.89.97.165.bfd-control: BCM-LI-SHIM: direction unused, pkt-type unknown, pkt-subtype single VLAN tag, li-id 2097944\n11:09:47.111266 ARP, Request who-has 10.89.97.166 tell 10.89.97.162, length 42\n11:09:47.575002 ARP, Request who-has 10.89.97.167 tell 10.89.97.162, length 42\n11:09:47.614353 IP 10.89.97.162.49152 &gt; 10.89.97.165.bfd-control: BCM-LI-SHIM: direction unused, pkt-type unknown, pkt-subtype single VLAN tag, li-id 2097944\n11:09:48.110974 ARP, Request who-has 10.89.97.166 tell 10.89.97.162, length 42\n11:09:48.398615 IP 10.89.97.162.49152 &gt; 10.89.97.165.bfd-control: BCM-LI-SHIM: direction unused, pkt-type unknown, pkt-subtype single VLAN tag, li-id 2097944\n11:09:49.165561 IP 10.89.97.162.49152 &gt; 10.89.97.165.bfd-control: BCM-LI-SHIM: direction unused, pkt-type unknown, pkt-subtype single VLAN tag, li-id 2097944\n11:09:49.944591 IP 10.89.97.162.49152 &gt; 10.89.97.165.bfd-control: BCM-LI-SHIM: direction unused, pkt-type unknown, pkt-subtype single VLAN tag, li-id 2097944\n11:09:50.708761 IP 10.89.97.162.49152 &gt; 10.89.97.165.bfd-control: BCM-LI-SHIM: direction unused, pkt-type unknown, pkt-subtype single VLAN tag, li-id 2097944\n11:09:51.486902 IP 10.89.97.162.49152 &gt; 10.89.97.165.bfd-control: BCM-LI-SHIM: direction unused, pkt-type unknown, pkt-subtype single VLAN tag, li-id 2097944\n11:09:52.262277 IP 10.89.97.162.49152 &gt; 10.89.97.165.bfd-control: BCM-LI-SHIM: direction unused, pkt-type unknown, pkt-subtype single VLAN tag, li-id 2097944\n11:09:53.022279 IP 10.89.97.162.49152 &gt; 10.89.97.165.bfd-control: BCM-LI-SHIM: direction unused, pkt-type unknown, pkt-subtype single VLAN tag, li-id 2097944\n11:09:53.796498 IP 10.89.97.162.49152 &gt; 10.89.97.165.bfd-control: BCM-LI-SHIM: direction unused, pkt-type unknown, pkt-subtype single VLAN tag, li-id 2097944\n11:09:54.578752 IP 10.89.97.162.49152 &gt; 10.89.97.165.bfd-control: BCM-LI-SHIM: direction unused, pkt-type unknown, pkt-subtype single VLAN tag, li-id 2097944\n11:09:55.339790 IP 10.89.97.162.49152 &gt; 10.89.97.165.bfd-control: BCM-LI-SHIM: direction unused, pkt-type unknown, pkt-subtype single VLAN tag, li-id 2097944\n11:09:56.093946 IP 10.89.97.162.49152 &gt; 10.89.97.165.bfd-control: BCM-LI-SHIM: direction unused, pkt-type unknown, pkt-subtype single VLAN tag, li-id 2097944\n11:09:56.871023 IP 10.89.97.162.49152 &gt; 10.89.97.165.bfd-control: BCM-LI-SHIM: direction unused, pkt-type unknown, pkt-subtype single VLAN tag, li-id 2097944\n11:09:57.655185 IP 10.89.97.162.49152 &gt; 10.89.97.165.bfd-control: BCM-LI-SHIM: direction unused, pkt-type unknown, pkt-subtype single VLAN tag, li-id 2097944\n11:09:58.416364 IP 10.89.97.162.49152 &gt; 10.89.97.165.bfd-control: BCM-LI-SHIM: direction unused, pkt-type unknown, pkt-subtype single VLAN tag, li-id 2097944\n11:09:59.198508 IP 10.89.97.162.49152 &gt; 10.89.97.165.bfd-control: BCM-LI-SHIM: direction unused, pkt-type unknown, pkt-subtype single VLAN tag, li-id 2097944\n11:09:59.976760 IP 10.89.97.162.49152 &gt; 10.89.97.165.bfd-control: BCM-LI-SHIM: direction unused, pkt-type unknown, pkt-subtype single VLAN tag, li-id 2097944\n11:10:00.160910 IP 10.89.97.165.37680 &gt; 10.89.97.162.bgp: Flags [P.], seq 207097212:207097231, ack 3177745402, win 64, options [nop,nop,TS val 4011175388 ecr 3700362321], length 19: BGP\n11:10:00.162044 IP 10.89.97.162.bgp &gt; 10.89.97.165.37680: Flags [.], ack 19, win 23411, options [nop,nop,TS val 3700389294 ecr 4011175388], length 0\n11:10:00.738822 IP 10.89.97.162.49152 &gt; 10.89.97.165.bfd-control: BCM-LI-SHIM: direction unused, pkt-type unknown, pkt-subtype single VLAN tag, li-id 2097944\n11:10:01.497752 IP 10.89.97.162.49152 &gt; 10.89.97.165.bfd-control: BCM-LI-SHIM: direction unused, pkt-type unknown, pkt-subtype single VLAN tag, li-id 2097944\n11:10:02.254817 IP 10.89.97.162.49152 &gt; 10.89.97.165.bfd-control: BCM-LI-SHIM: direction unused, pkt-type unknown, pkt-subtype single VLAN tag, li-id 2097944\n11:10:03.028798 IP 10.89.97.162.49152 &gt; 10.89.97.165.bfd-control: BCM-LI-SHIM: direction unused, pkt-type unknown, pkt-subtype single VLAN tag, li-id 2097944\n11:10:03.209179 IP 10.89.97.162.bgp &gt; 10.89.97.165.37680: Flags [P.], seq 1:20, ack 19, win 23411, options [nop,nop,TS val 3700392341 ecr 4011175388], length 19: BGP\n11:10:03.209205 IP 10.89.97.165.37680 &gt; 10.89.97.162.bgp: Flags [.], ack 20, win 64, options [nop,nop,TS val 4011178436 ecr 3700392341], length 0\n11:10:03.798017 IP 10.89.97.162.49152 &gt; 10.89.97.165.bfd-control: BCM-LI-SHIM: direction unused, pkt-type unknown, pkt-subtype single VLAN tag, li-id 2097944\n11:10:04.086263 ARP, Request who-has 10.89.97.163 tell 10.89.97.162, length 42\n11:10:04.564104 IP 10.89.97.162.49152 &gt; 10.89.97.165.bfd-control: BCM-LI-SHIM: direction unused, pkt-type unknown, pkt-subtype single VLAN tag, li-id 2097944\n11:10:05.086213 ARP, Request who-has 10.89.97.163 tell 10.89.97.162, length 42\n11:10:05.343276 IP 10.89.97.162.49152 &gt; 10.89.97.165.bfd-control: BCM-LI-SHIM: direction unused, pkt-type unknown, pkt-subtype single VLAN tag, li-id 2097944\n11:10:06.086632 ARP, Request who-has 10.89.97.163 tell 10.89.97.162, length 42\n11:10:06.110449 IP 10.89.97.162.49152 &gt; 10.89.97.165.bfd-control: BCM-LI-SHIM: direction unused, pkt-type unknown, pkt-subtype single VLAN tag, li-id 2097944\n11:10:06.906665 IP 10.89.97.162.49152 &gt; 10.89.97.165.bfd-control: BCM-LI-SHIM: direction unused, pkt-type unknown, pkt-subtype single VLAN tag, li-id 2097944\n11:10:07.686809 IP 10.89.97.162.49152 &gt; 10.89.97.165.bfd-control: BCM-LI-SHIM: direction unused, pkt-type unknown, pkt-subtype single VLAN tag, li-id 2097944\n11:10:08.455977 IP 10.89.97.162.49152 &gt; 10.89.97.165.bfd-control: BCM-LI-SHIM: direction unused, pkt-type unknown, pkt-subtype single VLAN tag, li-id 2097944\n11:10:09.231136 IP 10.89.97.162.49152 &gt; 10.89.97.165.bfd-control: BCM-LI-SHIM: direction unused, pkt-type unknown, pkt-subtype single VLAN tag, li-id 2097944\n11:10:10.002335 IP 10.89.97.162.49152 &gt; 10.89.97.165.bfd-control: BCM-LI-SHIM: direction unused, pkt-type unknown, pkt-subtype single VLAN tag, li-id 2097944\n11:10:10.792658 IP 10.89.97.162.49152 &gt; 10.89.97.165.bfd-control: BCM-LI-SHIM: direction unused, pkt-type unknown, pkt-subtype single VLAN tag, li-id 2097944\n11:10:11.561799 IP 10.89.97.162.49152 &gt; 10.89.97.165.bfd-control: BCM-LI-SHIM: direction unused, pkt-type unknown, pkt-subtype single VLAN tag, li-id 2097944\n^C\n44 packets captured\n44 packets received by filter\n0 packets dropped by kernel\n[root@gateway2 /]# \n</code></pre>"},{"location":"openshift/tools-management-ts/ts-tools/#sos-report","title":"sos report","text":"<p>1) login to node via ssh or debug utitiy </p> <pre><code>[root@ncputility ~ pancwl_rc]$ ssh core@gateway2.panclypcwl01.mnc020.mcc714\nRed Hat Enterprise Linux CoreOS 416.94.202407081958-0\n  Part of OpenShift 4.16, RHCOS is a Kubernetes-native operating system\n  managed by the Machine Config Operator (`clusteroperator/machine-config`).\n\nWARNING: Direct SSH access to machines is not recommended; instead,\nmake configuration changes via `machineconfig` objects:\n  https://docs.openshift.com/container-platform/4.16/architecture/architecture-rhcos.html\n\n---\n[core@gateway2 ~]$\n</code></pre> <p>2) become root and update the toolbox rc file here.     a. use your hub cluster quay to avoid ssl certificate trust error. </p> <pre><code>[root@gateway2 ~]# cat /root/.toolboxrc\n#REGISTRY=ncputility.panclyphub01.mnc020.mcc714:8443/ocmirror/rhel9\nREGISTRY=quay-registry.apps.panclyphub01.mnc020.mcc714/ocmirror/rhel9\nIMAGE=support-tools:latest\n[root@gateway2 ~]#\n</code></pre> <p>3) now trigger toolbox command to execute into a shell which contains all required tools like tcpdump, sosreport etc. </p> <pre><code>[root@gateway2 ~]# toolbox\n.toolboxrc file detected, overriding defaults...\nTrying to pull quay-registry.apps.panclyphub01.mnc020.mcc714/ocmirror/rhel9/support-tools:latest...\nGetting image source signatures\nCopying blob f5e6502d2728 done   |\nCopying blob ebc7dc32a098 done   |\nCopying config affd08d3be done   |\nWriting manifest to image destination\naffd08d3bead20c55f40f08270d477b1524d9d7a2db25235956c7858755ef5f3\nSpawning a container 'toolbox-root' with image 'quay-registry.apps.panclyphub01.mnc020.mcc714/ocmirror/rhel9/support-tools:latest'\nDetected RUN label in the container image. Using that as the default...\n6bdff24c2e5da044965e2cec8eea58c3d86668f3a5bbe1e2d34495e956fdf0d7\ntoolbox-root\nContainer started successfully. To exit, type 'exit'.\n[root@gateway2 /]#\n</code></pre> <p>4) now use sosreport from here </p> <pre><code>sos report -k crio.all=on -k crio.logs=on  -k podman.all=on -k podman.logs=on\n</code></pre>"},{"location":"openshift/tools-management-ts/ts-tools/#references","title":"References","text":"<ul> <li>Recovering a node that has lost all networking in OpenShift 4</li> </ul>"},{"location":"openshift/troubleshooting/ncomsa/","title":"NCOM Caas Issue","text":""},{"location":"openshift/troubleshooting/ncomsa/#ncom-caas-fluctuate-every-1-hours-once-and-here-is-the-process-to-create-a-service-account-for-ncom-to-resolve-this-issue","title":"NCOM CAAS fluctuate every 1 hours once and here is the process to create a service account for ncom to resolve this issue.","text":"<p>Caas using user/passwd based registation is fluctuate</p>"},{"location":"openshift/troubleshooting/ncomsa/#problem-describe","title":"Problem describe:","text":"<ul> <li>CAAS flucatuate on the ncom, CNF onboarding may fail. in millicom, 30mins once it;s fluctuate.  </li> </ul>"},{"location":"openshift/troubleshooting/ncomsa/#solution-describe","title":"Solution describe:","text":"<ul> <li>The auto discovery mode NCOM uses a token after discovery to integrate with the workload cluster instead of userID/password. This connection appears to be stable and does not fluctuate. So ncom want to use this token based access as a solution to our problem. instead of user id based. </li> </ul>"},{"location":"openshift/troubleshooting/ncomsa/#limitation","title":"limitation:","text":"<ul> <li>based on NCP security hardening, system token may expire within 24hrs. </li> </ul>"},{"location":"openshift/troubleshooting/ncomsa/#solution-steps","title":"Solution steps:","text":"<p>1) You need to be logged in with a user who has cluster-admin privileges:</p> <p>2) Create an service account on the desired project. </p> <pre><code>[root@ncputility ~ pancwl_rc]$ oc project ncom01pan\nNow using project \"ncom01pan\" on server \"https://api.panclypcwl01.mnc020.mcc714:6443\".\n[root@ncputility ~ pancwl_rc]$ \n\n[root@ncputility ~ pancwl_rc]$ oc create serviceaccount ncom-sa\nserviceaccount/ncom-sa created\n[root@ncputility ~ pancwl_rc]$\n</code></pre> <p>3)  Assing <code>cluster-admin</code> role to this <code>SA</code> now. </p> <pre><code>[root@ncputility ~ pancwl_rc]$ oc adm policy add-cluster-role-to-user cluster-admin -z  ncom-sa -n ncom01pan\nclusterrole.rbac.authorization.k8s.io/cluster-admin added: \"ncom-sa\"\n[root@ncputility ~ pancwl_rc]$ \n</code></pre> <p>4) describe sa here, you wont see the token and it's expected on this version OCP. </p> <pre><code>[root@ncputility ~ pancwl_rc]$ oc describe sa ncom-sa\nName:                ncom-sa\nNamespace:           ncom01pan\nLabels:              &lt;none&gt;\nAnnotations:         &lt;none&gt;\nImage pull secrets:  &lt;none&gt;\nMountable secrets:   &lt;none&gt;\nTokens:              &lt;none&gt;\nEvents:              &lt;none&gt;\n[root@ncputility ~ pancwl_rc]$\n</code></pre> <p>5) Now Manually Create a Token for the ServiceAccount. This tells Kubernetes/OpenShift to generate a token for ncom-sa and store it in the secret ncom-sa-secret.</p> <pre><code>[root@ncputility ~ pancwl_rc]$ cat &gt; secret.yaml\napiVersion: v1\nkind: Secret\nmetadata:\n  name: ncom-sa-secret\n  annotations:\n    kubernetes.io/service-account.name: ncom-sa\ntype: kubernetes.io/service-account-token\n\n[root@ncputility ~ pancwl_rc]$\n</code></pre> <p>6) describe sa here again. you can see the secret created. </p> <pre><code>[root@ncputility ~ pancwl_rc]$ oc describe sa ncom-sa\nName:                ncom-sa\nNamespace:           ncom01pan\nLabels:              &lt;none&gt;\nAnnotations:         &lt;none&gt;\nImage pull secrets:  &lt;none&gt;\nMountable secrets:   &lt;none&gt;\nTokens:              ncom-sa-secret\nEvents:              &lt;none&gt;\n[root@ncputility ~ pancwl_rc]$\n</code></pre> <p>7) retrive the token and share with ncom team here</p> <p><code>oc get secret ncom-sa-secret -n ncom01pan -o jsonpath=\"{.data.token}\" | base64 -d</code></p> <pre><code> [root@ncputility ~ pancwl_rc]$ oc get secret ncom-sa-secret -n ncom01pan -o jsonpath=\"{.data.token}\" | base64 -d\n\neyJhbGciOiJSUzI1NiIsImtpZCI6IjBpd2lFcjdSU3ktY25uRDl3YTVhU0M2V0wtZ0pUWXBXM0RzMmpUTFp1N28ifQ.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZX        Rlcy5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJuY29tMDFwYW4iLCJrdWJlcm5ldGVzLmlvL3NlcnZpY2VhY2NvdW50L3NlY3JldC5uYW1lIjoibmNvbS1zYS1zZWNyZXQiLCJrdWJlcm5        ldGVzLmlvL3NlcnZpY2VhY2NvdW50L3NlcnZpY2UtYWNjb3VudC5uYW1lIjoibmNvbS1zYSIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VydmljZS1hY2NvdW50LnVpZCI6IjU0MDli        ZTIyLTA4YTMtNDVkNS1iNGRlLWMyOGNlMGZjYmIwMyIsInN1YiI6InN5c3RlbTpzZXJ2aWNlYWNjb3VudDpuY29tMDFwYW46bmNvbS1zYSJ9.H7EpGT0Le2evHG2UW96OO3-mvY7-Dg6nzVAgr98c        SUALecFyrEC9g-f9lYvgncS4mgQINV7voTaIYQ211AvGn6Adeu-PE3vPObxcIUA5_KJmRbKl7O1TMzqpJmIa5TnDeSLt2xN6D3bn74n1JpaGq3-VhBicA9j0jDbLVTi5EE_JGX2PkpJ-vvMhrnsGF        YzEA7oOAWwyyPMy2RPeEzKYK0bubvQaRLf2T-oZkWgXejWeLV0Z1mU12e75husjrEdu-FEfzyAEU_CQIuPAHHBC7U1OMLAuN_ehImmoLzObSCKqruLxeqIZamr6cNzZAKouc2bcdvLuDWmG3nVZRP        ZRGnRMKPGaFNPnetav0Nq5MvYy4zOAAaDWq1_B5b8iYFxCsycceqZKySe-Z_lmEw1x1lyf2I8Z5fpfaoPW_QxndSuxBrV4h6O9igZpwzoCjwq8vB838vkVMlLIcDGTViLAnLd8pl763-1coSzqsnt        rO_eUUTZICvTzp-dA6QdZOb5SWMXj9-vqvW469rjzgaopSeS7hmUO_6BGMS7O5-nZC4PG-QPISrUKup9eAM62jssTmu2QL4-y3FbY1vjOFvJAbkEwzBhPN-2EPEY9zUu44ZximqFwNkuR4T66u_jG        \n[root@ncputility ~ pancwl_rc]$\n</code></pre>"},{"location":"openshift/troubleshooting/ncomsa/#reference","title":"reference","text":"<ul> <li>Creating an SA and map cluster admin role</li> </ul>"},{"location":"openshift/troubleshooting/nsenter/","title":"Method to use nsenter on the OCP","text":"<p>Please find the method to use nsenter, so that you will not struggle during your deployment. </p> <p>1) Login to the ocp cluster with cluster admin role and find the pod name which you want to login inside the container using nsenter. <code>ncom01pan-caas-plugin-9bd7755bb-bb5fs</code> is selected.</p> <pre><code>[root@ncputility ~ pancwl_rc]$ source /root/pancwlrc\nWARNING: Using insecure TLS client config. Setting this option is not supported!\n\nLogin successful.\n\nYou have access to 119 projects, the list has been suppressed. You can list all projects with 'oc projects'\n\nUsing project \"ncom01pan\".\n[root@ncputility ~ pancwl_rc]$ oc get pods -A -o wide |grep -i ^C\n[root@ncputility ~ pancwl_rc]$ oc get pods -n ncom01pan -o wide |grep -i caas\nncom01pan-caas-plugin-9bd7755bb-bb5fs                       1/1     Running             0          3h15m   172.17.18.34    appworker23.panclypcwl01.mnc020.mcc714   &lt;none&gt;           &lt;none&gt;\nncom01pan-caas-plugin-9bd7755bb-cwzmm                       1/1     Running             0          3h15m   172.18.8.78     appworker16.panclypcwl01.mnc020.mcc714   &lt;none&gt;           &lt;none&gt;\n[root@ncputility ~ pancwl_rc]$\n</code></pre> <p>2) execute to that node where your pod hosted, and this will be indentified from the previous command. </p> <pre><code>[root@ncputility ~ pancwl_rc]$ oc debug -t node/appworker0.panclypcwl01.mnc020.mcc714\nTemporary namespace openshift-debug-vz9qc is created for debugging node...\nStarting pod/appworker0panclypcwl01mnc020mcc714-debug-87f7l ...\nTo use host binaries, run `chroot /host`\nPod IP: 10.89.96.26\nIf you don't see a command prompt, try pressing enter.\nsh-5.1# chroot /host\nsh-5.1#\n</code></pre> <p>3) now find out the container id using crictl command here </p> <pre><code>sh-5.1# crictl ps |grep -i ncom01pan-caas-plugin-7654b86fdb-mz5r7\n2b61910d5eb23       quay-registry.apps.panclyphub01.mnc020.mcc714/ncom01pan/ncom/caas-plugin@sha256:d6d9506d14d756ecafe7d93debcb9eeb498cc805506fb1480002713d17ce64d6   19 minutes ago      Running             cjee-wildfly                         0                   8ca17869e45fa       ncom01pan-caas-plugin-7654b86fdb-mz5r7\n</code></pre> <p>4) find out the pid of the container using inspect command. </p> <pre><code>sh-5.1# crictl inpsect 2b61910d5eb23 |grep -i pid\nNo help topic for 'inpsect'\nsh-5.1# crictl inspect 2b61910d5eb23 |grep -i pid\n    \"pid\": 60545,\n          \"pids\": {\n            \"type\": \"pid\"\n                \"getpid\",\n                \"getppid\",\n                \"pidfd_getfd\",\n                \"pidfd_open\",\n                \"pidfd_send_signal\",\n                \"waitpid\",\n</code></pre> <p>5) now use the toolbox command, since tcpdump is not configured on the host os level. </p> <pre><code>sh-5.1# toolbox\n.toolboxrc file detected, overriding defaults...\nChecking if there is a newer version of quay-registry.apps.panclyphub01.mnc020.mcc714/ocmirror/rhel9/support-tools:latest available...\nContainer 'toolbox-root' already exists. Trying to start...\n(To remove the container and start with a fresh toolbox, run: sudo podman rm 'toolbox-root')\ntoolbox-root\nContainer started successfully. To exit, type 'exit'.\n[root@appworker0 /]# nsenter -t 60545 -n ip a\n1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000\n    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00\n    inet 127.0.0.1/8 scope host lo\n       valid_lft forever preferred_lft forever\n    inet6 ::1/128 scope host\n       valid_lft forever preferred_lft forever\n2: gre0@NONE: &lt;NOARP&gt; mtu 1476 qdisc noop state DOWN group default qlen 1000\n    link/gre 0.0.0.0 brd 0.0.0.0\n3: gretap0@NONE: &lt;BROADCAST,MULTICAST&gt; mtu 1462 qdisc noop state DOWN group default qlen 1000\n    link/ether 00:00:00:00:00:00 brd ff:ff:ff:ff:ff:ff\n4: erspan0@NONE: &lt;BROADCAST,MULTICAST&gt; mtu 1450 qdisc noop state DOWN group default qlen 1000\n    link/ether 00:00:00:00:00:00 brd ff:ff:ff:ff:ff:ff\n5: ip6tnl0@NONE: &lt;NOARP&gt; mtu 1452 qdisc noop state DOWN group default qlen 1000\n    link/tunnel6 :: brd :: permaddr 3eac:e266:df07::\n6: ip6gre0@NONE: &lt;NOARP&gt; mtu 1448 qdisc noop state DOWN group default qlen 1000\n    link/gre6 :: brd :: permaddr 6679:afbc:3648::\n7: eth0@if609: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 8900 qdisc noqueue state UP group default\n    link/ether 0a:58:ac:10:04:0f brd ff:ff:ff:ff:ff:ff link-netns d00bdece-6c79-4840-87d1-10d3103ecdd7\n    inet 172.16.4.15/23 brd 172.16.5.255 scope global eth0\n       valid_lft forever preferred_lft forever\n    inet6 fe80::858:acff:fe10:40f/64 scope link\n       valid_lft forever preferred_lft forever\n[root@appworker0 /]# nsenter -t 60545 -n ping  quay-registry.apps.panclyphub01.mnc020.mcc714\nPING quay-registry.apps.panclyphub01.mnc020.mcc714 (10.89.97.143) 56(84) bytes of data.\nFrom 10.89.97.168 (10.89.97.168) icmp_seq=1 Destination Host Unreachable\nFrom 10.89.97.168 (10.89.97.168) icmp_seq=2 Destination Host Unreachable\nFrom 10.89.97.168 (10.89.97.168) icmp_seq=3 Destination Host Unreachable\nFrom 10.89.97.168 (10.89.97.168) icmp_seq=4 Destination Host Unreachable\n\n^C\n--- quay-registry.apps.panclyphub01.mnc020.mcc714 ping statistics ---\n5 packets transmitted, 0 received, +4 errors, 100% packet loss, time 4089ms\npipe 4\n[root@appworker0 /]# nsenter -t 60545 -n ping  10.89.97.143\nPING 10.89.97.143 (10.89.97.143) 56(84) bytes of data.\nFrom 10.89.97.168 icmp_seq=1 Destination Host Unreachable\nFrom 10.89.97.168 icmp_seq=2 Destination Host Unreachable\nFrom 10.89.97.168 icmp_seq=3 Destination Host Unreachable\nFrom 10.89.97.168 icmp_seq=4 Destination Host Unreachable\n\n^C\n--- 10.89.97.143 ping statistics ---\n4 packets transmitted, 0 received, +4 errors, 100% packet loss, time 3088ms\npipe 4\n[root@appworker0 /]# nsenter -t 60545 -n tracepath 10.89.97.143\n 1?: [LOCALHOST]                      pmtu 8900\n 1:  *.apps.panclyphub01.mnc020.mcc714                     1.683ms asymm  2\n 1:  *.apps.panclyphub01.mnc020.mcc714                     0.878ms asymm  2\n 2:  100.88.0.7                                            1.903ms asymm  3\n 3:  172.17.2.2                                            2.086ms\n 4:  no reply\n 4:  10.89.97.168                                        3087.371ms !H\n     Resume: pmtu 8900\n[root@appworker0 /]#\n</code></pre>"},{"location":"openshift/usermanagement/remove-kubeadmin/","title":"Removing the kubeadmin user","text":""},{"location":"openshift/usermanagement/remove-kubeadmin/#the-kubeadmin-user","title":"The kubeadmin user","text":"<p>OpenShift Container Platform creates a cluster administrator, kubeadmin, after the installation process completes.</p> <p>This user has the cluster-admin role automatically applied and is treated as the root user for the cluster. The password is dynamically generated and unique to your OpenShift Container Platform environment. After installation completes the password is provided in the installation program\u2019s output. For example:</p> <pre><code>INFO Install complete!\nINFO Run 'export KUBECONFIG=&lt;your working directory&gt;/auth/kubeconfig' to manage the cluster with 'oc', the OpenShift CLI.\nINFO The cluster is ready when 'oc login -u kubeadmin -p &lt;provided&gt;' succeeds (wait a few minutes).\nINFO Access the OpenShift web-console here: https://console-openshift-console.apps.demo1.openshift4-beta-abcorp.com\nINFO Login to the console with user: kubeadmin, password: &lt;provided&gt;\n</code></pre>"},{"location":"openshift/usermanagement/remove-kubeadmin/#removing-the-kubeadmin-user_1","title":"Removing the kubeadmin user","text":"<ol> <li>After you define an identity provider and create a new cluster-admin user, you can remove the kubeadmin to improve cluster security.</li> </ol>"},{"location":"openshift/usermanagement/remove-kubeadmin/#warning","title":"Warning","text":"<p><code>If you follow this procedure before another user is a cluster-admin, then OpenShift Container Platform must be reinstalled. It is not possible to undo this command.</code></p>"},{"location":"openshift/usermanagement/remove-kubeadmin/#prerequisites","title":"Prerequisites","text":"<ul> <li>You must have configured at least one identity provider.</li> <li>You must have added the cluster-admin role to a user.</li> <li>You must be logged in as an administrator.</li> </ul>"},{"location":"openshift/usermanagement/remove-kubeadmin/#procedure","title":"Procedure","text":"<ol> <li>Remove the kubeadmin secrets:</li> </ol> <pre><code>oc delete secrets kubeadmin -n kube-system\n</code></pre>"},{"location":"openshift/usermanagement/remove-kubeadmin/#references","title":"References","text":"<ul> <li>Remove kubeadmin id</li> </ul>"},{"location":"openshift/usermanagement/user-management/","title":"OCP User management using htpassword","text":""},{"location":"openshift/usermanagement/user-management/#configure-htpasswd-as-an-identity-provider-for-ocp","title":"Configure HTPASSWD as an identity provider for OCP","text":""},{"location":"openshift/usermanagement/user-management/#add-update-remove-users-for-ocp-using-htpasswd-as-indentity-provider","title":"Add, update, remove users for OCP using htpasswd as indentity provider.","text":""},{"location":"openshift/usermanagement/user-management/#procedure-to-add-an-additional-users","title":"Procedure to <code>Add</code> an additional users","text":"<p>1) Retrieve the htpasswd file from the htpass-secret Secret object and save the file to your file system:</p> <pre><code>[root@dom14npv101-infra-manager ~ hub]# oc get secret htpass-secret -ojsonpath={.data.htpasswd} -n openshift-config | base64 --decode &gt; users.htpasswd\n[root@dom14npv101-infra-manager ~ hub]# cat users.htpasswd\nncpadmin:$2y$05$DYlpXiwFzfyRioO4hRNq1.ZdFLO3yMz3Pl3gs7.yUpEUKeOGoHX9K\n[root@dom14npv101-infra-manager ~ hub]# \n</code></pre> <p>2) Add or remove users from the users.htpasswd file.</p> <pre><code>[root@dom14npv101-infra-manager ~ hub]# htpasswd -bB users.htpasswd nokia nokia@123\nAdding password for user nokia\n[root@dom14npv101-infra-manager ~ hub]# \n</code></pre> <p>3) Replace the htpass-secret Secret object with the updated users in the users.htpasswd file:</p> <pre><code>[root@dom14npv101-infra-manager ~ hub]# oc create secret generic htpass-secret --from-file=htpasswd=users.htpasswd --dry-run=client -o yaml -n openshift-config | oc replace -f -\nsecret/htpass-secret replaced\n[root@dom14npv101-infra-manager ~ hub]# \n</code></pre> <p>4) Wait for all these pods to be restarted </p> <pre><code>[root@dom14npv101-infra-manager ~ hub]# oc get pods -n openshift-authentication -o wide |grep -i oauth\noauth-openshift-f446bd5b-58cps   1/1     Running   0          82s   172.20.2.190   ncpvnpvhub-hubmaster-101.ncpvnpvhub.pnwlab.nsn-rdnet.net   &lt;none&gt;           &lt;none&gt;\noauth-openshift-f446bd5b-k8dqx   0/1     Running   0          27s   172.21.0.241   ncpvnpvhub-hubmaster-103.ncpvnpvhub.pnwlab.nsn-rdnet.net   &lt;none&gt;           &lt;none&gt;\noauth-openshift-f446bd5b-v4n6m   1/1     Running   0          55s   172.20.0.134   ncpvnpvhub-hubmaster-102.ncpvnpvhub.pnwlab.nsn-rdnet.net   &lt;none&gt;           &lt;none&gt;\n[root@dom14npv101-infra-manager ~ hub]# \n</code></pre> <p>5) Validate the login now. </p> <pre><code>[root@dom14npv101-infra-manager ~ hub]# oc login -u nokia -p nokia@123\nWARNING: Using insecure TLS client config. Setting this option is not supported!\n\nLogin successful.\n\nYou don't have any projects. You can try to create a new project, by running\n\n    oc new-project &lt;projectname&gt;\n\n[root@dom14npv101-infra-manager ~ hub]# oc whoami\nnokia\n[root@dom14npv101-infra-manager ~ hub]# \n</code></pre>"},{"location":"openshift/usermanagement/user-management/#procedure-to-update-the-password-of-an-existing-users","title":"Procedure to <code>update</code> the password of an existing users","text":"<p>1) Retrieve the htpasswd file from the htpass-secret Secret object and save the file to your file system:</p> <pre><code>[root@dom14npv101-infra-manager ~ hub]# oc get secret htpass-secret -ojsonpath={.data.htpasswd} -n openshift-config | base64 --decode &gt; users.htpasswd\n[root@dom14npv101-infra-manager ~ hub]# cat users.htpasswd\nncpadmin:$2y$05$DYlpXiwFzfyRioO4hRNq1.ZdFLO3yMz3Pl3gs7.yUpEUKeOGoHX9K\nnokia:$2y$05$SLpgZb3AE.fE7WQEGIMOXesoZdY9pV9OHVpXb5CuITdZO5RVId8Ye\n[root@dom14npv101-infra-manager ~ hub]# \n</code></pre> <p>2) update the users password from the users.htpasswd file.</p> <pre><code>[root@dom14npv101-infra-manager ~ hub]# htpasswd -bB users.htpasswd nokia nokia@1234\nUpdating password for user nokia\n[root@dom14npv101-infra-manager ~ hub]# \n</code></pre> <p>3) Replace the htpass-secret Secret object with the updated users in the users.htpasswd file:</p> <pre><code>[root@dom14npv101-infra-manager ~ hub]# oc create secret generic htpass-secret --from-file=htpasswd=users.htpasswd --dry-run=client -o yaml -n openshift-config | oc replace -f -\nsecret/htpass-secret replaced\n[root@dom14npv101-infra-manager ~ hub]# \n</code></pre> <p>4) Wait for all these pods to be restarted </p> <pre><code>[root@dom14npv101-infra-manager ~ hub]# oc get pods -n openshift-authentication -o wide -w\nNAME                             READY   STATUS    RESTARTS   AGE     IP             NODE                                                       NOMINATED NODE   READINESS GATES\noauth-openshift-f446bd5b-58cps   1/1     Running   0          6m48s   172.20.2.190   ncpvnpvhub-hubmaster-101.ncpvnpvhub.pnwlab.nsn-rdnet.net   &lt;none&gt;           &lt;none&gt;\noauth-openshift-f446bd5b-k8dqx   1/1     Running   0          5m53s   172.21.0.241   ncpvnpvhub-hubmaster-103.ncpvnpvhub.pnwlab.nsn-rdnet.net   &lt;none&gt;           &lt;none&gt;\noauth-openshift-f446bd5b-v4n6m   1/1     Running   0          6m21s   172.20.0.134   ncpvnpvhub-hubmaster-102.ncpvnpvhub.pnwlab.nsn-rdnet.net   &lt;none&gt;           &lt;none&gt;\noauth-openshift-f446bd5b-v4n6m   1/1     Terminating   0          6m24s   172.20.0.134   ncpvnpvhub-hubmaster-102.ncpvnpvhub.pnwlab.nsn-rdnet.net   &lt;none&gt;           &lt;none&gt;\noauth-openshift-6497ccb4f5-d5wxd   0/1     Pending       0          0s      &lt;none&gt;         &lt;none&gt;                                                     &lt;none&gt;           &lt;none&gt;\noauth-openshift-6497ccb4f5-d5wxd   0/1     Pending       0          0s      &lt;none&gt;         &lt;none&gt;                                                     &lt;none&gt;           &lt;none&gt;\n</code></pre> <p>5) Validate the login now. </p> <pre><code>[root@dom14npv101-infra-manager ~ hub]# oc login -u nokia -p nokia@123\nWARNING: Using insecure TLS client config. Setting this option is not supported!\n\nLogin failed (401 Unauthorized)\nVerify you have provided the correct credentials.\n[root@dom14npv101-infra-manager ~ hub]# oc login -u nokia -p nokia@1234\nWARNING: Using insecure TLS client config. Setting this option is not supported!\n\nLogin successful.\n\nYou don't have any projects. You can try to create a new project, by running\n\n    oc new-project &lt;projectname&gt;\n\n[root@dom14npv101-infra-manager ~ hub]# \n</code></pre>"},{"location":"openshift/usermanagement/user-management/#procedure-to-delete-an-user-completely","title":"Procedure to <code>delete</code> an user completely","text":"<p>1) Retrieve the htpasswd file from the htpass-secret Secret object and save the file to your file system:</p> <pre><code>[root@dom14npv101-infra-manager ~ hub]# oc get secret htpass-secret -ojsonpath={.data.htpasswd} -n openshift-config | base64 --decode &gt; users.htpasswd\n[root@dom14npv101-infra-manager ~ hub]# cat users.htpasswd\nncpadmin:$2y$05$DYlpXiwFzfyRioO4hRNq1.ZdFLO3yMz3Pl3gs7.yUpEUKeOGoHX9K\nnokia:$2y$05$SLpgZb3AE.fE7WQEGIMOXesoZdY9pV9OHVpXb5CuITdZO5RVId8Ye\n[root@dom14npv101-infra-manager ~ hub]# \n</code></pre> <p>2) remove users from the users.htpasswd file.</p> <pre><code>[root@dom14npv101-infra-manager ~ hub]# htpasswd -D users.htpasswd nokia\nDeleting password for user nokia\n[root@dom14npv101-infra-manager ~ hub]#\n</code></pre> <p>3) Replace the htpass-secret Secret object with the updated users in the users.htpasswd file:</p> <pre><code>[root@dom14npv101-infra-manager ~ hub]# oc create secret generic htpass-secret --from-file=htpasswd=users.htpasswd --dry-run=client -o yaml -n openshift-config | oc replace -f -\nsecret/htpass-secret replaced\n[root@dom14npv101-infra-manager ~ hub]# \n</code></pre> <p>4) Wait for all these pods to be restarted </p> <pre><code>[root@dom14npv101-infra-manager ~ hub]# oc get pods -n openshift-authentication -o wide -w\nNAME                             READY   STATUS    RESTARTS   AGE     IP             NODE                                                       NOMINATED NODE   READINESS GATES\noauth-openshift-f446bd5b-58cps   1/1     Running   0          6m48s   172.20.2.190   ncpvnpvhub-hubmaster-101.ncpvnpvhub.pnwlab.nsn-rdnet.net   &lt;none&gt;           &lt;none&gt;\noauth-openshift-f446bd5b-k8dqx   1/1     Running   0          5m53s   172.21.0.241   ncpvnpvhub-hubmaster-103.ncpvnpvhub.pnwlab.nsn-rdnet.net   &lt;none&gt;           &lt;none&gt;\noauth-openshift-f446bd5b-v4n6m   1/1     Running   0          6m21s   172.20.0.134   ncpvnpvhub-hubmaster-102.ncpvnpvhub.pnwlab.nsn-rdnet.net   &lt;none&gt;           &lt;none&gt;\noauth-openshift-f446bd5b-v4n6m   1/1     Terminating   0          6m24s   172.20.0.134   ncpvnpvhub-hubmaster-102.ncpvnpvhub.pnwlab.nsn-rdnet.net   &lt;none&gt;           &lt;none&gt;\noauth-openshift-6497ccb4f5-d5wxd   0/1     Pending       0          0s      &lt;none&gt;         &lt;none&gt;                                                     &lt;none&gt;           &lt;none&gt;\noauth-openshift-6497ccb4f5-d5wxd   0/1     Pending       0          0s      &lt;none&gt;         &lt;none&gt;                                                     &lt;none&gt;           &lt;none&gt;\n</code></pre> <p>5) If you removed one or more users, you must additionally remove existing resources for each user.</p> <pre><code>a) Delete the User object:\n\n```\n[root@dom14npv101-infra-manager ~ hub]# oc get user \nNAME       UID                                    FULL NAME   IDENTITIES\nncpadmin   8e728883-a17f-4bde-8b0f-2eab78ccc6c3               my_htpasswd_provider:ncpadmin\nnokia      7e44ba3a-8d91-435f-800f-380a8e87f0d1               my_htpasswd_provider:nokia\n[root@dom14npv101-infra-manager ~ hub]# oc delete user nokia \nuser.user.openshift.io \"nokia\" deleted\n[root@dom14npv101-infra-manager ~ hub]#\n\n```\nb) Be sure to remove the user, otherwise the user can continue using their token as long as it has not expired.\n\n```\n[root@dom14npv101-infra-manager ~ hub]#  oc get identity \nNAME                            IDP NAME               IDP USER NAME   USER NAME   USER UID\nmy_htpasswd_provider:ncpadmin   my_htpasswd_provider   ncpadmin        ncpadmin    8e728883-a17f-4bde-8b0f-2eab78ccc6c3\nmy_htpasswd_provider:nokia      my_htpasswd_provider   nokia           nokia       7e44ba3a-8d91-435f-800f-380a8e87f0d1\n[root@dom14npv101-infra-manager ~ hub]# oc delete identity my_htpasswd_provider:nokia  \nidentity.user.openshift.io \"my_htpasswd_provider:nokia\" deleted\n[root@dom14npv101-infra-manager ~ hub]# \n```\n</code></pre> <p>5) Validate the login and it should not work. </p> <pre><code>[root@dom14npv101-infra-manager ~ hub]# oc login -u nokia -p nokia@1234\nWARNING: Using insecure TLS client config. Setting this option is not supported!\n\nLogin failed (401 Unauthorized)\nVerify you have provided the correct credentials.\n[root@dom14npv101-infra-manager ~ hub]# \n</code></pre>"},{"location":"openshift/usermanagement/user-management/#disclaimer","title":"Disclaimer","text":"<p>All these procedures are collected from redhat link given below. </p>"},{"location":"openshift/usermanagement/user-management/#references","title":"References","text":"<ul> <li> <p>Configuring an htpasswd identity provider </p> </li> <li> <p>To configure an HTPasswd identity provider in OpenShift 4</p> </li> <li> <p>Remove kubeadmin id</p> </li> </ul>"}]}