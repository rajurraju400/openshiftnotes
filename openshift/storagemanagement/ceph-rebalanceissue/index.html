
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
        <meta name="description" content="Documentation for OpenShift installation, backup, storage, troubleshooting, and more.">
      
      
        <meta name="author" content="venkatapathirajr">
      
      
      
        <link rel="prev" href="../../networking/metallb-configuration/">
      
      
        <link rel="next" href="../storage-node-replacement/">
      
      
      <link rel="icon" href="../../../assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.6.1, mkdocs-material-9.6.12">
    
    
      
        <title>Ceph Rebalance Issue - OpenShift Notes</title>
      
    
    
      <link rel="stylesheet" href="../../../assets/stylesheets/main.2afb09e1.min.css">
      
        
        <link rel="stylesheet" href="../../../assets/stylesheets/palette.06af60db.min.css">
      
      


    
    
      
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Red+Hat+Text:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Red Hat Text";--md-code-font:"Roboto Mono"}</style>
      
    
    
      <link rel="stylesheet" href="../../../stylesheets/redhat-style.css">
    
    <script>__md_scope=new URL("../../..",location),__md_hash=e=>[...e].reduce(((e,_)=>(e<<5)-e+_.charCodeAt(0)),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
    
  </head>
  
  
    
    
      
    
    
    
    
    <body dir="ltr" data-md-color-scheme="default" data-md-color-primary="custom" data-md-color-accent="custom">
  
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

  

<header class="md-header md-header--shadow" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href="../../.." title="OpenShift Notes" class="md-header__button md-logo" aria-label="OpenShift Notes" data-md-component="logo">
      
  <img src="../../../images/logo.png" alt="logo">

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3zm0 5h18v2H3zm0 5h18v2H3z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            OpenShift Notes
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              Ceph Rebalance Issue
            
          </span>
        </div>
      </div>
    </div>
    
      
        <form class="md-header__option" data-md-component="palette">
  
    
    
    
    <input class="md-option" data-md-color-media="" data-md-color-scheme="default" data-md-color-primary="custom" data-md-color-accent="custom"  aria-hidden="true"  type="radio" name="__palette" id="__palette_0">
    
  
</form>
      
    
    
      <script>var palette=__md_get("__palette");if(palette&&palette.color){if("(prefers-color-scheme)"===palette.color.media){var media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']");palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent")}for(var[key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script>
    
    
    
      
      
        <label class="md-header__button md-icon" for="__search">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        </label>
        <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">
        
        <button type="reset" class="md-search__icon md-icon" title="Clear" aria-label="Clear" tabindex="-1">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"/></svg>
        </button>
      </nav>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" tabindex="0" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list" role="presentation"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
      
    
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    



<nav class="md-nav md-nav--primary" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href="../../.." title="OpenShift Notes" class="md-nav__button md-logo" aria-label="OpenShift Notes" data-md-component="logo">
      
  <img src="../../../images/logo.png" alt="logo">

    </a>
    OpenShift Notes
  </label>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../../.." class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Home
    
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2" >
        
          
          <label class="md-nav__link" for="__nav_2" id="__nav_2_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    Git Helper
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_2_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_2">
            <span class="md-nav__icon md-icon"></span>
            Git Helper
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../git-helper/ncd-git-backup/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Git Backup
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../git-helper/ncd-git-restore/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Git Restore
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2_3" >
        
          
          <label class="md-nav__link" for="__nav_2_3" id="__nav_2_3_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    Deployment Scripts
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_2_3_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_2_3">
            <span class="md-nav__icon md-icon"></span>
            Deployment Scripts
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../git-helper/Deployment/backupconfig.sh" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Backup Config
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../git-helper/Deployment/newgitserver.sh" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    New Git Server (v1)
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
    
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--active md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3" checked>
        
          
          <label class="md-nav__link" for="__nav_3" id="__nav_3_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    OpenShift
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_3_label" aria-expanded="true">
          <label class="md-nav__title" for="__nav_3">
            <span class="md-nav__icon md-icon"></span>
            OpenShift
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3_1" >
        
          
          <label class="md-nav__link" for="__nav_3_1" id="__nav_3_1_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    Backup & Restore
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_3_1_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_3_1">
            <span class="md-nav__icon md-icon"></span>
            Backup & Restore
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../backup-restore/ACM-GEO-RED/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    ACM GEO Red
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../backup-restore/ACM-localbackup/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    ACM Local Backup
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../backup-restore/AWS-S3/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    AWS S3 Backup
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../backup-restore/etcd-backup/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    ETCD Backup
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../backup-restore/etcd-restore/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    ETCD Restore
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3_2" >
        
          
          <label class="md-nav__link" for="__nav_3_2" id="__nav_3_2_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    CNF Onboarding Support
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_3_2_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_3_2">
            <span class="md-nav__icon md-icon"></span>
            CNF Onboarding Support
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../CNF-onboarding-support/image-tls-issue/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Image TLS Issue
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../CNF-onboarding-support/oclogin-tls-issue/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    OCLogin TLS Issue
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../CNF-onboarding-support/proxy-cache-pod/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Proxy Cache Pod
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../deployment/readme/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Deployment
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3_4" >
        
          
          <label class="md-nav__link" for="__nav_3_4" id="__nav_3_4_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    Maintenance
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_3_4_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_3_4">
            <span class="md-nav__icon md-icon"></span>
            Maintenance
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../maintenace/cluster-stop-start/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Cluster Stop/Start
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../maintenace/reboot-nodes/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Reboot Nodes
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3_5" >
        
          
          <label class="md-nav__link" for="__nav_3_5" id="__nav_3_5_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    Networking
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_3_5_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_3_5">
            <span class="md-nav__icon md-icon"></span>
            Networking
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../networking/metalb-troubleshooting/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    MetalLB Troubleshooting
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../networking/metallb-configuration/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    MetalLB Configuration
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
    
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--active md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3_6" checked>
        
          
          <label class="md-nav__link" for="__nav_3_6" id="__nav_3_6_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    Storage Management
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_3_6_label" aria-expanded="true">
          <label class="md-nav__title" for="__nav_3_6">
            <span class="md-nav__icon md-icon"></span>
            Storage Management
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
    
  
  
  
    <li class="md-nav__item md-nav__item--active">
      
      <input class="md-nav__toggle md-toggle" type="checkbox" id="__toc">
      
      
      
      <a href="./" class="md-nav__link md-nav__link--active">
        
  
  
  <span class="md-ellipsis">
    Ceph Rebalance Issue
    
  </span>
  

      </a>
      
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../storage-node-replacement/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Storage Node Replacement
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../tools-management-ts/ts-tools/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Tools Management TS
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3_8" >
        
          
          <label class="md-nav__link" for="__nav_3_8" id="__nav_3_8_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    Troubleshooting
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_3_8_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_3_8">
            <span class="md-nav__icon md-icon"></span>
            Troubleshooting
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../troubleshooting/nsenter/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Nsenter Usage
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3_9" >
        
          
          <label class="md-nav__link" for="__nav_3_9" id="__nav_3_9_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    User Management
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_3_9_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_3_9">
            <span class="md-nav__icon md-icon"></span>
            User Management
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../usermanagement/remove-kubeadmin/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Remove Kubeadmin
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../usermanagement/user-management/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    User Management
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_4" >
        
          
          <label class="md-nav__link" for="__nav_4" id="__nav_4_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    OpenStack
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_4_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_4">
            <span class="md-nav__icon md-icon"></span>
            OpenStack
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../(Add OpenStack documents here when ready)" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    None
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
  
</nav>
                  </div>
                </div>
              </div>
            
          
          
            <div class="md-content" data-md-component="content">
              <article class="md-content__inner md-typeset">
                
                  



  <h1>Ceph Rebalance Issue</h1>

<p>[root@dom14npv101-infra-manager ~ vlabrc]# oc rsh -n openshift-storage pod/$CEPH_TOOL_POD ceph health detail
HEALTH_WARN 1 filesystem is degraded; 11 osds down; 3 hosts (23 osds) down; Reduced data availability: 247 pgs inactive; Degraded data redundancy: 5609127/17836029 objects degraded (31.448%), 811 pgs degraded, 839 pgs undersized; 1597 slow ops, oldest one blocked for 16139 sec, daemons [osd.0,osd.10,osd.13,osd.19,osd.22,osd.27,osd.29,osd.3,osd.35,osd.36]... have slow ops.
[WRN] FS_DEGRADED: 1 filesystem is degraded
    fs ocs-storagecluster-cephfilesystem is degraded
[WRN] OSD_DOWN: 11 osds down
    osd.7 (root=default,host=ncpvnpvlab1-storage-102-ncpvnpvlab1-pnwlab-nsn-rdnet-net) is down
    osd.9 (root=default,host=ncpvnpvlab1-storage-101-ncpvnpvlab1-pnwlab-nsn-rdnet-net) is down
    osd.28 (root=default,host=ncpvnpvlab1-storage-101-ncpvnpvlab1-pnwlab-nsn-rdnet-net) is down
    osd.30 (root=default,host=ncpvnpvlab1-storage-203-ncpvnpvlab1-pnwlab-nsn-rdnet-net) is down
    osd.31 (root=default,host=ncpvnpvlab1-storage-102-ncpvnpvlab1-pnwlab-nsn-rdnet-net) is down
    osd.33 (root=default,host=ncpvnpvlab1-storage-102-ncpvnpvlab1-pnwlab-nsn-rdnet-net) is down
    osd.34 (root=default,host=ncpvnpvlab1-storage-101-ncpvnpvlab1-pnwlab-nsn-rdnet-net) is down
    osd.37 (root=default,host=ncpvnpvlab1-storage-203-ncpvnpvlab1-pnwlab-nsn-rdnet-net) is down
    osd.40 (root=default,host=ncpvnpvlab1-storage-203-ncpvnpvlab1-pnwlab-nsn-rdnet-net) is down
    osd.41 (root=default,host=ncpvnpvlab1-storage-101-ncpvnpvlab1-pnwlab-nsn-rdnet-net) is down
    osd.44 (root=default,host=ncpvnpvlab1-storage-101-ncpvnpvlab1-pnwlab-nsn-rdnet-net) is down
[WRN] OSD_HOST_DOWN: 3 hosts (23 osds) down
    host ncpvnpvlab1-storage-101-ncpvnpvlab1-pnwlab-nsn-rdnet-net (root=default) (8 osds) is down
    host ncpvnpvlab1-storage-203-ncpvnpvlab1-pnwlab-nsn-rdnet-net (root=default) (8 osds) is down
    host ncpvnpvlab1-storage-102-ncpvnpvlab1-pnwlab-nsn-rdnet-net (root=default) (7 osds) is down
[WRN] PG_AVAILABILITY: Reduced data availability: 247 pgs inactive
    pg 9.91 is stuck inactive for 4h, current state unknown, last acting []
    pg 9.93 is stuck inactive for 4h, current state undersized+degraded+peered, last acting [45]
    pg 9.97 is stuck inactive for 4h, current state undersized+degraded+peered, last acting [47]
    pg 9.9e is stuck inactive for 4h, current state unknown, last acting []
    pg 9.9f is stuck inactive for 4h, current state undersized+degraded+peered, last acting [5]
    pg 9.a0 is stuck inactive for 4h, current state unknown, last acting []
    pg 9.a6 is stuck inactive for 4h, current state undersized+degraded+peered, last acting [22]
    pg 9.ab is stuck inactive for 4h, current state undersized+degraded+peered, last acting [35]
    pg 9.ae is stuck inactive for 4h, current state undersized+degraded+peered, last acting [25]
    pg 9.af is stuck inactive for 4h, current state undersized+degraded+peered, last acting [8]
    pg 9.b2 is stuck inactive for 4h, current state unknown, last acting []
    pg 9.b3 is stuck inactive for 4h, current state undersized+degraded+peered, last acting [35]
    pg 9.b4 is stuck inactive for 4h, current state unknown, last acting []
    pg 9.c5 is stuck inactive for 4h, current state undersized+degraded+peered, last acting [42]
    pg 9.c6 is stuck inactive for 4h, current state undersized+degraded+peered, last acting [3]
    pg 9.c7 is stuck inactive for 4h, current state undersized+degraded+peered, last acting [46]
    pg 9.1e7 is stuck inactive for 4h, current state undersized+degraded+peered, last acting [46]
    pg 9.1eb is stuck inactive for 4h, current state undersized+degraded+peered, last acting [29]
    pg 9.1f5 is stuck inactive for 4h, current state undersized+degraded+peered, last acting [0]
    pg 9.1fa is stuck inactive for 4h, current state undersized+degraded+peered, last acting [22]
    pg 9.1fb is stuck inactive for 4h, current state unknown, last acting []
    pg 11.95 is stuck inactive for 4h, current state undersized+degraded+peered, last acting [46]
    pg 11.98 is stuck inactive for 4h, current state undersized+degraded+peered, last acting [19]
    pg 11.9b is stuck inactive for 4h, current state undersized+degraded+peered, last acting [32]
    pg 11.a0 is stuck inactive for 4h, current state undersized+degraded+peered, last acting [36]
    pg 11.a1 is stuck inactive for 4h, current state unknown, last acting []
    pg 11.a3 is stuck inactive for 4h, current state undersized+degraded+peered, last acting [3]
    pg 11.a8 is stuck inactive for 4h, current state undersized+degraded+peered, last acting [23]
    pg 11.ac is stuck inactive for 4h, current state undersized+degraded+peered, last acting [43]
    pg 11.b0 is stuck inactive for 4h, current state undersized+degraded+peered, last acting [39]
    pg 11.b3 is stuck inactive for 4h, current state undersized+degraded+peered, last acting [3]
    pg 11.b5 is stuck inactive for 4h, current state undersized+degraded+peered, last acting [10]
    pg 11.b9 is stuck inactive for 4h, current state undersized+degraded+peered, last acting [43]
    pg 11.bb is stuck inactive for 4h, current state unknown, last acting []
    pg 11.bc is stuck inactive for 4h, current state undersized+degraded+peered, last acting [42]
    pg 11.c0 is stuck inactive for 4h, current state undersized+degraded+peered, last acting [15]
    pg 11.c2 is stuck inactive for 4h, current state undersized+degraded+peered, last acting [3]
    pg 11.c3 is stuck inactive for 4h, current state undersized+degraded+peered, last acting [3]
    pg 12.96 is stuck inactive for 4h, current state undersized+degraded+peered, last acting [23]
    pg 12.9b is stuck inactive for 4h, current state undersized+degraded+peered, last acting [8]
    pg 12.9c is stuck inactive for 4h, current state undersized+degraded+peered, last acting [38]
    pg 12.9d is stuck inactive for 4h, current state undersized+degraded+peered, last acting [10]
    pg 12.ab is stuck inactive for 4h, current state undersized+degraded+peered, last acting [36]
    pg 12.af is stuck inactive for 4h, current state undersized+degraded+peered, last acting [20]
    pg 12.b1 is stuck inactive for 4h, current state undersized+degraded+peered, last acting [0]
    pg 12.b4 is stuck inactive for 4h, current state undersized+degraded+peered, last acting [13]
    pg 12.b5 is stuck inactive for 4h, current state undersized+degraded+peered, last acting [38]
    pg 12.be is stuck inactive for 4h, current state undersized+degraded+peered, last acting [3]
    pg 12.c4 is stuck inactive for 4h, current state undersized+degraded+peered, last acting [23]
    pg 12.c5 is stuck inactive for 4h, current state undersized+degraded+peered, last acting [39]
    pg 12.c7 is stuck inactive for 4h, current state undersized+degraded+peered, last acting [13]
[WRN] PG_DEGRADED: Degraded data redundancy: 5609127/17836029 objects degraded (31.448%), 811 pgs degraded, 839 pgs undersized
    pg 9.bf is active+undersized+degraded, acting [29,35]
    pg 9.c0 is stuck undersized for 4h, current state active+undersized+degraded, last acting [0,45]
    pg 9.c1 is stuck undersized for 3h, current state active+undersized+remapped, last acting [46,45]
    pg 9.c2 is stuck undersized for 4h, current state active+undersized+degraded, last acting [10,13]
    pg 9.c3 is stuck undersized for 4h, current state active+undersized+degraded, last acting [27,22]
    pg 9.c4 is stuck undersized for 4h, current state active+undersized+degraded, last acting [36,45]
    pg 9.c5 is stuck undersized for 4h, current state undersized+degraded+peered, last acting [42]
    pg 9.c6 is stuck undersized for 4h, current state undersized+degraded+peered, last acting [3]
    pg 9.c7 is stuck undersized for 4h, current state undersized+degraded+peered, last acting [46]
    pg 9.1e4 is stuck undersized for 4h, current state active+undersized+degraded, last acting [39,25]
    pg 9.1e5 is stuck undersized for 4h, current state active+undersized+degraded, last acting [10,43]
    pg 9.1e6 is stuck undersized for 4h, current state active+undersized+degraded, last acting [39,5]
    pg 9.1e7 is stuck undersized for 4h, current state undersized+degraded+peered, last acting [46]
    pg 9.1e8 is stuck undersized for 4h, current state active+undersized+degraded, last acting [23,8]
    pg 9.1e9 is stuck undersized for 4h, current state active+undersized+degraded, last acting [47,27]
    pg 9.1ea is stuck undersized for 4h, current state active+undersized+degraded, last acting [19,36]
    pg 9.1eb is stuck undersized for 4h, current state undersized+degraded+peered, last acting [29]
    pg 9.1ec is stuck undersized for 4h, current state active+undersized+degraded, last acting [3,42]
    pg 9.1ed is stuck undersized for 4h, current state active+undersized+degraded, last acting [15,43]
    pg 9.1ee is stuck undersized for 4h, current state active+undersized+degraded, last acting [19,36]
    pg 9.1ef is stuck undersized for 4h, current state active+undersized+degraded, last acting [0,20]
    pg 9.1f1 is stuck undersized for 4h, current state active+undersized+degraded, last acting [47,0]
    pg 9.1f2 is stuck undersized for 4h, current state active+undersized+degraded, last acting [42,35]
    pg 9.1f3 is stuck undersized for 4h, current state active+undersized+degraded, last acting [47,27]
    pg 9.1f4 is stuck undersized for 4h, current state active+undersized+degraded, last acting [36,46]
    pg 9.1f5 is stuck undersized for 4h, current state undersized+degraded+peered, last acting [0]
    pg 9.1f6 is stuck undersized for 3h, current state active+undersized+degraded, last acting [38,46]
    pg 9.1f7 is stuck undersized for 4h, current state active+undersized+degraded, last acting [3,0]
    pg 9.1f8 is stuck undersized for 4h, current state active+undersized+degraded, last acting [3,15]
    pg 9.1f9 is stuck undersized for 4h, current state active+undersized+degraded, last acting [15,43]
    pg 9.1fa is stuck undersized for 4h, current state undersized+degraded+peered, last acting [22]
    pg 9.1fc is stuck undersized for 4h, current state active+undersized+degraded, last acting [35,29]
    pg 9.1fd is stuck undersized for 4h, current state active+undersized+degraded, last acting [5,19]
    pg 9.1fe is stuck undersized for 4h, current state active+undersized+degraded, last acting [29,0]
    pg 9.1ff is stuck undersized for 4h, current state active+undersized+degraded, last acting [43,27]
    pg 11.bd is stuck undersized for 4h, current state active+undersized+degraded, last acting [10,42]
    pg 11.c0 is stuck undersized for 4h, current state undersized+degraded+peered, last acting [15]
    pg 11.c1 is stuck undersized for 4h, current state active+undersized+degraded, last acting [5,32]
    pg 11.c2 is stuck undersized for 4h, current state undersized+degraded+peered, last acting [3]
    pg 11.c3 is stuck undersized for 4h, current state undersized+degraded+peered, last acting [3]
    pg 11.c4 is stuck undersized for 4h, current state active+undersized+degraded, last acting [42,39]
    pg 11.c5 is stuck undersized for 4h, current state active+undersized+degraded, last acting [46,27]
    pg 11.c7 is stuck undersized for 4h, current state active+undersized+degraded, last acting [42,36]
    pg 12.c0 is stuck undersized for 4h, current state active+undersized+degraded, last acting [47,23]
    pg 12.c1 is stuck undersized for 4h, current state active+undersized+degraded, last acting [25,32]
    pg 12.c2 is stuck undersized for 4h, current state active+undersized+degraded, last acting [20,0]
    pg 12.c3 is stuck undersized for 4h, current state active+undersized+degraded, last acting [22,13]
    pg 12.c4 is stuck undersized for 4h, current state undersized+degraded+peered, last acting [23]
    pg 12.c5 is stuck undersized for 4h, current state undersized+degraded+peered, last acting [39]
    pg 12.c6 is stuck undersized for 4h, current state active+undersized+degraded, last acting [29,32]
    pg 12.c7 is stuck undersized for 4h, current state undersized+degraded+peered, last acting [13]
[WRN] SLOW_OPS: 1597 slow ops, oldest one blocked for 16139 sec, daemons [osd.0,osd.10,osd.13,osd.19,osd.22,osd.27,osd.29,osd.3,osd.35,osd.36]... have slow ops.
[root@dom14npv101-infra-manager ~ vlabrc]# oc rsh -n openshift-storage pod/$CEPH_TOOL_POD ceph -s
  cluster:
    id:     a2b0c334-ba7c-4ae1-b3f5-c6d514f19bec
    health: HEALTH_WARN
            1 filesystem is degraded
            11 osds down
            3 hosts (23 osds) down
            Reduced data availability: 247 pgs inactive
            Degraded data redundancy: 5609127/17836029 objects degraded (31.448%), 811 pgs degraded, 839 pgs undersized
            1597 slow ops, oldest one blocked for 16159 sec, daemons [osd.0,osd.10,osd.13,osd.19,osd.22,osd.27,osd.29,osd.3,osd.35,osd.36]... have slow ops.</p>
<p>services:
    mon: 3 daemons, quorum h,j,k (age 4h)
    mgr: a(active, since 4h), standbys: b
    mds: 1/1 daemons up, 1 standby
    osd: 47 osds: 24 up (since 4h), 35 in (since 4h); 42 remapped pgs</p>
<p>data:
    volumes: 0/1 healthy, 1 recovering
    pools:   12 pools, 1097 pgs
    objects: 5.95M objects, 20 TiB
    usage:   41 TiB used, 99 TiB / 140 TiB avail
    pgs:     3.829% pgs unknown
             18.687% pgs not active
             5609127/17836029 objects degraded (31.448%)
             279094/17836029 objects misplaced (1.565%)
             610 active+undersized+degraded
             201 undersized+degraded+peered
             190 active+clean
             42  unknown
             26  active+clean+remapped
             16  active+undersized+remapped
             8   active+undersized
             4   undersized+peered</p>
<p>[root@dom14npv101-infra-manager ~ vlabrc]# oc rsh -n openshift-storage pod/$CEPH_TOOL_POD ceph -s
E0501 23:49:07.749890 2538728 memcache.go:265] couldn't get current server API group list: the server has asked for the client to provide credentials
E0501 23:49:07.787031 2538728 memcache.go:265] couldn't get current server API group list: the server has asked for the client to provide credentials
E0501 23:49:07.822712 2538728 memcache.go:265] couldn't get current server API group list: the server has asked for the client to provide credentials
E0501 23:49:07.855100 2538728 memcache.go:265] couldn't get current server API group list: the server has asked for the client to provide credentials
error: You must be logged in to the server (the server has asked for the client to provide credentials)
[root@dom14npv101-infra-manager ~ vlabrc]# source /root/raj/
alarms/                                health.py                              ingress_ca.crt                         ncp-health.py                          security/
amc-backup/                            htpasswdhub                            install-config                         new.py                                 storage-logging-loki-compactor-0.yaml
backup-etcd/                           htpasswdmang                           kubeadmin.yaml                         oauth.yaml                             testing.txt
beacon.k8s.worker.tar                  htpasswdnlab                           localcert.crt                          pull_secret_cwl_dockerconfigjson.json  users.htpasswd
cephvlanrc.yaml                        hubconfig                              management_new_health_output.txt       registry-cas.yaml                      vlab1config
fedora-tools.tar.gz                    hub_new_health_output.txt              managementrc                           resourcequota.yaml                     vlabrc
git/                                   hubrc                                  managementrcconfig                     rr.txt                                 vlabrc_new_health_output.txt
[root@dom14npv101-infra-manager ~ vlabrc]# source /root/raj/vlabrc
WARNING: Using insecure TLS client config. Setting this option is not supported!</p>
<p>Login successful.</p>
<p>You have access to 115 projects, the list has been suppressed. You can list all projects with 'oc projects'</p>
<p>Using project "openshift-storage".
[root@dom14npv101-infra-manager ~ vlabrc]# oc rsh -n openshift-storage pod/$CEPH_TOOL_POD ceph -s
  cluster:
    id:     a2b0c334-ba7c-4ae1-b3f5-c6d514f19bec
    health: HEALTH_WARN
            1 filesystem is degraded
            11 osds down
            3 hosts (23 osds) down
            Reduced data availability: 247 pgs inactive
            Degraded data redundancy: 5609127/17836029 objects degraded (31.448%), 811 pgs degraded, 839 pgs undersized
            1597 slow ops, oldest one blocked for 24624 sec, daemons [osd.0,osd.10,osd.13,osd.19,osd.22,osd.27,osd.29,osd.3,osd.35,osd.36]... have slow ops.</p>
<p>services:
    mon: 3 daemons, quorum h,j,k (age 6h)
    mgr: a(active, since 6h), standbys: b
    mds: 1/1 daemons up, 1 standby
    osd: 47 osds: 24 up (since 7h), 35 in (since 6h); 42 remapped pgs</p>
<p>data:
    volumes: 0/1 healthy, 1 recovering
    pools:   12 pools, 1097 pgs
    objects: 5.95M objects, 20 TiB
    usage:   41 TiB used, 99 TiB / 140 TiB avail
    pgs:     3.829% pgs unknown
             18.687% pgs not active
             5609127/17836029 objects degraded (31.448%)
             279094/17836029 objects misplaced (1.565%)
             610 active+undersized+degraded
             201 undersized+degraded+peered
             190 active+clean
             42  unknown
             26  active+clean+remapped
             16  active+undersized+remapped
             8   active+undersized
             4   undersized+peered</p>
<p>[root@dom14npv101-infra-manager ~ vlabrc]# oc rsh -n openshift-storage pod/$CEPH_TOOL_POD ceph osd tree
ID   CLASS  WEIGHT     TYPE NAME                                                          STATUS  REWEIGHT  PRI-AFF
 -1         273.62927  root default
 -3          46.57520      host ncpvnpvlab1-storage-101-ncpvnpvlab1-pnwlab-nsn-rdnet-net
  4    ssd    5.82190          osd.4                                                        down         0  1.00000
  9    ssd    5.82190          osd.9                                                        down   1.00000  1.00000
 14    ssd    5.82190          osd.14                                                       down         0  1.00000
 21    ssd    5.82190          osd.21                                                       down         0  1.00000
 28    ssd    5.82190          osd.28                                                       down   1.00000  1.00000
 34    ssd    5.82190          osd.34                                                       down   1.00000  1.00000
 41    ssd    5.82190          osd.41                                                       down   1.00000  1.00000
 44    ssd    5.82190          osd.44                                                       down   1.00000  1.00000
 -7          40.75330      host ncpvnpvlab1-storage-102-ncpvnpvlab1-pnwlab-nsn-rdnet-net
  7    ssd    5.82190          osd.7                                                        down   1.00000  1.00000
 11    ssd    5.82190          osd.11                                                       down         0  1.00000
 17    ssd    5.82190          osd.17                                                       down         0  1.00000
 18    ssd    5.82190          osd.18                                                       down         0  1.00000
 24    ssd    5.82190          osd.24                                                       down         0  1.00000
 31    ssd    5.82190          osd.31                                                       down   1.00000  1.00000
 33    ssd    5.82190          osd.33                                                       down   1.00000  1.00000
 -9          46.57520      host ncpvnpvlab1-storage-103-ncpvnpvlab1-pnwlab-nsn-rdnet-net
  0    ssd    5.82190          osd.0                                                          up   1.00000  1.00000
  5    ssd    5.82190          osd.5                                                          up   1.00000  1.00000
 10    ssd    5.82190          osd.10                                                         up   1.00000  1.00000
 15    ssd    5.82190          osd.15                                                         up   1.00000  1.00000
 23    ssd    5.82190          osd.23                                                         up   1.00000  1.00000
 25    ssd    5.82190          osd.25                                                         up   1.00000  1.00000
 35    ssd    5.82190          osd.35                                                         up   1.00000  1.00000
 36    ssd    5.82190          osd.36                                                         up   1.00000  1.00000
-11          46.57520      host ncpvnpvlab1-storage-201-ncpvnpvlab1-pnwlab-nsn-rdnet-net
 19    ssd    5.82190          osd.19                                                         up   1.00000  1.00000
 22    ssd    5.82190          osd.22                                                         up   1.00000  1.00000
 32    ssd    5.82190          osd.32                                                         up   1.00000  1.00000
 42    ssd    5.82190          osd.42                                                         up   1.00000  1.00000
 43    ssd    5.82190          osd.43                                                         up   1.00000  1.00000
 45    ssd    5.82190          osd.45                                                         up   1.00000  1.00000
 46    ssd    5.82190          osd.46                                                         up   1.00000  1.00000
 47    ssd    5.82190          osd.47                                                         up   1.00000  1.00000
-13          46.57520      host ncpvnpvlab1-storage-202-ncpvnpvlab1-pnwlab-nsn-rdnet-net
  3    ssd    5.82190          osd.3                                                          up   1.00000  1.00000
  8    ssd    5.82190          osd.8                                                          up   1.00000  1.00000
 13    ssd    5.82190          osd.13                                                         up   1.00000  1.00000
 20    ssd    5.82190          osd.20                                                         up   1.00000  1.00000
 27    ssd    5.82190          osd.27                                                         up   1.00000  1.00000
 29    ssd    5.82190          osd.29                                                         up   1.00000  1.00000
 38    ssd    5.82190          osd.38                                                         up   1.00000  1.00000
 39    ssd    5.82190          osd.39                                                         up   1.00000  1.00000
 -5          46.57520      host ncpvnpvlab1-storage-203-ncpvnpvlab1-pnwlab-nsn-rdnet-net
  2    ssd    5.82190          osd.2                                                        down         0  1.00000
  6    ssd    5.82190          osd.6                                                        down         0  1.00000
 12    ssd    5.82190          osd.12                                                       down         0  1.00000
 16    ssd    5.82190          osd.16                                                       down         0  1.00000
 26    ssd    5.82190          osd.26                                                       down         0  1.00000
 30    ssd    5.82190          osd.30                                                       down   1.00000  1.00000
 37    ssd    5.82190          osd.37                                                       down   1.00000  1.00000
 40    ssd    5.82190          osd.40                                                       down   1.00000  1.00000
[root@dom14npv101-infra-manager ~ vlabrc]# oc rsh -n openshift-storage pod/$CEPH_TOOL_POD ceph osd out 4,9
Error EINVAL: Expected option value to be integer, got '4,9'invalid osd id-1
command terminated with exit code 22
[root@dom14npv101-infra-manager ~ vlabrc]# oc rsh -n openshift-storage pod/$CEPH_TOOL_POD ceph osd out 4
osd.4 is already out.
[root@dom14npv101-infra-manager ~ vlabrc]# oc rsh -n openshift-storage pod/$CEPH_TOOL_POD ceph osd out 9
marked out osd.9.
[root@dom14npv101-infra-manager ~ vlabrc]# oc rsh -n openshift-storage pod/$CEPH_TOOL_POD ceph osd out 14
osd.14 is already out.
[root@dom14npv101-infra-manager ~ vlabrc]# oc rsh -n openshift-storage pod/$CEPH_TOOL_POD ceph osd out  ^C
[root@dom14npv101-infra-manager ~ vlabrc]#
[root@dom14npv101-infra-manager ~ vlabrc]#
[root@dom14npv101-infra-manager ~ vlabrc]#
[root@dom14npv101-infra-manager ~ vlabrc]# oc rsh -n openshift-storage pod/$CEPH_TOOL_POD ceph osd out 4
oc rsh -n openshift-storage pod/$CEPH_TOOL_POD ceph osd out 9
oc rsh -n openshift-storage pod/$CEPH_TOOL_POD ceph osd out 14
oc rsh -n openshift-storage pod/$CEPH_TOOL_POD ceph osd out 21
oc rsh -n openshift-storage pod/$CEPH_TOOL_POD ceph osd out 28
oc rsh -n openshift-storage pod/$CEPH_TOOL_POD ceph osd out 34
oc rsh -n openshift-storage pod/$CEPH_TOOL_POD ceph osd out 41
oc rsh -n openshift-storage pod/$CEPH_TOOL_POD ceph osd out 44
oc rsh -n openshift-storage pod/$CEPH_TOOL_POD ceph osd out 7
oc rsh -n openshift-storage pod/$CEPH_TOOL_POD ceph osd out 11
oc rsh -n openshift-storage pod/$CEPH_TOOL_POD ceph osd out 17
oc rsh -n openshift-storage pod/$CEPH_TOOL_POD ceph osd out 18
oc rsh -n openshift-storage pod/$CEPH_TOOL_POD ceph osd out 24
oc rsh -n openshift-storage pod/$CEPH_TOOL_POD ceph osd out 31
oc rsh -n openshift-storage pod/$CEPH_TOOL_POD ceph osd out 33
oc rsh -n openshift-storage pod/$CEPH_TOOL_POD ceph osd out 2
oc rsh -n openshift-storage pod/$CEPH_TOOL_POD ceph osd out 6
oc rsh -n openshift-storage pod/$CEPH_TOOL_POD ceph osd out 12
oc rsh -n openshift-storage pod/$CEPH_TOOL_POD ceph osd out 16
oc rsh -n openshift-storage pod/$CEPH_TOOL_POD ceph osd out 26
oc rsh -n openshift-storage pod/$CEPH_TOOL_POD ceph osd out 30
oc rsh -n openshift-storage pod/$CEPH_TOOL_POD ceph osd out 37
oc rsh -n openshift-storage pod/$CEPH_TOOL_POD ceph osd out 40
osd.4 is already out.
osd.9 is already out.
osd.14 is already out.
osd.21 is already out.
marked out osd.28.
marked out osd.34.
marked out osd.41.
marked out osd.44.
marked out osd.7.
osd.11 is already out.
osd.17 is already out.
osd.18 is already out.
osd.24 is already out.
marked out osd.31.
marked out osd.33.
osd.2 is already out.
osd.6 is already out.
osd.12 is already out.
osd.16 is already out.
osd.26 is already out.
marked out osd.30.
marked out osd.37.
marked out osd.40.
[root@dom14npv101-infra-manager ~ vlabrc]# oc rsh -n openshift-storage pod/$CEPH_TOOL_POD ceph osd purge 4  --yes-i-really-mean-it
oc rsh -n openshift-storage pod/$CEPH_TOOL_POD ceph osd purge 9  --yes-i-really-mean-it
oc rsh -n openshift-storage pod/$CEPH_TOOL_POD ceph osd purge 14 --yes-i-really-mean-it
oc rsh -n openshift-storage pod/$CEPH_TOOL_POD ceph osd purge 21 --yes-i-really-mean-it
oc rsh -n openshift-storage pod/$CEPH_TOOL_POD ceph osd purge 28 --yes-i-really-mean-it
oc rsh -n openshift-storage pod/$CEPH_TOOL_POD ceph osd purge 34 --yes-i-really-mean-it
oc rsh -n openshift-storage pod/$CEPH_TOOL_POD ceph osd purge 41 --yes-i-really-mean-it
oc rsh -n openshift-storage pod/$CEPH_TOOL_POD ceph osd purge 44 --yes-i-really-mean-it
oc rsh -n openshift-storage pod/$CEPH_TOOL_POD ceph osd purge 7  --yes-i-really-mean-it
oc rsh -n openshift-storage pod/$CEPH_TOOL_POD ceph osd purge 11 --yes-i-really-mean-it
oc rsh -n openshift-storage pod/$CEPH_TOOL_POD ceph osd purge 17 --yes-i-really-mean-it
oc rsh -n openshift-storage pod/$CEPH_TOOL_POD ceph osd purge 18 --yes-i-really-mean-it
oc rsh -n openshift-storage pod/$CEPH_TOOL_POD ceph osd purge 24 --yes-i-really-mean-it
oc rsh -n openshift-storage pod/$CEPH_TOOL_POD ceph osd purge 31 --yes-i-really-mean-it
oc rsh -n openshift-storage pod/$CEPH_TOOL_POD ceph osd purge 33 --yes-i-really-mean-it
purged osd.4
purged osd.9
purged osd.14
purged osd.21
purged osd.28
purged osd.34
purged osd.41
purged osd.44
purged osd.7
purged osd.11
purged osd.17
purged osd.18
purged osd.24
purged osd.31
purged osd.33
[root@dom14npv101-infra-manager ~ vlabrc]#
[root@dom14npv101-infra-manager ~ vlabrc]#
[root@dom14npv101-infra-manager ~ vlabrc]#
[root@dom14npv101-infra-manager ~ vlabrc]# oc rsh -n openshift-storage pod/$CEPH_TOOL_POD ceph -s
  cluster:
    id:     a2b0c334-ba7c-4ae1-b3f5-c6d514f19bec
    health: HEALTH_WARN
            1 filesystem is degraded
            Reduced data availability: 203 pgs inactive
            Degraded data redundancy: 5434540/17836062 objects degraded (30.469%), 800 pgs degraded, 218 pgs undersized
            62 slow ops, oldest one blocked for 24934 sec, daemons [osd.10,osd.13,osd.19,osd.22,osd.35,osd.36,osd.42,osd.45,osd.5] have slow ops.</p>
<p>services:
    mon: 3 daemons, quorum h,j,k (age 6h)
    mgr: a(active, since 6h), standbys: b
    mds: 1/1 daemons up, 1 standby
    osd: 32 osds: 24 up (since 7h), 24 in (since 119s); 879 remapped pgs</p>
<p>data:
    volumes: 0/1 healthy, 1 recovering
    pools:   12 pools, 1097 pgs
    objects: 5.95M objects, 20 TiB
    usage:   42 TiB used, 98 TiB / 140 TiB avail
    pgs:     3.829% pgs unknown
             14.676% pgs not active
             5434540/17836062 objects degraded (30.469%)
             1857921/17836062 objects misplaced (10.417%)
             639 active+undersized+degraded+remapped+backfill_wait
             176 active+clean
             157 undersized+degraded+remapped+backfill_wait+peered
             79  active+remapped+backfill_wait
             42  unknown
             4   undersized+degraded+remapped+backfilling+peered</p>
<p>io:
    client:   2.3 MiB/s wr, 0 op/s rd, 6 op/s wr
    recovery: 1.4 GiB/s, 0 keys/s, 455 objects/s</p>
<p>[root@dom14npv101-infra-manager ~ vlabrc]# oc rsh -n openshift-storage pod/$CEPH_TOOL_POD ceph -s
  cluster:
    id:     a2b0c334-ba7c-4ae1-b3f5-c6d514f19bec
    health: HEALTH_WARN
            1 filesystem is degraded
            Reduced data availability: 203 pgs inactive
            Degraded data redundancy: 5429732/17836062 objects degraded (30.442%), 800 pgs degraded, 218 pgs undersized
            62 slow ops, oldest one blocked for 24939 sec, daemons [osd.10,osd.13,osd.19,osd.22,osd.35,osd.36,osd.42,osd.45,osd.5] have slow ops.</p>
<p>services:
    mon: 3 daemons, quorum h,j,k (age 6h)
    mgr: a(active, since 6h), standbys: b
    mds: 1/1 daemons up, 1 standby
    osd: 32 osds: 24 up (since 7h), 24 in (since 2m); 879 remapped pgs</p>
<p>data:
    volumes: 0/1 healthy, 1 recovering
    pools:   12 pools, 1097 pgs
    objects: 5.95M objects, 20 TiB
    usage:   42 TiB used, 98 TiB / 140 TiB avail
    pgs:     3.829% pgs unknown
             14.676% pgs not active
             5429732/17836062 objects degraded (30.442%)
             1857921/17836062 objects misplaced (10.417%)
             639 active+undersized+degraded+remapped+backfill_wait
             176 active+clean
             157 undersized+degraded+remapped+backfill_wait+peered
             79  active+remapped+backfill_wait
             42  unknown
             4   undersized+degraded+remapped+backfilling+peered</p>
<p>io:
    client:   1.5 MiB/s wr, 0 op/s rd, 6 op/s wr
    recovery: 1.7 GiB/s, 0 keys/s, 507 objects/s</p>
<p>[root@dom14npv101-infra-manager ~ vlabrc]# </p>












                
              </article>
            </div>
          
          
<script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script>
        </div>
        
      </main>
      
        <footer class="md-footer">
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
      Material for MkDocs
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    
    
      
      <script id="__config" type="application/json">{"base": "../../..", "features": [], "search": "../../../assets/javascripts/workers/search.f8cc74c7.min.js", "tags": null, "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}, "version": null}</script>
    
    
      <script src="../../../assets/javascripts/bundle.c8b220af.min.js"></script>
      
    
  </body>
</html>